@article{1812.02900v3,
Author        = {Scott Fujimoto and David Meger and Doina Precup},
Title         = {Off-Policy Deep Reinforcement Learning without Exploration},
Eprint        = {1812.02900v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Many practical applications of reinforcement learning constrain agents to
learn from a fixed batch of data which has already been gathered, without
offering further possibility for data collection. In this paper, we demonstrate
that due to errors introduced by extrapolation, standard off-policy deep
reinforcement learning algorithms, such as DQN and DDPG, are incapable of
learning with data uncorrelated to the distribution under the current policy,
making them ineffective for this fixed batch setting. We introduce a novel
class of off-policy algorithms, batch-constrained reinforcement learning, which
restricts the action space in order to force the agent towards behaving close
to on-policy with respect to a subset of the given data. We present the first
continuous control deep reinforcement learning algorithm which can learn
effectively from arbitrary, fixed batch data, and empirically demonstrate the
quality of its behavior in several tasks.},
Year          = {2018},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1812.02900v3},
File          = {1812.02900v3.pdf}
}
@article{2105.00371v1,
Author        = {Zhiqi Yin and Zeshi Yang and Michiel van de Panne and KangKang Yin},
Title         = {Discovering Diverse Athletic Jumping Strategies},
Eprint        = {2105.00371v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We present a framework that enables the discovery of diverse and
natural-looking motion strategies for athletic skills such as the high jump.
The strategies are realized as control policies for physics-based characters.
Given a task objective and an initial character configuration, the combination
of physics simulation and deep reinforcement learning (DRL) provides a suitable
starting point for automatic control policy training. To facilitate the
learning of realistic human motions, we propose a Pose Variational Autoencoder
(P-VAE) to constrain the actions to a subspace of natural poses. In contrast to
motion imitation methods, a rich variety of novel strategies can naturally
emerge by exploring initial character states through a sample-efficient
Bayesian diversity search (BDS) algorithm. A second stage of optimization that
encourages novel policies can further enrich the unique strategies discovered.
Our method allows for the discovery of diverse and novel strategies for
athletic jumping motions such as high jumps and obstacle jumps with no motion
examples and less reward engineering than prior work.},
Year          = {2021},
Month         = {May},
Note          = {ACM Trans. Graph. 40, 4, Article 91 (August 2021), 17 pages (2021)},
Url           = {http://arxiv.org/abs/2105.00371v1},
File          = {2105.00371v1.pdf}
}
@article{1706.10295v3,
Author        = {Meire Fortunato and Mohammad Gheshlaghi Azar and Bilal Piot and Jacob Menick and Ian Osband and Alex Graves and Vlad Mnih and Remi Munos and Demis Hassabis and Olivier Pietquin and Charles Blundell and Shane Legg},
Title         = {Noisy Networks for Exploration},
Eprint        = {1706.10295v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We introduce NoisyNet, a deep reinforcement learning agent with parametric
noise added to its weights, and show that the induced stochasticity of the
agent's policy can be used to aid efficient exploration. The parameters of the
noise are learned with gradient descent along with the remaining network
weights. NoisyNet is straightforward to implement and adds little computational
overhead. We find that replacing the conventional exploration heuristics for
A3C, DQN and dueling agents (entropy reward and $\epsilon$-greedy respectively)
with NoisyNet yields substantially higher scores for a wide range of Atari
games, in some cases advancing the agent from sub to super-human performance.},
Year          = {2017},
Month         = {Jun},
Url           = {http://arxiv.org/abs/1706.10295v3},
File          = {1706.10295v3.pdf}
}
@article{1706.03762v5,
Author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
Title         = {Attention Is All You Need},
Eprint        = {1706.03762v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks in an encoder-decoder configuration. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer, based
solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to be
superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014
English-to-German translation task, improving over the existing best results,
including ensembles by over 2 BLEU. On the WMT 2014 English-to-French
translation task, our model establishes a new single-model state-of-the-art
BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction
of the training costs of the best models from the literature. We show that the
Transformer generalizes well to other tasks by applying it successfully to
English constituency parsing both with large and limited training data.},
Year          = {2017},
Month         = {Jun},
Url           = {http://arxiv.org/abs/1706.03762v5},
File          = {1706.03762v5.pdf}
}
@article{1801.01290v2,
Author        = {Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
Title         = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement
  Learning with a Stochastic Actor},
Eprint        = {1801.01290v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated
on a range of challenging decision making and control tasks. However, these
methods typically suffer from two major challenges: very high sample complexity
and brittle convergence properties, which necessitate meticulous hyperparameter
tuning. Both of these challenges severely limit the applicability of such
methods to complex, real-world domains. In this paper, we propose soft
actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum
entropy reinforcement learning framework. In this framework, the actor aims to
maximize expected reward while also maximizing entropy. That is, to succeed at
the task while acting as randomly as possible. Prior deep RL methods based on
this framework have been formulated as Q-learning methods. By combining
off-policy updates with a stable stochastic actor-critic formulation, our
method achieves state-of-the-art performance on a range of continuous control
benchmark tasks, outperforming prior on-policy and off-policy methods.
Furthermore, we demonstrate that, in contrast to other off-policy algorithms,
our approach is very stable, achieving very similar performance across
different random seeds.},
Year          = {2018},
Month         = {Jan},
Url           = {http://arxiv.org/abs/1801.01290v2},
File          = {1801.01290v2.pdf}
}
@article{1906.04493v3,
Author        = {Juergen Schmidhuber},
Title         = {Generative Adversarial Networks are Special Cases of Artificial
  Curiosity (1990) and also Closely Related to Predictability Minimization
  (1991)},
Eprint        = {1906.04493v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.NE},
Abstract      = {I review unsupervised or self-supervised neural networks playing minimax
games in game-theoretic settings: (i) Artificial Curiosity (AC, 1990) is based
on two such networks. One network learns to generate a probability distribution
over outputs, the other learns to predict effects of the outputs. Each network
minimizes the objective function maximized by the other. (ii) Generative
Adversarial Networks (GANs, 2010-2014) are an application of AC where the
effect of an output is 1 if the output is in a given set, and 0 otherwise.
(iii) Predictability Minimization (PM, 1990s) models data distributions through
a neural encoder that maximizes the objective function minimized by a neural
predictor of the code components. I correct a previously published claim that
PM is not based on a minimax game.},
Year          = {2019},
Month         = {Jun},
Note          = {Neural Networks, Volume 127, July 2020, Pages 58-66},
Url           = {http://arxiv.org/abs/1906.04493v3},
File          = {1906.04493v3.pdf}
}
@article{1504.04909v1,
Author        = {Jean-Baptiste Mouret and Jeff Clune},
Title         = {Illuminating search spaces by mapping elites},
Eprint        = {1504.04909v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Many fields use search algorithms, which automatically explore a search space
to find high-performing solutions: chemists search through the space of
molecules to discover new drugs; engineers search for stronger, cheaper, safer
designs, scientists search for models that best explain data, etc. The goal of
search algorithms has traditionally been to return the single
highest-performing solution in a search space. Here we describe a new,
fundamentally different type of algorithm that is more useful because it
provides a holistic view of how high-performing solutions are distributed
throughout a search space. It creates a map of high-performing solutions at
each point in a space defined by dimensions of variation that a user gets to
choose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites)
algorithm illuminates search spaces, allowing researchers to understand how
interesting attributes of solutions combine to affect performance, either
positively or, equally of interest, negatively. For example, a drug company may
wish to understand how performance changes as the size of molecules and their
cost-to-produce vary. MAP-Elites produces a large diversity of high-performing,
yet qualitatively different solutions, which can be more helpful than a single,
high-performing solution. Interestingly, because MAP-Elites explores more of
the search space, it also tends to find a better overall solution than
state-of-the-art search algorithms. We demonstrate the benefits of this new
algorithm in three different problem domains ranging from producing modular
neural networks to designing simulated and real soft robots. Because MAP-
Elites (1) illuminates the relationship between performance and dimensions of
interest in solutions, (2) returns a set of high-performing, yet diverse
solutions, and (3) improves finding a single, best solution, it will advance
science and engineering.},
Year          = {2015},
Month         = {Apr},
Url           = {http://arxiv.org/abs/1504.04909v1},
File          = {1504.04909v1.pdf}
}
@article{1804.00379v2,
Author        = {Anirudh Goyal and Philemon Brakel and William Fedus and Soumye Singhal and Timothy Lillicrap and Sergey Levine and Hugo Larochelle and Yoshua Bengio},
Title         = {Recall Traces: Backtracking Models for Efficient Reinforcement Learning},
Eprint        = {1804.00379v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {In many environments only a tiny subset of all states yield high reward. In
these cases, few of the interactions with the environment provide a relevant
learning signal. Hence, we may want to preferentially train on those
high-reward states and the probable trajectories leading to them. To this end,
we advocate for the use of a backtracking model that predicts the preceding
states that terminate at a given high-reward state. We can train a model which,
starting from a high value state (or one that is estimated to have high value),
predicts and sample for which the (state, action)-tuples may have led to that
high value state. These traces of (state, action) pairs, which we refer to as
Recall Traces, sampled from this backtracking model starting from a high value
state, are informative as they terminate in good states, and hence we can use
these traces to improve a policy. We provide a variational interpretation for
this idea and a practical algorithm in which the backtracking model samples
from an approximate posterior distribution over trajectories which lead to
large rewards. Our method improves the sample efficiency of both on- and
off-policy RL algorithms across several environments and tasks.},
Year          = {2018},
Month         = {Apr},
Url           = {http://arxiv.org/abs/1804.00379v2},
File          = {1804.00379v2.pdf}
}
@article{1811.11711v2,
Author        = {Josh Merel and Leonard Hasenclever and Alexandre Galashov and Arun Ahuja and Vu Pham and Greg Wayne and Yee Whye Teh and Nicolas Heess},
Title         = {Neural probabilistic motor primitives for humanoid control},
Eprint        = {1811.11711v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We focus on the problem of learning a single motor module that can flexibly
express a range of behaviors for the control of high-dimensional physically
simulated humanoids. To do this, we propose a motor architecture that has the
general structure of an inverse model with a latent-variable bottleneck. We
show that it is possible to train this model entirely offline to compress
thousands of expert policies and learn a motor primitive embedding space. The
trained neural probabilistic motor primitive system can perform one-shot
imitation of whole-body humanoid behaviors, robustly mimicking unseen
trajectories. Additionally, we demonstrate that it is also straightforward to
train controllers to reuse the learned motor primitive space to solve tasks,
and the resulting movements are relatively naturalistic. To support the
training of our model, we compare two approaches for offline policy cloning,
including an experience efficient method which we call linear feedback policy
cloning. We encourage readers to view a supplementary video (
https://youtu.be/CaDEf-QcKwA ) summarizing our results.},
Year          = {2018},
Month         = {Nov},
Url           = {http://arxiv.org/abs/1811.11711v2},
File          = {1811.11711v2.pdf}
}
@article{2103.12656v2,
Author        = {Benjamin Eysenbach and Sergey Levine and Ruslan Salakhutdinov},
Title         = {Replacing Rewards with Examples: Example-Based Policy Search via
  Recursive Classification},
Eprint        = {2103.12656v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Reinforcement learning (RL) algorithms assume that users specify tasks by
manually writing down a reward function. However, this process can be laborious
and demands considerable technical expertise. Can we devise RL algorithms that
instead enable users to specify tasks simply by providing examples of
successful outcomes? In this paper, we derive a control algorithm that
maximizes the future probability of these successful outcome examples. Prior
work has approached similar problems with a two-stage process, first learning a
reward function and then optimizing this reward function using another RL
algorithm. In contrast, our method directly learns a value function from
transitions and successful outcomes, without learning this intermediate reward
function. Our method therefore requires fewer hyperparameters to tune and lines
of code to debug. We show that our method satisfies a new data-driven Bellman
equation, where examples take the place of the typical reward function term.
Experiments show that our approach outperforms prior methods that learn
explicit reward functions.},
Year          = {2021},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2103.12656v2},
File          = {2103.12656v2.pdf}
}
@article{1412.1897v4,
Author        = {Anh Nguyen and Jason Yosinski and Jeff Clune},
Title         = {Deep Neural Networks are Easily Fooled: High Confidence Predictions for
  Unrecognizable Images},
Eprint        = {1412.1897v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Deep neural networks (DNNs) have recently been achieving state-of-the-art
performance on a variety of pattern-recognition tasks, most notably visual
classification problems. Given that DNNs are now able to classify objects in
images with near-human-level performance, questions naturally arise as to what
differences remain between computer and human vision. A recent study revealed
that changing an image (e.g. of a lion) in a way imperceptible to humans can
cause a DNN to label the image as something else entirely (e.g. mislabeling a
lion a library). Here we show a related result: it is easy to produce images
that are completely unrecognizable to humans, but that state-of-the-art DNNs
believe to be recognizable objects with 99.99% confidence (e.g. labeling with
certainty that white noise static is a lion). Specifically, we take
convolutional neural networks trained to perform well on either the ImageNet or
MNIST datasets and then find images with evolutionary algorithms or gradient
ascent that DNNs label with high confidence as belonging to each dataset class.
It is possible to produce images totally unrecognizable to human eyes that DNNs
believe with near certainty are familiar objects, which we call "fooling
images" (more generally, fooling examples). Our results shed light on
interesting differences between human vision and current DNNs, and raise
questions about the generality of DNN computer vision.},
Year          = {2014},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1412.1897v4},
File          = {1412.1897v4.pdf}
}
@article{1106.3708v4,
Author        = {Yann Ollivier and Ludovic Arnold and Anne Auger and Nikolaus Hansen},
Title         = {Information-Geometric Optimization Algorithms: A Unifying Picture via
  Invariance Principles},
Eprint        = {1106.3708v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {math.OC},
Abstract      = {We present a canonical way to turn any smooth parametric family of
probability distributions on an arbitrary search space $X$ into a
continuous-time black-box optimization method on $X$, the
\emph{information-geometric optimization} (IGO) method. Invariance as a design
principle minimizes the number of arbitrary choices. The resulting \emph{IGO
flow} conducts the natural gradient ascent of an adaptive, time-dependent,
quantile-based transformation of the objective function. It makes no
assumptions on the objective function to be optimized.
  The IGO method produces explicit IGO algorithms through time discretization.
It naturally recovers versions of known algorithms and offers a systematic way
to derive new ones. The cross-entropy method is recovered in a particular case,
and can be extended into a smoothed, parametrization-independent maximum
likelihood update (IGO-ML). For Gaussian distributions on $\mathbb{R}^d$, IGO
is related to natural evolution strategies (NES) and recovers a version of the
CMA-ES algorithm. For Bernoulli distributions on $\{0,1\}^d$, we recover the
PBIL algorithm. From restricted Boltzmann machines, we obtain a novel algorithm
for optimization on $\{0,1\}^d$. All these algorithms are unified under a
single information-geometric optimization framework.
  Thanks to its intrinsic formulation, the IGO method achieves invariance under
reparametrization of the search space $X$, under a change of parameters of the
probability distributions, and under increasing transformations of the
objective function.
  Theory strongly suggests that IGO algorithms have minimal loss in diversity
during optimization, provided the initial diversity is high. First experiments
using restricted Boltzmann machines confirm this insight. Thus IGO seems to
provide, from information theory, an elegant way to spontaneously explore
several valleys of a fitness landscape in a single run.},
Year          = {2011},
Month         = {Jun},
Note          = {J. Machine Learning Research 18 (2017), no 18, 1-65},
Url           = {http://arxiv.org/abs/1106.3708v4},
File          = {1106.3708v4.pdf}
}
@article{1912.01603v3,
Author        = {Danijar Hafner and Timothy Lillicrap and Jimmy Ba and Mohammad Norouzi},
Title         = {Dream to Control: Learning Behaviors by Latent Imagination},
Eprint        = {1912.01603v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Learned world models summarize an agent's experience to facilitate learning
complex behaviors. While learning world models from high-dimensional sensory
inputs is becoming feasible through deep learning, there are many potential
ways for deriving behaviors from them. We present Dreamer, a reinforcement
learning agent that solves long-horizon tasks from images purely by latent
imagination. We efficiently learn behaviors by propagating analytic gradients
of learned state values back through trajectories imagined in the compact state
space of a learned world model. On 20 challenging visual control tasks, Dreamer
exceeds existing approaches in data-efficiency, computation time, and final
performance.},
Year          = {2019},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1912.01603v3},
File          = {1912.01603v3.pdf}
}
@article{1709.10087v2,
Author        = {Aravind Rajeswaran and Vikash Kumar and Abhishek Gupta and Giulia Vezzani and John Schulman and Emanuel Todorov and Sergey Levine},
Title         = {Learning Complex Dexterous Manipulation with Deep Reinforcement Learning
  and Demonstrations},
Eprint        = {1709.10087v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Dexterous multi-fingered hands are extremely versatile and provide a generic
way to perform a multitude of tasks in human-centric environments. However,
effectively controlling them remains challenging due to their high
dimensionality and large number of potential contacts. Deep reinforcement
learning (DRL) provides a model-agnostic approach to control complex dynamical
systems, but has not been shown to scale to high-dimensional dexterous
manipulation. Furthermore, deployment of DRL on physical systems remains
challenging due to sample inefficiency. Consequently, the success of DRL in
robotics has thus far been limited to simpler manipulators and tasks. In this
work, we show that model-free DRL can effectively scale up to complex
manipulation tasks with a high-dimensional 24-DoF hand, and solve them from
scratch in simulated experiments. Furthermore, with the use of a small number
of human demonstrations, the sample complexity can be significantly reduced,
which enables learning with sample sizes equivalent to a few hours of robot
experience. The use of demonstrations result in policies that exhibit very
natural movements and, surprisingly, are also substantially more robust.},
Year          = {2017},
Month         = {Sep},
Url           = {http://arxiv.org/abs/1709.10087v2},
File          = {1709.10087v2.pdf}
}
@article{1502.03167v3,
Author        = {Sergey Ioffe and Christian Szegedy},
Title         = {Batch Normalization: Accelerating Deep Network Training by Reducing
  Internal Covariate Shift},
Eprint        = {1502.03167v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Training Deep Neural Networks is complicated by the fact that the
distribution of each layer's inputs changes during training, as the parameters
of the previous layers change. This slows down the training by requiring lower
learning rates and careful parameter initialization, and makes it notoriously
hard to train models with saturating nonlinearities. We refer to this
phenomenon as internal covariate shift, and address the problem by normalizing
layer inputs. Our method draws its strength from making normalization a part of
the model architecture and performing the normalization for each training
mini-batch. Batch Normalization allows us to use much higher learning rates and
be less careful about initialization. It also acts as a regularizer, in some
cases eliminating the need for Dropout. Applied to a state-of-the-art image
classification model, Batch Normalization achieves the same accuracy with 14
times fewer training steps, and beats the original model by a significant
margin. Using an ensemble of batch-normalized networks, we improve upon the
best published result on ImageNet classification: reaching 4.9% top-5
validation error (and 4.8% test error), exceeding the accuracy of human raters.},
Year          = {2015},
Month         = {Feb},
Url           = {http://arxiv.org/abs/1502.03167v3},
File          = {1502.03167v3.pdf}
}
@article{1511.06279v4,
Author        = {Scott Reed and Nando de Freitas},
Title         = {Neural Programmer-Interpreters},
Eprint        = {1511.06279v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We propose the neural programmer-interpreter (NPI): a recurrent and
compositional neural network that learns to represent and execute programs. NPI
has three learnable components: a task-agnostic recurrent core, a persistent
key-value program memory, and domain-specific encoders that enable a single NPI
to operate in multiple perceptually diverse environments with distinct
affordances. By learning to compose lower-level programs to express
higher-level programs, NPI reduces sample complexity and increases
generalization ability compared to sequence-to-sequence LSTMs. The program
memory allows efficient learning of additional tasks by building on existing
programs. NPI can also harness the environment (e.g. a scratch pad with
read-write pointers) to cache intermediate results of computation, lessening
the long-term memory burden on recurrent hidden units. In this work we train
the NPI with fully-supervised execution traces; each program has example
sequences of calls to the immediate subprograms conditioned on the input.
Rather than training on a huge number of relatively weak labels, NPI learns
from a small number of rich examples. We demonstrate the capability of our
model to learn several types of compositional programs: addition, sorting, and
canonicalizing 3D models. Furthermore, a single NPI learns to execute these
programs and all 21 associated subprograms.},
Year          = {2015},
Month         = {Nov},
Url           = {http://arxiv.org/abs/1511.06279v4},
File          = {1511.06279v4.pdf}
}
@article{1901.11530v2,
Author        = {Marc G. Bellemare and Will Dabney and Robert Dadashi and Adrien Ali Taiga and Pablo Samuel Castro and Nicolas Le Roux and Dale Schuurmans and Tor Lattimore and Clare Lyle},
Title         = {A Geometric Perspective on Optimal Representations for Reinforcement
  Learning},
Eprint        = {1901.11530v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We propose a new perspective on representation learning in reinforcement
learning based on geometric properties of the space of value functions. We
leverage this perspective to provide formal evidence regarding the usefulness
of value functions as auxiliary tasks. Our formulation considers adapting the
representation to minimize the (linear) approximation of the value function of
all stationary policies for a given environment. We show that this optimization
reduces to making accurate predictions regarding a special class of value
functions which we call adversarial value functions (AVFs). We demonstrate that
using value functions as auxiliary tasks corresponds to an expected-error
relaxation of our formulation, with AVFs a natural candidate, and identify a
close relationship with proto-value functions (Mahadevan, 2005). We highlight
characteristics of AVFs and their usefulness as auxiliary tasks in a series of
experiments on the four-room domain.},
Year          = {2019},
Month         = {Jan},
Url           = {http://arxiv.org/abs/1901.11530v2},
File          = {1901.11530v2.pdf}
}
@article{1609.03759v2,
Author        = {Stephen James and Edward Johns},
Title         = {3D Simulation for Robot Arm Control with Deep Q-Learning},
Eprint        = {1609.03759v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Recent trends in robot arm control have seen a shift towards end-to-end
solutions, using deep reinforcement learning to learn a controller directly
from raw sensor data, rather than relying on a hand-crafted, modular pipeline.
However, the high dimensionality of the state space often means that it is
impractical to generate sufficient training data with real-world experiments.
As an alternative solution, we propose to learn a robot controller in
simulation, with the potential of then transferring this to a real robot.
Building upon the recent success of deep Q-networks, we present an approach
which uses 3D simulations to train a 7-DOF robotic arm in a control task
without any prior knowledge. The controller accepts images of the environment
as its only input, and outputs motor actions for the task of locating and
grasping a cube, over a range of initial configurations. To encourage efficient
learning, a structured reward function is designed with intermediate rewards.
We also present preliminary results in direct transfer of policies over to a
real robot, without any further training.},
Year          = {2016},
Month         = {Sep},
Url           = {http://arxiv.org/abs/1609.03759v2},
File          = {1609.03759v2.pdf}
}
@article{1709.02878v2,
Author        = {Danijar Hafner and James Davidson and Vincent Vanhoucke},
Title         = {TensorFlow Agents: Efficient Batched Reinforcement Learning in
  TensorFlow},
Eprint        = {1709.02878v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We introduce TensorFlow Agents, an efficient infrastructure paradigm for
building parallel reinforcement learning algorithms in TensorFlow. We simulate
multiple environments in parallel, and group them to perform the neural network
computation on a batch rather than individual observations. This allows the
TensorFlow execution engine to parallelize computation, without the need for
manual synchronization. Environments are stepped in separate Python processes
to progress them in parallel without interference of the global interpreter
lock. As part of this project, we introduce BatchPPO, an efficient
implementation of the proximal policy optimization algorithm. By open sourcing
TensorFlow Agents, we hope to provide a flexible starting point for future
projects that accelerates future research in the field.},
Year          = {2017},
Month         = {Sep},
Url           = {http://arxiv.org/abs/1709.02878v2},
File          = {1709.02878v2.pdf}
}
@article{1511.02540v1,
Author        = {Pierre-Yves Massé and Yann Ollivier},
Title         = {Speed learning on the fly},
Eprint        = {1511.02540v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {math.OC},
Abstract      = {The practical performance of online stochastic gradient descent algorithms is
highly dependent on the chosen step size, which must be tediously hand-tuned in
many applications. The same is true for more advanced variants of stochastic
gradients, such as SAGA, SVRG, or AdaGrad. Here we propose to adapt the step
size by performing a gradient descent on the step size itself, viewing the
whole performance of the learning trajectory as a function of step size.
Importantly, this adaptation can be computed online at little cost, without
having to iterate backward passes over the full data.},
Year          = {2015},
Month         = {Nov},
Url           = {http://arxiv.org/abs/1511.02540v1},
File          = {1511.02540v1.pdf}
}
@article{2005.13143v2,
Author        = {Muhammad Asif Rana and Anqi Li and Dieter Fox and Byron Boots and Fabio Ramos and Nathan Ratliff},
Title         = {Euclideanizing Flows: Diffeomorphic Reduction for Learning Stable
  Dynamical Systems},
Eprint        = {2005.13143v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Robotic tasks often require motions with complex geometric structures. We
present an approach to learn such motions from a limited number of human
demonstrations by exploiting the regularity properties of human motions e.g.
stability, smoothness, and boundedness. The complex motions are encoded as
rollouts of a stable dynamical system, which, under a change of coordinates
defined by a diffeomorphism, is equivalent to a simple, hand-specified
dynamical system. As an immediate result of using diffeomorphisms, the
stability property of the hand-specified dynamical system directly carry over
to the learned dynamical system. Inspired by recent works in density
estimation, we propose to represent the diffeomorphism as a composition of
simple parameterized diffeomorphisms. Additional structure is imposed to
provide guarantees on the smoothness of the generated motions. The efficacy of
this approach is demonstrated through validation on an established benchmark as
well demonstrations collected on a real-world robotic system.},
Year          = {2020},
Month         = {May},
Url           = {http://arxiv.org/abs/2005.13143v2},
File          = {2005.13143v2.pdf}
}
@article{1611.02635v4,
Author        = {Ashia C. Wilson and Benjamin Recht and Michael I. Jordan},
Title         = {A Lyapunov Analysis of Momentum Methods in Optimization},
Eprint        = {1611.02635v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {math.OC},
Abstract      = {Momentum methods play a significant role in optimization. Examples include
Nesterov's accelerated gradient method and the conditional gradient algorithm.
Several momentum methods are provably optimal under standard oracle models, and
all use a technique called estimate sequences to analyze their convergence
properties. The technique of estimate sequences has long been considered
difficult to understand, leading many researchers to generate alternative,
"more intuitive" methods and analyses. We show there is an equivalence between
the technique of estimate sequences and a family of Lyapunov functions in both
continuous and discrete time. This connection allows us to develop a simple and
unified analysis of many existing momentum algorithms, introduce several new
algorithms, and strengthen the connection between algorithms and
continuous-time dynamical systems.},
Year          = {2016},
Month         = {Nov},
Url           = {http://arxiv.org/abs/1611.02635v4},
File          = {1611.02635v4.pdf}
}
@article{1810.01257v2,
Author        = {Ofir Nachum and Shixiang Gu and Honglak Lee and Sergey Levine},
Title         = {Near-Optimal Representation Learning for Hierarchical Reinforcement
  Learning},
Eprint        = {1810.01257v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We study the problem of representation learning in goal-conditioned
hierarchical reinforcement learning. In such hierarchical structures, a
higher-level controller solves tasks by iteratively communicating goals which a
lower-level policy is trained to reach. Accordingly, the choice of
representation -- the mapping of observation space to goal space -- is crucial.
To study this problem, we develop a notion of sub-optimality of a
representation, defined in terms of expected reward of the optimal hierarchical
policy using this representation. We derive expressions which bound the
sub-optimality and show how these expressions can be translated to
representation learning objectives which may be optimized in practice. Results
on a number of difficult continuous-control tasks show that our approach to
representation learning yields qualitatively better representations as well as
quantitatively better hierarchical policies, compared to existing methods (see
videos at https://sites.google.com/view/representation-hrl).},
Year          = {2018},
Month         = {Oct},
Url           = {http://arxiv.org/abs/1810.01257v2},
File          = {1810.01257v2.pdf}
}
@article{1811.11359v1,
Author        = {David Warde-Farley and Tom Van de Wiele and Tejas Kulkarni and Catalin Ionescu and Steven Hansen and Volodymyr Mnih},
Title         = {Unsupervised Control Through Non-Parametric Discriminative Rewards},
Eprint        = {1811.11359v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Learning to control an environment without hand-crafted rewards or expert
data remains challenging and is at the frontier of reinforcement learning
research. We present an unsupervised learning algorithm to train agents to
achieve perceptually-specified goals using only a stream of observations and
actions. Our agent simultaneously learns a goal-conditioned policy and a goal
achievement reward function that measures how similar a state is to the goal
state. This dual optimization leads to a co-operative game, giving rise to a
learned reward function that reflects similarity in controllable aspects of the
environment instead of distance in the space of observations. We demonstrate
the efficacy of our agent to learn, in an unsupervised manner, to reach a
diverse set of goals on three domains -- Atari, the DeepMind Control Suite and
DeepMind Lab.},
Year          = {2018},
Month         = {Nov},
Url           = {http://arxiv.org/abs/1811.11359v1},
File          = {1811.11359v1.pdf}
}
@article{1412.1193v11,
Author        = {James Martens},
Title         = {New insights and perspectives on the natural gradient method},
Eprint        = {1412.1193v11},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Natural gradient descent is an optimization method traditionally motivated
from the perspective of information geometry, and works well for many
applications as an alternative to stochastic gradient descent. In this paper we
critically analyze this method and its properties, and show how it can be
viewed as a type of 2nd-order optimization method, with the Fisher information
matrix acting as a substitute for the Hessian. In many important cases, the
Fisher information matrix is shown to be equivalent to the Generalized
Gauss-Newton matrix, which both approximates the Hessian, but also has certain
properties that favor its use over the Hessian. This perspective turns out to
have significant implications for the design of a practical and robust natural
gradient optimizer, as it motivates the use of techniques like trust regions
and Tikhonov regularization. Additionally, we make a series of contributions to
the understanding of natural gradient and 2nd-order methods, including: a
thorough analysis of the convergence speed of stochastic natural gradient
descent (and more general stochastic 2nd-order methods) as applied to convex
quadratics, a critical examination of the oft-used "empirical" approximation of
the Fisher matrix, and an analysis of the (approximate) parameterization
invariance property possessed by natural gradient methods (which we show also
holds for certain other curvature, but notably not the Hessian).},
Year          = {2014},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1412.1193v11},
File          = {1412.1193v11.pdf}
}
@article{2006.04678v2,
Author        = {Robert Dadashi and Léonard Hussenot and Matthieu Geist and Olivier Pietquin},
Title         = {Primal Wasserstein Imitation Learning},
Eprint        = {2006.04678v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Imitation Learning (IL) methods seek to match the behavior of an agent with
that of an expert. In the present work, we propose a new IL method based on a
conceptually simple algorithm: Primal Wasserstein Imitation Learning (PWIL),
which ties to the primal form of the Wasserstein distance between the expert
and the agent state-action distributions. We present a reward function which is
derived offline, as opposed to recent adversarial IL algorithms that learn a
reward function through interactions with the environment, and which requires
little fine-tuning. We show that we can recover expert behavior on a variety of
continuous control tasks of the MuJoCo domain in a sample efficient manner in
terms of agent interactions and of expert interactions with the environment.
Finally, we show that the behavior of the agent we train matches the behavior
of the expert with the Wasserstein distance, rather than the commonly used
proxy of performance.},
Year          = {2020},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2006.04678v2},
File          = {2006.04678v2.pdf}
}
@article{1710.09829v2,
Author        = {Sara Sabour and Nicholas Frosst and Geoffrey E Hinton},
Title         = {Dynamic Routing Between Capsules},
Eprint        = {1710.09829v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {A capsule is a group of neurons whose activity vector represents the
instantiation parameters of a specific type of entity such as an object or an
object part. We use the length of the activity vector to represent the
probability that the entity exists and its orientation to represent the
instantiation parameters. Active capsules at one level make predictions, via
transformation matrices, for the instantiation parameters of higher-level
capsules. When multiple predictions agree, a higher level capsule becomes
active. We show that a discrimininatively trained, multi-layer capsule system
achieves state-of-the-art performance on MNIST and is considerably better than
a convolutional net at recognizing highly overlapping digits. To achieve these
results we use an iterative routing-by-agreement mechanism: A lower-level
capsule prefers to send its output to higher level capsules whose activity
vectors have a big scalar product with the prediction coming from the
lower-level capsule.},
Year          = {2017},
Month         = {Oct},
Url           = {http://arxiv.org/abs/1710.09829v2},
File          = {1710.09829v2.pdf}
}
@article{1807.06919v4,
Author        = {Cinjon Resnick and Roberta Raileanu and Sanyam Kapoor and Alexander Peysakhovich and Kyunghyun Cho and Joan Bruna},
Title         = {Backplay: "Man muss immer umkehren"},
Eprint        = {1807.06919v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Model-free reinforcement learning (RL) requires a large number of trials to
learn a good policy, especially in environments with sparse rewards. We explore
a method to improve the sample efficiency when we have access to
demonstrations. Our approach, Backplay, uses a single demonstration to
construct a curriculum for a given task. Rather than starting each training
episode in the environment's fixed initial state, we start the agent near the
end of the demonstration and move the starting point backwards during the
course of training until we reach the initial state. Our contributions are that
we analytically characterize the types of environments where Backplay can
improve training speed, demonstrate the effectiveness of Backplay both in large
grid worlds and a complex four player zero-sum game (Pommerman), and show that
Backplay compares favorably to other competitive methods known to improve
sample efficiency. This includes reward shaping, behavioral cloning, and
reverse curriculum generation.},
Year          = {2018},
Month         = {Jul},
Url           = {http://arxiv.org/abs/1807.06919v4},
File          = {1807.06919v4.pdf}
}
@article{1010.3013v1,
Author        = {Mark M. Tobenkin and Ian R. Manchester and Russ Tedrake},
Title         = {Invariant Funnels around Trajectories using Sum-of-Squares Programming},
Eprint        = {1010.3013v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {math.DS},
Abstract      = {This paper presents numerical methods for computing regions of finite-time
invariance (funnels) around solutions of polynomial differential equations.
First, we present a method which exactly certifies sufficient conditions for
invariance despite relying on approximate trajectories from numerical
integration. Our second method relaxes the constraints of the first by sampling
in time. In applications, this can recover almost identical funnels but is much
faster to compute. In both cases, funnels are verified using Sum-of-Squares
programming to search over a family of time-varying polynomial Lyapunov
functions. Initial candidate Lyapunov functions are constructed using the
linearization about the trajectory, and associated time-varying Lyapunov and
Riccati differential equations. The methods are compared on stabilized
trajectories of a six-state model of a satellite.},
Year          = {2010},
Month         = {Oct},
Url           = {http://arxiv.org/abs/1010.3013v1},
File          = {1010.3013v1.pdf}
}
@article{1807.03748v2,
Author        = {Aaron van den Oord and Yazhe Li and Oriol Vinyals},
Title         = {Representation Learning with Contrastive Predictive Coding},
Eprint        = {1807.03748v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {While supervised learning has enabled great progress in many applications,
unsupervised learning has not seen such widespread adoption, and remains an
important and challenging endeavor for artificial intelligence. In this work,
we propose a universal unsupervised learning approach to extract useful
representations from high-dimensional data, which we call Contrastive
Predictive Coding. The key insight of our model is to learn such
representations by predicting the future in latent space by using powerful
autoregressive models. We use a probabilistic contrastive loss which induces
the latent space to capture information that is maximally useful to predict
future samples. It also makes the model tractable by using negative sampling.
While most prior work has focused on evaluating representations for a
particular modality, we demonstrate that our approach is able to learn useful
representations achieving strong performance on four distinct domains: speech,
images, text and reinforcement learning in 3D environments.},
Year          = {2018},
Month         = {Jul},
Url           = {http://arxiv.org/abs/1807.03748v2},
File          = {1807.03748v2.pdf}
}
@article{1901.10691v2,
Author        = {Casey Chu and Jose Blanchet and Peter Glynn},
Title         = {Probability Functional Descent: A Unifying Perspective on GANs,
  Variational Inference, and Reinforcement Learning},
Eprint        = {1901.10691v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {This paper provides a unifying view of a wide range of problems of interest
in machine learning by framing them as the minimization of functionals defined
on the space of probability measures. In particular, we show that generative
adversarial networks, variational inference, and actor-critic methods in
reinforcement learning can all be seen through the lens of our framework. We
then discuss a generic optimization algorithm for our formulation, called
probability functional descent (PFD), and show how this algorithm recovers
existing methods developed independently in the settings mentioned earlier.},
Year          = {2019},
Month         = {Jan},
Url           = {http://arxiv.org/abs/1901.10691v2},
File          = {1901.10691v2.pdf}
}
@article{1802.04181v2,
Author        = {Timothée Lesort and Natalia Díaz-Rodríguez and Jean-François Goudou and David Filliat},
Title         = {State Representation Learning for Control: An Overview},
Eprint        = {1802.04181v2},
DOI           = {10.1016/j.neunet.2018.07.006},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Representation learning algorithms are designed to learn abstract features
that characterize data. State representation learning (SRL) focuses on a
particular kind of representation learning where learned features are in low
dimension, evolve through time, and are influenced by actions of an agent. The
representation is learned to capture the variation in the environment generated
by the agent's actions; this kind of representation is particularly suitable
for robotics and control scenarios. In particular, the low dimension
characteristic of the representation helps to overcome the curse of
dimensionality, provides easier interpretation and utilization by humans and
can help improve performance and speed in policy learning algorithms such as
reinforcement learning.
  This survey aims at covering the state-of-the-art on state representation
learning in the most recent years. It reviews different SRL methods that
involve interaction with the environment, their implementations and their
applications in robotics control tasks (simulated or real). In particular, it
highlights how generic learning objectives are differently exploited in the
reviewed algorithms. Finally, it discusses evaluation methods to assess the
representation learned and summarizes current and future lines of research.},
Year          = {2018},
Month         = {Feb},
Url           = {http://arxiv.org/abs/1802.04181v2},
File          = {1802.04181v2.pdf}
}
@article{1801.06159v2,
Author        = {Lam M. Nguyen and Nam H. Nguyen and Dzung T. Phan and Jayant R. Kalagnanam and Katya Scheinberg},
Title         = {When Does Stochastic Gradient Algorithm Work Well?},
Eprint        = {1801.06159v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.ML},
Abstract      = {In this paper, we consider a general stochastic optimization problem which is
often at the core of supervised learning, such as deep learning and linear
classification. We consider a standard stochastic gradient descent (SGD) method
with a fixed, large step size and propose a novel assumption on the objective
function, under which this method has the improved convergence rates (to a
neighborhood of the optimal solutions). We then empirically demonstrate that
these assumptions hold for logistic regression and standard deep neural
networks on classical data sets. Thus our analysis helps to explain when
efficient behavior can be expected from the SGD method in training
classification models and deep neural networks.},
Year          = {2018},
Month         = {Jan},
Url           = {http://arxiv.org/abs/1801.06159v2},
File          = {1801.06159v2.pdf}
}
@article{1801.03954v2,
Author        = {Glen Berseth and Michiel van de Panne},
Title         = {Model-Based Action Exploration for Learning Dynamic Motion Skills},
Eprint        = {1801.03954v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Deep reinforcement learning has achieved great strides in solving challenging
motion control tasks. Recently, there has been significant work on methods for
exploiting the data gathered during training, but there has been less work on
how to best generate the data to learn from. For continuous action domains, the
most common method for generating exploratory actions involves sampling from a
Gaussian distribution centred around the mean action output by a policy.
Although these methods can be quite capable, they do not scale well with the
dimensionality of the action space, and can be dangerous to apply on hardware.
We consider learning a forward dynamics model to predict the result,
($x_{t+1}$), of taking a particular action, ($u$), given a specific observation
of the state, ($x_{t}$). With this model we perform internal look-ahead
predictions of outcomes and seek actions we believe have a reasonable chance of
success. This method alters the exploratory action space, thereby increasing
learning speed and enables higher quality solutions to difficult problems, such
as robotic locomotion and juggling.},
Year          = {2018},
Month         = {Jan},
Url           = {http://arxiv.org/abs/1801.03954v2},
File          = {1801.03954v2.pdf}
}
@article{1906.07987v1,
Author        = {Hugo Penedones and Carlos Riquelme and Damien Vincent and Hartmut Maennel and Timothy Mann and Andre Barreto and Sylvain Gelly and Gergely Neu},
Title         = {Adaptive Temporal-Difference Learning for Policy Evaluation with
  Per-State Uncertainty Estimates},
Eprint        = {1906.07987v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We consider the core reinforcement-learning problem of on-policy value
function approximation from a batch of trajectory data, and focus on various
issues of Temporal Difference (TD) learning and Monte Carlo (MC) policy
evaluation. The two methods are known to achieve complementary bias-variance
trade-off properties, with TD tending to achieve lower variance but potentially
higher bias. In this paper, we argue that the larger bias of TD can be a result
of the amplification of local approximation errors. We address this by
proposing an algorithm that adaptively switches between TD and MC in each
state, thus mitigating the propagation of errors. Our method is based on
learned confidence intervals that detect biases of TD estimates. We demonstrate
in a variety of policy evaluation tasks that this simple adaptive algorithm
performs competitively with the best approach in hindsight, suggesting that
learned confidence intervals are a powerful technique for adapting policy
evaluation to use TD or MC returns in a data-driven way.},
Year          = {2019},
Month         = {Jun},
Url           = {http://arxiv.org/abs/1906.07987v1},
File          = {1906.07987v1.pdf}
}
@article{1606.05908v3,
Author        = {Carl Doersch},
Title         = {Tutorial on Variational Autoencoders},
Eprint        = {1606.05908v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.ML},
Abstract      = {In just three years, Variational Autoencoders (VAEs) have emerged as one of
the most popular approaches to unsupervised learning of complicated
distributions. VAEs are appealing because they are built on top of standard
function approximators (neural networks), and can be trained with stochastic
gradient descent. VAEs have already shown promise in generating many kinds of
complicated data, including handwritten digits, faces, house numbers, CIFAR
images, physical models of scenes, segmentation, and predicting the future from
static images. This tutorial introduces the intuitions behind VAEs, explains
the mathematics behind them, and describes some empirical behavior. No prior
knowledge of variational Bayesian methods is assumed.},
Year          = {2016},
Month         = {Jun},
Url           = {http://arxiv.org/abs/1606.05908v3},
File          = {1606.05908v3.pdf}
}
@article{1810.12894v1,
Author        = {Yuri Burda and Harrison Edwards and Amos Storkey and Oleg Klimov},
Title         = {Exploration by Random Network Distillation},
Eprint        = {1810.12894v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We introduce an exploration bonus for deep reinforcement learning methods
that is easy to implement and adds minimal overhead to the computation
performed. The bonus is the error of a neural network predicting features of
the observations given by a fixed randomly initialized neural network. We also
introduce a method to flexibly combine intrinsic and extrinsic rewards. We find
that the random network distillation (RND) bonus combined with this increased
flexibility enables significant progress on several hard exploration Atari
games. In particular we establish state of the art performance on Montezuma's
Revenge, a game famously difficult for deep reinforcement learning methods. To
the best of our knowledge, this is the first method that achieves better than
average human performance on this game without using demonstrations or having
access to the underlying state of the game, and occasionally completes the
first level.},
Year          = {2018},
Month         = {Oct},
Url           = {http://arxiv.org/abs/1810.12894v1},
File          = {1810.12894v1.pdf}
}
@article{1906.04556v2,
Author        = {Matthieu Zimmer and Paul Weng},
Title         = {Exploiting the Sign of the Advantage Function to Learn Deterministic
  Policies in Continuous Domains},
Eprint        = {1906.04556v2},
DOI           = {10.24963/ijcai.2019/625},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {In the context of learning deterministic policies in continuous domains, we
revisit an approach, which was first proposed in Continuous Actor Critic
Learning Automaton (CACLA) and later extended in Neural Fitted Actor Critic
(NFAC). This approach is based on a policy update different from that of
deterministic policy gradient (DPG). Previous work has observed its excellent
performance empirically, but a theoretical justification is lacking. To fill
this gap, we provide a theoretical explanation to motivate this unorthodox
policy update by relating it to another update and making explicit the
objective function of the latter. We furthermore discuss in depth the
properties of these updates to get a deeper understanding of the overall
approach. In addition, we extend it and propose a new trust region algorithm,
Penalized NFAC (PeNFAC). Finally, we experimentally demonstrate in several
classic control problems that it surpasses the state-of-the-art algorithms to
learn deterministic policies.},
Year          = {2019},
Month         = {Jun},
Url           = {http://arxiv.org/abs/1906.04556v2},
File          = {1906.04556v2.pdf}
}
@article{1709.07932v3,
Author        = {Visak C. V. Kumar and Sehoon Ha and C. Karen Liu},
Title         = {Expanding Motor Skills through Relay Neural Networks},
Eprint        = {1709.07932v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {While the recent advances in deep reinforcement learning have achieved
impressive results in learning motor skills, many of the trained policies are
only capable within a limited set of initial states. We propose a technique to
break down a complex robotic task to simpler subtasks and train them
sequentially such that the robot can expand its existing skill set gradually.
Our key idea is to build a tree of local control policies represented by neural
networks, which we refer as Relay Neural Networks. Starting from the root
policy that attempts to achieve the task from a small set of initial states,
each subsequent policy expands the set of successful initial states by driving
the new states to existing "good" states. Our algorithm utilizes the value
function of the policy to determine whether a state is "good" under each
policy. We take advantage of many existing policy search algorithms that learn
the value function simultaneously with the policy, such as those that use
actor-critic representations or those that use the advantage function to reduce
variance. We demonstrate that the relay networks can solve complex continuous
control problems for underactuated dynamic systems.},
Year          = {2017},
Month         = {Sep},
Url           = {http://arxiv.org/abs/1709.07932v3},
File          = {1709.07932v3.pdf}
}
@article{1802.01561v3,
Author        = {Lasse Espeholt and Hubert Soyer and Remi Munos and Karen Simonyan and Volodymir Mnih and Tom Ward and Yotam Doron and Vlad Firoiu and Tim Harley and Iain Dunning and Shane Legg and Koray Kavukcuoglu},
Title         = {IMPALA: Scalable Distributed Deep-RL with Importance Weighted
  Actor-Learner Architectures},
Eprint        = {1802.01561v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {In this work we aim to solve a large collection of tasks using a single
reinforcement learning agent with a single set of parameters. A key challenge
is to handle the increased amount of data and extended training time. We have
developed a new distributed agent IMPALA (Importance Weighted Actor-Learner
Architecture) that not only uses resources more efficiently in single-machine
training but also scales to thousands of machines without sacrificing data
efficiency or resource utilisation. We achieve stable learning at high
throughput by combining decoupled acting and learning with a novel off-policy
correction method called V-trace. We demonstrate the effectiveness of IMPALA
for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the
DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available
Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our
results show that IMPALA is able to achieve better performance than previous
agents with less data, and crucially exhibits positive transfer between tasks
as a result of its multi-task approach.},
Year          = {2018},
Month         = {Feb},
Url           = {http://arxiv.org/abs/1802.01561v3},
File          = {1802.01561v3.pdf}
}
@article{1806.03884v2,
Author        = {Thomas George and César Laurent and Xavier Bouthillier and Nicolas Ballas and Pascal Vincent},
Title         = {Fast Approximate Natural Gradient Descent in a Kronecker-factored
  Eigenbasis},
Eprint        = {1806.03884v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Optimization algorithms that leverage gradient covariance information, such
as variants of natural gradient descent (Amari, 1998), offer the prospect of
yielding more effective descent directions. For models with many parameters,
the covariance matrix they are based on becomes gigantic, making them
inapplicable in their original form. This has motivated research into both
simple diagonal approximations and more sophisticated factored approximations
such as KFAC (Heskes, 2000; Martens & Grosse, 2015; Grosse & Martens, 2016). In
the present work we draw inspiration from both to propose a novel approximation
that is provably better than KFAC and amendable to cheap partial updates. It
consists in tracking a diagonal variance, not in parameter coordinates, but in
a Kronecker-factored eigenbasis, in which the diagonal approximation is likely
to be more effective. Experiments show improvements over KFAC in optimization
speed for several deep network architectures.},
Year          = {2018},
Month         = {Jun},
Note          = {Advances in Neural Information Processing Systems 2018},
Url           = {http://arxiv.org/abs/1806.03884v2},
File          = {1806.03884v2.pdf}
}
@article{1211.0358v2,
Author        = {Andreas C. Damianou and Neil D. Lawrence},
Title         = {Deep Gaussian Processes},
Eprint        = {1211.0358v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.ML},
Abstract      = {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a
deep belief network based on Gaussian process mappings. The data is modeled as
the output of a multivariate GP. The inputs to that Gaussian process are then
governed by another GP. A single layer model is equivalent to a standard GP or
the GP latent variable model (GP-LVM). We perform inference in the model by
approximate variational marginalization. This results in a strict lower bound
on the marginal likelihood of the model which we use for model selection
(number of layers and nodes per layer). Deep belief networks are typically
applied to relatively large data sets using stochastic gradient descent for
optimization. Our fully Bayesian treatment allows for the application of deep
models even when data is scarce. Model selection by our variational bound shows
that a five layer hierarchy is justified even when modelling a digit data set
containing only 150 examples.},
Year          = {2012},
Month         = {Nov},
Url           = {http://arxiv.org/abs/1211.0358v2},
File          = {1211.0358v2.pdf}
}
@article{1805.08380v4,
Author        = {Yifan Chen and Wuchen Li},
Title         = {Optimal transport natural gradient for statistical manifolds with
  continuous sample space},
Eprint        = {1805.08380v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {math.OC},
Abstract      = {We study the Wasserstein natural gradient in parametric statistical models
with continuous sample spaces. Our approach is to pull back the
$L^2$-Wasserstein metric tensor in the probability density space to a parameter
space, equipping the latter with a positive definite metric tensor, under which
it becomes a Riemannian manifold, named the Wasserstein statistical manifold.
In general, it is not a totally geodesic sub-manifold of the density space, and
therefore its geodesics will differ from the Wasserstein geodesics, except for
the well-known Gaussian distribution case, a fact which can also be validated
under our framework. We use the sub-manifold geometry to derive a gradient flow
and natural gradient descent method in the parameter space. When parametrized
densities lie in $\bR$, the induced metric tensor establishes an explicit
formula. In optimization problems, we observe that the natural gradient descent
outperforms the standard gradient descent when the Wasserstein distance is the
objective function. In such a case, we prove that the resulting algorithm
behaves similarly to the Newton method in the asymptotic regime. The proof
calculates the exact Hessian formula for the Wasserstein distance, which
further motivates another preconditioner for the optimization process. To the
end, we present examples to illustrate the effectiveness of the natural
gradient in several parametric statistical models, including the Gaussian
measure, Gaussian mixture, Gamma distribution, and Laplace distribution.},
Year          = {2018},
Month         = {May},
Url           = {http://arxiv.org/abs/1805.08380v4},
File          = {1805.08380v4.pdf}
}
@article{1909.01387v1,
Author        = {Tom Le Paine and Caglar Gulcehre and Bobak Shahriari and Misha Denil and Matt Hoffman and Hubert Soyer and Richard Tanburn and Steven Kapturowski and Neil Rabinowitz and Duncan Williams and Gabriel Barth-Maron and Ziyu Wang and Nando de Freitas and Worlds Team},
Title         = {Making Efficient Use of Demonstrations to Solve Hard Exploration
  Problems},
Eprint        = {1909.01387v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {This paper introduces R2D3, an agent that makes efficient use of
demonstrations to solve hard exploration problems in partially observable
environments with highly variable initial conditions. We also introduce a suite
of eight tasks that combine these three properties, and show that R2D3 can
solve several of the tasks where other state of the art methods (both with and
without demonstrations) fail to see even a single successful trajectory after
tens of billions of steps of exploration.},
Year          = {2019},
Month         = {Sep},
Url           = {http://arxiv.org/abs/1909.01387v1},
File          = {1909.01387v1.pdf}
}
@article{1909.12397v3,
Author        = {Moonkyung Ryu and Yinlam Chow and Ross Anderson and Christian Tjandraatmadja and Craig Boutilier},
Title         = {CAQL: Continuous Action Q-Learning},
Eprint        = {1909.12397v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Value-based reinforcement learning (RL) methods like Q-learning have shown
success in a variety of domains. One challenge in applying Q-learning to
continuous-action RL problems, however, is the continuous action maximization
(max-Q) required for optimal Bellman backup. In this work, we develop CAQL, a
(class of) algorithm(s) for continuous-action Q-learning that can use several
plug-and-play optimizers for the max-Q problem. Leveraging recent optimization
results for deep neural networks, we show that max-Q can be solved optimally
using mixed-integer programming (MIP). When the Q-function representation has
sufficient power, MIP-based optimization gives rise to better policies and is
more robust than approximate methods (e.g., gradient ascent, cross-entropy
search). We further develop several techniques to accelerate inference in CAQL,
which despite their approximate nature, perform well. We compare CAQL with
state-of-the-art RL algorithms on benchmark continuous-control problems that
have different degrees of action constraints and show that CAQL outperforms
policy-based methods in heavily constrained environments, often dramatically.},
Year          = {2019},
Month         = {Sep},
Url           = {http://arxiv.org/abs/1909.12397v3},
File          = {1909.12397v3.pdf}
}
@article{1708.05866v2,
Author        = {Kai Arulkumaran and Marc Peter Deisenroth and Miles Brundage and Anil Anthony Bharath},
Title         = {A Brief Survey of Deep Reinforcement Learning},
Eprint        = {1708.05866v2},
DOI           = {10.1109/MSP.2017.2743240},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Deep reinforcement learning is poised to revolutionise the field of AI and
represents a step towards building autonomous systems with a higher level
understanding of the visual world. Currently, deep learning is enabling
reinforcement learning to scale to problems that were previously intractable,
such as learning to play video games directly from pixels. Deep reinforcement
learning algorithms are also applied to robotics, allowing control policies for
robots to be learned directly from camera inputs in the real world. In this
survey, we begin with an introduction to the general field of reinforcement
learning, then progress to the main streams of value-based and policy-based
methods. Our survey will cover central algorithms in deep reinforcement
learning, including the deep $Q$-network, trust region policy optimisation, and
asynchronous advantage actor-critic. In parallel, we highlight the unique
advantages of deep neural networks, focusing on visual understanding via
reinforcement learning. To conclude, we describe several current areas of
research within the field.},
Year          = {2017},
Month         = {Aug},
Url           = {http://arxiv.org/abs/1708.05866v2},
File          = {1708.05866v2.pdf}
}
@article{1707.01891v3,
Author        = {Ofir Nachum and Mohammad Norouzi and Kelvin Xu and Dale Schuurmans},
Title         = {Trust-PCL: An Off-Policy Trust Region Method for Continuous Control},
Eprint        = {1707.01891v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Trust region methods, such as TRPO, are often used to stabilize policy
optimization algorithms in reinforcement learning (RL). While current trust
region strategies are effective for continuous control, they typically require
a prohibitively large amount of on-policy interaction with the environment. To
address this problem, we propose an off-policy trust region method, Trust-PCL.
The algorithm is the result of observing that the optimal policy and state
values of a maximum reward objective with a relative-entropy regularizer
satisfy a set of multi-step pathwise consistencies along any path. Thus,
Trust-PCL is able to maintain optimization stability while exploiting
off-policy data to improve sample efficiency. When evaluated on a number of
continuous control tasks, Trust-PCL improves the solution quality and sample
efficiency of TRPO.},
Year          = {2017},
Month         = {Jul},
Url           = {http://arxiv.org/abs/1707.01891v3},
File          = {1707.01891v3.pdf}
}
@article{1806.07366v5,
Author        = {Ricky T. Q. Chen and Yulia Rubanova and Jesse Bettencourt and David Duvenaud},
Title         = {Neural Ordinary Differential Equations},
Eprint        = {1806.07366v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We introduce a new family of deep neural network models. Instead of
specifying a discrete sequence of hidden layers, we parameterize the derivative
of the hidden state using a neural network. The output of the network is
computed using a black-box differential equation solver. These continuous-depth
models have constant memory cost, adapt their evaluation strategy to each
input, and can explicitly trade numerical precision for speed. We demonstrate
these properties in continuous-depth residual networks and continuous-time
latent variable models. We also construct continuous normalizing flows, a
generative model that can train by maximum likelihood, without partitioning or
ordering the data dimensions. For training, we show how to scalably
backpropagate through any ODE solver, without access to its internal
operations. This allows end-to-end training of ODEs within larger models.},
Year          = {2018},
Month         = {Jun},
Url           = {http://arxiv.org/abs/1806.07366v5},
File          = {1806.07366v5.pdf}
}
@article{1311.1839v2,
Author        = {Scott Kuindersma and Frank Permenter and Russ Tedrake},
Title         = {An Efficiently Solvable Quadratic Program for Stabilizing Dynamic
  Locomotion},
Eprint        = {1311.1839v2},
DOI           = {10.1109/ICRA.2014.6907230},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {We describe a whole-body dynamic walking controller implemented as a convex
quadratic program. The controller solves an optimal control problem using an
approximate value function derived from a simple walking model while respecting
the dynamic, input, and contact constraints of the full robot dynamics. By
exploiting sparsity and temporal structure in the optimization with a custom
active-set algorithm, we surpass the performance of the best available
off-the-shelf solvers and achieve 1kHz control rates for a 34-DOF humanoid. We
describe applications to balancing and walking tasks using the simulated Atlas
robot in the DARPA Virtual Robotics Challenge.},
Year          = {2013},
Month         = {Nov},
Note          = {In Proceedings of the International Conference on Robotics and
  Automation (ICRA), 2589-2594 (2014)},
Url           = {http://arxiv.org/abs/1311.1839v2},
File          = {1311.1839v2.pdf}
}
@article{1507.07680v2,
Author        = {Yann Ollivier and Corentin Tallec and Guillaume Charpiat},
Title         = {Training recurrent networks online without backtracking},
Eprint        = {1507.07680v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.NE},
Abstract      = {We introduce the "NoBackTrack" algorithm to train the parameters of dynamical
systems such as recurrent neural networks. This algorithm works in an online,
memoryless setting, thus requiring no backpropagation through time, and is
scalable, avoiding the large computational and memory cost of maintaining the
full gradient of the current state with respect to the parameters.
  The algorithm essentially maintains, at each time, a single search direction
in parameter space. The evolution of this search direction is partly stochastic
and is constructed in such a way to provide, at every time, an unbiased random
estimate of the gradient of the loss function with respect to the parameters.
Because the gradient estimate is unbiased, on average over time the parameter
is updated as it should.
  The resulting gradient estimate can then be fed to a lightweight Kalman-like
filter to yield an improved algorithm. For recurrent neural networks, the
resulting algorithms scale linearly with the number of parameters.
  Small-scale experiments confirm the suitability of the approach, showing that
the stochastic approximation of the gradient introduced in the algorithm is not
detrimental to learning. In particular, the Kalman-like version of NoBackTrack
is superior to backpropagation through time (BPTT) when the time span of
dependencies in the data is longer than the truncation span for BPTT.},
Year          = {2015},
Month         = {Jul},
Url           = {http://arxiv.org/abs/1507.07680v2},
File          = {1507.07680v2.pdf}
}
@article{2010.06491v1,
Author        = {Brian Ichter and Pierre Sermanet and Corey Lynch},
Title         = {Broadly-Exploring, Local-Policy Trees for Long-Horizon Task Planning},
Eprint        = {2010.06491v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Long-horizon planning in realistic environments requires the ability to
reason over sequential tasks in high-dimensional state spaces with complex
dynamics. Classical motion planning algorithms, such as rapidly-exploring
random trees, are capable of efficiently exploring large state spaces and
computing long-horizon, sequential plans. However, these algorithms are
generally challenged with complex, stochastic, and high-dimensional state
spaces as well as in the presence of narrow passages, which naturally emerge in
tasks that interact with the environment. Machine learning offers a promising
solution for its ability to learn general policies that can handle complex
interactions and high-dimensional observations. However, these policies are
generally limited in horizon length. Our approach, Broadly-Exploring,
Local-policy Trees (BELT), merges these two approaches to leverage the
strengths of both through a task-conditioned, model-based tree search. BELT
uses an RRT-inspired tree search to efficiently explore the state space.
Locally, the exploration is guided by a task-conditioned, learned policy
capable of performing general short-horizon tasks. This task space can be quite
general and abstract; its only requirements are to be sampleable and to
well-cover the space of useful tasks. This search is aided by a
task-conditioned model that temporally extends dynamics propagation to allow
long-horizon search and sequential reasoning over tasks. BELT is demonstrated
experimentally to be able to plan long-horizon, sequential trajectories with a
goal conditioned policy and generate plans that are robust.},
Year          = {2020},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2010.06491v1},
File          = {2010.06491v1.pdf}
}
@article{1803.06773v1,
Author        = {Tuomas Haarnoja and Vitchyr Pong and Aurick Zhou and Murtaza Dalal and Pieter Abbeel and Sergey Levine},
Title         = {Composable Deep Reinforcement Learning for Robotic Manipulation},
Eprint        = {1803.06773v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Model-free deep reinforcement learning has been shown to exhibit good
performance in domains ranging from video games to simulated robotic
manipulation and locomotion. However, model-free methods are known to perform
poorly when the interaction time with the environment is limited, as is the
case for most real-world robotic tasks. In this paper, we study how maximum
entropy policies trained using soft Q-learning can be applied to real-world
robotic manipulation. The application of this method to real-world manipulation
is facilitated by two important features of soft Q-learning. First, soft
Q-learning can learn multimodal exploration strategies by learning policies
represented by expressive energy-based models. Second, we show that policies
learned with soft Q-learning can be composed to create new policies, and that
the optimality of the resulting policy can be bounded in terms of the
divergence between the composed policies. This compositionality provides an
especially valuable tool for real-world manipulation, where constructing new
policies by composing existing skills can provide a large gain in efficiency
over training from scratch. Our experimental evaluation demonstrates that soft
Q-learning is substantially more sample efficient than prior model-free deep
reinforcement learning methods, and that compositionality can be performed for
both simulated and real-world tasks.},
Year          = {2018},
Month         = {Mar},
Url           = {http://arxiv.org/abs/1803.06773v1},
File          = {1803.06773v1.pdf}
}
@article{1806.01240v3,
Author        = {Laurent Younes},
Title         = {Diffeomorphic Learning},
Eprint        = {1806.01240v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.ML},
Abstract      = {We introduce in this paper a learning paradigm in which the training data is
transformed by a diffeomorphic transformation before prediction. The learning
algorithm minimizes a cost function evaluating the prediction error on the
training set penalized by the distance between the diffeomorphism and the
identity. The approach borrows ideas from shape analysis where diffeomorphisms
are estimated for shape and image alignment, and brings them in a previously
unexplored setting, estimating, in particular diffeomorphisms in much larger
dimensions. After introducing the concept and describing a learning algorithm,
we present diverse applications, mostly with synthetic examples, demonstrating
the potential of the approach, as well as some insight on how it can be
improved.},
Year          = {2018},
Month         = {Jun},
Url           = {http://arxiv.org/abs/1806.01240v3},
File          = {1806.01240v3.pdf}
}
@article{1603.00748v1,
Author        = {Shixiang Gu and Timothy Lillicrap and Ilya Sutskever and Sergey Levine},
Title         = {Continuous Deep Q-Learning with Model-based Acceleration},
Eprint        = {1603.00748v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Model-free reinforcement learning has been successfully applied to a range of
challenging problems, and has recently been extended to handle large neural
network policies and value functions. However, the sample complexity of
model-free algorithms, particularly when using high-dimensional function
approximators, tends to limit their applicability to physical systems. In this
paper, we explore algorithms and representations to reduce the sample
complexity of deep reinforcement learning for continuous control tasks. We
propose two complementary techniques for improving the efficiency of such
algorithms. First, we derive a continuous variant of the Q-learning algorithm,
which we call normalized adantage functions (NAF), as an alternative to the
more commonly used policy gradient and actor-critic methods. NAF representation
allows us to apply Q-learning with experience replay to continuous tasks, and
substantially improves performance on a set of simulated robotic control tasks.
To further improve the efficiency of our approach, we explore the use of
learned models for accelerating model-free reinforcement learning. We show that
iteratively refitted local linear models are especially effective for this, and
demonstrate substantially faster learning on domains where such models are
applicable.},
Year          = {2016},
Month         = {Mar},
Url           = {http://arxiv.org/abs/1603.00748v1},
File          = {1603.00748v1.pdf}
}
@article{1803.02348v3,
Author        = {Ofir Nachum and Mohammad Norouzi and George Tucker and Dale Schuurmans},
Title         = {Smoothed Action Value Functions for Learning Gaussian Policies},
Eprint        = {1803.02348v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {State-action value functions (i.e., Q-values) are ubiquitous in reinforcement
learning (RL), giving rise to popular algorithms such as SARSA and Q-learning.
We propose a new notion of action value defined by a Gaussian smoothed version
of the expected Q-value. We show that such smoothed Q-values still satisfy a
Bellman equation, making them learnable from experience sampled from an
environment. Moreover, the gradients of expected reward with respect to the
mean and covariance of a parameterized Gaussian policy can be recovered from
the gradient and Hessian of the smoothed Q-value function. Based on these
relationships, we develop new algorithms for training a Gaussian policy
directly from a learned smoothed Q-value approximator. The approach is
additionally amenable to proximal optimization by augmenting the objective with
a penalty on KL-divergence from a previous policy. We find that the ability to
learn both a mean and covariance during training leads to significantly
improved results on standard continuous control benchmarks.},
Year          = {2018},
Month         = {Mar},
Url           = {http://arxiv.org/abs/1803.02348v3},
File          = {1803.02348v3.pdf}
}
@article{1705.10743v1,
Author        = {Marc G. Bellemare and Ivo Danihelka and Will Dabney and Shakir Mohamed and Balaji Lakshminarayanan and Stephan Hoyer and Rémi Munos},
Title         = {The Cramer Distance as a Solution to Biased Wasserstein Gradients},
Eprint        = {1705.10743v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {The Wasserstein probability metric has received much attention from the
machine learning community. Unlike the Kullback-Leibler divergence, which
strictly measures change in probability, the Wasserstein metric reflects the
underlying geometry between outcomes. The value of being sensitive to this
geometry has been demonstrated, among others, in ordinal regression and
generative modelling. In this paper we describe three natural properties of
probability divergences that reflect requirements from machine learning: sum
invariance, scale sensitivity, and unbiased sample gradients. The Wasserstein
metric possesses the first two properties but, unlike the Kullback-Leibler
divergence, does not possess the third. We provide empirical evidence
suggesting that this is a serious issue in practice. Leveraging insights from
probabilistic forecasting we propose an alternative to the Wasserstein metric,
the Cram\'er distance. We show that the Cram\'er distance possesses all three
desired properties, combining the best of the Wasserstein and Kullback-Leibler
divergences. To illustrate the relevance of the Cram\'er distance in practice
we design a new algorithm, the Cram\'er Generative Adversarial Network (GAN),
and show that it performs significantly better than the related Wasserstein
GAN.},
Year          = {2017},
Month         = {May},
Url           = {http://arxiv.org/abs/1705.10743v1},
File          = {1705.10743v1.pdf}
}
@article{1607.00485v1,
Author        = {Simone Scardapane and Danilo Comminiello and Amir Hussain and Aurelio Uncini},
Title         = {Group Sparse Regularization for Deep Neural Networks},
Eprint        = {1607.00485v1},
DOI           = {10.1016/j.neucom.2017.02.029},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.ML},
Abstract      = {In this paper, we consider the joint task of simultaneously optimizing (i)
the weights of a deep neural network, (ii) the number of neurons for each
hidden layer, and (iii) the subset of active input features (i.e., feature
selection). While these problems are generally dealt with separately, we
present a simple regularized formulation allowing to solve all three of them in
parallel, using standard optimization routines. Specifically, we extend the
group Lasso penalty (originated in the linear regression literature) in order
to impose group-level sparsity on the network's connections, where each group
is defined as the set of outgoing weights from a unit. Depending on the
specific case, the weights can be related to an input variable, to a hidden
neuron, or to a bias unit, thus performing simultaneously all the
aforementioned tasks in order to obtain a compact network. We perform an
extensive experimental evaluation, by comparing with classical weight decay and
Lasso penalties. We show that a sparse version of the group Lasso penalty is
able to achieve competitive performances, while at the same time resulting in
extremely compact networks with a smaller number of input features. We evaluate
both on a toy dataset for handwritten digit recognition, and on multiple
realistic large-scale classification problems.},
Year          = {2016},
Month         = {Jul},
Url           = {http://arxiv.org/abs/1607.00485v1},
File          = {1607.00485v1.pdf}
}
@article{1811.11214v5,
Author        = {Zafarali Ahmed and Nicolas Le Roux and Mohammad Norouzi and Dale Schuurmans},
Title         = {Understanding the impact of entropy on policy optimization},
Eprint        = {1811.11214v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Entropy regularization is commonly used to improve policy optimization in
reinforcement learning. It is believed to help with \emph{exploration} by
encouraging the selection of more stochastic policies. In this work, we analyze
this claim using new visualizations of the optimization landscape based on
randomly perturbing the loss function. We first show that even with access to
the exact gradient, policy optimization is difficult due to the geometry of the
objective function. Then, we qualitatively show that in some environments, a
policy with higher entropy can make the optimization landscape smoother,
thereby connecting local optima and enabling the use of larger learning rates.
This paper presents new tools for understanding the optimization landscape,
shows that policy entropy serves as a regularizer, and highlights the challenge
of designing general-purpose policy optimization algorithms.},
Year          = {2018},
Month         = {Nov},
Url           = {http://arxiv.org/abs/1811.11214v5},
File          = {1811.11214v5.pdf}
}
@article{1809.02721v3,
Author        = {Marcelo O. R. Prates and Pedro H. C. Avelar and Henrique Lemos and Luis Lamb and Moshe Vardi},
Title         = {Learning to Solve NP-Complete Problems - A Graph Neural Network for
  Decision TSP},
Eprint        = {1809.02721v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Graph Neural Networks (GNN) are a promising technique for bridging
differential programming and combinatorial domains. GNNs employ trainable
modules which can be assembled in different configurations that reflect the
relational structure of each problem instance. In this paper, we show that GNNs
can learn to solve, with very little supervision, the decision variant of the
Traveling Salesperson Problem (TSP), a highly relevant $\mathcal{NP}$-Complete
problem. Our model is trained to function as an effective message-passing
algorithm in which edges (embedded with their weights) communicate with
vertices for a number of iterations after which the model is asked to decide
whether a route with cost $<C$ exists. We show that such a network can be
trained with sets of dual examples: given the optimal tour cost $C^{*}$, we
produce one decision instance with target cost $x\%$ smaller and one with
target cost $x\%$ larger than $C^{*}$. We were able to obtain $80\%$ accuracy
training with $-2\%,+2\%$ deviations, and the same trained model can generalize
for more relaxed deviations with increasing performance. We also show that the
model is capable of generalizing for larger problem sizes. Finally, we provide
a method for predicting the optimal route cost within $2\%$ deviation from the
ground truth. In summary, our work shows that Graph Neural Networks are
powerful enough to solve $\mathcal{NP}$-Complete problems which combine
symbolic and numeric data.},
Year          = {2018},
Month         = {Sep},
Url           = {http://arxiv.org/abs/1809.02721v3},
File          = {1809.02721v3.pdf}
}
@article{1811.12359v4,
Author        = {Francesco Locatello and Stefan Bauer and Mario Lucic and Gunnar Rätsch and Sylvain Gelly and Bernhard Schölkopf and Olivier Bachem},
Title         = {Challenging Common Assumptions in the Unsupervised Learning of
  Disentangled Representations},
Eprint        = {1811.12359v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {The key idea behind the unsupervised learning of disentangled representations
is that real-world data is generated by a few explanatory factors of variation
which can be recovered by unsupervised learning algorithms. In this paper, we
provide a sober look at recent progress in the field and challenge some common
assumptions. We first theoretically show that the unsupervised learning of
disentangled representations is fundamentally impossible without inductive
biases on both the models and the data. Then, we train more than 12000 models
covering most prominent methods and evaluation metrics in a reproducible
large-scale experimental study on seven different data sets. We observe that
while the different methods successfully enforce properties ``encouraged'' by
the corresponding losses, well-disentangled models seemingly cannot be
identified without supervision. Furthermore, increased disentanglement does not
seem to lead to a decreased sample complexity of learning for downstream tasks.
Our results suggest that future work on disentanglement learning should be
explicit about the role of inductive biases and (implicit) supervision,
investigate concrete benefits of enforcing disentanglement of the learned
representations, and consider a reproducible experimental setup covering
several data sets.},
Year          = {2018},
Month         = {Nov},
Note          = {Proceedings of the 36th International Conference on Machine
  Learning (ICML 2019)},
Url           = {http://arxiv.org/abs/1811.12359v4},
File          = {1811.12359v4.pdf}
}
@article{1611.07507v1,
Author        = {Karol Gregor and Danilo Jimenez Rezende and Daan Wierstra},
Title         = {Variational Intrinsic Control},
Eprint        = {1611.07507v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {In this paper we introduce a new unsupervised reinforcement learning method
for discovering the set of intrinsic options available to an agent. This set is
learned by maximizing the number of different states an agent can reliably
reach, as measured by the mutual information between the set of options and
option termination states. To this end, we instantiate two policy gradient
based algorithms, one that creates an explicit embedding space of options and
one that represents options implicitly. The algorithms also provide an explicit
measure of empowerment in a given state that can be used by an empowerment
maximizing agent. The algorithm scales well with function approximation and we
demonstrate the applicability of the algorithm on a range of tasks.},
Year          = {2016},
Month         = {Nov},
Url           = {http://arxiv.org/abs/1611.07507v1},
File          = {1611.07507v1.pdf}
}
@article{1908.04211v4,
Author        = {Gino Brunner and Yang Liu and Damián Pascual and Oliver Richter and Massimiliano Ciaramita and Roger Wattenhofer},
Title         = {On Identifiability in Transformers},
Eprint        = {1908.04211v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this paper we delve deep in the Transformer architecture by investigating
two of its core components: self-attention and contextual embeddings. In
particular, we study the identifiability of attention weights and token
embeddings, and the aggregation of context into hidden tokens. We show that,
for sequences longer than the attention head dimension, attention weights are
not identifiable. We propose effective attention as a complementary tool for
improving explanatory interpretations based on attention. Furthermore, we show
that input tokens retain to a large degree their identity across the model. We
also find evidence suggesting that identity information is mainly encoded in
the angle of the embeddings and gradually decreases with depth. Finally, we
demonstrate strong mixing of input information in the generation of contextual
embeddings by means of a novel quantification method based on gradient
attribution. Overall, we show that self-attention distributions are not
directly interpretable and present tools to better understand and further
investigate Transformer models.},
Year          = {2019},
Month         = {Aug},
Url           = {http://arxiv.org/abs/1908.04211v4},
File          = {1908.04211v4.pdf}
}
@article{1909.10618v2,
Author        = {Ofir Nachum and Haoran Tang and Xingyu Lu and Shixiang Gu and Honglak Lee and Sergey Levine},
Title         = {Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?},
Eprint        = {1909.10618v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Hierarchical reinforcement learning has demonstrated significant success at
solving difficult reinforcement learning (RL) tasks. Previous works have
motivated the use of hierarchy by appealing to a number of intuitive benefits,
including learning over temporally extended transitions, exploring over
temporally extended periods, and training and exploring in a more semantically
meaningful action space, among others. However, in fully observed, Markovian
settings, it is not immediately clear why hierarchical RL should provide
benefits over standard "shallow" RL architectures. In this work, we isolate and
evaluate the claimed benefits of hierarchical RL on a suite of tasks
encompassing locomotion, navigation, and manipulation. Surprisingly, we find
that most of the observed benefits of hierarchy can be attributed to improved
exploration, as opposed to easier policy learning or imposed hierarchical
structures. Given this insight, we present exploration techniques inspired by
hierarchy that achieve performance competitive with hierarchical RL while at
the same time being much simpler to use and implement.},
Year          = {2019},
Month         = {Sep},
Url           = {http://arxiv.org/abs/1909.10618v2},
File          = {1909.10618v2.pdf}
}
@article{1906.01563v3,
Author        = {Sam Greydanus and Misko Dzamba and Jason Yosinski},
Title         = {Hamiltonian Neural Networks},
Eprint        = {1906.01563v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.NE},
Abstract      = {Even though neural networks enjoy widespread use, they still struggle to
learn the basic laws of physics. How might we endow them with better inductive
biases? In this paper, we draw inspiration from Hamiltonian mechanics to train
models that learn and respect exact conservation laws in an unsupervised
manner. We evaluate our models on problems where conservation of energy is
important, including the two-body problem and pixel observations of a pendulum.
Our model trains faster and generalizes better than a regular neural network.
An interesting side effect is that our model is perfectly reversible in time.},
Year          = {2019},
Month         = {Jun},
Url           = {http://arxiv.org/abs/1906.01563v3},
File          = {1906.01563v3.pdf}
}
@article{1906.02691v3,
Author        = {Diederik P. Kingma and Max Welling},
Title         = {An Introduction to Variational Autoencoders},
Eprint        = {1906.02691v3},
DOI           = {10.1561/2200000056},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Variational autoencoders provide a principled framework for learning deep
latent-variable models and corresponding inference models. In this work, we
provide an introduction to variational autoencoders and some important
extensions.},
Year          = {2019},
Month         = {Jun},
Note          = {Foundations and Trends in Machine Learning: Vol. 12 (2019): No. 4,
  pp 307-392},
Url           = {http://arxiv.org/abs/1906.02691v3},
File          = {1906.02691v3.pdf}
}
@article{1504.00702v5,
Author        = {Sergey Levine and Chelsea Finn and Trevor Darrell and Pieter Abbeel},
Title         = {End-to-End Training of Deep Visuomotor Policies},
Eprint        = {1504.00702v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Policy search methods can allow robots to learn control policies for a wide
range of tasks, but practical applications of policy search often require
hand-engineered components for perception, state estimation, and low-level
control. In this paper, we aim to answer the following question: does training
the perception and control systems jointly end-to-end provide better
performance than training each component separately? To this end, we develop a
method that can be used to learn policies that map raw image observations
directly to torques at the robot's motors. The policies are represented by deep
convolutional neural networks (CNNs) with 92,000 parameters, and are trained
using a partially observed guided policy search method, which transforms policy
search into supervised learning, with supervision provided by a simple
trajectory-centric reinforcement learning method. We evaluate our method on a
range of real-world manipulation tasks that require close coordination between
vision and control, such as screwing a cap onto a bottle, and present simulated
comparisons to a range of prior policy search methods.},
Year          = {2015},
Month         = {Apr},
Url           = {http://arxiv.org/abs/1504.00702v5},
File          = {1504.00702v5.pdf}
}
@article{1611.01224v2,
Author        = {Ziyu Wang and Victor Bapst and Nicolas Heess and Volodymyr Mnih and Remi Munos and Koray Kavukcuoglu and Nando de Freitas},
Title         = {Sample Efficient Actor-Critic with Experience Replay},
Eprint        = {1611.01224v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {This paper presents an actor-critic deep reinforcement learning agent with
experience replay that is stable, sample efficient, and performs remarkably
well on challenging environments, including the discrete 57-game Atari domain
and several continuous control problems. To achieve this, the paper introduces
several innovations, including truncated importance sampling with bias
correction, stochastic dueling network architectures, and a new trust region
policy optimization method.},
Year          = {2016},
Month         = {Nov},
Url           = {http://arxiv.org/abs/1611.01224v2},
File          = {1611.01224v2.pdf}
}
@article{1412.7009v3,
Author        = {Jan Rudy and Graham Taylor},
Title         = {Generative Class-conditional Autoencoders},
Eprint        = {1412.7009v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.NE},
Abstract      = {Recent work by Bengio et al. (2013) proposes a sampling procedure for
denoising autoencoders which involves learning the transition operator of a
Markov chain. The transition operator is typically unimodal, which limits its
capacity to model complex data. In order to perform efficient sampling from
conditional distributions, we extend this work, both theoretically and
algorithmically, to gated autoencoders (Memisevic, 2013), The proposed model is
able to generate convincing class-conditional samples when trained on both the
MNIST and TFD datasets.},
Year          = {2014},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1412.7009v3},
File          = {1412.7009v3.pdf}
}
@article{1509.02971v6,
Author        = {Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
Title         = {Continuous control with deep reinforcement learning},
Eprint        = {1509.02971v6},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We adapt the ideas underlying the success of Deep Q-Learning to the
continuous action domain. We present an actor-critic, model-free algorithm
based on the deterministic policy gradient that can operate over continuous
action spaces. Using the same learning algorithm, network architecture and
hyper-parameters, our algorithm robustly solves more than 20 simulated physics
tasks, including classic problems such as cartpole swing-up, dexterous
manipulation, legged locomotion and car driving. Our algorithm is able to find
policies whose performance is competitive with those found by a planning
algorithm with full access to the dynamics of the domain and its derivatives.
We further demonstrate that for many of the tasks the algorithm can learn
policies end-to-end: directly from raw pixel inputs.},
Year          = {2015},
Month         = {Sep},
Url           = {http://arxiv.org/abs/1509.02971v6},
File          = {1509.02971v6.pdf}
}
@article{1212.1524v2,
Author        = {Ludovic Arnold and Yann Ollivier},
Title         = {Layer-wise learning of deep generative models},
Eprint        = {1212.1524v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.NE},
Abstract      = {When using deep, multi-layered architectures to build generative models of
data, it is difficult to train all layers at once. We propose a layer-wise
training procedure admitting a performance guarantee compared to the global
optimum. It is based on an optimistic proxy of future performance, the best
latent marginal. We interpret auto-encoders in this setting as generative
models, by showing that they train a lower bound of this criterion. We test the
new learning procedure against a state of the art method (stacked RBMs), and
find it to improve performance. Both theory and experiments highlight the
importance, when training deep architectures, of using an inference model (from
data to hidden variables) richer than the generative model (from hidden
variables to data).},
Year          = {2012},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1212.1524v2},
File          = {1212.1524v2.pdf}
}
@article{2109.08522v1,
Author        = {Bryan Lim and Luca Grillotti and Lorenzo Bernasconi and Antoine Cully},
Title         = {Dynamics-Aware Quality-Diversity for Efficient Learning of Skill
  Repertoires},
Eprint        = {2109.08522v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Quality-Diversity (QD) algorithms are powerful exploration algorithms that
allow robots to discover large repertoires of diverse and high-performing
skills. However, QD algorithms are sample inefficient and require millions of
evaluations. In this paper, we propose Dynamics-Aware Quality-Diversity
(DA-QD), a framework to improve the sample efficiency of QD algorithms through
the use of dynamics models. We also show how DA-QD can then be used for
continual acquisition of new skill repertoires. To do so, we incrementally
train a deep dynamics model from experience obtained when performing skill
discovery using QD. We can then perform QD exploration in imagination with an
imagined skill repertoire. We evaluate our approach on three robotic
experiments. First, our experiments show DA-QD is 20 times more sample
efficient than existing QD approaches for skill discovery. Second, we
demonstrate learning an entirely new skill repertoire in imagination to perform
zero-shot learning. Finally, we show how DA-QD is useful and effective for
solving a long horizon navigation task and for damage adaptation in the real
world. Videos and source code are available at:
https://sites.google.com/view/da-qd.},
Year          = {2021},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2109.08522v1},
File          = {2109.08522v1.pdf}
}
@article{1703.09312v3,
Author        = {Jeffrey Mahler and Jacky Liang and Sherdil Niyaz and Michael Laskey and Richard Doan and Xinyu Liu and Juan Aparicio Ojea and Ken Goldberg},
Title         = {Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point
  Clouds and Analytic Grasp Metrics},
Eprint        = {1703.09312v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {To reduce data collection time for deep learning of robust robotic grasp
plans, we explore training from a synthetic dataset of 6.7 million point
clouds, grasps, and analytic grasp metrics generated from thousands of 3D
models from Dex-Net 1.0 in randomized poses on a table. We use the resulting
dataset, Dex-Net 2.0, to train a Grasp Quality Convolutional Neural Network
(GQ-CNN) model that rapidly predicts the probability of success of grasps from
depth images, where grasps are specified as the planar position, angle, and
depth of a gripper relative to an RGB-D sensor. Experiments with over 1,000
trials on an ABB YuMi comparing grasp planning methods on singulated objects
suggest that a GQ-CNN trained with only synthetic data from Dex-Net 2.0 can be
used to plan grasps in 0.8sec with a success rate of 93% on eight known objects
with adversarial geometry and is 3x faster than registering point clouds to a
precomputed dataset of objects and indexing grasps. The Dex-Net 2.0 grasp
planner also has the highest success rate on a dataset of 10 novel rigid
objects and achieves 99% precision (one false positive out of 69 grasps
classified as robust) on a dataset of 40 novel household objects, some of which
are articulated or deformable. Code, datasets, videos, and supplementary
material are available at http://berkeleyautomation.github.io/dex-net .},
Year          = {2017},
Month         = {Mar},
Url           = {http://arxiv.org/abs/1703.09312v3},
File          = {1703.09312v3.pdf}
}
@article{1812.06298v2,
Author        = {Tom Silver and Kelsey Allen and Josh Tenenbaum and Leslie Kaelbling},
Title         = {Residual Policy Learning},
Eprint        = {1812.06298v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {We present Residual Policy Learning (RPL): a simple method for improving
nondifferentiable policies using model-free deep reinforcement learning. RPL
thrives in complex robotic manipulation tasks where good but imperfect
controllers are available. In these tasks, reinforcement learning from scratch
remains data-inefficient or intractable, but learning a residual on top of the
initial controller can yield substantial improvements. We study RPL in six
challenging MuJoCo tasks involving partial observability, sensor noise, model
misspecification, and controller miscalibration. For initial controllers, we
consider both hand-designed policies and model-predictive controllers with
known or learned transition models. By combining learning with control
algorithms, RPL can perform long-horizon, sparse-reward tasks for which
reinforcement learning alone fails. Moreover, we find that RPL consistently and
substantially improves on the initial controllers. We argue that RPL is a
promising approach for combining the complementary strengths of deep
reinforcement learning and robotic control, pushing the boundaries of what
either can achieve independently. Video and code at
https://k-r-allen.github.io/residual-policy-learning/.},
Year          = {2018},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1812.06298v2},
File          = {1812.06298v2.pdf}
}
@article{1606.02396v1,
Author        = {Tejas D. Kulkarni and Ardavan Saeedi and Simanta Gautam and Samuel J. Gershman},
Title         = {Deep Successor Reinforcement Learning},
Eprint        = {1606.02396v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.ML},
Abstract      = {Learning robust value functions given raw observations and rewards is now
possible with model-free and model-based deep reinforcement learning
algorithms. There is a third alternative, called Successor Representations
(SR), which decomposes the value function into two components -- a reward
predictor and a successor map. The successor map represents the expected future
state occupancy from any given state and the reward predictor maps states to
scalar rewards. The value function of a state can be computed as the inner
product between the successor map and the reward weights. In this paper, we
present DSR, which generalizes SR within an end-to-end deep reinforcement
learning framework. DSR has several appealing properties including: increased
sensitivity to distal reward changes due to factorization of reward and world
dynamics, and the ability to extract bottleneck states (subgoals) given
successor maps trained under a random policy. We show the efficacy of our
approach on two diverse environments given raw pixel observations -- simple
grid-world domains (MazeBase) and the Doom game engine.},
Year          = {2016},
Month         = {Jun},
Url           = {http://arxiv.org/abs/1606.02396v1},
File          = {1606.02396v1.pdf}
}
@article{1612.07139v4,
Author        = {Lei Tai and Jingwei Zhang and Ming Liu and Joschka Boedecker and Wolfram Burgard},
Title         = {A Survey of Deep Network Solutions for Learning Control in Robotics:
  From Reinforcement to Imitation},
Eprint        = {1612.07139v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Deep learning techniques have been widely applied, achieving state-of-the-art
results in various fields of study. This survey focuses on deep learning
solutions that target learning control policies for robotics applications. We
carry out our discussions on the two main paradigms for learning control with
deep networks: deep reinforcement learning and imitation learning. For deep
reinforcement learning (DRL), we begin from traditional reinforcement learning
algorithms, showing how they are extended to the deep context and effective
mechanisms that could be added on top of the DRL algorithms. We then introduce
representative works that utilize DRL to solve navigation and manipulation
tasks in robotics. We continue our discussion on methods addressing the
challenge of the reality gap for transferring DRL policies trained in
simulation to real-world scenarios, and summarize robotics simulation platforms
for conducting DRL research. For imitation leaning, we go through its three
main categories, behavior cloning, inverse reinforcement learning and
generative adversarial imitation learning, by introducing their formulations
and their corresponding robotics applications. Finally, we discuss the open
challenges and research frontiers.},
Year          = {2016},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1612.07139v4},
File          = {1612.07139v4.pdf}
}
@article{1707.08817v2,
Author        = {Mel Vecerik and Todd Hester and Jonathan Scholz and Fumin Wang and Olivier Pietquin and Bilal Piot and Nicolas Heess and Thomas Rothörl and Thomas Lampe and Martin Riedmiller},
Title         = {Leveraging Demonstrations for Deep Reinforcement Learning on Robotics
  Problems with Sparse Rewards},
Eprint        = {1707.08817v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We propose a general and model-free approach for Reinforcement Learning (RL)
on real robotics with sparse rewards. We build upon the Deep Deterministic
Policy Gradient (DDPG) algorithm to use demonstrations. Both demonstrations and
actual interactions are used to fill a replay buffer and the sampling ratio
between demonstrations and transitions is automatically tuned via a prioritized
replay mechanism. Typically, carefully engineered shaping rewards are required
to enable the agents to efficiently explore on high dimensional control
problems such as robotics. They are also required for model-based acceleration
methods relying on local solvers such as iLQG (e.g. Guided Policy Search and
Normalized Advantage Function). The demonstrations replace the need for
carefully engineered rewards, and reduce the exploration problem encountered by
classical RL approaches in these domains. Demonstrations are collected by a
robot kinesthetically force-controlled by a human demonstrator. Results on four
simulated insertion tasks show that DDPG from demonstrations out-performs DDPG,
and does not require engineered rewards. Finally, we demonstrate the method on
a real robotics task consisting of inserting a clip (flexible object) into a
rigid object.},
Year          = {2017},
Month         = {Jul},
Url           = {http://arxiv.org/abs/1707.08817v2},
File          = {1707.08817v2.pdf}
}
@article{1802.08163v1,
Author        = {Mark Rowland and Marc G. Bellemare and Will Dabney and Rémi Munos and Yee Whye Teh},
Title         = {An Analysis of Categorical Distributional Reinforcement Learning},
Eprint        = {1802.08163v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.ML},
Abstract      = {Distributional approaches to value-based reinforcement learning model the
entire distribution of returns, rather than just their expected values, and
have recently been shown to yield state-of-the-art empirical performance. This
was demonstrated by the recently proposed C51 algorithm, based on categorical
distributional reinforcement learning (CDRL) [Bellemare et al., 2017]. However,
the theoretical properties of CDRL algorithms are not yet well understood. In
this paper, we introduce a framework to analyse CDRL algorithms, establish the
importance of the projected distributional Bellman operator in distributional
RL, draw fundamental connections between CDRL and the Cram\'er distance, and
give a proof of convergence for sample-based categorical distributional
reinforcement learning algorithms.},
Year          = {2018},
Month         = {Feb},
Url           = {http://arxiv.org/abs/1802.08163v1},
File          = {1802.08163v1.pdf}
}
@article{1707.06347v2,
Author        = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
Title         = {Proximal Policy Optimization Algorithms},
Eprint        = {1707.06347v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We propose a new family of policy gradient methods for reinforcement
learning, which alternate between sampling data through interaction with the
environment, and optimizing a "surrogate" objective function using stochastic
gradient ascent. Whereas standard policy gradient methods perform one gradient
update per data sample, we propose a novel objective function that enables
multiple epochs of minibatch updates. The new methods, which we call proximal
policy optimization (PPO), have some of the benefits of trust region policy
optimization (TRPO), but they are much simpler to implement, more general, and
have better sample complexity (empirically). Our experiments test PPO on a
collection of benchmark tasks, including simulated robotic locomotion and Atari
game playing, and we show that PPO outperforms other online policy gradient
methods, and overall strikes a favorable balance between sample complexity,
simplicity, and wall-time.},
Year          = {2017},
Month         = {Jul},
Url           = {http://arxiv.org/abs/1707.06347v2},
File          = {1707.06347v2.pdf}
}
@article{1902.04706v2,
Author        = {Devin Schwab and Tobias Springenberg and Murilo F. Martins and Thomas Lampe and Michael Neunert and Abbas Abdolmaleki and Tim Hertweck and Roland Hafner and Francesco Nori and Martin Riedmiller},
Title         = {Simultaneously Learning Vision and Feature-based Control Policies for
  Real-world Ball-in-a-Cup},
Eprint        = {1902.04706v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We present a method for fast training of vision based control policies on
real robots. The key idea behind our method is to perform multi-task
Reinforcement Learning with auxiliary tasks that differ not only in the reward
to be optimized but also in the state-space in which they operate. In
particular, we allow auxiliary task policies to utilize task features that are
available only at training-time. This allows for fast learning of auxiliary
policies, which subsequently generate good data for training the main,
vision-based control policies. This method can be seen as an extension of the
Scheduled Auxiliary Control (SAC-X) framework. We demonstrate the efficacy of
our method by using both a simulated and real-world Ball-in-a-Cup game
controlled by a robot arm. In simulation, our approach leads to significant
learning speed-ups when compared to standard SAC-X. On the real robot we show
that the task can be learned from-scratch, i.e., with no transfer from
simulation and no imitation learning. Videos of our learned policies running on
the real robot can be found at
https://sites.google.com/view/rss-2019-sawyer-bic/.},
Year          = {2019},
Month         = {Feb},
Url           = {http://arxiv.org/abs/1902.04706v2},
File          = {1902.04706v2.pdf}
}
@article{1912.11032v1,
Author        = {Richard Li and Allan Jabri and Trevor Darrell and Pulkit Agrawal},
Title         = {Towards Practical Multi-Object Manipulation using Relational
  Reinforcement Learning},
Eprint        = {1912.11032v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Learning robotic manipulation tasks using reinforcement learning with sparse
rewards is currently impractical due to the outrageous data requirements. Many
practical tasks require manipulation of multiple objects, and the complexity of
such tasks increases with the number of objects. Learning from a curriculum of
increasingly complex tasks appears to be a natural solution, but unfortunately,
does not work for many scenarios. We hypothesize that the inability of the
state-of-the-art algorithms to effectively utilize a task curriculum stems from
the absence of inductive biases for transferring knowledge from simpler to
complex tasks. We show that graph-based relational architectures overcome this
limitation and enable learning of complex tasks when provided with a simple
curriculum of tasks with increasing numbers of objects. We demonstrate the
utility of our framework on a simulated block stacking task. Starting from
scratch, our agent learns to stack six blocks into a tower. Despite using
step-wise sparse rewards, our method is orders of magnitude more data-efficient
and outperforms the existing state-of-the-art method that utilizes human
demonstrations. Furthermore, the learned policy exhibits zero-shot
generalization, successfully stacking blocks into taller towers and previously
unseen configurations such as pyramids, without any further training.},
Year          = {2019},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1912.11032v1},
File          = {1912.11032v1.pdf}
}
@article{2001.02907v1,
Author        = {Whiyoung Jung and Giseung Park and Youngchul Sung},
Title         = {Population-Guided Parallel Policy Search for Reinforcement Learning},
Eprint        = {2001.02907v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {In this paper, a new population-guided parallel learning scheme is proposed
to enhance the performance of off-policy reinforcement learning (RL). In the
proposed scheme, multiple identical learners with their own value-functions and
policies share a common experience replay buffer, and search a good policy in
collaboration with the guidance of the best policy information. The key point
is that the information of the best policy is fused in a soft manner by
constructing an augmented loss function for policy update to enlarge the
overall search region by the multiple learners. The guidance by the previous
best policy and the enlarged range enable faster and better policy search.
Monotone improvement of the expected cumulative return by the proposed scheme
is proved theoretically. Working algorithms are constructed by applying the
proposed scheme to the twin delayed deep deterministic (TD3) policy gradient
algorithm. Numerical results show that the constructed algorithm outperforms
most of the current state-of-the-art RL algorithms, and the gain is significant
in the case of sparse reward environment.},
Year          = {2020},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2001.02907v1},
File          = {2001.02907v1.pdf}
}
@article{1903.02219v1,
Author        = {Guillaume Bellegarda and Katie Byl},
Title         = {Training in Task Space to Speed Up and Guide Reinforcement Learning},
Eprint        = {1903.02219v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Recent breakthroughs in the reinforcement learning (RL) community have made
significant advances towards learning and deploying policies on real world
robotic systems. However, even with the current state-of-the-art algorithms and
computational resources, these algorithms are still plagued with high sample
complexity, and thus long training times, especially for high degree of freedom
(DOF) systems. There are also concerns arising from lack of perceived stability
or robustness guarantees from emerging policies. This paper aims at mitigating
these drawbacks by: (1) modeling a complex, high DOF system with a
representative simple one, (2) making explicit use of forward and inverse
kinematics without forcing the RL algorithm to "learn" them on its own, and (3)
learning locomotion policies in Cartesian space instead of joint space. In this
paper these methods are applied to JPL's Robosimian, but can be readily used on
any system with a base and end effector(s). These locomotion policies can be
produced in just a few minutes, trained on a single laptop. We compare the
robustness of the resulting learned policies to those of other control methods.
An accompanying video for this paper can be found at
https://youtu.be/xDxxSw5ahnc .},
Year          = {2019},
Month         = {Mar},
Url           = {http://arxiv.org/abs/1903.02219v1},
File          = {1903.02219v1.pdf}
}
@article{1811.06407v2,
Author        = {Zhaohan Daniel Guo and Mohammad Gheshlaghi Azar and Bilal Piot and Bernardo A. Pires and Rémi Munos},
Title         = {Neural Predictive Belief Representations},
Eprint        = {1811.06407v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Unsupervised representation learning has succeeded with excellent results in
many applications. It is an especially powerful tool to learn a good
representation of environments with partial or noisy observations. In partially
observable domains it is important for the representation to encode a belief
state, a sufficient statistic of the observations seen so far. In this paper,
we investigate whether it is possible to learn such a belief representation
using modern neural architectures. Specifically, we focus on one-step frame
prediction and two variants of contrastive predictive coding (CPC) as the
objective functions to learn the representations. To evaluate these learned
representations, we test how well they can predict various pieces of
information about the underlying state of the environment, e.g., position of
the agent in a 3D maze. We show that all three methods are able to learn belief
representations of the environment, they encode not only the state information,
but also its uncertainty, a crucial aspect of belief states. We also find that
for CPC multi-step predictions and action-conditioning are critical for
accurate belief representations in visually complex environments. The ability
of neural representations to capture the belief information has the potential
to spur new advances for learning and planning in partially observable domains,
where leveraging uncertainty is essential for optimal decision making.},
Year          = {2018},
Month         = {Nov},
Url           = {http://arxiv.org/abs/1811.06407v2},
File          = {1811.06407v2.pdf}
}
@article{1609.04747v2,
Author        = {Sebastian Ruder},
Title         = {An overview of gradient descent optimization algorithms},
Eprint        = {1609.04747v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Gradient descent optimization algorithms, while increasingly popular, are
often used as black-box optimizers, as practical explanations of their
strengths and weaknesses are hard to come by. This article aims to provide the
reader with intuitions with regard to the behaviour of different algorithms
that will allow her to put them to use. In the course of this overview, we look
at different variants of gradient descent, summarize challenges, introduce the
most common optimization algorithms, review architectures in a parallel and
distributed setting, and investigate additional strategies for optimizing
gradient descent.},
Year          = {2016},
Month         = {Sep},
Url           = {http://arxiv.org/abs/1609.04747v2},
File          = {1609.04747v2.pdf}
}
@article{1301.3584v7,
Author        = {Razvan Pascanu and Yoshua Bengio},
Title         = {Revisiting Natural Gradient for Deep Networks},
Eprint        = {1301.3584v7},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We evaluate natural gradient, an algorithm originally proposed in Amari
(1997), for learning deep models. The contributions of this paper are as
follows. We show the connection between natural gradient and three other
recently proposed methods for training deep models: Hessian-Free (Martens,
2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA (Le Roux et
al., 2008). We describe how one can use unlabeled data to improve the
generalization error obtained by natural gradient and empirically evaluate the
robustness of the algorithm to the ordering of the training set compared to
stochastic gradient descent. Finally we extend natural gradient to incorporate
second order information alongside the manifold information and provide a
benchmark of the new algorithm using a truncated Newton approach for inverting
the metric matrix instead of using a diagonal approximation of it.},
Year          = {2013},
Month         = {Jan},
Url           = {http://arxiv.org/abs/1301.3584v7},
File          = {1301.3584v7.pdf}
}
@article{2107.12808v2,
Author        = { Open Ended Learning Team and Adam Stooke and Anuj Mahajan and Catarina Barros and Charlie Deck and Jakob Bauer and Jakub Sygnowski and Maja Trebacz and Max Jaderberg and Michael Mathieu and Nat McAleese and Nathalie Bradley-Schmieg and Nathaniel Wong and Nicolas Porcel and Roberta Raileanu and Steph Hughes-Fitt and Valentin Dalibard and Wojciech Marian Czarnecki},
Title         = {Open-Ended Learning Leads to Generally Capable Agents},
Eprint        = {2107.12808v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {In this work we create agents that can perform well beyond a single,
individual task, that exhibit much wider generalisation of behaviour to a
massive, rich space of challenges. We define a universe of tasks within an
environment domain and demonstrate the ability to train agents that are
generally capable across this vast space and beyond. The environment is
natively multi-agent, spanning the continuum of competitive, cooperative, and
independent games, which are situated within procedurally generated physical 3D
worlds. The resulting space is exceptionally diverse in terms of the challenges
posed to agents, and as such, even measuring the learning progress of an agent
is an open research problem. We propose an iterative notion of improvement
between successive generations of agents, rather than seeking to maximise a
singular objective, allowing us to quantify progress despite tasks being
incomparable in terms of achievable rewards. We show that through constructing
an open-ended learning process, which dynamically changes the training task
distributions and training objectives such that the agent never stops learning,
we achieve consistent learning of new behaviours. The resulting agent is able
to score reward in every one of our humanly solvable evaluation levels, with
behaviour generalising to many held-out points in the universe of tasks.
Examples of this zero-shot generalisation include good performance on Hide and
Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks
we characterise the behaviour of our agent, and find interesting emergent
heuristic behaviours such as trial-and-error experimentation, simple tool use,
option switching, and cooperation. Finally, we demonstrate that the general
capabilities of this agent could unlock larger scale transfer of behaviour
through cheap finetuning.},
Year          = {2021},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2107.12808v2},
File          = {2107.12808v2.pdf}
}
@article{1907.08225v4,
Author        = {Kristian Hartikainen and Xinyang Geng and Tuomas Haarnoja and Sergey Levine},
Title         = {Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill
  Discovery},
Eprint        = {1907.08225v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Reinforcement learning requires manual specification of a reward function to
learn a task. While in principle this reward function only needs to specify the
task goal, in practice reinforcement learning can be very time-consuming or
even infeasible unless the reward function is shaped so as to provide a smooth
gradient towards a successful outcome. This shaping is difficult to specify by
hand, particularly when the task is learned from raw observations, such as
images. In this paper, we study how we can automatically learn dynamical
distances: a measure of the expected number of time steps to reach a given goal
state from any other state. These dynamical distances can be used to provide
well-shaped reward functions for reaching new goals, making it possible to
learn complex tasks efficiently. We show that dynamical distances can be used
in a semi-supervised regime, where unsupervised interaction with the
environment is used to learn the dynamical distances, while a small amount of
preference supervision is used to determine the task goal, without any manually
engineered reward function or goal examples. We evaluate our method both on a
real-world robot and in simulation. We show that our method can learn to turn a
valve with a real-world 9-DoF hand, using raw image observations and just ten
preference labels, without any other supervision. Videos of the learned skills
can be found on the project website:
https://sites.google.com/view/dynamical-distance-learning.},
Year          = {2019},
Month         = {Jul},
Url           = {http://arxiv.org/abs/1907.08225v4},
File          = {1907.08225v4.pdf}
}
@article{1903.01973v2,
Author        = {Corey Lynch and Mohi Khansari and Ted Xiao and Vikash Kumar and Jonathan Tompson and Sergey Levine and Pierre Sermanet},
Title         = {Learning Latent Plans from Play},
Eprint        = {1903.01973v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Acquiring a diverse repertoire of general-purpose skills remains an open
challenge for robotics. In this work, we propose self-supervising control on
top of human teleoperated play data as a way to scale up skill learning. Play
has two properties that make it attractive compared to conventional task
demonstrations. Play is cheap, as it can be collected in large quantities
quickly without task segmenting, labeling, or resetting to an initial state.
Play is naturally rich, covering ~4x more interaction space than task
demonstrations for the same amount of collection time. To learn control from
play, we introduce Play-LMP, a self-supervised method that learns to organize
play behaviors in a latent space, then reuse them at test time to achieve
specific goals. Combining self-supervised control with a diverse play dataset
shifts the focus of skill learning from a narrow and discrete set of tasks to
the full continuum of behaviors available in an environment. We find that this
combination generalizes well empirically---after self-supervising on unlabeled
play, our method substantially outperforms individual expert-trained policies
on 18 difficult user-specified visual manipulation tasks in a simulated robotic
tabletop environment. We additionally find that play-supervised models, unlike
their expert-trained counterparts, are more robust to perturbations and exhibit
retrying-till-success behaviors. Finally, we find that our agent organizes its
latent plan space around functional tasks, despite never being trained with
task labels. Videos, code and data are available at
learning-from-play.github.io},
Year          = {2019},
Month         = {Mar},
Url           = {http://arxiv.org/abs/1903.01973v2},
File          = {1903.01973v2.pdf}
}
@article{1802.06070v6,
Author        = {Benjamin Eysenbach and Abhishek Gupta and Julian Ibarz and Sergey Levine},
Title         = {Diversity is All You Need: Learning Skills without a Reward Function},
Eprint        = {1802.06070v6},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Intelligent creatures can explore their environments and learn useful skills
without supervision. In this paper, we propose DIAYN ('Diversity is All You
Need'), a method for learning useful skills without a reward function. Our
proposed method learns skills by maximizing an information theoretic objective
using a maximum entropy policy. On a variety of simulated robotic tasks, we
show that this simple objective results in the unsupervised emergence of
diverse skills, such as walking and jumping. In a number of reinforcement
learning benchmark environments, our method is able to learn a skill that
solves the benchmark task despite never receiving the true task reward. We show
how pretrained skills can provide a good parameter initialization for
downstream tasks, and can be composed hierarchically to solve complex, sparse
reward tasks. Our results suggest that unsupervised discovery of skills can
serve as an effective pretraining mechanism for overcoming challenges of
exploration and data efficiency in reinforcement learning.},
Year          = {2018},
Month         = {Feb},
Url           = {http://arxiv.org/abs/1802.06070v6},
File          = {1802.06070v6.pdf}
}
@article{1810.12281v1,
Author        = {Guodong Zhang and Chaoqi Wang and Bowen Xu and Roger Grosse},
Title         = {Three Mechanisms of Weight Decay Regularization},
Eprint        = {1810.12281v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Weight decay is one of the standard tricks in the neural network toolbox, but
the reasons for its regularization effect are poorly understood, and recent
results have cast doubt on the traditional interpretation in terms of $L_2$
regularization. Literal weight decay has been shown to outperform $L_2$
regularization for optimizers for which they differ. We empirically investigate
weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a
variety of network architectures. We identify three distinct mechanisms by
which weight decay exerts a regularization effect, depending on the particular
optimization algorithm and architecture: (1) increasing the effective learning
rate, (2) approximately regularizing the input-output Jacobian norm, and (3)
reducing the effective damping coefficient for second-order optimization. Our
results provide insight into how to improve the regularization of neural
networks.},
Year          = {2018},
Month         = {Oct},
Url           = {http://arxiv.org/abs/1810.12281v1},
File          = {1810.12281v1.pdf}
}
@article{2010.14274v1,
Author        = {Dhruva Tirumala and Alexandre Galashov and Hyeonwoo Noh and Leonard Hasenclever and Razvan Pascanu and Jonathan Schwarz and Guillaume Desjardins and Wojciech Marian Czarnecki and Arun Ahuja and Yee Whye Teh and Nicolas Heess},
Title         = {Behavior Priors for Efficient Reinforcement Learning},
Eprint        = {2010.14274v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {As we deploy reinforcement learning agents to solve increasingly challenging
problems, methods that allow us to inject prior knowledge about the structure
of the world and effective solution strategies becomes increasingly important.
In this work we consider how information and architectural constraints can be
combined with ideas from the probabilistic modeling literature to learn
behavior priors that capture the common movement and interaction patterns that
are shared across a set of related tasks or contexts. For example the day-to
day behavior of humans comprises distinctive locomotion and manipulation
patterns that recur across many different situations and goals. We discuss how
such behavior patterns can be captured using probabilistic trajectory models
and how these can be integrated effectively into reinforcement learning
schemes, e.g.\ to facilitate multi-task and transfer learning. We then extend
these ideas to latent variable models and consider a formulation to learn
hierarchical priors that capture different aspects of the behavior in reusable
modules. We discuss how such latent variable formulations connect to related
work on hierarchical reinforcement learning (HRL) and mutual information and
curiosity based objectives, thereby offering an alternative perspective on
existing ideas. We demonstrate the effectiveness of our framework by applying
it to a range of simulated continuous control domains.},
Year          = {2020},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2010.14274v1},
File          = {2010.14274v1.pdf}
}
@article{1910.01215v4,
Author        = {Xingyou Song and Wenbo Gao and Yuxiang Yang and Krzysztof Choromanski and Aldo Pacchiano and Yunhao Tang},
Title         = {ES-MAML: Simple Hessian-Free Meta Learning},
Eprint        = {1910.01215v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We introduce ES-MAML, a new framework for solving the model agnostic meta
learning (MAML) problem based on Evolution Strategies (ES). Existing algorithms
for MAML are based on policy gradients, and incur significant difficulties when
attempting to estimate second derivatives using backpropagation on stochastic
policies. We show how ES can be applied to MAML to obtain an algorithm which
avoids the problem of estimating second derivatives, and is also conceptually
simple and easy to implement. Moreover, ES-MAML can handle new types of
nonsmooth adaptation operators, and other techniques for improving performance
and estimation of ES methods become applicable. We show empirically that
ES-MAML is competitive with existing methods and often yields better adaptation
with fewer queries.},
Year          = {2019},
Month         = {Sep},
Url           = {http://arxiv.org/abs/1910.01215v4},
File          = {1910.01215v4.pdf}
}
@article{1707.05300v3,
Author        = {Carlos Florensa and David Held and Markus Wulfmeier and Michael Zhang and Pieter Abbeel},
Title         = {Reverse Curriculum Generation for Reinforcement Learning},
Eprint        = {1707.05300v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Many relevant tasks require an agent to reach a certain state, or to
manipulate objects into a desired configuration. For example, we might want a
robot to align and assemble a gear onto an axle or insert and turn a key in a
lock. These goal-oriented tasks present a considerable challenge for
reinforcement learning, since their natural reward function is sparse and
prohibitive amounts of exploration are required to reach the goal and receive
some learning signal. Past approaches tackle these problems by exploiting
expert demonstrations or by manually designing a task-specific reward shaping
function to guide the learning agent. Instead, we propose a method to learn
these tasks without requiring any prior knowledge other than obtaining a single
state in which the task is achieved. The robot is trained in reverse, gradually
learning to reach the goal from a set of start states increasingly far from the
goal. Our method automatically generates a curriculum of start states that
adapts to the agent's performance, leading to efficient training on
goal-oriented tasks. We demonstrate our approach on difficult simulated
navigation and fine-grained manipulation problems, not solvable by
state-of-the-art reinforcement learning methods.},
Year          = {2017},
Month         = {Jul},
Url           = {http://arxiv.org/abs/1707.05300v3},
File          = {1707.05300v3.pdf}
}
@article{1602.07868v3,
Author        = {Tim Salimans and Diederik P. Kingma},
Title         = {Weight Normalization: A Simple Reparameterization to Accelerate Training
  of Deep Neural Networks},
Eprint        = {1602.07868v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We present weight normalization: a reparameterization of the weight vectors
in a neural network that decouples the length of those weight vectors from
their direction. By reparameterizing the weights in this way we improve the
conditioning of the optimization problem and we speed up convergence of
stochastic gradient descent. Our reparameterization is inspired by batch
normalization but does not introduce any dependencies between the examples in a
minibatch. This means that our method can also be applied successfully to
recurrent models such as LSTMs and to noise-sensitive applications such as deep
reinforcement learning or generative models, for which batch normalization is
less well suited. Although our method is much simpler, it still provides much
of the speed-up of full batch normalization. In addition, the computational
overhead of our method is lower, permitting more optimization steps to be taken
in the same amount of time. We demonstrate the usefulness of our method on
applications in supervised image recognition, generative modelling, and deep
reinforcement learning.},
Year          = {2016},
Month         = {Feb},
Url           = {http://arxiv.org/abs/1602.07868v3},
File          = {1602.07868v3.pdf}
}
@article{1812.05905v2,
Author        = {Tuomas Haarnoja and Aurick Zhou and Kristian Hartikainen and George Tucker and Sehoon Ha and Jie Tan and Vikash Kumar and Henry Zhu and Abhishek Gupta and Pieter Abbeel and Sergey Levine},
Title         = {Soft Actor-Critic Algorithms and Applications},
Eprint        = {1812.05905v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Model-free deep reinforcement learning (RL) algorithms have been successfully
applied to a range of challenging sequential decision making and control tasks.
However, these methods typically suffer from two major challenges: high sample
complexity and brittleness to hyperparameters. Both of these challenges limit
the applicability of such methods to real-world domains. In this paper, we
describe Soft Actor-Critic (SAC), our recently introduced off-policy
actor-critic algorithm based on the maximum entropy RL framework. In this
framework, the actor aims to simultaneously maximize expected return and
entropy. That is, to succeed at the task while acting as randomly as possible.
We extend SAC to incorporate a number of modifications that accelerate training
and improve stability with respect to the hyperparameters, including a
constrained formulation that automatically tunes the temperature
hyperparameter. We systematically evaluate SAC on a range of benchmark tasks,
as well as real-world challenging tasks such as locomotion for a quadrupedal
robot and robotic manipulation with a dexterous hand. With these improvements,
SAC achieves state-of-the-art performance, outperforming prior on-policy and
off-policy methods in sample-efficiency and asymptotic performance.
Furthermore, we demonstrate that, in contrast to other off-policy algorithms,
our approach is very stable, achieving similar performance across different
random seeds. These results suggest that SAC is a promising candidate for
learning in real-world robotics tasks.},
Year          = {2018},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1812.05905v2},
File          = {1812.05905v2.pdf}
}
@article{1811.02553v4,
Author        = {Andrew Ilyas and Logan Engstrom and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},
Title         = {A Closer Look at Deep Policy Gradients},
Eprint        = {1811.02553v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We study how the behavior of deep policy gradient algorithms reflects the
conceptual framework motivating their development. To this end, we propose a
fine-grained analysis of state-of-the-art methods based on key elements of this
framework: gradient estimation, value prediction, and optimization landscapes.
Our results show that the behavior of deep policy gradient algorithms often
deviates from what their motivating framework would predict: the surrogate
objective does not match the true reward landscape, learned value estimators
fail to fit the true value function, and gradient estimates poorly correlate
with the "true" gradient. The mismatch between predicted and empirical behavior
we uncover highlights our poor understanding of current methods, and indicates
the need to move beyond current benchmark-centric evaluation methods.},
Year          = {2018},
Month         = {Nov},
Url           = {http://arxiv.org/abs/1811.02553v4},
File          = {1811.02553v4.pdf}
}
@article{1812.02648v1,
Author        = {Hado van Hasselt and Yotam Doron and Florian Strub and Matteo Hessel and Nicolas Sonnerat and Joseph Modayil},
Title         = {Deep Reinforcement Learning and the Deadly Triad},
Eprint        = {1812.02648v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We know from reinforcement learning theory that temporal difference learning
can fail in certain cases. Sutton and Barto (2018) identify a deadly triad of
function approximation, bootstrapping, and off-policy learning. When these
three properties are combined, learning can diverge with the value estimates
becoming unbounded. However, several algorithms successfully combine these
three properties, which indicates that there is at least a partial gap in our
understanding. In this work, we investigate the impact of the deadly triad in
practice, in the context of a family of popular deep reinforcement learning
models - deep Q-networks trained with experience replay - analysing how the
components of this system play a role in the emergence of the deadly triad, and
in the agent's performance},
Year          = {2018},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1812.02648v1},
File          = {1812.02648v1.pdf}
}
@article{1811.01848v3,
Author        = {Kendall Lowrey and Aravind Rajeswaran and Sham Kakade and Emanuel Todorov and Igor Mordatch},
Title         = {Plan Online, Learn Offline: Efficient Learning and Exploration via
  Model-Based Control},
Eprint        = {1811.01848v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We propose a plan online and learn offline (POLO) framework for the setting
where an agent, with an internal model, needs to continually act and learn in
the world. Our work builds on the synergistic relationship between local
model-based control, global value function learning, and exploration. We study
how local trajectory optimization can cope with approximation errors in the
value function, and can stabilize and accelerate value function learning.
Conversely, we also study how approximate value functions can help reduce the
planning horizon and allow for better policies beyond local solutions. Finally,
we also demonstrate how trajectory optimization can be used to perform
temporally coordinated exploration in conjunction with estimating uncertainty
in value function approximation. This exploration is critical for fast and
stable learning of the value function. Combining these components enable
solutions to complex simulated control tasks, like humanoid locomotion and
dexterous in-hand manipulation, in the equivalent of a few minutes of
experience in the real world.},
Year          = {2018},
Month         = {Nov},
Url           = {http://arxiv.org/abs/1811.01848v3},
File          = {1811.01848v3.pdf}
}
@article{1911.01417v1,
Author        = {Alexander Trott and Stephan Zheng and Caiming Xiong and Richard Socher},
Title         = {Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing
  Shaped Rewards},
Eprint        = {1911.01417v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {While using shaped rewards can be beneficial when solving sparse reward
tasks, their successful application often requires careful engineering and is
problem specific. For instance, in tasks where the agent must achieve some goal
state, simple distance-to-goal reward shaping often fails, as it renders
learning vulnerable to local optima. We introduce a simple and effective
model-free method to learn from shaped distance-to-goal rewards on tasks where
success depends on reaching a goal state. Our method introduces an auxiliary
distance-based reward based on pairs of rollouts to encourage diverse
exploration. This approach effectively prevents learning dynamics from
stabilizing around local optima induced by the naive distance-to-goal reward
shaping and enables policies to efficiently solve sparse reward tasks. Our
augmented objective does not require any additional reward engineering or
domain expertise to implement and converges to the original sparse objective as
the agent learns to solve the task. We demonstrate that our method successfully
solves a variety of hard-exploration tasks (including maze navigation and 3D
construction in a Minecraft environment), where naive distance-based reward
shaping otherwise fails, and intrinsic curiosity and reward relabeling
strategies exhibit poor performance.},
Year          = {2019},
Month         = {Nov},
Url           = {http://arxiv.org/abs/1911.01417v1},
File          = {1911.01417v1.pdf}
}
@article{1607.05077v1,
Author        = {Ionel-Alexandru Hosu and Traian Rebedea},
Title         = {Playing Atari Games with Deep Reinforcement Learning and Human
  Checkpoint Replay},
Eprint        = {1607.05077v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper introduces a novel method for learning how to play the most
difficult Atari 2600 games from the Arcade Learning Environment using deep
reinforcement learning. The proposed method, human checkpoint replay, consists
in using checkpoints sampled from human gameplay as starting points for the
learning process. This is meant to compensate for the difficulties of current
exploration strategies, such as epsilon-greedy, to find successful control
policies in games with sparse rewards. Like other deep reinforcement learning
architectures, our model uses a convolutional neural network that receives only
raw pixel inputs to estimate the state value function. We tested our method on
Montezuma's Revenge and Private Eye, two of the most challenging games from the
Atari platform. The results we obtained show a substantial improvement compared
to previous learning approaches, as well as over a random player. We also
propose a method for training deep reinforcement learning agents using human
gameplay experience, which we call human experience replay.},
Year          = {2016},
Month         = {Jul},
Url           = {http://arxiv.org/abs/1607.05077v1},
File          = {1607.05077v1.pdf}
}
@article{1712.08449v1,
Author        = {Yann Ollivier},
Title         = {True Asymptotic Natural Gradient Optimization},
Eprint        = {1712.08449v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.ML},
Abstract      = {We introduce a simple algorithm, True Asymptotic Natural Gradient
Optimization (TANGO), that converges to a true natural gradient descent in the
limit of small learning rates, without explicit Fisher matrix estimation.
  For quadratic models the algorithm is also an instance of averaged stochastic
gradient, where the parameter is a moving average of a "fast", constant-rate
gradient descent. TANGO appears as a particular de-linearization of averaged
SGD, and is sometimes quite different on non-quadratic models. This further
connects averaged SGD and natural gradient, both of which are arguably optimal
asymptotically.
  In large dimension, small learning rates will be required to approximate the
natural gradient well. Still, this shows it is possible to get arbitrarily
close to exact natural gradient descent with a lightweight algorithm.},
Year          = {2017},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1712.08449v1},
File          = {1712.08449v1.pdf}
}
@article{1703.02949v1,
Author        = {Abhishek Gupta and Coline Devin and YuXuan Liu and Pieter Abbeel and Sergey Levine},
Title         = {Learning Invariant Feature Spaces to Transfer Skills with Reinforcement
  Learning},
Eprint        = {1703.02949v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {People can learn a wide range of tasks from their own experience, but can
also learn from observing other creatures. This can accelerate acquisition of
new skills even when the observed agent differs substantially from the learning
agent in terms of morphology. In this paper, we examine how reinforcement
learning algorithms can transfer knowledge between morphologically different
agents (e.g., different robots). We introduce a problem formulation where two
agents are tasked with learning multiple skills by sharing information. Our
method uses the skills that were learned by both agents to train invariant
feature spaces that can then be used to transfer other skills from one agent to
another. The process of learning these invariant feature spaces can be viewed
as a kind of "analogy making", or implicit learning of partial correspondences
between two distinct domains. We evaluate our transfer learning algorithm in
two simulated robotic manipulation skills, and illustrate that we can transfer
knowledge between simulated robotic arms with different numbers of links, as
well as simulated arms with different actuation mechanisms, where one robot is
torque-driven while the other is tendon-driven.},
Year          = {2017},
Month         = {Mar},
Url           = {http://arxiv.org/abs/1703.02949v1},
File          = {1703.02949v1.pdf}
}
@article{1707.02286v2,
Author        = {Nicolas Heess and Dhruva TB and Srinivasan Sriram and Jay Lemmon and Josh Merel and Greg Wayne and Yuval Tassa and Tom Erez and Ziyu Wang and S. M. Ali Eslami and Martin Riedmiller and David Silver},
Title         = {Emergence of Locomotion Behaviours in Rich Environments},
Eprint        = {1707.02286v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The reinforcement learning paradigm allows, in principle, for complex
behaviours to be learned directly from simple reward signals. In practice,
however, it is common to carefully hand-design the reward function to encourage
a particular solution, or to derive it from demonstration data. In this paper
explore how a rich environment can help to promote the learning of complex
behavior. Specifically, we train agents in diverse environmental contexts, and
find that this encourages the emergence of robust behaviours that perform well
across a suite of tasks. We demonstrate this principle for locomotion --
behaviours that are known for their sensitivity to the choice of reward. We
train several simulated bodies on a diverse set of challenging terrains and
obstacles, using a simple reward function based on forward progress. Using a
novel scalable variant of policy gradient reinforcement learning, our agents
learn to run, jump, crouch and turn as required by the environment without
explicit reward-based guidance. A visual depiction of highlights of the learned
behavior can be viewed following https://youtu.be/hx_bgoTF7bs .},
Year          = {2017},
Month         = {Jul},
Url           = {http://arxiv.org/abs/1707.02286v2},
File          = {1707.02286v2.pdf}
}
@article{1910.00177v3,
Author        = {Xue Bin Peng and Aviral Kumar and Grace Zhang and Sergey Levine},
Title         = {Advantage-Weighted Regression: Simple and Scalable Off-Policy
  Reinforcement Learning},
Eprint        = {1910.00177v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {In this paper, we aim to develop a simple and scalable reinforcement learning
algorithm that uses standard supervised learning methods as subroutines. Our
goal is an algorithm that utilizes only simple and convergent maximum
likelihood loss functions, while also being able to leverage off-policy data.
Our proposed approach, which we refer to as advantage-weighted regression
(AWR), consists of two standard supervised learning steps: one to regress onto
target values for a value function, and another to regress onto weighted target
actions for the policy. The method is simple and general, can accommodate
continuous and discrete actions, and can be implemented in just a few lines of
code on top of standard supervised learning methods. We provide a theoretical
motivation for AWR and analyze its properties when incorporating off-policy
data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym
benchmark tasks, and show that it achieves competitive performance compared to
a number of well-established state-of-the-art RL algorithms. AWR is also able
to acquire more effective policies than most off-policy algorithms when
learning from purely static datasets with no additional environmental
interactions. Furthermore, we demonstrate our algorithm on challenging
continuous control tasks with highly complex simulated characters.},
Year          = {2019},
Month         = {Oct},
Url           = {http://arxiv.org/abs/1910.00177v3},
File          = {1910.00177v3.pdf}
}
@article{1901.10995v4,
Author        = {Adrien Ecoffet and Joost Huizinga and Joel Lehman and Kenneth O. Stanley and Jeff Clune},
Title         = {Go-Explore: a New Approach for Hard-Exploration Problems},
Eprint        = {1901.10995v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {A grand challenge in reinforcement learning is intelligent exploration,
especially when rewards are sparse or deceptive. Two Atari games serve as
benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall.
On both games, current RL algorithms perform poorly, even those with intrinsic
motivation, which is the dominant method to improve performance on
hard-exploration domains. To address this shortfall, we introduce a new
algorithm called Go-Explore. It exploits the following principles: (1) remember
previously visited states, (2) first return to a promising state (without
exploration), then explore from it, and (3) solve simulated environments
through any available means (including by introducing determinism), then
robustify via imitation learning. The combined effect of these principles is a
dramatic performance improvement on hard-exploration problems. On Montezuma's
Revenge, Go-Explore scores a mean of over 43k points, almost 4 times the
previous state of the art. Go-Explore can also harness human-provided domain
knowledge and, when augmented with it, scores a mean of over 650k points on
Montezuma's Revenge. Its max performance of nearly 18 million surpasses the
human world record, meeting even the strictest definition of "superhuman"
performance. On Pitfall, Go-Explore with domain knowledge is the first
algorithm to score above zero. Its mean score of almost 60k points exceeds
expert human performance. Because Go-Explore produces high-performing
demonstrations automatically and cheaply, it also outperforms imitation
learning work where humans provide solution demonstrations. Go-Explore opens up
many new research directions into improving it and weaving its insights into
current RL algorithms. It may also enable progress on previously unsolvable
hard-exploration problems in many domains, especially those that harness a
simulator during training (e.g. robotics).},
Year          = {2019},
Month         = {Jan},
Url           = {http://arxiv.org/abs/1901.10995v4},
File          = {1901.10995v4.pdf}
}
@article{2004.12524v1,
Author        = {Benjamin Shickel and Parisa Rashidi},
Title         = {Sequential Interpretability: Methods, Applications, and Future Direction
  for Understanding Deep Learning Models in the Context of Sequential Data},
Eprint        = {2004.12524v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Deep learning continues to revolutionize an ever-growing number of critical
application areas including healthcare, transportation, finance, and basic
sciences. Despite their increased predictive power, model transparency and
human explainability remain a significant challenge due to the "black box"
nature of modern deep learning models. In many cases the desired balance
between interpretability and performance is predominately task specific.
Human-centric domains such as healthcare necessitate a renewed focus on
understanding how and why these frameworks are arriving at critical and
potentially life-or-death decisions. Given the quantity of research and
empirical successes of deep learning for computer vision, most of the existing
interpretability research has focused on image processing techniques.
Comparatively, less attention has been paid to interpreting deep learning
frameworks using sequential data. Given recent deep learning advancements in
highly sequential domains such as natural language processing and physiological
signal processing, the need for deep sequential explanations is at an all-time
high. In this paper, we review current techniques for interpreting deep
learning techniques involving sequential data, identify similarities to
non-sequential methods, and discuss current limitations and future avenues of
sequential interpretability research.},
Year          = {2020},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2004.12524v1},
File          = {2004.12524v1.pdf}
}
@article{1911.08265v2,
Author        = {Julian Schrittwieser and Ioannis Antonoglou and Thomas Hubert and Karen Simonyan and Laurent Sifre and Simon Schmitt and Arthur Guez and Edward Lockhart and Demis Hassabis and Thore Graepel and Timothy Lillicrap and David Silver},
Title         = {Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
Eprint        = {1911.08265v2},
DOI           = {10.1038/s41586-020-03051-4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Constructing agents with planning capabilities has long been one of the main
challenges in the pursuit of artificial intelligence. Tree-based planning
methods have enjoyed huge success in challenging domains, such as chess and Go,
where a perfect simulator is available. However, in real-world problems the
dynamics governing the environment are often complex and unknown. In this work
we present the MuZero algorithm which, by combining a tree-based search with a
learned model, achieves superhuman performance in a range of challenging and
visually complex domains, without any knowledge of their underlying dynamics.
MuZero learns a model that, when applied iteratively, predicts the quantities
most directly relevant to planning: the reward, the action-selection policy,
and the value function. When evaluated on 57 different Atari games - the
canonical video game environment for testing AI techniques, in which
model-based planning approaches have historically struggled - our new algorithm
achieved a new state of the art. When evaluated on Go, chess and shogi, without
any knowledge of the game rules, MuZero matched the superhuman performance of
the AlphaZero algorithm that was supplied with the game rules.},
Year          = {2019},
Month         = {Nov},
Url           = {http://arxiv.org/abs/1911.08265v2},
File          = {1911.08265v2.pdf}
}
@article{1712.00948v5,
Author        = {Andrew Levy and George Konidaris and Robert Platt and Kate Saenko},
Title         = {Learning Multi-Level Hierarchies with Hindsight},
Eprint        = {1712.00948v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Hierarchical agents have the potential to solve sequential decision making
tasks with greater sample efficiency than their non-hierarchical counterparts
because hierarchical agents can break down tasks into sets of subtasks that
only require short sequences of decisions. In order to realize this potential
of faster learning, hierarchical agents need to be able to learn their multiple
levels of policies in parallel so these simpler subproblems can be solved
simultaneously. Yet, learning multiple levels of policies in parallel is hard
because it is inherently unstable: changes in a policy at one level of the
hierarchy may cause changes in the transition and reward functions at higher
levels in the hierarchy, making it difficult to jointly learn multiple levels
of policies. In this paper, we introduce a new Hierarchical Reinforcement
Learning (HRL) framework, Hierarchical Actor-Critic (HAC), that can overcome
the instability issues that arise when agents try to jointly learn multiple
levels of policies. The main idea behind HAC is to train each level of the
hierarchy independently of the lower levels by training each level as if the
lower level policies are already optimal. We demonstrate experimentally in both
grid world and simulated robotics domains that our approach can significantly
accelerate learning relative to other non-hierarchical and hierarchical
methods. Indeed, our framework is the first to successfully learn 3-level
hierarchies in parallel in tasks with continuous state and action spaces.},
Year          = {2017},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1712.00948v5},
File          = {1712.00948v5.pdf}
}
@article{1812.02256v1,
Author        = {Abbas Abdolmaleki and Jost Tobias Springenberg and Jonas Degrave and Steven Bohez and Yuval Tassa and Dan Belov and Nicolas Heess and Martin Riedmiller},
Title         = {Relative Entropy Regularized Policy Iteration},
Eprint        = {1812.02256v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We present an off-policy actor-critic algorithm for Reinforcement Learning
(RL) that combines ideas from gradient-free optimization via stochastic search
with learned action-value function. The result is a simple procedure consisting
of three steps: i) policy evaluation by estimating a parametric action-value
function; ii) policy improvement via the estimation of a local non-parametric
policy; and iii) generalization by fitting a parametric policy. Each step can
be implemented in different ways, giving rise to several algorithm variants.
Our algorithm draws on connections to existing literature on black-box
optimization and 'RL as an inference' and it can be seen either as an extension
of the Maximum a Posteriori Policy Optimisation algorithm (MPO) [Abdolmaleki et
al., 2018a], or as an extension of Trust Region Covariance Matrix Adaptation
Evolutionary Strategy (CMA-ES) [Abdolmaleki et al., 2017b; Hansen et al., 1997]
to a policy iteration scheme. Our comparison on 31 continuous control tasks
from parkour suite [Heess et al., 2017], DeepMind control suite [Tassa et al.,
2018] and OpenAI Gym [Brockman et al., 2016] with diverse properties, limited
amount of compute and a single set of hyperparameters, demonstrate the
effectiveness of our method and the state of art results. Videos, summarizing
results, can be found at goo.gl/HtvJKR .},
Year          = {2018},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1812.02256v1},
File          = {1812.02256v1.pdf}
}
@article{1606.03657v1,
Author        = {Xi Chen and Yan Duan and Rein Houthooft and John Schulman and Ilya Sutskever and Pieter Abbeel},
Title         = {InfoGAN: Interpretable Representation Learning by Information Maximizing
  Generative Adversarial Nets},
Eprint        = {1606.03657v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {This paper describes InfoGAN, an information-theoretic extension to the
Generative Adversarial Network that is able to learn disentangled
representations in a completely unsupervised manner. InfoGAN is a generative
adversarial network that also maximizes the mutual information between a small
subset of the latent variables and the observation. We derive a lower bound to
the mutual information objective that can be optimized efficiently, and show
that our training procedure can be interpreted as a variation of the Wake-Sleep
algorithm. Specifically, InfoGAN successfully disentangles writing styles from
digit shapes on the MNIST dataset, pose from lighting of 3D rendered images,
and background digits from the central digit on the SVHN dataset. It also
discovers visual concepts that include hair styles, presence/absence of
eyeglasses, and emotions on the CelebA face dataset. Experiments show that
InfoGAN learns interpretable representations that are competitive with
representations learned by existing fully supervised methods.},
Year          = {2016},
Month         = {Jun},
Url           = {http://arxiv.org/abs/1606.03657v1},
File          = {1606.03657v1.pdf}
}
@article{1901.03909v1,
Author        = {Jascha Sohl-Dickstein and Kenji Kawaguchi},
Title         = {Eliminating all bad Local Minima from Loss Landscapes without even
  adding an Extra Unit},
Eprint        = {1901.03909v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.ML},
Abstract      = {Recent work has noted that all bad local minima can be removed from neural
network loss landscapes, by adding a single unit with a particular
parameterization. We show that the core technique from these papers can be used
to remove all bad local minima from any loss landscape, so long as the global
minimum has a loss of zero. This procedure does not require the addition of
auxiliary units, or even that the loss be associated with a neural network. The
method of action involves all bad local minima being converted into bad
(non-local) minima at infinity in terms of auxiliary parameters.},
Year          = {2019},
Month         = {Jan},
Url           = {http://arxiv.org/abs/1901.03909v1},
File          = {1901.03909v1.pdf}
}
@article{1509.03005v1,
Author        = {David Balduzzi and Muhammad Ghifary},
Title         = {Compatible Value Gradients for Reinforcement Learning of Continuous Deep
  Policies},
Eprint        = {1509.03005v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {This paper proposes GProp, a deep reinforcement learning algorithm for
continuous policies with compatible function approximation. The algorithm is
based on two innovations. Firstly, we present a temporal-difference based
method for learning the gradient of the value-function. Secondly, we present
the deviator-actor-critic (DAC) model, which comprises three neural networks
that estimate the value function, its gradient, and determine the actor's
policy respectively. We evaluate GProp on two challenging tasks: a contextual
bandit problem constructed from nonparametric regression datasets that is
designed to probe the ability of reinforcement learning algorithms to
accurately estimate gradients; and the octopus arm, a challenging reinforcement
learning benchmark. GProp is competitive with fully supervised methods on the
bandit task and achieves the best performance to date on the octopus arm.},
Year          = {2015},
Month         = {Sep},
Url           = {http://arxiv.org/abs/1509.03005v1},
File          = {1509.03005v1.pdf}
}
@article{1711.09846v2,
Author        = {Max Jaderberg and Valentin Dalibard and Simon Osindero and Wojciech M. Czarnecki and Jeff Donahue and Ali Razavi and Oriol Vinyals and Tim Green and Iain Dunning and Karen Simonyan and Chrisantha Fernando and Koray Kavukcuoglu},
Title         = {Population Based Training of Neural Networks},
Eprint        = {1711.09846v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Neural networks dominate the modern machine learning landscape, but their
training and success still suffer from sensitivity to empirical choices of
hyperparameters such as model architecture, loss function, and optimisation
algorithm. In this work we present \emph{Population Based Training (PBT)}, a
simple asynchronous optimisation algorithm which effectively utilises a fixed
computational budget to jointly optimise a population of models and their
hyperparameters to maximise performance. Importantly, PBT discovers a schedule
of hyperparameter settings rather than following the generally sub-optimal
strategy of trying to find a single fixed set to use for the whole course of
training. With just a small modification to a typical distributed
hyperparameter training framework, our method allows robust and reliable
training of models. We demonstrate the effectiveness of PBT on deep
reinforcement learning problems, showing faster wall-clock convergence and
higher final performance of agents by optimising over a suite of
hyperparameters. In addition, we show the same method can be applied to
supervised learning for machine translation, where PBT is used to maximise the
BLEU score directly, and also to training of Generative Adversarial Networks to
maximise the Inception score of generated images. In all cases PBT results in
the automatic discovery of hyperparameter schedules and model selection which
results in stable training and better final performance.},
Year          = {2017},
Month         = {Nov},
Url           = {http://arxiv.org/abs/1711.09846v2},
File          = {1711.09846v2.pdf}
}
@article{2001.00449v1,
Author        = {Michael Neunert and Abbas Abdolmaleki and Markus Wulfmeier and Thomas Lampe and Jost Tobias Springenberg and Roland Hafner and Francesco Romano and Jonas Buchli and Nicolas Heess and Martin Riedmiller},
Title         = {Continuous-Discrete Reinforcement Learning for Hybrid Control in
  Robotics},
Eprint        = {2001.00449v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Many real-world control problems involve both discrete decision variables -
such as the choice of control modes, gear switching or digital outputs - as
well as continuous decision variables - such as velocity setpoints, control
gains or analogue outputs. However, when defining the corresponding optimal
control or reinforcement learning problem, it is commonly approximated with
fully continuous or fully discrete action spaces. These simplifications aim at
tailoring the problem to a particular algorithm or solver which may only
support one type of action space. Alternatively, expert heuristics are used to
remove discrete actions from an otherwise continuous space. In contrast, we
propose to treat hybrid problems in their 'native' form by solving them with
hybrid reinforcement learning, which optimizes for discrete and continuous
actions simultaneously. In our experiments, we first demonstrate that the
proposed approach efficiently solves such natively hybrid reinforcement
learning problems. We then show, both in simulation and on robotic hardware,
the benefits of removing possibly imperfect expert-designed heuristics. Lastly,
hybrid reinforcement learning encourages us to rethink problem definitions. We
propose reformulating control problems, e.g. by adding meta actions, to improve
exploration or reduce mechanical wear and tear.},
Year          = {2020},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2001.00449v1},
File          = {2001.00449v1.pdf}
}
@article{1803.02999v3,
Author        = {Alex Nichol and Joshua Achiam and John Schulman},
Title         = {On First-Order Meta-Learning Algorithms},
Eprint        = {1803.02999v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {This paper considers meta-learning problems, where there is a distribution of
tasks, and we would like to obtain an agent that performs well (i.e., learns
quickly) when presented with a previously unseen task sampled from this
distribution. We analyze a family of algorithms for learning a parameter
initialization that can be fine-tuned quickly on a new task, using only
first-order derivatives for the meta-learning updates. This family includes and
generalizes first-order MAML, an approximation to MAML obtained by ignoring
second-order derivatives. It also includes Reptile, a new algorithm that we
introduce here, which works by repeatedly sampling a task, training on it, and
moving the initialization towards the trained weights on that task. We expand
on the results from Finn et al. showing that first-order meta-learning
algorithms perform well on some well-established benchmarks for few-shot
classification, and we provide theoretical analysis aimed at understanding why
these algorithms work.},
Year          = {2018},
Month         = {Mar},
Url           = {http://arxiv.org/abs/1803.02999v3},
File          = {1803.02999v3.pdf}
}
@article{1506.02438v6,
Author        = {John Schulman and Philipp Moritz and Sergey Levine and Michael Jordan and Pieter Abbeel},
Title         = {High-Dimensional Continuous Control Using Generalized Advantage
  Estimation},
Eprint        = {1506.02438v6},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Policy gradient methods are an appealing approach in reinforcement learning
because they directly optimize the cumulative reward and can straightforwardly
be used with nonlinear function approximators such as neural networks. The two
main challenges are the large number of samples typically required, and the
difficulty of obtaining stable and steady improvement despite the
nonstationarity of the incoming data. We address the first challenge by using
value functions to substantially reduce the variance of policy gradient
estimates at the cost of some bias, with an exponentially-weighted estimator of
the advantage function that is analogous to TD(lambda). We address the second
challenge by using trust region optimization procedure for both the policy and
the value function, which are represented by neural networks.
  Our approach yields strong empirical results on highly challenging 3D
locomotion tasks, learning running gaits for bipedal and quadrupedal simulated
robots, and learning a policy for getting the biped to stand up from starting
out lying on the ground. In contrast to a body of prior work that uses
hand-crafted policy representations, our neural network policies map directly
from raw kinematics to joint torques. Our algorithm is fully model-free, and
the amount of simulated experience required for the learning tasks on 3D bipeds
corresponds to 1-2 weeks of real time.},
Year          = {2015},
Month         = {Jun},
Url           = {http://arxiv.org/abs/1506.02438v6},
File          = {1506.02438v6.pdf}
}
@article{2005.10934v3,
Author        = {Homanga Bharadhwaj and Animesh Garg and Florian Shkurti},
Title         = {LEAF: Latent Exploration Along the Frontier},
Eprint        = {2005.10934v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Self-supervised goal proposal and reaching is a key component for exploration
and efficient policy learning algorithms. Such a self-supervised approach
without access to any oracle goal sampling distribution requires deep
exploration and commitment so that long horizon plans can be efficiently
discovered. In this paper, we propose an exploration framework, which learns a
dynamics-aware manifold of reachable states. For a goal, our proposed method
deterministically visits a state at the current frontier of reachable states
(commitment/reaching) and then stochastically explores to reach the goal
(exploration). This allocates exploration budget near the frontier of the
reachable region instead of its interior. We target the challenging problem of
policy learning from initial and goal states specified as images, and do not
assume any access to the underlying ground-truth states of the robot and the
environment. To keep track of reachable latent states, we propose a
distance-conditioned reachability network that is trained to infer whether one
state is reachable from another within the specified latent space distance.
Given an initial state, we obtain a frontier of reachable states from that
state. By incorporating a curriculum for sampling easier goals (closer to the
start state) before more difficult goals, we demonstrate that the proposed
self-supervised exploration algorithm, superior performance compared to
existing baselines on a set of challenging robotic
environments.https://sites.google.com/view/leaf-exploration},
Year          = {2020},
Month         = {May},
Url           = {http://arxiv.org/abs/2005.10934v3},
File          = {2005.10934v3.pdf}
}
@article{2006.13258v6,
Author        = {Paul Barde and Julien Roy and Wonseok Jeon and Joelle Pineau and Christopher Pal and Derek Nowrouzezahrai},
Title         = {Adversarial Soft Advantage Fitting: Imitation Learning without Policy
  Optimization},
Eprint        = {2006.13258v6},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Adversarial Imitation Learning alternates between learning a discriminator --
which tells apart expert's demonstrations from generated ones -- and a
generator's policy to produce trajectories that can fool this discriminator.
This alternated optimization is known to be delicate in practice since it
compounds unstable adversarial training with brittle and sample-inefficient
reinforcement learning. We propose to remove the burden of the policy
optimization steps by leveraging a novel discriminator formulation.
Specifically, our discriminator is explicitly conditioned on two policies: the
one from the previous generator's iteration and a learnable policy. When
optimized, this discriminator directly learns the optimal generator's policy.
Consequently, our discriminator's update solves the generator's optimization
problem for free: learning a policy that imitates the expert does not require
an additional optimization loop. This formulation effectively cuts by half the
implementation and computational burden of Adversarial Imitation Learning
algorithms by removing the Reinforcement Learning phase altogether. We show on
a variety of tasks that our simpler approach is competitive to prevalent
Imitation Learning methods.},
Year          = {2020},
Month         = {Jun},
Note          = {Advances in Neural Information Processing Systems 33 (2020)},
Url           = {http://arxiv.org/abs/2006.13258v6},
File          = {2006.13258v6.pdf}
}
@article{1907.01298v2,
Author        = {Erinc Merdivan and Sten Hanke and Matthieu Geist},
Title         = {Modified Actor-Critics},
Eprint        = {1907.01298v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Recent successful deep reinforcement learning algorithms, such as Trust
Region Policy Optimization (TRPO) or Proximal Policy Optimization (PPO), are
fundamentally variations of conservative policy iteration (CPI). These
algorithms iterate policy evaluation followed by a softened policy improvement
step. As so, they are naturally on-policy. In this paper, we propose to combine
(any kind of) soft greediness with Modified Policy Iteration (MPI). The
proposed abstract framework applies repeatedly: (i) a partial policy evaluation
step that allows off-policy learning and (ii) any softened greedy step. Our
contribution can be seen as a new generic tool for the deep reinforcement
learning toolbox. As a proof of concept, we instantiate this framework with the
PPO greediness. Comparison to the original PPO shows that our algorithm is much
more sample efficient. We also show that it is competitive with the
state-of-art off-policy algorithm Soft Actor Critic (SAC).},
Year          = {2019},
Month         = {Jul},
Url           = {http://arxiv.org/abs/1907.01298v2},
File          = {1907.01298v2.pdf}
}
@article{1705.07874v2,
Author        = {Scott Lundberg and Su-In Lee},
Title         = {A Unified Approach to Interpreting Model Predictions},
Eprint        = {1705.07874v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Understanding why a model makes a certain prediction can be as crucial as the
prediction's accuracy in many applications. However, the highest accuracy for
large modern datasets is often achieved by complex models that even experts
struggle to interpret, such as ensemble or deep learning models, creating a
tension between accuracy and interpretability. In response, various methods
have recently been proposed to help users interpret the predictions of complex
models, but it is often unclear how these methods are related and when one
method is preferable over another. To address this problem, we present a
unified framework for interpreting predictions, SHAP (SHapley Additive
exPlanations). SHAP assigns each feature an importance value for a particular
prediction. Its novel components include: (1) the identification of a new class
of additive feature importance measures, and (2) theoretical results showing
there is a unique solution in this class with a set of desirable properties.
The new class unifies six existing methods, notable because several recent
methods in the class lack the proposed desirable properties. Based on insights
from this unification, we present new methods that show improved computational
performance and/or better consistency with human intuition than previous
approaches.},
Year          = {2017},
Month         = {May},
Url           = {http://arxiv.org/abs/1705.07874v2},
File          = {1705.07874v2.pdf}
}
@article{1806.01261v3,
Author        = {Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Vinicius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Caglar Gulcehre and Francis Song and Andrew Ballard and Justin Gilmer and George Dahl and Ashish Vaswani and Kelsey Allen and Charles Nash and Victoria Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matt Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
Title         = {Relational inductive biases, deep learning, and graph networks},
Eprint        = {1806.01261v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Artificial intelligence (AI) has undergone a renaissance recently, making
major progress in key domains such as vision, language, control, and
decision-making. This has been due, in part, to cheap data and cheap compute
resources, which have fit the natural strengths of deep learning. However, many
defining characteristics of human intelligence, which developed under much
different pressures, remain out of reach for current approaches. In particular,
generalizing beyond one's experiences--a hallmark of human intelligence from
infancy--remains a formidable challenge for modern AI.
  The following is part position paper, part review, and part unification. We
argue that combinatorial generalization must be a top priority for AI to
achieve human-like abilities, and that structured representations and
computations are key to realizing this objective. Just as biology uses nature
and nurture cooperatively, we reject the false choice between
"hand-engineering" and "end-to-end" learning, and instead advocate for an
approach which benefits from their complementary strengths. We explore how
using relational inductive biases within deep learning architectures can
facilitate learning about entities, relations, and rules for composing them. We
present a new building block for the AI toolkit with a strong relational
inductive bias--the graph network--which generalizes and extends various
approaches for neural networks that operate on graphs, and provides a
straightforward interface for manipulating structured knowledge and producing
structured behaviors. We discuss how graph networks can support relational
reasoning and combinatorial generalization, laying the foundation for more
sophisticated, interpretable, and flexible patterns of reasoning. As a
companion to this paper, we have released an open-source software library for
building graph networks, with demonstrations of how to use them in practice.},
Year          = {2018},
Month         = {Jun},
Url           = {http://arxiv.org/abs/1806.01261v3},
File          = {1806.01261v3.pdf}
}
@article{0911.4625v1,
Author        = {Kostas Margellos and John Lygeros},
Title         = {Hamilton-Jacobi formulation for reach-avoid differential games},
Eprint        = {0911.4625v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {math.OC},
Abstract      = {A new framework for formulating reachability problems with competing inputs,
nonlinear dynamics and state constraints as optimal control problems is
developed. Such reach-avoid problems arise in, among others, the study of
safety problems in hybrid systems. Earlier approaches to reach-avoid
computations are either restricted to linear systems, or face numerical
difficulties due to possible discontinuities in the Hamiltonian of the optimal
control problem. The main advantage of the approach proposed in this paper is
that it can be applied to a general class of target hitting continuous dynamic
games with nonlinear dynamics, and has very good properties in terms of its
numerical solution, since the value function and the Hamiltonian of the system
are both continuous. The performance of the proposed method is demonstrated by
applying it to a two aircraft collision avoidance scenario under target window
constraints and in the presence of wind disturbance. Target Windows are a novel
concept in air traffic management, and represent spatial and temporal
constraints, that the aircraft have to respect to meet their schedule.},
Year          = {2009},
Month         = {Nov},
Url           = {http://arxiv.org/abs/0911.4625v1},
File          = {0911.4625v1.pdf}
}
