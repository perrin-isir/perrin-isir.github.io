<?xml version="1.0" encoding="UTF-8"?>
<library>
	<manage_target braces="false" utf8="false"></manage_target>
	<library_folder monitor="false"></library_folder>
	<taglist/>
	<doclist>
		<doc>
			<filename></filename>
			<key>2107.10253v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Demonstration-Guided Reinforcement Learning with Learned Skills</bib_title>
			<bib_authors>Pertsch, Karl and Lee, Youngwoon and Wu, Yue and Lim, Joseph J.</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Demonstration-guided reinforcement learning (RL) is a promising approach for learning complex behaviors by leveraging both reward feedback and a set of target task demonstrations. Prior approaches for demonstration-guided RL treat every new task as an independent learning problem and attempt to follow the provided demonstrations step-by-step, akin to a human trying to imitate a completely unseen behavior by following the demonstrator’s exact muscle movements. Naturally, such learning will be slow, but often new behaviors are not completely unseen: they share subtasks with behaviors we have previously learned. In this work, we aim to exploit this shared subtask structure to increase the efficiency of demonstration-guided RL. We first learn a set of reusable skills from large offline datasets of prior experience collected across many tasks. We then propose Skill-based Learning with Demonstrations (SkiLD), an algorithm for demonstration-guided RL that efficiently leverages the provided demonstrations by following the demonstrated skills instead of the primitive actions, resulting in substantial performance improvements over prior demonstration-guided RL approaches. We validate the effectiveness of our approach on long-horizon maze navigation and complex robot manipulation tasks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2107.10253v1</bib_extra>
			<bib_extra key="File">2107.10253v1.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2107.10253v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1909.07543v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Attraction-Repulsion Actor-Critic for Continuous Control Reinforcement Learning</bib_title>
			<bib_authors>Doan, Thang and Mazoure, Bogdan and Abdar, Moloud and Durand, Audrey and Pineau, Joelle and Hjelm, R. Devon</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Continuous control tasks in reinforcement learning are important because they provide an important framework for learning in high-dimensional state spaces with deceptive rewards, where the agent can easily become trapped into suboptimal solutions. One way to avoid local optima is to use a population of agents to ensure coverage of the policy space, yet learning a population with the &quot;best&quot; coverage is still an open problem. In this work, we present a novel approach to population-based RL in continuous control that leverages properties of normalizing flows to perform attractive and repulsive operations between current members of the population and previously observed policies. Empirical results on the MuJoCo suite demonstrate a high performance gain for our algorithm compared to prior work, including Soft-Actor Critic (SAC).</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1909.07543v3</bib_extra>
			<bib_extra key="File">1909.07543v3.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1909.07543v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1504.04909v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Illuminating search spaces by mapping elites</bib_title>
			<bib_authors>Mouret, Jean-Baptiste and Clune, Jeff</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2015</bib_year>
			<bib_extra key="Abstract">Many fields use search algorithms, which automatically explore a search space to find high-performing solutions: chemists search through the space of molecules to discover new drugs; engineers search for stronger, cheaper, safer designs, scientists search for models that best explain data, etc. The goal of search algorithms has traditionally been to return the single highest-performing solution in a search space. Here we describe a new, fundamentally different type of algorithm that is more useful because it provides a holistic view of how high-performing solutions are distributed throughout a search space. It creates a map of high-performing solutions at each point in a space defined by dimensions of variation that a user gets to choose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) algorithm illuminates search spaces, allowing researchers to understand how interesting attributes of solutions combine to affect performance, either positively or, equally of interest, negatively. For example, a drug company may wish to understand how performance changes as the size of molecules and their cost-to-produce vary. MAP-Elites produces a large diversity of high-performing, yet qualitatively different solutions, which can be more helpful than a single, high-performing solution. Interestingly, because MAP-Elites explores more of the search space, it also tends to find a better overall solution than state-of-the-art search algorithms. We demonstrate the benefits of this new algorithm in three different problem domains ranging from producing modular neural networks to designing simulated and real soft robots. Because MAP- Elites (1) illuminates the relationship between performance and dimensions of interest in solutions, (2) returns a set of high-performing, yet diverse solutions, and (3) improves finding a single, best solution, it will advance science and engineering.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1504.04909v1</bib_extra>
			<bib_extra key="File">1504.04909v1.pdf</bib_extra>
			<bib_extra key="Month">Apr</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1504.04909v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1905.06750v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation</bib_title>
			<bib_authors>Wang, Ruohan and Ciliberto, Carlo and Amadori, Pierluigi and Demiris, Yiannis</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">We consider the problem of imitation learning from a finite set of expert trajectories, without access to reinforcement signals. The classical approach of extracting the expert’s reward function via inverse reinforcement learning, followed by reinforcement learning is indirect and may be computationally expensive. Recent generative adversarial methods based on matching the policy distribution between the expert and the agent could be unstable during training. We propose a new framework for imitation learning by estimating the support of the expert policy to compute a fixed reward function, which allows us to re-frame imitation learning within the standard reinforcement learning setting. We demonstrate the efficacy of our reward function on both discrete and continuous domains, achieving comparable or better performance than the state of the art under different reinforcement learning algorithms.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1905.06750v2</bib_extra>
			<bib_extra key="File">1905.06750v2.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1905.06750v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1707.06347v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Proximal Policy Optimization Algorithms</bib_title>
			<bib_authors>Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a &quot;surrogate&quot; objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1707.06347v2</bib_extra>
			<bib_extra key="File">1707.06347v2.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1707.06347v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1909.10618v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?</bib_title>
			<bib_authors>Nachum, Ofir and Tang, Haoran and Lu, Xingyu and Gu, Shixiang and Lee, Honglak and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Hierarchical reinforcement learning has demonstrated significant success at solving difficult reinforcement learning (RL) tasks. Previous works have motivated the use of hierarchy by appealing to a number of intuitive benefits, including learning over temporally extended transitions, exploring over temporally extended periods, and training and exploring in a more semantically meaningful action space, among others. However, in fully observed, Markovian settings, it is not immediately clear why hierarchical RL should provide benefits over standard &quot;shallow&quot; RL architectures. In this work, we isolate and evaluate the claimed benefits of hierarchical RL on a suite of tasks encompassing locomotion, navigation, and manipulation. Surprisingly, we find that most of the observed benefits of hierarchy can be attributed to improved exploration, as opposed to easier policy learning or imposed hierarchical structures. Given this insight, we present exploration techniques inspired by hierarchy that achieve performance competitive with hierarchical RL while at the same time being much simpler to use and implement.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1909.10618v2</bib_extra>
			<bib_extra key="File">1909.10618v2.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1909.10618v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2106.04615v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Vector Quantized Models for Planning</bib_title>
			<bib_authors>Ozair, Sherjil and Li, Yazhe and Razavi, Ali and Antonoglou, Ioannis and Oord, Aäron van den and Vinyals, Oriol</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Recent developments in the field of model-based RL have proven successful in a range of environments, especially ones where planning is essential. However, such successes have been limited to deterministic fully-observed environments. We present a new approach that handles stochastic and partially-observable environments. Our key insight is to use discrete autoencoders to capture the multiple possible effects of an action in a stochastic environment. We use a stochastic variant of Monte Carlo tree search to plan over both the agent’s actions and the discrete latent variables representing the environment’s response. Our approach significantly outperforms an offline version of MuZero on a stochastic interpretation of chess where the opponent is considered part of the environment. We also show that our approach scales to DeepMind Lab, a first-person 3D environment with large visual observations and partial observability.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2106.04615v2</bib_extra>
			<bib_extra key="File">2106.04615v2.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2106.04615v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1310.5726v5</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Correlation and variable importance in random forests</bib_title>
			<bib_authors>Gregorutti, Baptiste and Michel, Bertrand and Saint-Pierre, Philippe</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2013</bib_year>
			<bib_extra key="Abstract">This paper is about variable selection with the random forests algorithm in presence of correlated predictors. In high-dimensional regression or classification frameworks, variable selection is a difficult task, that becomes even more challenging in the presence of highly correlated predictors. Firstly we provide a theoretical study of the permutation importance measure for an additive regression model. This allows us to describe how the correlation between predictors impacts the permutation importance. Our results motivate the use of the Recursive Feature Elimination (RFE) algorithm for variable selection in this context. This algorithm recursively eliminates the variables using permutation importance measure as a ranking criterion. Next various simulation experiments illustrate the efficiency of the RFE algorithm for selecting a small number of variables together with a good prediction error. Finally, this selection algorithm is tested on the Landsat Satellite data from the UCI Machine Learning Repository.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1007/s11222-016-9646-1</bib_extra>
			<bib_extra key="Eprint">1310.5726v5</bib_extra>
			<bib_extra key="File">1310.5726v5.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">stat.ME</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1310.5726v5</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1807.06919v5</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Backplay: &quot;Man muss immer umkehren&quot;</bib_title>
			<bib_authors>Resnick, Cinjon and Raileanu, Roberta and Kapoor, Sanyam and Peysakhovich, Alexander and Cho, Kyunghyun and Bruna, Joan</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Model-free reinforcement learning (RL) requires a large number of trials to learn a good policy, especially in environments with sparse rewards. We explore a method to improve the sample efficiency when we have access to demonstrations. Our approach, Backplay, uses a single demonstration to construct a curriculum for a given task. Rather than starting each training episode in the environment’s fixed initial state, we start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state. Our contributions are that we analytically characterize the types of environments where Backplay can improve training speed, demonstrate the effectiveness of Backplay both in large grid worlds and a complex four player zero-sum game (Pommerman), and show that Backplay compares favorably to other competitive methods known to improve sample efficiency. This includes reward shaping, behavioral cloning, and reverse curriculum generation.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1807.06919v5</bib_extra>
			<bib_extra key="File">1807.06919v5.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1807.06919v5</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1412.8690v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Breaking the Curse of Dimensionality with Convex Neural Networks</bib_title>
			<bib_authors>Bach, Francis</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2014</bib_year>
			<bib_extra key="Abstract">We consider neural networks with a single hidden layer and non-decreasing homogeneous activa-tion functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity-inducing norms on the input weights, we show that high-dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of ob-servations. In addition, we provide a simple geometric interpretation to the non-convex problem of addition of a new unit, which is the core potentially hard computational element in the framework of learning from continuously many basis functions. We provide simple conditions for convex relaxations to achieve the same generalization error bounds, even when constant-factor approxi-mations cannot be found (e.g., because it is NP-hard such as for the zero-homogeneous activation function). We were not able to find strong enough convex relaxations and leave open the existence or non-existence of polynomial-time algorithms.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1412.8690v2</bib_extra>
			<bib_extra key="File">1412.8690v2.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1412.8690v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1611.02635v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>A Lyapunov Analysis of Momentum Methods in Optimization</bib_title>
			<bib_authors>Wilson, Ashia C. and Recht, Benjamin and Jordan, Michael I.</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">Momentum methods play a significant role in optimization. Examples include Nesterov’s accelerated gradient method and the conditional gradient algorithm. Several momentum methods are provably optimal under standard oracle models, and all use a technique called estimate sequences to analyze their convergence properties. The technique of estimate sequences has long been considered difficult to understand, leading many researchers to generate alternative, &quot;more intuitive&quot; methods and analyses. We show there is an equivalence between the technique of estimate sequences and a family of Lyapunov functions in both continuous and discrete time. This connection allows us to develop a simple and unified analysis of many existing momentum algorithms, introduce several new algorithms, and strengthen the connection between algorithms and continuous-time dynamical systems.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1611.02635v4</bib_extra>
			<bib_extra key="File">1611.02635v4.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">math.OC</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1611.02635v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1712.06560v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents</bib_title>
			<bib_authors>Conti, Edoardo and Madhavan, Vashisht and Such, Felipe Petroski and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1712.06560v3</bib_extra>
			<bib_extra key="File">1712.06560v3.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1712.06560v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1907.08225v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery</bib_title>
			<bib_authors>Hartikainen, Kristian and Geng, Xinyang and Haarnoja, Tuomas and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Reinforcement learning requires manual specification of a reward function to learn a task. While in principle this reward function only needs to specify the task goal, in practice reinforcement learning can be very time-consuming or even infeasible unless the reward function is shaped so as to provide a smooth gradient towards a successful outcome. This shaping is difficult to specify by hand, particularly when the task is learned from raw observations, such as images. In this paper, we study how we can automatically learn dynamical distances: a measure of the expected number of time steps to reach a given goal state from any other state. These dynamical distances can be used to provide well-shaped reward functions for reaching new goals, making it possible to learn complex tasks efficiently. We show that dynamical distances can be used in a semi-supervised regime, where unsupervised interaction with the environment is used to learn the dynamical distances, while a small amount of preference supervision is used to determine the task goal, without any manually engineered reward function or goal examples. We evaluate our method both on a real-world robot and in simulation. We show that our method can learn to turn a valve with a real-world 9-DoF hand, using raw image observations and just ten preference labels, without any other supervision. Videos of the learned skills can be found on the project website: https://sites.google.com/view/dynamical-distance-learning.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1907.08225v4</bib_extra>
			<bib_extra key="File">1907.08225v4.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1907.08225v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1905.11108v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards</bib_title>
			<bib_authors>Reddy, Siddharth and Dragan, Anca D. and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Learning to imitate expert behavior from demonstrations can be challenging, especially in environments with high-dimensional, continuous observations and unknown dynamics. Supervised learning methods based on behavioral cloning (BC) suffer from distribution shift: because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation. Recent methods based on reinforcement learning (RL), such as inverse RL and generative adversarial imitation learning (GAIL), overcome this issue by training an RL agent to match the demonstrations over a long horizon. Since the true reward function for the task is unknown, these methods learn a reward function from the demonstrations, often using complex and brittle approximation techniques that involve adversarial training. We propose a simple alternative that still uses RL, but does not require learning a reward function. The key idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. We accomplish this by giving the agent a constant reward of r=+1 for matching the demonstrated action in a demonstrated state, and a constant reward of r=0 for all other behavior. Our method, which we call soft Q imitation learning (SQIL), can be implemented with a handful of minor modifications to any standard Q-learning or off-policy actor-critic algorithm. Theoretically, we show that SQIL can be interpreted as a regularized variant of BC that uses a sparsity prior to encourage long-horizon imitation. Empirically, we show that SQIL outperforms BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks in Box2D, Atari, and MuJoCo.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1905.11108v3</bib_extra>
			<bib_extra key="File">1905.11108v3.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1905.11108v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1712.08449v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>True Asymptotic Natural Gradient Optimization</bib_title>
			<bib_authors>Ollivier, Yann</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">We introduce a simple algorithm, True Asymptotic Natural Gradient Optimization (TANGO), that converges to a true natural gradient descent in the limit of small learning rates, without explicit Fisher matrix estimation. For quadratic models the algorithm is also an instance of averaged stochastic gradient, where the parameter is a moving average of a &quot;fast&quot;, constant-rate gradient descent. TANGO appears as a particular de-linearization of averaged SGD, and is sometimes quite different on non-quadratic models. This further connects averaged SGD and natural gradient, both of which are arguably optimal asymptotically. In large dimension, small learning rates will be required to approximate the natural gradient well. Still, this shows it is possible to get arbitrarily close to exact natural gradient descent with a lightweight algorithm.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1712.08449v1</bib_extra>
			<bib_extra key="File">1712.08449v1.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">stat.ML</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1712.08449v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1606.01868v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Unifying Count-Based Exploration and Intrinsic Motivation</bib_title>
			<bib_authors>Bellemare, Marc G. and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">We consider an agent’s uncertainty about its environment and the problem of generalizing this uncertainty across observations. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma’s Revenge.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1606.01868v2</bib_extra>
			<bib_extra key="File">1606.01868v2.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1606.01868v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2110.14457v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Direct then Diffuse: Incremental Unsupervised Skill Discovery for State Covering and Goal Reaching</bib_title>
			<bib_authors>Kamienny, Pierre-Alexandre and Tarbouriech, Jean and Lamprier, Sylvain and Lazaric, Alessandro and Denoyer, Ludovic</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Learning meaningful behaviors in the absence of reward is a difficult problem in reinforcement learning. A desirable and challenging unsupervised objective is to learn a set of diverse skills that provide a thorough coverage of the state space while being directed, i.e., reliably reaching distinct regions of the environment. In this paper, we build on the mutual information framework for skill discovery and introduce UPSIDE, which addresses the coverage-directedness trade-off in the following ways: 1) We design policies with a decoupled structure of a directed skill, trained to reach a specific region, followed by a diffusing part that induces a local coverage. 2) We optimize policies by maximizing their number under the constraint that each of them reaches distinct regions of the environment (i.e., they are sufficiently discriminable) and prove that this serves as a lower bound to the original mutual information objective. 3) Finally, we compose the learned directed skills into a growing tree that adaptively covers the environment. We illustrate in several navigation and control environments how the skills learned by UPSIDE solve sparse-reward downstream tasks better than existing baselines.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2110.14457v2</bib_extra>
			<bib_extra key="File">2110.14457v2.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2110.14457v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2009.07888v5</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Transfer Learning in Deep Reinforcement Learning: A Survey</bib_title>
			<bib_authors>Zhu, Zhuangdi and Lin, Kaixiang and Jain, Anil K. and Zhou, Jiayu</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the efficiency and effectiveness of the learning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning. Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications. We also draw connections between transfer learning and other relevant topics from the reinforcement learning perspective and explore their potential challenges that await future research progress.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2009.07888v5</bib_extra>
			<bib_extra key="File">2009.07888v5.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2009.07888v5</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2110.14770v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>TRAIL: Near-Optimal Imitation Learning with Suboptimal Data</bib_title>
			<bib_authors>Yang, Mengjiao and Levine, Sergey and Nachum, Ofir</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">The aim in imitation learning is to learn effective policies by utilizing near-optimal expert demonstrations. However, high-quality demonstrations from human experts can be expensive to obtain in large numbers. On the other hand, it is often much easier to obtain large quantities of suboptimal or task-agnostic trajectories, which are not useful for direct imitation, but can nevertheless provide insight into the dynamical structure of the environment, showing what could be done in the environment even if not what should be done. We ask the question, is it possible to utilize such suboptimal offline datasets to facilitate provably improved downstream imitation learning? In this work, we answer this question affirmatively and present training objectives that use offline datasets to learn a factored transition model whose structure enables the extraction of a latent action space. Our theoretical analysis shows that the learned latent action space can boost the sample-efficiency of downstream imitation learning, effectively reducing the need for large near-optimal expert datasets through the use of auxiliary non-expert data. To learn the latent action space in practice, we propose TRAIL (Transition-Reparametrized Actions for Imitation Learning), an algorithm that learns an energy-based transition model contrastively, and uses the transition model to reparametrize the action space for sample-efficient imitation learning. We evaluate the practicality of our objective through experiments on a set of navigation and locomotion tasks. Our results verify the benefits suggested by our theory and show that TRAIL is able to improve baseline imitation learning by up to 4x in performance.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2110.14770v1</bib_extra>
			<bib_extra key="File">2110.14770v1.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2110.14770v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2004.11667v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>PBCS : Efficient Exploration and Exploitation Using a Synergy between Reinforcement Learning and Motion Planning</bib_title>
			<bib_authors>Matheron, Guillaume and Perrin, Nicolas and Sigaud, Olivier</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">The exploration-exploitation trade-off is at the heart of reinforcement learning (RL). However, most continuous control benchmarks used in recent RL research only require local exploration. This led to the development of algorithms that have basic exploration capabilities, and behave poorly in benchmarks that require more versatile exploration. For instance, as demonstrated in our empirical study, state-of-the-art RL algorithms such as DDPG and TD3 are unable to steer a point mass in even small 2D mazes. In this paper, we propose a new algorithm called &quot;Plan, Backplay, Chain Skills&quot; (PBCS) that combines motion planning and reinforcement learning to solve hard exploration environments. In a first phase, a motion planning algorithm is used to find a single good trajectory, then an RL algorithm is trained using a curriculum derived from the trajectory, by combining a variant of the Backplay algorithm and skill chaining. We show that this method outperforms state-of-the-art RL algorithms in 2D maze environments of various sizes, and is able to improve on the trajectory obtained by the motion planning phase.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1007/978-3-030-61616-8_24</bib_extra>
			<bib_extra key="Eprint">2004.11667v1</bib_extra>
			<bib_extra key="File">2004.11667v1.pdf</bib_extra>
			<bib_extra key="Month">Apr</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2004.11667v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1812.07626v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Universal Successor Features Approximators</bib_title>
			<bib_authors>Borsa, Diana and Barreto, André and Quan, John and Mankowitz, Daniel and Munos, Rémi and Hasselt, Hado van and Silver, David and Schaul, Tom</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">The ability of a reinforcement learning (RL) agent to learn about many reward functions at the same time has many potential benefits, such as the decomposition of complex tasks into simpler ones, the exchange of information between tasks, and the reuse of skills. We focus on one aspect in particular, namely the ability to generalise to unseen tasks. Parametric generalisation relies on the interpolation power of a function approximator that is given the task description as input; one of its most common form are universal value function approximators (UVFAs). Another way to generalise to new tasks is to exploit structure in the RL problem itself. Generalised policy improvement (GPI) combines solutions of previous tasks into a policy for the unseen task; this relies on instantaneous policy evaluation of old policies under the new reward function, which is made possible through successor features (SFs). Our proposed universal successor features approximators (USFAs) combine the advantages of all of these, namely the scalability of UVFAs, the instant inference of SFs, and the strong generalisation of GPI. We discuss the challenges involved in training a USFA, its generalisation properties and demonstrate its practical benefits and transfer abilities on a large-scale domain in which the agent has to navigate in a first-person perspective three-dimensional environment.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1812.07626v1</bib_extra>
			<bib_extra key="File">1812.07626v1.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1812.07626v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1806.06923v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Implicit Quantile Networks for Distributional Reinforcement Learning</bib_title>
			<bib_authors>Dabney, Will and Ostrovski, Georg and Silver, David and Munos, Rémi</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm’s implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1806.06923v1</bib_extra>
			<bib_extra key="File">1806.06923v1.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1806.06923v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1908.04211v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>On Identifiability in Transformers</bib_title>
			<bib_authors>Brunner, Gino and Liu, Yang and Pascual, Damián and Richter, Oliver and Ciaramita, Massimiliano and Wattenhofer, Roger</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">In this paper we delve deep in the Transformer architecture by investigating two of its core components: self-attention and contextual embeddings. In particular, we study the identifiability of attention weights and token embeddings, and the aggregation of context into hidden tokens. We show that, for sequences longer than the attention head dimension, attention weights are not identifiable. We propose effective attention as a complementary tool for improving explanatory interpretations based on attention. Furthermore, we show that input tokens retain to a large degree their identity across the model. We also find evidence suggesting that identity information is mainly encoded in the angle of the embeddings and gradually decreases with depth. Finally, we demonstrate strong mixing of input information in the generation of contextual embeddings by means of a novel quantification method based on gradient attribution. Overall, we show that self-attention distributions are not directly interpretable and present tools to better understand and further investigate Transformer models.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1908.04211v4</bib_extra>
			<bib_extra key="File">1908.04211v4.pdf</bib_extra>
			<bib_extra key="Month">Aug</bib_extra>
			<bib_extra key="Primaryclass">cs.CL</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1908.04211v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1703.02949v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning</bib_title>
			<bib_authors>Gupta, Abhishek and Devin, Coline and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">People can learn a wide range of tasks from their own experience, but can also learn from observing other creatures. This can accelerate acquisition of new skills even when the observed agent differs substantially from the learning agent in terms of morphology. In this paper, we examine how reinforcement learning algorithms can transfer knowledge between morphologically different agents (e.g., different robots). We introduce a problem formulation where two agents are tasked with learning multiple skills by sharing information. Our method uses the skills that were learned by both agents to train invariant feature spaces that can then be used to transfer other skills from one agent to another. The process of learning these invariant feature spaces can be viewed as a kind of &quot;analogy making&quot;, or implicit learning of partial correspondences between two distinct domains. We evaluate our transfer learning algorithm in two simulated robotic manipulation skills, and illustrate that we can transfer knowledge between simulated robotic arms with different numbers of links, as well as simulated arms with different actuation mechanisms, where one robot is torque-driven while the other is tendon-driven.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1703.02949v1</bib_extra>
			<bib_extra key="File">1703.02949v1.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1703.02949v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1901.03909v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Eliminating all bad Local Minima from Loss Landscapes without even adding an Extra Unit</bib_title>
			<bib_authors>Sohl-Dickstein, Jascha and Kawaguchi, Kenji</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Recent work has noted that all bad local minima can be removed from neural network loss landscapes, by adding a single unit with a particular parameterization. We show that the core technique from these papers can be used to remove all bad local minima from any loss landscape, so long as the global minimum has a loss of zero. This procedure does not require the addition of auxiliary units, or even that the loss be associated with a neural network. The method of action involves all bad local minima being converted into bad (non-local) minima at infinity in terms of auxiliary parameters.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1901.03909v1</bib_extra>
			<bib_extra key="File">1901.03909v1.pdf</bib_extra>
			<bib_extra key="Month">Jan</bib_extra>
			<bib_extra key="Primaryclass">stat.ML</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1901.03909v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1712.00948v5</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Learning Multi-Level Hierarchies with Hindsight</bib_title>
			<bib_authors>Levy, Andrew and Konidaris, George and Platt, Robert and Saenko, Kate</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">Hierarchical agents have the potential to solve sequential decision making tasks with greater sample efficiency than their non-hierarchical counterparts because hierarchical agents can break down tasks into sets of subtasks that only require short sequences of decisions. In order to realize this potential of faster learning, hierarchical agents need to be able to learn their multiple levels of policies in parallel so these simpler subproblems can be solved simultaneously. Yet, learning multiple levels of policies in parallel is hard because it is inherently unstable: changes in a policy at one level of the hierarchy may cause changes in the transition and reward functions at higher levels in the hierarchy, making it difficult to jointly learn multiple levels of policies. In this paper, we introduce a new Hierarchical Reinforcement Learning (HRL) framework, Hierarchical Actor-Critic (HAC), that can overcome the instability issues that arise when agents try to jointly learn multiple levels of policies. The main idea behind HAC is to train each level of the hierarchy independently of the lower levels by training each level as if the lower level policies are already optimal. We demonstrate experimentally in both grid world and simulated robotics domains that our approach can significantly accelerate learning relative to other non-hierarchical and hierarchical methods. Indeed, our framework is the first to successfully learn 3-level hierarchies in parallel in tasks with continuous state and action spaces.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1712.00948v5</bib_extra>
			<bib_extra key="File">1712.00948v5.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1712.00948v5</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1809.02721v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Learning to Solve NP-Complete Problems - A Graph Neural Network for Decision TSP</bib_title>
			<bib_authors>Prates, Marcelo O. R. and Avelar, Pedro H. C. and Lemos, Henrique and Lamb, Luis and Vardi, Moshe</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Graph Neural Networks (GNN) are a promising technique for bridging differential programming and combinatorial domains. GNNs employ trainable modules which can be assembled in different configurations that reflect the relational structure of each problem instance. In this paper, we show that GNNs can learn to solve, with very little supervision, the decision variant of the Traveling Salesperson Problem (TSP), a highly relevant $\mathcalNP$-Complete problem. Our model is trained to function as an effective message-passing algorithm in which edges (embedded with their weights) communicate with vertices for a number of iterations after which the model is asked to decide whether a route with cost $&lt;C$ exists. We show that such a network can be trained with sets of dual examples: given the optimal tour cost $C^*$, we produce one decision instance with target cost $x%$ smaller and one with target cost $x%$ larger than $C^*$. We were able to obtain $80%$ accuracy training with $-2%,+2%$ deviations, and the same trained model can generalize for more relaxed deviations with increasing performance. We also show that the model is capable of generalizing for larger problem sizes. Finally, we provide a method for predicting the optimal route cost within $2%$ deviation from the ground truth. In summary, our work shows that Graph Neural Networks are powerful enough to solve $\mathcalNP$-Complete problems which combine symbolic and numeric data.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1809.02721v3</bib_extra>
			<bib_extra key="File">1809.02721v3.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1809.02721v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1706.10295v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Noisy Networks for Exploration</bib_title>
			<bib_authors>Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and Menick, Jacob and Osband, Ian and Graves, Alex and Mnih, Vlad and Munos, Remi and Hassabis, Demis and Pietquin, Olivier and Blundell, Charles and Legg, Shane</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent’s policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and ε-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1706.10295v3</bib_extra>
			<bib_extra key="File">1706.10295v3.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1706.10295v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1705.07241v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Diffusion-based neuromodulation can eliminate catastrophic forgetting in simple neural networks</bib_title>
			<bib_authors>Velez, Roby and Clune, Jeff</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">A long-term goal of AI is to produce agents that can learn a diversity of skills throughout their lifetimes and continuously improve those skills via experience. A longstanding obstacle towards that goal is catastrophic forgetting, which is when learning new information erases previously learned information. Catastrophic forgetting occurs in artificial neural networks (ANNs), which have fueled most recent advances in AI. A recent paper proposed that catastrophic forgetting in ANNs can be reduced by promoting modularity, which can limit forgetting by isolating task information to specific clusters of nodes and connections (functional modules). While the prior work did show that modular ANNs suffered less from catastrophic forgetting, it was not able to produce ANNs that possessed task-specific functional modules, thereby leaving the main theory regarding modularity and forgetting untested. We introduce diffusion-based neuromodulation, which simulates the release of diffusing, neuromodulatory chemicals within an ANN that can modulate (i.e. up or down regulate) learning in a spatial region. On the simple diagnostic problem from the prior work, diffusion-based neuromodulation 1) induces task-specific learning in groups of nodes and connections (task-specific localized learning), which 2) produces functional modules for each subtask, and 3) yields higher performance by eliminating catastrophic forgetting. Overall, our results suggest that diffusion-based neuromodulation promotes task-specific localized learning and functional modularity, which can help solve the challenging, but important problem of catastrophic forgetting.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1371/journal.pone.0187736</bib_extra>
			<bib_extra key="Eprint">1705.07241v3</bib_extra>
			<bib_extra key="File">1705.07241v3.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">cs.NE</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1705.07241v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2102.09430v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>State Entropy Maximization with Random Encoders for Efficient Exploration</bib_title>
			<bib_authors>Seo, Younggyo and Chen, Lili and Shin, Jinwoo and Lee, Honglak and Abbeel, Pieter and Lee, Kimin</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Recent exploration methods have proven to be a recipe for improving sample-efficiency in deep reinforcement learning (RL). However, efficient exploration in high-dimensional observation spaces still remains a challenge. This paper presents Random Encoders for Efficient Exploration (RE3), an exploration method that utilizes state entropy as an intrinsic reward. In order to estimate state entropy in environments with high-dimensional observations, we utilize a k-nearest neighbor entropy estimator in the low-dimensional representation space of a convolutional encoder. In particular, we find that the state entropy can be estimated in a stable and compute-efficient manner by utilizing a randomly initialized encoder, which is fixed throughout training. Our experiments show that RE3 significantly improves the sample-efficiency of both model-free and model-based RL methods on locomotion and navigation tasks from DeepMind Control Suite and MiniGrid benchmarks. We also show that RE3 allows learning diverse behaviors without extrinsic rewards, effectively improving sample-efficiency in downstream tasks. Source code and videos are available at https://sites.google.com/view/re3-rl.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2102.09430v4</bib_extra>
			<bib_extra key="File">2102.09430v4.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2102.09430v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1709.07932v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Expanding Motor Skills through Relay Neural Networks</bib_title>
			<bib_authors>Kumar, Visak C. V. and Ha, Sehoon and Liu, C. Karen</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">While the recent advances in deep reinforcement learning have achieved impressive results in learning motor skills, many of the trained policies are only capable within a limited set of initial states. We propose a technique to break down a complex robotic task to simpler subtasks and train them sequentially such that the robot can expand its existing skill set gradually. Our key idea is to build a tree of local control policies represented by neural networks, which we refer as Relay Neural Networks. Starting from the root policy that attempts to achieve the task from a small set of initial states, each subsequent policy expands the set of successful initial states by driving the new states to existing &quot;good&quot; states. Our algorithm utilizes the value function of the policy to determine whether a state is &quot;good&quot; under each policy. We take advantage of many existing policy search algorithms that learn the value function simultaneously with the policy, such as those that use actor-critic representations or those that use the advantage function to reduce variance. We demonstrate that the relay networks can solve complex continuous control problems for underactuated dynamic systems.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1709.07932v3</bib_extra>
			<bib_extra key="File">1709.07932v3.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1709.07932v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1801.01290v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</bib_title>
			<bib_authors>Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1801.01290v2</bib_extra>
			<bib_extra key="File">1801.01290v2.pdf</bib_extra>
			<bib_extra key="Month">Jan</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1801.01290v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1906.07794v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Convolutional neural network models for cancer type prediction based on gene expression</bib_title>
			<bib_authors>Mostavi, Milad and Chiu, Yu-Chiao and Huang, Yufei and Chen, Yidong</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Background Precise prediction of cancer types is vital for cancer diagnosis and therapy. Important cancer marker genes can be inferred through predictive model. Several studies have attempted to build machine learning models for this task however none has taken into consideration the effects of tissue of origin that can potentially bias the identification of cancer markers. Results In this paper, we introduced several Convolutional Neural Network (CNN) models that take unstructured gene expression inputs to classify tumor and non-tumor samples into their designated cancer types or as normal. Based on different designs of gene embeddings and convolution schemes, we implemented three CNN models: 1D-CNN, 2D-Vanilla-CNN, and 2D-Hybrid-CNN. The models were trained and tested on combined 10,340 samples of 33 cancer types and 731 matched normal tissues of The Cancer Genome Atlas (TCGA). Our models achieved excellent prediction accuracies (93.9-95.0%) among 34 classes (33 cancers and normal). Furthermore, we interpreted one of the models, known as 1D-CNN model, with a guided saliency technique and identified a total of 2,090 cancer markers (108 per class). The concordance of differential expression of these markers between the cancer type they represent and others is confirmed. In breast cancer, for instance, our model identified well-known markers, such as GATA3 and ESR1. Finally, we extended the 1D-CNN model for prediction of breast cancer subtypes and achieved an average accuracy of 88.42% among 5 subtypes. The codes can be found at https://github.com/chenlabgccri/CancerTypePrediction.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1906.07794v1</bib_extra>
			<bib_extra key="File">1906.07794v1.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">q-bio.GN</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1906.07794v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1902.10250v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Diagnosing Bottlenecks in Deep Q-learning Algorithms</bib_title>
			<bib_authors>Fu, Justin and Kumar, Aviral and Soh, Matthew and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Q-learning methods represent a commonly used class of algorithms in reinforcement learning: they are generally efficient and simple, and can be combined readily with function approximators for deep reinforcement learning (RL). However, the behavior of Q-learning methods with function approximation is poorly understood, both theoretically and empirically. In this work, we aim to experimentally investigate potential issues in Q-learning, by means of a &quot;unit testing&quot; framework where we can utilize oracles to disentangle sources of error. Specifically, we investigate questions related to function approximation, sampling error and nonstationarity, and where available, verify if trends found in oracle settings hold true with modern deep RL methods. We find that large neural network architectures have many benefits with regards to learning stability; offer several practical compensations for overfitting; and develop a novel sampling method based on explicitly compensating for function approximation error that yields fair improvement on high-dimensional continuous control domains.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1902.10250v1</bib_extra>
			<bib_extra key="File">1902.10250v1.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1902.10250v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2105.00371v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Discovering Diverse Athletic Jumping Strategies</bib_title>
			<bib_authors>Yin, Zhiqi and Yang, Zeshi and Panne, Michiel van de and Yin, KangKang</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">We present a framework that enables the discovery of diverse and natural-looking motion strategies for athletic skills such as the high jump. The strategies are realized as control policies for physics-based characters. Given a task objective and an initial character configuration, the combination of physics simulation and deep reinforcement learning (DRL) provides a suitable starting point for automatic control policy training. To facilitate the learning of realistic human motions, we propose a Pose Variational Autoencoder (P-VAE) to constrain the actions to a subspace of natural poses. In contrast to motion imitation methods, a rich variety of novel strategies can naturally emerge by exploring initial character states through a sample-efficient Bayesian diversity search (BDS) algorithm. A second stage of optimization that encourages novel policies can further enrich the unique strategies discovered. Our method allows for the discovery of diverse and novel strategies for athletic jumping motions such as high jumps and obstacle jumps with no motion examples and less reward engineering than prior work.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2105.00371v1</bib_extra>
			<bib_extra key="File">2105.00371v1.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Note">ACM Trans. Graph. 40, 4, Article 91 (August 2021), 17 pages (2021)</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2105.00371v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1711.01012v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Policy Optimization by Genetic Distillation</bib_title>
			<bib_authors>Gangwani, Tanmay and Peng, Jian</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">Genetic algorithms have been widely used in many practical optimization problems. Inspired by natural selection, operators, including mutation, crossover and selection, provide effective heuristics for search and black-box optimization. However, they have not been shown useful for deep reinforcement learning, possibly due to the catastrophic consequence of parameter crossovers of neural networks. Here, we present Genetic Policy Optimization (GPO), a new genetic algorithm for sample-efficient deep policy optimization. GPO uses imitation learning for policy crossover in the state space and applies policy gradient methods for mutation. Our experiments on MuJoCo tasks show that GPO as a genetic algorithm is able to provide superior performance over the state-of-the-art policy gradient methods and achieves comparable or higher sample efficiency.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1711.01012v2</bib_extra>
			<bib_extra key="File">1711.01012v2.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">stat.ML</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1711.01012v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1506.02438v6</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>High-Dimensional Continuous Control Using Generalized Advantage Estimation</bib_title>
			<bib_authors>Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2015</bib_year>
			<bib_extra key="Abstract">Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1506.02438v6</bib_extra>
			<bib_extra key="File">1506.02438v6.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1506.02438v6</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1811.12359v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations</bib_title>
			<bib_authors>Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Rätsch, Gunnar and Gelly, Sylvain and Schölkopf, Bernhard and Bachem, Olivier</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties “encouraged” by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1811.12359v4</bib_extra>
			<bib_extra key="File">1811.12359v4.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Note">Proceedings of the 36th International Conference on Machine Learning (ICML 2019)</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1811.12359v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1509.06113v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Deep Spatial Autoencoders for Visuomotor Learning</bib_title>
			<bib_authors>Finn, Chelsea and Tan, Xin Yu and Duan, Yan and Darrell, Trevor and Levine, Sergey and Abbeel, Pieter</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2015</bib_year>
			<bib_extra key="Abstract">Reinforcement learning provides a powerful and flexible framework for automated acquisition of robotic motion skills. However, applying reinforcement learning requires a sufficiently detailed representation of the state, including the configuration of task-relevant objects. We present an approach that automates state-space construction by learning a state representation directly from camera images. Our method uses a deep spatial autoencoder to acquire a set of feature points that describe the environment for the current task, such as the positions of objects, and then learns a motion skill with these feature points using an efficient reinforcement learning method based on local linear models. The resulting controller reacts continuously to the learned feature points, allowing the robot to dynamically manipulate objects in the world with closed-loop control. We demonstrate our method with a PR2 robot on tasks that include pushing a free-standing toy block, picking up a bag of rice using a spatula, and hanging a loop of rope on a hook at various positions. In each task, our method automatically learns to track task-relevant objects and manipulate their configuration with the robot’s arm.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1509.06113v3</bib_extra>
			<bib_extra key="File">1509.06113v3.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1509.06113v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1812.05905v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Soft Actor-Critic Algorithms and Applications</bib_title>
			<bib_authors>Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1812.05905v2</bib_extra>
			<bib_extra key="File">1812.05905v2.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1812.05905v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1906.02771v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Improving Exploration in Soft-Actor-Critic with Normalizing Flows Policies</bib_title>
			<bib_authors>Ward, Patrick Nadeem and Smofsky, Ariella and Bose, Avishek Joey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Deep Reinforcement Learning (DRL) algorithms for continuous action spaces are known to be brittle toward hyperparameters as well as \cutbeingsample inefficient. Soft Actor Critic (SAC) proposes an off-policy deep actor critic algorithm within the maximum entropy RL framework which offers greater stability and empirical gains. The choice of policy distribution, a factored Gaussian, is motivated by \cutchosen dueits easy re-parametrization rather than its modeling power. We introduce Normalizing Flow policies within the SAC framework that learn more expressive classes of policies than simple factored Gaussians. \cutWe also present a series of stabilization tricks that enable effective training of these policies in the RL setting.We show empirically on continuous grid world tasks that our approach increases stability and is better suited to difficult exploration in sparse reward settings.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1906.02771v1</bib_extra>
			<bib_extra key="File">1906.02771v1.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1906.02771v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1606.03657v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</bib_title>
			<bib_authors>Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1606.03657v1</bib_extra>
			<bib_extra key="File">1606.03657v1.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1606.03657v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1910.00177v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning</bib_title>
			<bib_authors>Peng, Xue Bin and Kumar, Aviral and Zhang, Grace and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1910.00177v3</bib_extra>
			<bib_extra key="File">1910.00177v3.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1910.00177v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2004.07219v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>D4RL: Datasets for Deep Data-Driven Reinforcement Learning</bib_title>
			<bib_authors>Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">The offline reinforcement learning (RL) setting (also known as full batch RL), where a policy is learned from a static dataset, is compelling as progress enables RL methods to take advantage of large, previously-collected datasets, much like how the rise of large datasets has fueled results in supervised learning. However, existing online RL benchmarks are not tailored towards the offline setting and existing offline RL benchmarks are restricted to data generated by partially-trained agents, making progress in offline RL difficult to measure. In this work, we introduce benchmarks specifically designed for the offline setting, guided by key properties of datasets relevant to real-world applications of offline RL. With a focus on dataset collection, examples of such properties include: datasets generated via hand-designed controllers and human demonstrators, multitask datasets where an agent performs different tasks in the same environment, and datasets collected with mixtures of policies. By moving beyond simple benchmark tasks and data collected by partially-trained RL agents, we reveal important and unappreciated deficiencies of existing algorithms. To facilitate research, we have released our benchmark tasks and datasets with a comprehensive evaluation of existing algorithms, an evaluation protocol, and open-source examples. This serves as a common starting point for the community to identify shortcomings in existing offline RL methods and a collaborative route for progress in this emerging area.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2004.07219v4</bib_extra>
			<bib_extra key="File">2004.07219v4.pdf</bib_extra>
			<bib_extra key="Month">Apr</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2004.07219v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2002.06473v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Universal Value Density Estimation for Imitation Learning and Goal-Conditioned Reinforcement Learning</bib_title>
			<bib_authors>Schroecker, Yannick and Isbell, Charles</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">This work considers two distinct settings: imitation learning and goal-conditioned reinforcement learning. In either case, effective solutions require the agent to reliably reach a specified state (a goal), or set of states (a demonstration). Drawing a connection between probabilistic long-term dynamics and the desired value function, this work introduces an approach which utilizes recent advances in density estimation to effectively learn to reach a given state. As our first contribution, we use this approach for goal-conditioned reinforcement learning and show that it is both efficient and does not suffer from hindsight bias in stochastic domains. As our second contribution, we extend the approach to imitation learning and show that it achieves state-of-the art demonstration sample-efficiency on standard benchmark tasks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2002.06473v1</bib_extra>
			<bib_extra key="File">2002.06473v1.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2002.06473v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1901.10691v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Probability Functional Descent: A Unifying Perspective on GANs, Variational Inference, and Reinforcement Learning</bib_title>
			<bib_authors>Chu, Casey and Blanchet, Jose and Glynn, Peter</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">This paper provides a unifying view of a wide range of problems of interest in machine learning by framing them as the minimization of functionals defined on the space of probability measures. In particular, we show that generative adversarial networks, variational inference, and actor-critic methods in reinforcement learning can all be seen through the lens of our framework. We then discuss a generic optimization algorithm for our formulation, called probability functional descent (PFD), and show how this algorithm recovers existing methods developed independently in the settings mentioned earlier.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1901.10691v2</bib_extra>
			<bib_extra key="File">1901.10691v2.pdf</bib_extra>
			<bib_extra key="Month">Jan</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1901.10691v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1802.06070v6</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Diversity is All You Need: Learning Skills without a Reward Function</bib_title>
			<bib_authors>Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN (’Diversity is All You Need’), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1802.06070v6</bib_extra>
			<bib_extra key="File">1802.06070v6.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1802.06070v6</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1909.12397v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>CAQL: Continuous Action Q-Learning</bib_title>
			<bib_authors>Ryu, Moonkyung and Chow, Yinlam and Anderson, Ross and Tjandraatmadja, Christian and Boutilier, Craig</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Value-based reinforcement learning (RL) methods like Q-learning have shown success in a variety of domains. One challenge in applying Q-learning to continuous-action RL problems, however, is the continuous action maximization (max-Q) required for optimal Bellman backup. In this work, we develop CAQL, a (class of) algorithm(s) for continuous-action Q-learning that can use several plug-and-play optimizers for the max-Q problem. Leveraging recent optimization results for deep neural networks, we show that max-Q can be solved optimally using mixed-integer programming (MIP). When the Q-function representation has sufficient power, MIP-based optimization gives rise to better policies and is more robust than approximate methods (e.g., gradient ascent, cross-entropy search). We further develop several techniques to accelerate inference in CAQL, which despite their approximate nature, perform well. We compare CAQL with state-of-the-art RL algorithms on benchmark continuous-control problems that have different degrees of action constraints and show that CAQL outperforms policy-based methods in heavily constrained environments, often dramatically.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1909.12397v3</bib_extra>
			<bib_extra key="File">1909.12397v3.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1909.12397v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1606.05908v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Tutorial on Variational Autoencoders</bib_title>
			<bib_authors>Doersch, Carl</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1606.05908v3</bib_extra>
			<bib_extra key="File">1606.05908v3.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">stat.ML</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1606.05908v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2109.11052v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>On Bonus-Based Exploration Methods in the Arcade Learning Environment</bib_title>
			<bib_authors>Taïga, Adrien Ali and Fedus, William and Machado, Marlos C. and Courville, Aaron and Bellemare, Marc G.</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Research on exploration in reinforcement learning, as applied to Atari 2600 game-playing, has emphasized tackling difficult exploration problems such as Montezuma’s Revenge (Bellemare et al., 2016). Recently, bonus-based exploration methods, which explore by augmenting the environment reward, have reached above-human average performance on such domains. In this paper we reassess popular bonus-based exploration methods within a common evaluation framework. We combine Rainbow (Hessel et al., 2018) with different exploration bonuses and evaluate its performance on Montezuma’s Revenge, Bellemare et al.’s set of hard of exploration games with sparse rewards, and the whole Atari 2600 suite. We find that while exploration bonuses lead to higher score on Montezuma’s Revenge they do not provide meaningful gains over the simpler ε-greedy scheme. In fact, we find that methods that perform best on that game often underperform ε-greedy on easy exploration Atari 2600 games. We find that our conclusions remain valid even when hyperparameters are tuned for these easy-exploration games. Finally, we find that none of the methods surveyed benefit from additional training samples (1 billion frames, versus Rainbow’s 200 million) on Bellemare et al.’s hard exploration games. Our results suggest that recent gains in Montezuma’s Revenge may be better attributed to architecture change, rather than better exploration schemes; and that the real pace of progress in exploration research for Atari 2600 games may have been obfuscated by good results on a single domain.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2109.11052v1</bib_extra>
			<bib_extra key="File">2109.11052v1.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Note">Published as a conference paper at ICLR 2020</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2109.11052v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1811.06407v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Neural Predictive Belief Representations</bib_title>
			<bib_authors>Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Pires, Bernardo A. and Munos, Rémi</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Unsupervised representation learning has succeeded with excellent results in many applications. It is an especially powerful tool to learn a good representation of environments with partial or noisy observations. In partially observable domains it is important for the representation to encode a belief state, a sufficient statistic of the observations seen so far. In this paper, we investigate whether it is possible to learn such a belief representation using modern neural architectures. Specifically, we focus on one-step frame prediction and two variants of contrastive predictive coding (CPC) as the objective functions to learn the representations. To evaluate these learned representations, we test how well they can predict various pieces of information about the underlying state of the environment, e.g., position of the agent in a 3D maze. We show that all three methods are able to learn belief representations of the environment, they encode not only the state information, but also its uncertainty, a crucial aspect of belief states. We also find that for CPC multi-step predictions and action-conditioning are critical for accurate belief representations in visually complex environments. The ability of neural representations to capture the belief information has the potential to spur new advances for learning and planning in partially observable domains, where leveraging uncertainty is essential for optimal decision making.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1811.06407v2</bib_extra>
			<bib_extra key="File">1811.06407v2.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1811.06407v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1810.08102v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>First-order and second-order variants of the gradient descent in a unified framework</bib_title>
			<bib_authors>Pierrot, Thomas and Perrin, Nicolas and Sigaud, Olivier</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">In this paper, we provide an overview of first-order and second-order variants of the gradient descent method that are commonly used in machine learning. We propose a general framework in which 6 of these variants can be interpreted as different instances of the same approach. They are the vanilla gradient descent, the classical and generalized Gauss-Newton methods, the natural gradient descent method, the gradient covariance matrix approach, and Newton’s method. Besides interpreting these methods within a single framework, we explain their specificities and show under which conditions some of them coincide.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1810.08102v4</bib_extra>
			<bib_extra key="File">1810.08102v4.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1810.08102v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1507.07680v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Training recurrent networks online without backtracking</bib_title>
			<bib_authors>Ollivier, Yann and Tallec, Corentin and Charpiat, Guillaume</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2015</bib_year>
			<bib_extra key="Abstract">We introduce the &quot;NoBackTrack&quot; algorithm to train the parameters of dynamical systems such as recurrent neural networks. This algorithm works in an online, memoryless setting, thus requiring no backpropagation through time, and is scalable, avoiding the large computational and memory cost of maintaining the full gradient of the current state with respect to the parameters. The algorithm essentially maintains, at each time, a single search direction in parameter space. The evolution of this search direction is partly stochastic and is constructed in such a way to provide, at every time, an unbiased random estimate of the gradient of the loss function with respect to the parameters. Because the gradient estimate is unbiased, on average over time the parameter is updated as it should. The resulting gradient estimate can then be fed to a lightweight Kalman-like filter to yield an improved algorithm. For recurrent neural networks, the resulting algorithms scale linearly with the number of parameters. Small-scale experiments confirm the suitability of the approach, showing that the stochastic approximation of the gradient introduced in the algorithm is not detrimental to learning. In particular, the Kalman-like version of NoBackTrack is superior to backpropagation through time (BPTT) when the time span of dependencies in the data is longer than the truncation span for BPTT.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1507.07680v2</bib_extra>
			<bib_extra key="File">1507.07680v2.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.NE</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1507.07680v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2002.08803v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Support-weighted Adversarial Imitation Learning</bib_title>
			<bib_authors>Wang, Ruohan and Ciliberto, Carlo and Amadori, Pierluigi and Demiris, Yiannis</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Adversarial Imitation Learning (AIL) is a broad family of imitation learning methods designed to mimic expert behaviors from demonstrations. While AIL has shown state-of-the-art performance on imitation learning with only small number of demonstrations, it faces several practical challenges such as potential training instability and implicit reward bias. To address the challenges, we propose Support-weighted Adversarial Imitation Learning (SAIL), a general framework that extends a given AIL algorithm with information derived from support estimation of the expert policies. SAIL improves the quality of the reinforcement signals by weighing the adversarial reward with a confidence score from support estimation of the expert policy. We also show that SAIL is always at least as efficient as the underlying AIL algorithm that SAIL uses for learning the adversarial reward. Empirically, we show that the proposed method achieves better performance and training stability than baseline methods on a wide range of benchmark control tasks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2002.08803v1</bib_extra>
			<bib_extra key="File">2002.08803v1.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2002.08803v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2104.02646v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>gradSim: Differentiable simulation for system identification and visuomotor control</bib_title>
			<bib_authors>Jatavallabhula, Krishna Murthy and Macklin, Miles and Golemo, Florian and Voleti, Vikram and Petrini, Linda and Weiss, Martin and Considine, Breandan and Parent-Levesque, Jerome and Xie, Kevin and Erleben, Kenny and Paull, Liam and Shkurti, Florian and Nowrouzezahrai, Derek and Fidler, Sanja</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">We consider the problem of estimating an object’s physical properties such as mass, friction, and elasticity directly from video sequences. Such a system identification problem is fundamentally ill-posed due to the loss of information during image formation. Current solutions require precise 3D labels which are labor-intensive to gather, and infeasible to create for many systems such as deformable solids or cloth. We present gradSim, a framework that overcomes the dependence on 3D supervision by leveraging differentiable multiphysics simulation and differentiable rendering to jointly model the evolution of scene dynamics and image formation. This novel combination enables backpropagation from pixels in a video sequence through to the underlying physical attributes that generated them. Moreover, our unified computation graph – spanning from the dynamics and through the rendering process – enables learning in challenging visuomotor control tasks, without relying on state-based (3D) supervision, while obtaining performance competitive to or better than techniques that rely on precise 3D labels.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2104.02646v1</bib_extra>
			<bib_extra key="File">2104.02646v1.pdf</bib_extra>
			<bib_extra key="Month">Apr</bib_extra>
			<bib_extra key="Primaryclass">cs.CV</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2104.02646v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1903.08254v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables</bib_title>
			<bib_authors>Rakelly, Kate and Zhou, Aurick and Quillen, Deirdre and Finn, Chelsea and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While in principle meta-reinforcement learning (meta-RL) algorithms enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. The also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness in sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1903.08254v1</bib_extra>
			<bib_extra key="File">1903.08254v1.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1903.08254v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2004.12919v6</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>First return, then explore</bib_title>
			<bib_authors>Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">The promise of reinforcement learning is to solve complex sequential decision problems autonomously by specifying a high-level reward function only. However, reinforcement learning algorithms struggle when, as is often the case, simple and intuitive rewards provide sparse and deceptive feedback. Avoiding these pitfalls requires thoroughly exploring the environment, but creating algorithms that can do so remains one of the central challenges of the field. We hypothesise that the main impediment to effective exploration originates from algorithms forgetting how to reach previously visited states (&quot;detachment&quot;) and from failing to first return to a state before exploring from it (&quot;derailment&quot;). We introduce Go-Explore, a family of algorithms that addresses these two challenges directly through the simple principles of explicitly remembering promising states and first returning to such states before intentionally exploring. Go-Explore solves all heretofore unsolved Atari games and surpasses the state of the art on all hard-exploration games, with orders of magnitude improvements on the grand challenges Montezuma’s Revenge and Pitfall. We also demonstrate the practical potential of Go-Explore on a sparse-reward pick-and-place robotics task. Additionally, we show that adding a goal-conditioned policy can further improve Go-Explore’s exploration efficiency and enable it to handle stochasticity throughout training. The substantial performance gains from Go-Explore suggest that the simple principles of remembering states, returning to them, and exploring from them are a powerful and general approach to exploration, an insight that may prove critical to the creation of truly intelligent learning agents.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1038/s41586-020-03157-9</bib_extra>
			<bib_extra key="Eprint">2004.12919v6</bib_extra>
			<bib_extra key="File">2004.12919v6.pdf</bib_extra>
			<bib_extra key="Month">Apr</bib_extra>
			<bib_extra key="Note">Nature 590, 580-586 (2021)</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2004.12919v6</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1707.06887v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>A Distributional Perspective on Reinforcement Learning</bib_title>
			<bib_authors>Bellemare, Marc G. and Dabney, Will and Munos, Rémi</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman’s equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1707.06887v1</bib_extra>
			<bib_extra key="File">1707.06887v1.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1707.06887v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2002.09571v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Learning to Continually Learn</bib_title>
			<bib_authors>Beaulieu, Shawn and Frati, Lapo and Miconi, Thomas and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff and Cheney, Nick</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables context-dependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates).</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2002.09571v2</bib_extra>
			<bib_extra key="File">2002.09571v2.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2002.09571v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1509.03005v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Compatible Value Gradients for Reinforcement Learning of Continuous Deep Policies</bib_title>
			<bib_authors>Balduzzi, David and Ghifary, Muhammad</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2015</bib_year>
			<bib_extra key="Abstract">This paper proposes GProp, a deep reinforcement learning algorithm for continuous policies with compatible function approximation. The algorithm is based on two innovations. Firstly, we present a temporal-difference based method for learning the gradient of the value-function. Secondly, we present the deviator-actor-critic (DAC) model, which comprises three neural networks that estimate the value function, its gradient, and determine the actor’s policy respectively. We evaluate GProp on two challenging tasks: a contextual bandit problem constructed from nonparametric regression datasets that is designed to probe the ability of reinforcement learning algorithms to accurately estimate gradients; and the octopus arm, a challenging reinforcement learning benchmark. GProp is competitive with fully supervised methods on the bandit task and achieves the best performance to date on the octopus arm.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1509.03005v1</bib_extra>
			<bib_extra key="File">1509.03005v1.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1509.03005v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1602.01783v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Asynchronous Methods for Deep Reinforcement Learning</bib_title>
			<bib_authors>Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1602.01783v2</bib_extra>
			<bib_extra key="File">1602.01783v2.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Note">ICML 2016</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1602.01783v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1904.09237v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>On the Convergence of Adam and Beyond</bib_title>
			<bib_authors>Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ‘long-term memory’ of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1904.09237v1</bib_extra>
			<bib_extra key="File">1904.09237v1.pdf</bib_extra>
			<bib_extra key="Month">Apr</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1904.09237v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1811.02553v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>A Closer Look at Deep Policy Gradients</bib_title>
			<bib_authors>Ilyas, Andrew and Engstrom, Logan and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Madry, Aleksander</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: the surrogate objective does not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the &quot;true&quot; gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1811.02553v4</bib_extra>
			<bib_extra key="File">1811.02553v4.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1811.02553v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2002.02089v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Soft Hindsight Experience Replay</bib_title>
			<bib_authors>He, Qiwei and Zhuang, Liansheng and Li, Houqiang</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Efficient learning in the environment with sparse rewards is one of the most important challenges in Deep Reinforcement Learning (DRL). In continuous DRL environments such as robotic arms control, Hindsight Experience Replay (HER) has been shown an effective solution. However, due to the brittleness of deterministic methods, HER and its variants typically suffer from a major challenge for stability and convergence, which significantly affects the final performance. This challenge severely limits the applicability of such methods to complex real-world domains. To tackle this challenge, in this paper, we propose Soft Hindsight Experience Replay (SHER), a novel approach based on HER and Maximum Entropy Reinforcement Learning (MERL), combining the failed experiences reuse and maximum entropy probabilistic inference model. We evaluate SHER on Open AI Robotic manipulation tasks with sparse rewards. Experimental results show that, in contrast to HER and its variants, our proposed SHER achieves state-of-the-art performance, especially in the difficult HandManipulation tasks. Furthermore, our SHER method is more stable, achieving very similar performance across different random seeds.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2002.02089v1</bib_extra>
			<bib_extra key="File">2002.02089v1.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2002.02089v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2107.00541v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Goal-Conditioned Reinforcement Learning with Imagined Subgoals</bib_title>
			<bib_authors>Chane-Sane, Elliot and Schmid, Cordelia and Laptev, Ivan</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Goal-conditioned reinforcement learning endows an agent with a large variety of skills, but it often struggles to solve tasks that require more temporally extended reasoning. In this work, we propose to incorporate imagined subgoals into policy learning to facilitate learning of complex tasks. Imagined subgoals are predicted by a separate high-level policy, which is trained simultaneously with the policy and its critic. This high-level policy predicts intermediate states halfway to the goal using the value function as a reachability metric. We don’t require the policy to reach these subgoals explicitly. Instead, we use them to define a prior policy, and incorporate this prior into a KL-constrained policy iteration scheme to speed up and regularize learning. Imagined subgoals are used during policy learning, but not during test time, where we only apply the learned policy. We evaluate our approach on complex robotic navigation and manipulation tasks and show that it outperforms existing methods by a large margin.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2107.00541v1</bib_extra>
			<bib_extra key="File">2107.00541v1.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2107.00541v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1711.08946v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Action Branching Architectures for Deep Reinforcement Learning</bib_title>
			<bib_authors>Tavakoli, Arash and Pardo, Fabio and Kormushev, Petar</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">Discrete-action algorithms have been central to numerous recent successes of deep reinforcement learning. However, applying these algorithms to high-dimensional action tasks requires tackling the combinatorial increase of the number of possible actions with the number of action dimensions. This problem is further exacerbated for continuous-action tasks that require fine control of actions via discretization. In this paper, we propose a novel neural architecture featuring a shared decision module followed by several network branches, one for each action dimension. This approach achieves a linear increase of the number of network outputs with the number of degrees of freedom by allowing a level of independence for each individual action dimension. To illustrate the approach, we present a novel agent, called Branching Dueling Q-Network (BDQ), as a branching variant of the Dueling Double Deep Q-Network (Dueling DDQN). We evaluate the performance of our agent on a set of challenging continuous control tasks. The empirical results show that the proposed agent scales gracefully to environments with increasing action dimensionality and indicate the significance of the shared decision module in coordination of the distributed action branches. Furthermore, we show that the proposed agent performs competitively against a state-of-the-art continuous control algorithm, Deep Deterministic Policy Gradient (DDPG).</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1711.08946v2</bib_extra>
			<bib_extra key="File">1711.08946v2.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Note">AAAI 32: 4131-4138 (2018)</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1711.08946v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2006.13258v6</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization</bib_title>
			<bib_authors>Barde, Paul and Roy, Julien and Jeon, Wonseok and Pineau, Joelle and Pal, Christopher and Nowrouzezahrai, Derek</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Adversarial Imitation Learning alternates between learning a discriminator – which tells apart expert’s demonstrations from generated ones – and a generator’s policy to produce trajectories that can fool this discriminator. This alternated optimization is known to be delicate in practice since it compounds unstable adversarial training with brittle and sample-inefficient reinforcement learning. We propose to remove the burden of the policy optimization steps by leveraging a novel discriminator formulation. Specifically, our discriminator is explicitly conditioned on two policies: the one from the previous generator’s iteration and a learnable policy. When optimized, this discriminator directly learns the optimal generator’s policy. Consequently, our discriminator’s update solves the generator’s optimization problem for free: learning a policy that imitates the expert does not require an additional optimization loop. This formulation effectively cuts by half the implementation and computational burden of Adversarial Imitation Learning algorithms by removing the Reinforcement Learning phase altogether. We show on a variety of tasks that our simpler approach is competitive to prevalent Imitation Learning methods.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2006.13258v6</bib_extra>
			<bib_extra key="File">2006.13258v6.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Note">Advances in Neural Information Processing Systems 33 (2020)</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2006.13258v6</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1306.0514v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Riemannian metrics for neural networks II: recurrent networks and learning symbolic data sequences</bib_title>
			<bib_authors>Ollivier, Yann</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2013</bib_year>
			<bib_extra key="Abstract">Recurrent neural networks are powerful models for sequential data, able to represent complex dependencies in the sequence that simpler models such as hidden Markov models cannot handle. Yet they are notoriously hard to train. Here we introduce a training procedure using a gradient ascent in a Riemannian metric: this produces an algorithm independent from design choices such as the encoding of parameters and unit activities. This metric gradient ascent is designed to have an algorithmic cost close to backpropagation through time for sparsely connected networks. We use this procedure on gated leaky neural networks (GLNNs), a variant of recurrent neural networks with an architecture inspired by finite automata and an evolution equation inspired by continuous-time networks. GLNNs trained with a Riemannian gradient are demonstrated to effectively capture a variety of structures in synthetic problems: basic block nesting as in context-free grammars (an important feature of natural languages, but difficult to learn), intersections of multiple independent Markov-type relations, or long-distance relationships such as the distant-XOR problem. This method does not require adjusting the network structure or initial parameters: the network used is a sparse random graph and the initialization is identical for all problems considered.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1306.0514v4</bib_extra>
			<bib_extra key="File">1306.0514v4.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.NE</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1306.0514v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1603.02199v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection</bib_title>
			<bib_authors>Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Quillen, Deirdre</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1603.02199v4</bib_extra>
			<bib_extra key="File">1603.02199v4.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1603.02199v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1803.10122v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>World Models</bib_title>
			<bib_authors>Ha, David and Schmidhuber, Jürgen</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.5281/zenodo.1207631</bib_extra>
			<bib_extra key="Eprint">1803.10122v4</bib_extra>
			<bib_extra key="File">1803.10122v4.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1803.10122v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1502.03167v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</bib_title>
			<bib_authors>Ioffe, Sergey and Szegedy, Christian</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2015</bib_year>
			<bib_extra key="Abstract">Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1502.03167v3</bib_extra>
			<bib_extra key="File">1502.03167v3.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1502.03167v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1706.03762v5</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Attention Is All You Need</bib_title>
			<bib_authors>Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1706.03762v5</bib_extra>
			<bib_extra key="File">1706.03762v5.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.CL</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1706.03762v5</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1802.04821v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Evolved Policy Gradients</bib_title>
			<bib_authors>Houthooft, Rein and Chen, Richard Y. and Isola, Phillip and Stadie, Bradly C. and Wolski, Filip and Ho, Jonathan and Abbeel, Pieter</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">We propose a metalearning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal convolutions over the agent’s experience. Because this loss is highly flexible in its ability to take into account the agent’s history, it enables fast task learning. Empirical results show that our evolved policy gradient algorithm (EPG) achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method. We also demonstrate that EPG’s learned loss can generalize to out-of-distribution test time tasks, and exhibits qualitatively different behavior from other popular metalearning algorithms.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1802.04821v2</bib_extra>
			<bib_extra key="File">1802.04821v2.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1802.04821v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2103.05456v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Extended Tree Search for Robot Task and Motion Planning</bib_title>
			<bib_authors>Ren, Tianyu and Chalvatzaki, Georgia and Peters, Jan</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Integrated task and motion planning (TAMP) is desirable for generalized autonomy robots but it is challenging at the same time. TAMP requires the planner to not only search in both the large symbolic task space and the high-dimension motion space but also deal with the infeasible task actions due to its intrinsic hierarchical process. We propose a novel decision-making framework for TAMP by constructing an extended decision tree for both symbolic task planning and high-dimension motion variable binding. We integrate top-k planning for generating explicitly a skeleton space where a variety of candidate skeleton plans are at disposal. Moreover, we effectively combine this skeleton space with the resultant motion variable spaces into a single extended decision space. Accordingly, we use Monte-Carlo Tree Search (MCTS) to ensure an exploration-exploitation balance at each decision node and optimize globally to produce optimal solutions. The proposed seamless combination of symbolic top-k planning with streams, with the proved optimality of MCTS, leads to a powerful planning algorithm that can handle the combinatorial complexity of long-horizon manipulation tasks. We empirically evaluate our proposed algorithm in challenging robot tasks with different domains that require multi-stage decisions and show how our method can overcome the large task space and motion space through its effective tree search compared to its most competitive baseline method.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2103.05456v3</bib_extra>
			<bib_extra key="File">2103.05456v3.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2103.05456v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1909.01387v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Making Efficient Use of Demonstrations to Solve Hard Exploration Problems</bib_title>
			<bib_authors>Paine, Tom Le and Gulcehre, Caglar and Shahriari, Bobak and Denil, Misha and Hoffman, Matt and Soyer, Hubert and Tanburn, Richard and Kapturowski, Steven and Rabinowitz, Neil and Williams, Duncan and Barth-Maron, Gabriel and Wang, Ziyu and Freitas, Nando de and Team, Worlds</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1909.01387v1</bib_extra>
			<bib_extra key="File">1909.01387v1.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1909.01387v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2007.02832v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Maximum Entropy Gain Exploration for Long Horizon Multi-goal Reinforcement Learning</bib_title>
			<bib_authors>Pitis, Silviu and Chan, Harris and Zhao, Stephen and Stadie, Bradly and Ba, Jimmy</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">What goals should a multi-goal reinforcement learning agent pursue during training in long-horizon tasks? When the desired (test time) goal distribution is too distant to offer a useful learning signal, we argue that the agent should not pursue unobtainable goals. Instead, it should set its own intrinsic goals that maximize the entropy of the historical achieved goal distribution. We propose to optimize this objective by having the agent pursue past achieved goals in sparsely explored areas of the goal space, which focuses exploration on the frontier of the achievable goal set. We show that our strategy achieves an order of magnitude better sample efficiency than the prior state of the art on long-horizon multi-goal tasks including maze navigation and block stacking.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2007.02832v1</bib_extra>
			<bib_extra key="File">2007.02832v1.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2007.02832v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2206.07137v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt</bib_title>
			<bib_authors>Mindermann, Sören and Brauner, Jan and Razzak, Muhammed and Sharma, Mrinank and Kirsch, Andreas and Xu, Winnie and Höltgen, Benedikt and Gomez, Aidan N. and Morisot, Adrien and Farquhar, Sebastian and Gal, Yarin</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2022</bib_year>
			<bib_extra key="Abstract">Training on web-scale data can take months. But most computation and time is wasted on redundant and noisy points that are already learnt or not learnable. To accelerate training, we introduce Reducible Holdout Loss Selection (RHO-LOSS), a simple but principled technique which selects approximately those points for training that most reduce the model’s generalization loss. As a result, RHO-LOSS mitigates the weaknesses of existing data selection methods: techniques from the optimization literature typically select ’hard’ (e.g. high loss) points, but such points are often noisy (not learnable) or less task-relevant. Conversely, curriculum learning prioritizes ’easy’ points, but such points need not be trained on once learned. In contrast, RHO-LOSS selects points that are learnable, worth learning, and not yet learnt. RHO-LOSS trains in far fewer steps than prior art, improves accuracy, and speeds up training on a wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and BERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in 18x fewer steps and reaches 2% higher final accuracy than uniform data shuffling.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2206.07137v2</bib_extra>
			<bib_extra key="File">2206.07137v2.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2206.07137v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2011.04483v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>A Theory of Universal Learning</bib_title>
			<bib_authors>Bousquet, Olivier and Hanneke, Steve and Moran, Shay and Handel, Ramon van and Yehudayoff, Amir</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">How quickly can a given class of concepts be learned from examples? It is common to measure the performance of a supervised machine learning algorithm by plotting its &quot;learning curve&quot;, that is, the decay of the error rate as a function of the number of training examples. However, the classical theoretical framework for understanding learnability, the PAC model of Vapnik-Chervonenkis and Valiant, does not explain the behavior of learning curves: the distribution-free PAC model of learning can only bound the upper envelope of the learning curves over all possible data distributions. This does not match the practice of machine learning, where the data source is typically fixed in any given scenario, while the learner may choose the number of training examples on the basis of factors such as computational resources and desired accuracy. In this paper, we study an alternative learning model that better captures such practical aspects of machine learning, but still gives rise to a complete theory of the learnable in the spirit of the PAC model. More precisely, we consider the problem of universal learning, which aims to understand the performance of learning algorithms on every data distribution, but without requiring uniformity over the distribution. The main result of this paper is a remarkable trichotomy: there are only three possible rates of universal learning. More precisely, we show that the learning curves of any given concept class decay either at an exponential, linear, or arbitrarily slow rates. Moreover, each of these cases is completely characterized by appropriate combinatorial parameters, and we exhibit optimal learning algorithms that achieve the best possible rate in each case. For concreteness, we consider in this paper only the realizable case, though analogous results are expected to extend to more general learning scenarios.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2011.04483v1</bib_extra>
			<bib_extra key="File">2011.04483v1.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2011.04483v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1511.05952v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Prioritized Experience Replay</bib_title>
			<bib_authors>Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2015</bib_year>
			<bib_extra key="Abstract">Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1511.05952v4</bib_extra>
			<bib_extra key="File">1511.05952v4.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1511.05952v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1812.06298v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Residual Policy Learning</bib_title>
			<bib_authors>Silver, Tom and Allen, Kelsey and Tenenbaum, Josh and Kaelbling, Leslie</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">We present Residual Policy Learning (RPL): a simple method for improving nondifferentiable policies using model-free deep reinforcement learning. RPL thrives in complex robotic manipulation tasks where good but imperfect controllers are available. In these tasks, reinforcement learning from scratch remains data-inefficient or intractable, but learning a residual on top of the initial controller can yield substantial improvements. We study RPL in six challenging MuJoCo tasks involving partial observability, sensor noise, model misspecification, and controller miscalibration. For initial controllers, we consider both hand-designed policies and model-predictive controllers with known or learned transition models. By combining learning with control algorithms, RPL can perform long-horizon, sparse-reward tasks for which reinforcement learning alone fails. Moreover, we find that RPL consistently and substantially improves on the initial controllers. We argue that RPL is a promising approach for combining the complementary strengths of deep reinforcement learning and robotic control, pushing the boundaries of what either can achieve independently. Video and code at https://k-r-allen.github.io/residual-policy-learning/.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1812.06298v2</bib_extra>
			<bib_extra key="File">1812.06298v2.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1812.06298v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1811.01848v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control</bib_title>
			<bib_authors>Lowrey, Kendall and Rajeswaran, Aravind and Kakade, Sham and Todorov, Emanuel and Mordatch, Igor</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">We propose a plan online and learn offline (POLO) framework for the setting where an agent, with an internal model, needs to continually act and learn in the world. Our work builds on the synergistic relationship between local model-based control, global value function learning, and exploration. We study how local trajectory optimization can cope with approximation errors in the value function, and can stabilize and accelerate value function learning. Conversely, we also study how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, we also demonstrate how trajectory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning of the value function. Combining these components enable solutions to complex simulated control tasks, like humanoid locomotion and dexterous in-hand manipulation, in the equivalent of a few minutes of experience in the real world.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1811.01848v3</bib_extra>
			<bib_extra key="File">1811.01848v3.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1811.01848v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1211.0358v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Deep Gaussian Processes</bib_title>
			<bib_authors>Damianou, Andreas C. and Lawrence, Neil D.</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2012</bib_year>
			<bib_extra key="Abstract">In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1211.0358v2</bib_extra>
			<bib_extra key="File">1211.0358v2.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">stat.ML</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1211.0358v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1609.04747v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>An overview of gradient descent optimization algorithms</bib_title>
			<bib_authors>Ruder, Sebastian</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1609.04747v2</bib_extra>
			<bib_extra key="File">1609.04747v2.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1609.04747v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1705.10743v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>The Cramer Distance as a Solution to Biased Wasserstein Gradients</bib_title>
			<bib_authors>Bellemare, Marc G. and Danihelka, Ivo and Dabney, Will and Mohamed, Shakir and Lakshminarayanan, Balaji and Hoyer, Stephan and Munos, Rémi</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">The Wasserstein probability metric has received much attention from the machine learning community. Unlike the Kullback-Leibler divergence, which strictly measures change in probability, the Wasserstein metric reflects the underlying geometry between outcomes. The value of being sensitive to this geometry has been demonstrated, among others, in ordinal regression and generative modelling. In this paper we describe three natural properties of probability divergences that reflect requirements from machine learning: sum invariance, scale sensitivity, and unbiased sample gradients. The Wasserstein metric possesses the first two properties but, unlike the Kullback-Leibler divergence, does not possess the third. We provide empirical evidence suggesting that this is a serious issue in practice. Leveraging insights from probabilistic forecasting we propose an alternative to the Wasserstein metric, the Cramér distance. We show that the Cramér distance possesses all three desired properties, combining the best of the Wasserstein and Kullback-Leibler divergences. To illustrate the relevance of the Cramér distance in practice we design a new algorithm, the Cramér Generative Adversarial Network (GAN), and show that it performs significantly better than the related Wasserstein GAN.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1705.10743v1</bib_extra>
			<bib_extra key="File">1705.10743v1.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1705.10743v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2204.02372v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Jump-Start Reinforcement Learning</bib_title>
			<bib_authors>Uchendu, Ikechukwu and Xiao, Ted and Lu, Yao and Zhu, Banghua and Yan, Mengyuan and Simon, Joséphine and Bennice, Matthew and Fu, Chuyuan and Ma, Cong and Jiao, Jiantao and Levine, Sergey and Hausman, Karol</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2022</bib_year>
			<bib_extra key="Abstract">Reinforcement learning (RL) provides a theoretical framework for continuously improving an agent’s behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks with exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tasks. We show via experiments that JSRL is able to significantly outperform existing imitation and reinforcement learning algorithms, particularly in the small-data regime. In addition, we provide an upper bound on the sample complexity of JSRL and show that with the help of a guide-policy, one can improve the sample complexity for non-optimism exploration methods from exponential in horizon to polynomial.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2204.02372v1</bib_extra>
			<bib_extra key="File">2204.02372v1.pdf</bib_extra>
			<bib_extra key="Month">Apr</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2204.02372v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1806.05695v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Evolving simple programs for playing Atari games</bib_title>
			<bib_authors>Wilson, Dennis G. and Cussat-Blanc, Sylvain and Luga, Hervé and Miller, Julian F.</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Cartesian Genetic Programming (CGP) has previously shown capabilities in image processing tasks by evolving programs with a function set specialized for computer vision. A similar approach can be applied to Atari playing. Programs are evolved using mixed type CGP with a function set suited for matrix operations, including image processing, but allowing for controller behavior to emerge. While the programs are relatively small, many controllers are competitive with state of the art methods for the Atari benchmark set and require less training time. By evaluating the programs of the best evolved individuals, simple but effective strategies can be found.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1806.05695v1</bib_extra>
			<bib_extra key="File">1806.05695v1.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.NE</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1806.05695v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1903.08894v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Towards Characterizing Divergence in Deep Q-Learning</bib_title>
			<bib_authors>Achiam, Joshua and Knight, Ethan and Abbeel, Pieter</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Deep Q-Learning (DQL), a family of temporal difference algorithms for control, employs three techniques collectively known as the ‘deadly triad’ in reinforcement learning: bootstrapping, off-policy learning, and function approximation. Prior work has demonstrated that together these can lead to divergence in Q-learning algorithms, but the conditions under which divergence occurs are not well-understood. In this note, we give a simple analysis based on a linear approximation to the Q-value updates, which we believe provides insight into divergence under the deadly triad. The central point in our analysis is to consider when the leading order approximation to the deep-Q update is or is not a contraction in the sup norm. Based on this analysis, we develop an algorithm which permits stable deep Q-learning for continuous control without any of the tricks conventionally used (such as target networks, adaptive gradient optimizers, or using multiple Q functions). We demonstrate that our algorithm performs above or near state-of-the-art on standard MuJoCo benchmarks from the OpenAI Gym.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1903.08894v1</bib_extra>
			<bib_extra key="File">1903.08894v1.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1903.08894v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1703.02660v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Towards Generalization and Simplicity in Continuous Control</bib_title>
			<bib_authors>Rajeswaran, Aravind and Lowrey, Kendall and Todorov, Emanuel and Kakade, Sham</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">This work shows that policies with simple linear and RBF parameterizations can be trained to solve a variety of continuous control tasks, including the OpenAI gym benchmarks. The performance of these trained policies are competitive with state of the art results, obtained with more elaborate parameterizations such as fully connected neural networks. Furthermore, existing training and testing scenarios are shown to be very limited and prone to over-fitting, thus giving rise to only trajectory-centric policies. Training with a diverse initial state distribution is shown to produce more global policies with better generalization. This allows for interactive control scenarios where the system recovers from large on-line perturbations; as shown in the supplementary video.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1703.02660v2</bib_extra>
			<bib_extra key="File">1703.02660v2.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1703.02660v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1602.07868v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks</bib_title>
			<bib_authors>Salimans, Tim and Kingma, Diederik P.</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1602.07868v3</bib_extra>
			<bib_extra key="File">1602.07868v3.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1602.07868v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1712.09913v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Visualizing the Loss Landscape of Neural Nets</bib_title>
			<bib_authors>Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">Neural network training relies on our ability to find &quot;good&quot; minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple &quot;filter normalization&quot; method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1712.09913v3</bib_extra>
			<bib_extra key="File">1712.09913v3.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1712.09913v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2004.13649v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels</bib_title>
			<bib_authors>Kostrikov, Ilya and Yarats, Denis and Fergus, Rob</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC’s performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2004.13649v4</bib_extra>
			<bib_extra key="File">2004.13649v4.pdf</bib_extra>
			<bib_extra key="Month">Apr</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2004.13649v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1606.02396v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Deep Successor Reinforcement Learning</bib_title>
			<bib_authors>Kulkarni, Tejas D. and Saeedi, Ardavan and Gautam, Simanta and Gershman, Samuel J.</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">Learning robust value functions given raw observations and rewards is now possible with model-free and model-based deep reinforcement learning algorithms. There is a third alternative, called Successor Representations (SR), which decomposes the value function into two components – a reward predictor and a successor map. The successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards. The value function of a state can be computed as the inner product between the successor map and the reward weights. In this paper, we present DSR, which generalizes SR within an end-to-end deep reinforcement learning framework. DSR has several appealing properties including: increased sensitivity to distal reward changes due to factorization of reward and world dynamics, and the ability to extract bottleneck states (subgoals) given successor maps trained under a random policy. We show the efficacy of our approach on two diverse environments given raw pixel observations – simple grid-world domains (MazeBase) and the Doom game engine.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1606.02396v1</bib_extra>
			<bib_extra key="File">1606.02396v1.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">stat.ML</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1606.02396v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2106.01345v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Decision Transformer: Reinforcement Learning via Sequence Modeling</bib_title>
			<bib_authors>Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2106.01345v2</bib_extra>
			<bib_extra key="File">2106.01345v2.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2106.01345v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1812.02900v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Off-Policy Deep Reinforcement Learning without Exploration</bib_title>
			<bib_authors>Fujimoto, Scott and Meger, David and Precup, Doina</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with data uncorrelated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1812.02900v3</bib_extra>
			<bib_extra key="File">1812.02900v3.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1812.02900v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1505.05770v6</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Variational Inference with Normalizing Flows</bib_title>
			<bib_authors>Rezende, Danilo Jimenez and Mohamed, Shakir</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2015</bib_year>
			<bib_extra key="Abstract">The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1505.05770v6</bib_extra>
			<bib_extra key="File">1505.05770v6.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">stat.ML</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1505.05770v6</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1802.09477v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Addressing Function Approximation Error in Actor-Critic Methods</bib_title>
			<bib_authors>Fujimoto, Scott and Hoof, Herke van and Meger, David</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1802.09477v3</bib_extra>
			<bib_extra key="File">1802.09477v3.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1802.09477v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1511.06279v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Neural Programmer-Interpreters</bib_title>
			<bib_authors>Reed, Scott and Freitas, Nando de</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2015</bib_year>
			<bib_extra key="Abstract">We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-to-sequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1511.06279v4</bib_extra>
			<bib_extra key="File">1511.06279v4.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1511.06279v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2109.04504v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Bootstrapped Meta-Learning</bib_title>
			<bib_authors>Flennerhag, Sebastian and Schroecker, Yannick and Zahavy, Tom and Hasselt, Hado van and Silver, David and Singh, Satinder</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem. We propose an algorithm that tackles this problem by letting the meta-learner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that the metric can control meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta-learning. Finally, we explore how bootstrapping opens up new possibilities and find that it can meta-learn efficient exploration in an epsilon-greedy Q-learning agent, without backpropagating through the update rule.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2109.04504v2</bib_extra>
			<bib_extra key="File">2109.04504v2.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2109.04504v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2102.13651v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>On the Importance of Hyperparameter Optimization for Model-based Reinforcement Learning</bib_title>
			<bib_authors>Zhang, Baohe and Rajan, Raghu and Pineda, Luis and Lambert, Nathan and Biedenkapp, André and Chua, Kurtland and Hutter, Frank and Calandra, Roberto</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Model-based Reinforcement Learning (MBRL) is a promising framework for learning control in a data-efficient manner. MBRL algorithms can be fairly complex due to the separate dynamics modeling and the subsequent planning algorithm, and as a result, they often possess tens of hyperparameters and architectural choices. For this reason, MBRL typically requires significant human expertise before it can be applied to new problems and domains. To alleviate this problem, we propose to use automatic hyperparameter optimization (HPO). We demonstrate that this problem can be tackled effectively with automated HPO, which we demonstrate to yield significantly improved performance compared to human experts. In addition, we show that tuning of several MBRL hyperparameters dynamically, i.e. during the training itself, further improves the performance compared to using static hyperparameters which are kept fixed for the whole training. Finally, our experiments provide valuable insights into the effects of several hyperparameters, such as plan horizon or learning rate and their influence on the stability of training and resulting rewards.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2102.13651v1</bib_extra>
			<bib_extra key="File">2102.13651v1.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2102.13651v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2106.03894v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Differentiable Quality Diversity</bib_title>
			<bib_authors>Fontaine, Matthew C. and Nikolaidis, Stefanos</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Quality diversity (QD) is a growing branch of stochastic optimization research that studies the problem of generating an archive of solutions that maximize a given objective function but are also diverse with respect to a set of specified measure functions. However, even when these functions are differentiable, QD algorithms treat them as &quot;black boxes&quot;, ignoring gradient information. We present the differentiable quality diversity (DQD) problem, a special case of QD, where both the objective and measure functions are first order differentiable. We then present MAP-Elites via a Gradient Arborescence (MEGA), a DQD algorithm that leverages gradient information to efficiently explore the joint range of the objective and measure functions. Results in two QD benchmark domains and in searching the latent space of a StyleGAN show that MEGA significantly outperforms state-of-the-art QD algorithms, highlighting DQD’s promise for efficient quality diversity optimization when gradient information is available. Source code is available at https://github.com/icaros-usc/dqd.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2106.03894v3</bib_extra>
			<bib_extra key="File">2106.03894v3.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2106.03894v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2106.13281v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Brax – A Differentiable Physics Engine for Large Scale Rigid Body Simulation</bib_title>
			<bib_authors>Freeman, C. Daniel and Frey, Erik and Raichuk, Anton and Girgin, Sertan and Mordatch, Igor and Bachem, Olivier</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">We present Brax, an open source library for rigid body simulation with a focus on performance and parallelism on accelerators, written in JAX. We present results on a suite of tasks inspired by the existing reinforcement learning literature, but remade in our engine. Additionally, we provide reimplementations of PPO, SAC, ES, and direct policy optimization in JAX that compile alongside our environments, allowing the learning algorithm and the environment processing to occur on the same device, and to scale seamlessly on accelerators. Finally, we include notebooks that facilitate training of performant policies on common OpenAI Gym MuJoCo-like tasks in minutes.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2106.13281v1</bib_extra>
			<bib_extra key="File">2106.13281v1.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2106.13281v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1802.01561v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures</bib_title>
			<bib_authors>Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1802.01561v3</bib_extra>
			<bib_extra key="File">1802.01561v3.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1802.01561v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2206.03378v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Imitating Past Successes can be Very Suboptimal</bib_title>
			<bib_authors>Eysenbach, Benjamin and Udatha, Soumith and Levine, Sergey and Salakhutdinov, Ruslan</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2022</bib_year>
			<bib_extra key="Abstract">Prior work has proposed a simple strategy for reinforcement learning (RL): label experience with the outcomes achieved in that experience, and then imitate the relabeled experience. These outcome-conditioned imitation learning methods are appealing because of their simplicity, strong performance, and close ties with supervised learning. However, it remains unclear how these methods relate to the standard RL objective, reward maximization. In this paper, we prove that existing outcome-conditioned imitation learning methods do not necessarily improve the policy; rather, in some settings they can decrease the expected reward. Nonetheless, we show that a simple modification results in a method that does guarantee policy improvement, under some assumptions. Our aim is not to develop an entirely new method, but rather to explain how a variant of outcome-conditioned imitation learning can be used to maximize rewards.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2206.03378v1</bib_extra>
			<bib_extra key="File">2206.03378v1.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2206.03378v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2203.15755v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Demonstration-Bootstrapped Autonomous Practicing via Multi-Task Reinforcement Learning</bib_title>
			<bib_authors>Gupta, Abhishek and Lynch, Corey and Kinman, Brandon and Peake, Garrett and Levine, Sergey and Hausman, Karol</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2022</bib_year>
			<bib_extra key="Abstract">Reinforcement learning systems have the potential to enable continuous improvement in unstructured environments, leveraging data collected autonomously. However, in practice these systems require significant amounts of instrumentation or human intervention to learn in the real world. In this work, we propose a system for reinforcement learning that leverages multi-task reinforcement learning bootstrapped with prior data to enable continuous autonomous practicing, minimizing the number of resets needed while being able to learn temporally extended behaviors. We show how appropriately provided prior data can help bootstrap both low-level multi-task policies and strategies for sequencing these tasks one after another to enable learning with minimal resets. This mechanism enables our robotic system to practice with minimal human intervention at training time while being able to solve long horizon tasks at test time. We show the efficacy of the proposed system on a challenging kitchen manipulation task both in simulation and in the real world, demonstrating the ability to practice autonomously in order to solve temporally extended problems.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2203.15755v1</bib_extra>
			<bib_extra key="File">2203.15755v1.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2203.15755v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1905.09808v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies</bib_title>
			<bib_authors>Peng, Xue Bin and Chang, Michael and Zhang, Grace and Abbeel, Pieter and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Humans are able to perform a myriad of sophisticated tasks by drawing upon skills acquired through prior experience. For autonomous agents to have this capability, they must be able to extract reusable skills from past experience that can be recombined in new ways for subsequent tasks. Furthermore, when controlling complex high-dimensional morphologies, such as humanoid bodies, tasks often require coordination of multiple skills simultaneously. Learning discrete primitives for every combination of skills quickly becomes prohibitive. Composable primitives that can be recombined to create a large variety of behaviors can be more suitable for modeling this combinatorial explosion. In this work, we propose multiplicative compositional policies (MCP), a method for learning reusable motor skills that can be composed to produce a range of complex behaviors. Our method factorizes an agent’s skills into a collection of primitives, where multiple primitives can be activated simultaneously via multiplicative composition. This flexibility allows the primitives to be transferred and recombined to elicit new behaviors as necessary for novel tasks. We demonstrate that MCP is able to extract composable skills for highly complex simulated characters from pre-training tasks, such as motion imitation, and then reuse these skills to solve challenging continuous control tasks, such as dribbling a soccer ball to a goal, and picking up an object and transporting it to a target location.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1905.09808v1</bib_extra>
			<bib_extra key="File">1905.09808v1.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1905.09808v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1607.05077v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint Replay</bib_title>
			<bib_authors>Hosu, Ionel-Alexandru and Rebedea, Traian</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">This paper introduces a novel method for learning how to play the most difficult Atari 2600 games from the Arcade Learning Environment using deep reinforcement learning. The proposed method, human checkpoint replay, consists in using checkpoints sampled from human gameplay as starting points for the learning process. This is meant to compensate for the difficulties of current exploration strategies, such as epsilon-greedy, to find successful control policies in games with sparse rewards. Like other deep reinforcement learning architectures, our model uses a convolutional neural network that receives only raw pixel inputs to estimate the state value function. We tested our method on Montezuma’s Revenge and Private Eye, two of the most challenging games from the Atari platform. The results we obtained show a substantial improvement compared to previous learning approaches, as well as over a random player. We also propose a method for training deep reinforcement learning agents using human gameplay experience, which we call human experience replay.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1607.05077v1</bib_extra>
			<bib_extra key="File">1607.05077v1.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1607.05077v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2004.12524v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Sequential Interpretability: Methods, Applications, and Future Direction for Understanding Deep Learning Models in the Context of Sequential Data</bib_title>
			<bib_authors>Shickel, Benjamin and Rashidi, Parisa</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Deep learning continues to revolutionize an ever-growing number of critical application areas including healthcare, transportation, finance, and basic sciences. Despite their increased predictive power, model transparency and human explainability remain a significant challenge due to the &quot;black box&quot; nature of modern deep learning models. In many cases the desired balance between interpretability and performance is predominately task specific. Human-centric domains such as healthcare necessitate a renewed focus on understanding how and why these frameworks are arriving at critical and potentially life-or-death decisions. Given the quantity of research and empirical successes of deep learning for computer vision, most of the existing interpretability research has focused on image processing techniques. Comparatively, less attention has been paid to interpreting deep learning frameworks using sequential data. Given recent deep learning advancements in highly sequential domains such as natural language processing and physiological signal processing, the need for deep sequential explanations is at an all-time high. In this paper, we review current techniques for interpreting deep learning techniques involving sequential data, identify similarities to non-sequential methods, and discuss current limitations and future avenues of sequential interpretability research.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2004.12524v1</bib_extra>
			<bib_extra key="File">2004.12524v1.pdf</bib_extra>
			<bib_extra key="Month">Apr</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2004.12524v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2010.06491v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Broadly-Exploring, Local-Policy Trees for Long-Horizon Task Planning</bib_title>
			<bib_authors>Ichter, Brian and Sermanet, Pierre and Lynch, Corey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Long-horizon planning in realistic environments requires the ability to reason over sequential tasks in high-dimensional state spaces with complex dynamics. Classical motion planning algorithms, such as rapidly-exploring random trees, are capable of efficiently exploring large state spaces and computing long-horizon, sequential plans. However, these algorithms are generally challenged with complex, stochastic, and high-dimensional state spaces as well as in the presence of narrow passages, which naturally emerge in tasks that interact with the environment. Machine learning offers a promising solution for its ability to learn general policies that can handle complex interactions and high-dimensional observations. However, these policies are generally limited in horizon length. Our approach, Broadly-Exploring, Local-policy Trees (BELT), merges these two approaches to leverage the strengths of both through a task-conditioned, model-based tree search. BELT uses an RRT-inspired tree search to efficiently explore the state space. Locally, the exploration is guided by a task-conditioned, learned policy capable of performing general short-horizon tasks. This task space can be quite general and abstract; its only requirements are to be sampleable and to well-cover the space of useful tasks. This search is aided by a task-conditioned model that temporally extends dynamics propagation to allow long-horizon search and sequential reasoning over tasks. BELT is demonstrated experimentally to be able to plan long-horizon, sequential trajectories with a goal conditioned policy and generate plans that are robust.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2010.06491v1</bib_extra>
			<bib_extra key="File">2010.06491v1.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2010.06491v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1609.07152v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Input Convex Neural Networks</bib_title>
			<bib_authors>Amos, Brandon and Xu, Lei and Kolter, J. Zico</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made input-convex with a minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1609.07152v3</bib_extra>
			<bib_extra key="File">1609.07152v3.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1609.07152v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1801.00690v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>DeepMind Control Suite</bib_title>
			<bib_authors>Tassa, Yuval and Doron, Yotam and Muldal, Alistair and Erez, Tom and Li, Yazhe and Casas, Diego de Las and Budden, David and Abdolmaleki, Abbas and Merel, Josh and Lefrancq, Andrew and Lillicrap, Timothy and Riedmiller, Martin</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">The DeepMind Control Suite is a set of continuous control tasks with a standardised structure and interpretable rewards, intended to serve as performance benchmarks for reinforcement learning agents. The tasks are written in Python and powered by the MuJoCo physics engine, making them easy to use and modify. We include benchmarks for several learning algorithms. The Control Suite is publicly available at https://www.github.com/deepmind/dm_control . A video summary of all tasks is available at http://youtu.be/rAai4QzcYbs .</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1801.00690v1</bib_extra>
			<bib_extra key="File">1801.00690v1.pdf</bib_extra>
			<bib_extra key="Month">Jan</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1801.00690v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2002.00632v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Effective Diversity in Population Based Reinforcement Learning</bib_title>
			<bib_authors>Parker-Holder, Jack and Pacchiano, Aldo and Choromanski, Krzysztof and Roberts, Stephen</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Exploration is a key problem in reinforcement learning, since agents can only learn from data they acquire in the environment. With that in mind, maintaining a population of agents is an attractive method, as it allows data be collected with a diverse set of behaviors. This behavioral diversity is often boosted via multi-objective loss functions. However, those approaches typically leverage mean field updates based on pairwise distances, which makes them susceptible to cycling behaviors and increased redundancy. In addition, explicitly boosting diversity often has a detrimental impact on optimizing already fruitful behaviors for rewards. As such, the reward-diversity trade off typically relies on heuristics. Finally, such methods require behavioral representations, often handcrafted and domain specific. In this paper, we introduce an approach to optimize all members of a population simultaneously. Rather than using pairwise distance, we measure the volume of the entire population in a behavioral manifold, defined by task-agnostic behavioral embeddings. In addition, our algorithm Diversity via Determinants (DvD), adapts the degree of diversity during training using online learning techniques. We introduce both evolutionary and gradient-based instantiations of DvD and show they effectively improve exploration without reducing performance when better exploration is not required.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2002.00632v3</bib_extra>
			<bib_extra key="File">2002.00632v3.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2002.00632v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2010.05545v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Local Search for Policy Iteration in Continuous Control</bib_title>
			<bib_authors>Springenberg, Jost Tobias and Heess, Nicolas and Mankowitz, Daniel and Merel, Josh and Byravan, Arunkumar and Abdolmaleki, Abbas and Kay, Jackie and Degrave, Jonas and Schrittwieser, Julian and Tassa, Yuval and Buchli, Jonas and Belov, Dan and Riedmiller, Martin</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">We present an algorithm for local, regularized, policy improvement in reinforcement learning (RL) that allows us to formulate model-based and model-free variants in a single framework. Our algorithm can be interpreted as a natural extension of work on KL-regularized RL and introduces a form of tree search for continuous action spaces. We demonstrate that additional computation spent on model-based policy improvement during learning can improve data efficiency, and confirm that model-based policy improvement during action selection can also be beneficial. Quantitatively, our algorithm improves data efficiency on several continuous control benchmarks (when a model is learned in parallel), and it provides significant improvements in wall-clock time in high-dimensional domains (when a ground truth model is available). The unified framework also helps us to better understand the space of model-based and model-free algorithms. In particular, we demonstrate that some benefits attributed to model-based RL can be obtained without a model, simply by utilizing more computation.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2010.05545v1</bib_extra>
			<bib_extra key="File">2010.05545v1.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2010.05545v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2005.05719v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Smooth Exploration for Robotic Reinforcement Learning</bib_title>
			<bib_authors>Raffin, Antonin and Kober, Jens and Stulp, Freek</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Reinforcement learning (RL) enables robots to learn skills from interactions with the real world. In practice, the unstructured step-based exploration used in Deep RL – often very successful in simulation – leads to jerky motion patterns on real robots. Consequences of the resulting shaky behavior are poor exploration, or even damage to the robot. We address these issues by adapting state-dependent exploration (SDE) to current Deep RL algorithms. To enable this adaptation, we propose two extensions to the original SDE, using more general features and re-sampling the noise periodically, which leads to a new exploration method generalized state-dependent exploration (gSDE). We evaluate gSDE both in simulation, on PyBullet continuous control tasks, and directly on three different real robots: a tendon-driven elastic robot, a quadruped and an RC car. The noise sampling interval of gSDE permits to have a compromise between performance and smoothness, which allows training directly on the real robots without loss of performance. The code is available at https://github.com/DLR-RM/stable-baselines3.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2005.05719v2</bib_extra>
			<bib_extra key="File">2005.05719v2.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2005.05719v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1805.08380v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Optimal transport natural gradient for statistical manifolds with continuous sample space</bib_title>
			<bib_authors>Chen, Yifan and Li, Wuchen</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">We study the Wasserstein natural gradient in parametric statistical models with continuous sample spaces. Our approach is to pull back the $L^2$-Wasserstein metric tensor in the probability density space to a parameter space, equipping the latter with a positive definite metric tensor, under which it becomes a Riemannian manifold, named the Wasserstein statistical manifold. In general, it is not a totally geodesic sub-manifold of the density space, and therefore its geodesics will differ from the Wasserstein geodesics, except for the well-known Gaussian distribution case, a fact which can also be validated under our framework. We use the sub-manifold geometry to derive a gradient flow and natural gradient descent method in the parameter space. When parametrized densities lie in $\bR$, the induced metric tensor establishes an explicit formula. In optimization problems, we observe that the natural gradient descent outperforms the standard gradient descent when the Wasserstein distance is the objective function. In such a case, we prove that the resulting algorithm behaves similarly to the Newton method in the asymptotic regime. The proof calculates the exact Hessian formula for the Wasserstein distance, which further motivates another preconditioner for the optimization process. To the end, we present examples to illustrate the effectiveness of the natural gradient in several parametric statistical models, including the Gaussian measure, Gaussian mixture, Gamma distribution, and Laplace distribution.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1805.08380v4</bib_extra>
			<bib_extra key="File">1805.08380v4.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">math.OC</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1805.08380v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1611.05397v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Reinforcement Learning with Unsupervised Auxiliary Tasks</bib_title>
			<bib_authors>Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z. and Silver, David and Kavukcuoglu, Koray</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880% expert human performance, and a challenging suite of first-person, three-dimensional Labyrinth tasks leading to a mean speedup in learning of 10$\times$ and averaging 87% expert human performance on Labyrinth.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1611.05397v1</bib_extra>
			<bib_extra key="File">1611.05397v1.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1611.05397v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1201.4497v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>A geometrical introduction to screw theory</bib_title>
			<bib_authors>Minguzzi, E.</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2012</bib_year>
			<bib_extra key="Abstract">This work introduces screw theory, a venerable but yet little known theory aimed at describing rigid body dynamics. This formulation of mechanics unifies in the concept of screw the translational and rotational degrees of freedom of the body. It captures a remarkable mathematical analogy between mechanical momenta and linear velocities, and between forces and angular velocities. For instance, it clarifies that angular velocities should be treated as applied vectors and that, under the composition of motions, they sum with the same rules of applied forces. This work provides a short and rigorous introduction to screw theory intended to an undergraduate and general readership.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1088/0143-0807/34/3/613</bib_extra>
			<bib_extra key="Eprint">1201.4497v2</bib_extra>
			<bib_extra key="File">1201.4497v2.pdf</bib_extra>
			<bib_extra key="Month">Jan</bib_extra>
			<bib_extra key="Note">European Journal of Physics 34 (2013) 613 - 632</bib_extra>
			<bib_extra key="Primaryclass">math-ph</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1201.4497v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2001.02907v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Population-Guided Parallel Policy Search for Reinforcement Learning</bib_title>
			<bib_authors>Jung, Whiyoung and Park, Giseung and Sung, Youngchul</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">In this paper, a new population-guided parallel learning scheme is proposed to enhance the performance of off-policy reinforcement learning (RL). In the proposed scheme, multiple identical learners with their own value-functions and policies share a common experience replay buffer, and search a good policy in collaboration with the guidance of the best policy information. The key point is that the information of the best policy is fused in a soft manner by constructing an augmented loss function for policy update to enlarge the overall search region by the multiple learners. The guidance by the previous best policy and the enlarged range enable faster and better policy search. Monotone improvement of the expected cumulative return by the proposed scheme is proved theoretically. Working algorithms are constructed by applying the proposed scheme to the twin delayed deep deterministic (TD3) policy gradient algorithm. Numerical results show that the constructed algorithm outperforms most of the current state-of-the-art RL algorithms, and the gain is significant in the case of sparse reward environment.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2001.02907v1</bib_extra>
			<bib_extra key="File">2001.02907v1.pdf</bib_extra>
			<bib_extra key="Month">Jan</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2001.02907v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1805.12114v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models</bib_title>
			<bib_authors>Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1805.12114v2</bib_extra>
			<bib_extra key="File">1805.12114v2.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1805.12114v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1707.02286v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Emergence of Locomotion Behaviours in Rich Environments</bib_title>
			<bib_authors>Heess, Nicolas and TB, Dhruva and Sriram, Srinivasan and Lemmon, Jay and Merel, Josh and Wayne, Greg and Tassa, Yuval and Erez, Tom and Wang, Ziyu and Eslami, S. M. Ali and Riedmiller, Martin and Silver, David</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">The reinforcement learning paradigm allows, in principle, for complex behaviours to be learned directly from simple reward signals. In practice, however, it is common to carefully hand-design the reward function to encourage a particular solution, or to derive it from demonstration data. In this paper explore how a rich environment can help to promote the learning of complex behavior. Specifically, we train agents in diverse environmental contexts, and find that this encourages the emergence of robust behaviours that perform well across a suite of tasks. We demonstrate this principle for locomotion – behaviours that are known for their sensitivity to the choice of reward. We train several simulated bodies on a diverse set of challenging terrains and obstacles, using a simple reward function based on forward progress. Using a novel scalable variant of policy gradient reinforcement learning, our agents learn to run, jump, crouch and turn as required by the environment without explicit reward-based guidance. A visual depiction of highlights of the learned behavior can be viewed following https://youtu.be/hx_bgoTF7bs .</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1707.02286v2</bib_extra>
			<bib_extra key="File">1707.02286v2.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1707.02286v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1906.02691v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>An Introduction to Variational Autoencoders</bib_title>
			<bib_authors>Kingma, Diederik P. and Welling, Max</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1561/2200000056</bib_extra>
			<bib_extra key="Eprint">1906.02691v3</bib_extra>
			<bib_extra key="File">1906.02691v3.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Note">Foundations and Trends in Machine Learning: Vol. 12 (2019): No. 4, pp 307-392</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1906.02691v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>cs/0108021v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Computational Geometry Column 42</bib_title>
			<bib_authors>Mitchell, Joseph S. B. and O’Rourke, Joseph</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2001</bib_year>
			<bib_extra key="Abstract">A compendium of thirty previously published open problems in computational geometry is presented.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">cs/0108021v1</bib_extra>
			<bib_extra key="File">cs/0108021v1.pdf</bib_extra>
			<bib_extra key="Month">Aug</bib_extra>
			<bib_extra key="Note">SIGACT News, 32(3) Issue, 120 Sep. 2001, 63–72</bib_extra>
			<bib_extra key="Primaryclass">cs.CG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/cs/0108021v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2106.06860v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>A Minimalist Approach to Offline Reinforcement Learning</bib_title>
			<bib_authors>Fujimoto, Scott and Gu, Shixiang Shane</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Offline reinforcement learning (RL) defines the task of learning from a fixed batch of data. Due to errors in value estimation from out-of-distribution actions, most offline RL algorithms take the approach of constraining or regularizing the policy with the actions contained in the dataset. Built on pre-existing RL algorithms, modifications to make an RL algorithm work offline comes at the cost of additional complexity. Offline RL algorithms introduce new hyperparameters and often leverage secondary components such as generative models, while adjusting the underlying RL algorithm. In this paper we aim to make a deep RL algorithm work while making minimal changes. We find that we can match the performance of state-of-the-art offline RL algorithms by simply adding a behavior cloning term to the policy update of an online RL algorithm and normalizing the data. The resulting algorithm is a simple to implement and tune baseline, while more than halving the overall run time by removing the additional computational overhead of previous methods.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2106.06860v2</bib_extra>
			<bib_extra key="File">2106.06860v2.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2106.06860v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2006.14135v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Explainable CNN-attention Networks (C-Attention Network) for Automated Detection of Alzheimer’s Disease</bib_title>
			<bib_authors>Wang, Ning and Chen, Mingxuan and Subbalakshmi, K. P.</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">In this work, we propose three explainable deep learning architectures to automatically detect patients with Alzheimer‘s disease based on their language abilities. The architectures use: (1) only the part-of-speech features; (2) only language embedding features and (3) both of these feature classes via a unified architecture. We use self-attention mechanisms and interpretable 1-dimensional ConvolutionalNeural Network (CNN) to generate two types of explanations of the model‘s action: intra-class explanation and inter-class explanation. The inter-class explanation captures the relative importance of each of the different features in that class, while the inter-class explanation captures the relative importance between the classes. Note that although we have considered two classes of features in this paper, the architecture is easily expandable to more classes because of its modularity. Extensive experimentation and comparison with several recent models show that our method outperforms these methods with an accuracy of 92.2% and F1 score of 0.952on the DementiaBank dataset while being able to generate explanations. We show by examples, how to generate these explanations using attention values.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2006.14135v2</bib_extra>
			<bib_extra key="File">2006.14135v2.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.CL</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2006.14135v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1710.10044v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Distributional Reinforcement Learning with Quantile Regression</bib_title>
			<bib_authors>Dabney, Will and Rowland, Mark and Bellemare, Marc G. and Munos, Rémi</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">In reinforcement learning an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1710.10044v1</bib_extra>
			<bib_extra key="File">1710.10044v1.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1710.10044v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1901.10995v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Go-Explore: a New Approach for Hard-Exploration Problems</bib_title>
			<bib_authors>Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">A grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma’s Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to improve performance on hard-exploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember previously visited states, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through any available means (including by introducing determinism), then robustify via imitation learning. The combined effect of these principles is a dramatic performance improvement on hard-exploration problems. On Montezuma’s Revenge, Go-Explore scores a mean of over 43k points, almost 4 times the previous state of the art. Go-Explore can also harness human-provided domain knowledge and, when augmented with it, scores a mean of over 650k points on Montezuma’s Revenge. Its max performance of nearly 18 million surpasses the human world record, meeting even the strictest definition of &quot;superhuman&quot; performance. On Pitfall, Go-Explore with domain knowledge is the first algorithm to score above zero. Its mean score of almost 60k points exceeds expert human performance. Because Go-Explore produces high-performing demonstrations automatically and cheaply, it also outperforms imitation learning work where humans provide solution demonstrations. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in many domains, especially those that harness a simulator during training (e.g. robotics).</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1901.10995v4</bib_extra>
			<bib_extra key="File">1901.10995v4.pdf</bib_extra>
			<bib_extra key="Month">Jan</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1901.10995v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2205.11790v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Hierarchical Planning Through Goal-Conditioned Offline Reinforcement Learning</bib_title>
			<bib_authors>Li, Jinning and Tang, Chen and Tomizuka, Masayoshi and Zhan, Wei</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2022</bib_year>
			<bib_extra key="Abstract">Offline Reinforcement learning (RL) has shown potent in many safe-critical tasks in robotics where exploration is risky and expensive. However, it still struggles to acquire skills in temporally extended tasks. In this paper, we study the problem of offline RL for temporally extended tasks. We propose a hierarchical planning framework, consisting of a low-level goal-conditioned RL policy and a high-level goal planner. The low-level policy is trained via offline RL. We improve the offline training to deal with out-of-distribution goals by a perturbed goal sampling process. The high-level planner selects intermediate sub-goals by taking advantages of model-based planning methods. It plans over future sub-goal sequences based on the learned value function of the low-level policy. We adopt a Conditional Variational Autoencoder to sample meaningful high-dimensional sub-goal candidates and to solve the high-level long-term strategy optimization problem. We evaluate our proposed method in long-horizon driving and robot navigation tasks. Experiments show that our method outperforms baselines with different hierarchical designs and other regular planners without hierarchy in these complex tasks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2205.11790v1</bib_extra>
			<bib_extra key="File">2205.11790v1.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2205.11790v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1812.02256v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Relative Entropy Regularized Policy Iteration</bib_title>
			<bib_authors>Abdolmaleki, Abbas and Springenberg, Jost Tobias and Degrave, Jonas and Bohez, Steven and Tassa, Yuval and Belov, Dan and Heess, Nicolas and Riedmiller, Martin</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">We present an off-policy actor-critic algorithm for Reinforcement Learning (RL) that combines ideas from gradient-free optimization via stochastic search with learned action-value function. The result is a simple procedure consisting of three steps: i) policy evaluation by estimating a parametric action-value function; ii) policy improvement via the estimation of a local non-parametric policy; and iii) generalization by fitting a parametric policy. Each step can be implemented in different ways, giving rise to several algorithm variants. Our algorithm draws on connections to existing literature on black-box optimization and ’RL as an inference’ and it can be seen either as an extension of the Maximum a Posteriori Policy Optimisation algorithm (MPO) [Abdolmaleki et al., 2018a], or as an extension of Trust Region Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) [Abdolmaleki et al., 2017b; Hansen et al., 1997] to a policy iteration scheme. Our comparison on 31 continuous control tasks from parkour suite [Heess et al., 2017], DeepMind control suite [Tassa et al., 2018] and OpenAI Gym [Brockman et al., 2016] with diverse properties, limited amount of compute and a single set of hyperparameters, demonstrate the effectiveness of our method and the state of art results. Videos, summarizing results, can be found at goo.gl/HtvJKR .</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1812.02256v1</bib_extra>
			<bib_extra key="File">1812.02256v1.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1812.02256v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>0911.4625v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Hamilton-Jacobi formulation for reach-avoid differential games</bib_title>
			<bib_authors>Margellos, Kostas and Lygeros, John</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2009</bib_year>
			<bib_extra key="Abstract">A new framework for formulating reachability problems with competing inputs, nonlinear dynamics and state constraints as optimal control problems is developed. Such reach-avoid problems arise in, among others, the study of safety problems in hybrid systems. Earlier approaches to reach-avoid computations are either restricted to linear systems, or face numerical difficulties due to possible discontinuities in the Hamiltonian of the optimal control problem. The main advantage of the approach proposed in this paper is that it can be applied to a general class of target hitting continuous dynamic games with nonlinear dynamics, and has very good properties in terms of its numerical solution, since the value function and the Hamiltonian of the system are both continuous. The performance of the proposed method is demonstrated by applying it to a two aircraft collision avoidance scenario under target window constraints and in the presence of wind disturbance. Target Windows are a novel concept in air traffic management, and represent spatial and temporal constraints, that the aircraft have to respect to meet their schedule.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">0911.4625v1</bib_extra>
			<bib_extra key="File">0911.4625v1.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">math.OC</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/0911.4625v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1710.09829v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Dynamic Routing Between Capsules</bib_title>
			<bib_authors>Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1710.09829v2</bib_extra>
			<bib_extra key="File">1710.09829v2.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">cs.CV</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1710.09829v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1802.10567v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Learning by Playing - Solving Sparse Reward Tasks from Scratch</bib_title>
			<bib_authors>Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and Wiele, Tom Van de and Mnih, Volodymyr and Heess, Nicolas and Springenberg, Jost Tobias</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors - from scratch - in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment - enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1802.10567v1</bib_extra>
			<bib_extra key="File">1802.10567v1.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1802.10567v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1709.10089v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Overcoming Exploration in Reinforcement Learning with Demonstrations</bib_title>
			<bib_authors>Nair, Ashvin and McGrew, Bob and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1709.10089v2</bib_extra>
			<bib_extra key="File">1709.10089v2.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1709.10089v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1610.09038v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Professor Forcing: A New Algorithm for Training Recurrent Networks</bib_title>
			<bib_authors>Lamb, Alex and Goyal, Anirudh and Zhang, Ying and Zhang, Saizheng and Courville, Aaron and Bengio, Yoshua</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network’s own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1610.09038v1</bib_extra>
			<bib_extra key="File">1610.09038v1.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">stat.ML</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1610.09038v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1901.11530v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>A Geometric Perspective on Optimal Representations for Reinforcement Learning</bib_title>
			<bib_authors>Bellemare, Marc G. and Dabney, Will and Dadashi, Robert and Taiga, Adrien Ali and Castro, Pablo Samuel and Roux, Nicolas Le and Schuurmans, Dale and Lattimore, Tor and Lyle, Clare</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">We propose a new perspective on representation learning in reinforcement learning based on geometric properties of the space of value functions. We leverage this perspective to provide formal evidence regarding the usefulness of value functions as auxiliary tasks. Our formulation considers adapting the representation to minimize the (linear) approximation of the value function of all stationary policies for a given environment. We show that this optimization reduces to making accurate predictions regarding a special class of value functions which we call adversarial value functions (AVFs). We demonstrate that using value functions as auxiliary tasks corresponds to an expected-error relaxation of our formulation, with AVFs a natural candidate, and identify a close relationship with proto-value functions (Mahadevan, 2005). We highlight characteristics of AVFs and their usefulness as auxiliary tasks in a series of experiments on the four-room domain.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1901.11530v2</bib_extra>
			<bib_extra key="File">1901.11530v2.pdf</bib_extra>
			<bib_extra key="Month">Jan</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1901.11530v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2103.13834v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Self-Imitation Learning by Planning</bib_title>
			<bib_authors>Luo, Sha and Kasaei, Hamidreza and Schomaker, Lambert</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Imitation learning (IL) enables robots to acquire skills quickly by transferring expert knowledge, which is widely adopted in reinforcement learning (RL) to initialize exploration. However, in long-horizon motion planning tasks, a challenging problem in deploying IL and RL methods is how to generate and collect massive, broadly distributed data such that these methods can generalize effectively. In this work, we solve this problem using our proposed approach called self-imitation learning by planning (SILP), where demonstration data are collected automatically by planning on the visited states from the current policy. SILP is inspired by the observation that successfully visited states in the early reinforcement learning stage are collision-free nodes in the graph-search based motion planner, so we can plan and relabel robot’s own trials as demonstrations for policy learning. Due to these self-generated demonstrations, we relieve the human operator from the laborious data preparation process required by IL and RL methods in solving complex motion planning tasks. The evaluation results show that our SILP method achieves higher success rates and enhances sample efficiency compared to selected baselines, and the policy learned in simulation performs well in a real-world placement task with changing goals and obstacles.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2103.13834v2</bib_extra>
			<bib_extra key="File">2103.13834v2.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2103.13834v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2107.12808v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Open-Ended Learning Leads to Generally Capable Agents</bib_title>
			<bib_authors>Team, Open Ended Learning and Stooke, Adam and Mahajan, Anuj and Barros, Catarina and Deck, Charlie and Bauer, Jakob and Sygnowski, Jakub and Trebacz, Maja and Jaderberg, Max and Mathieu, Michael and McAleese, Nat and Bradley-Schmieg, Nathalie and Wong, Nathaniel and Porcel, Nicolas and Raileanu, Roberta and Hughes-Fitt, Steph and Dalibard, Valentin and Czarnecki, Wojciech Marian</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">In this work we create agents that can perform well beyond a single, individual task, that exhibit much wider generalisation of behaviour to a massive, rich space of challenges. We define a universe of tasks within an environment domain and demonstrate the ability to train agents that are generally capable across this vast space and beyond. The environment is natively multi-agent, spanning the continuum of competitive, cooperative, and independent games, which are situated within procedurally generated physical 3D worlds. The resulting space is exceptionally diverse in terms of the challenges posed to agents, and as such, even measuring the learning progress of an agent is an open research problem. We propose an iterative notion of improvement between successive generations of agents, rather than seeking to maximise a singular objective, allowing us to quantify progress despite tasks being incomparable in terms of achievable rewards. We show that through constructing an open-ended learning process, which dynamically changes the training task distributions and training objectives such that the agent never stops learning, we achieve consistent learning of new behaviours. The resulting agent is able to score reward in every one of our humanly solvable evaluation levels, with behaviour generalising to many held-out points in the universe of tasks. Examples of this zero-shot generalisation include good performance on Hide and Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks we characterise the behaviour of our agent, and find interesting emergent heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, and cooperation. Finally, we demonstrate that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2107.12808v2</bib_extra>
			<bib_extra key="File">2107.12808v2.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2107.12808v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1804.03906v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Discovering the Elite Hypervolume by Leveraging Interspecies Correlation</bib_title>
			<bib_authors>Vassiliades, Vassilis and Mouret, Jean-Baptiste</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Evolution has produced an astonishing diversity of species, each filling a different niche. Algorithms like MAP-Elites mimic this divergent evolutionary process to find a set of behaviorally diverse but high-performing solutions, called the elites. Our key insight is that species in nature often share a surprisingly large part of their genome, in spite of occupying very different niches; similarly, the elites are likely to be concentrated in a specific &quot;elite hypervolume&quot; whose shape is defined by their common features. In this paper, we first introduce the elite hypervolume concept and propose two metrics to characterize it: the genotypic spread and the genotypic similarity. We then introduce a new variation operator, called &quot;directional variation&quot;, that exploits interspecies (or inter-elites) correlations to accelerate the MAP-Elites algorithm. We demonstrate the effectiveness of this operator in three problems (a toy function, a redundant robotic arm, and a hexapod robot).</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1145/3205455.3205602</bib_extra>
			<bib_extra key="Eprint">1804.03906v1</bib_extra>
			<bib_extra key="File">1804.03906v1.pdf</bib_extra>
			<bib_extra key="Month">Apr</bib_extra>
			<bib_extra key="Primaryclass">cs.NE</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1804.03906v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1603.00448v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization</bib_title>
			<bib_authors>Finn, Chelsea and Levine, Sergey and Abbeel, Pieter</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1603.00448v3</bib_extra>
			<bib_extra key="File">1603.00448v3.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1603.00448v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1509.02971v6</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Continuous control with deep reinforcement learning</bib_title>
			<bib_authors>Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2015</bib_year>
			<bib_extra key="Abstract">We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1509.02971v6</bib_extra>
			<bib_extra key="File">1509.02971v6.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1509.02971v6</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1809.02925v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning</bib_title>
			<bib_authors>Kostrikov, Ilya and Agrawal, Kumar Krishna and Dwibedi, Debidatta and Levine, Sergey and Tompson, Jonathan</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">We identify two issues with the family of algorithms based on the Adversarial Imitation Learning framework. The first problem is implicit bias present in the reward functions used in these algorithms. While these biases might work well for some environments, they can also lead to sub-optimal behavior in others. Secondly, even though these algorithms can learn from few expert demonstrations, they require a prohibitively large number of interactions with the environment in order to imitate the expert for many real-world applications. In order to address these issues, we propose a new algorithm called Discriminator-Actor-Critic that uses off-policy Reinforcement Learning to reduce policy-environment interaction sample complexity by an average factor of 10. Furthermore, since our reward function is designed to be unbiased, we can apply our algorithm to many problems without making any task-specific adjustments.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1809.02925v2</bib_extra>
			<bib_extra key="File">1809.02925v2.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1809.02925v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1412.1193v11</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>New insights and perspectives on the natural gradient method</bib_title>
			<bib_authors>Martens, James</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2014</bib_year>
			<bib_extra key="Abstract">Natural gradient descent is an optimization method traditionally motivated from the perspective of information geometry, and works well for many applications as an alternative to stochastic gradient descent. In this paper we critically analyze this method and its properties, and show how it can be viewed as a type of 2nd-order optimization method, with the Fisher information matrix acting as a substitute for the Hessian. In many important cases, the Fisher information matrix is shown to be equivalent to the Generalized Gauss-Newton matrix, which both approximates the Hessian, but also has certain properties that favor its use over the Hessian. This perspective turns out to have significant implications for the design of a practical and robust natural gradient optimizer, as it motivates the use of techniques like trust regions and Tikhonov regularization. Additionally, we make a series of contributions to the understanding of natural gradient and 2nd-order methods, including: a thorough analysis of the convergence speed of stochastic natural gradient descent (and more general stochastic 2nd-order methods) as applied to convex quadratics, a critical examination of the oft-used &quot;empirical&quot; approximation of the Fisher matrix, and an analysis of the (approximate) parameterization invariance property possessed by natural gradient methods (which we show also holds for certain other curvature, but notably not the Hessian).</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1412.1193v11</bib_extra>
			<bib_extra key="File">1412.1193v11.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1412.1193v11</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2103.07607v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Solving Compositional Reinforcement Learning Problems via Task Reduction</bib_title>
			<bib_authors>Li, Yunfei and Wu, Yilin and Xu, Huazhe and Wang, Xiaolong and Wu, Yi</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/view/sir-compositional.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2103.07607v2</bib_extra>
			<bib_extra key="File">2103.07607v2.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2103.07607v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1301.3584v7</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Revisiting Natural Gradient for Deep Networks</bib_title>
			<bib_authors>Pascanu, Razvan and Bengio, Yoshua</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2013</bib_year>
			<bib_extra key="Abstract">We evaluate natural gradient, an algorithm originally proposed in Amari (1997), for learning deep models. The contributions of this paper are as follows. We show the connection between natural gradient and three other recently proposed methods for training deep models: Hessian-Free (Martens, 2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA (Le Roux et al., 2008). We describe how one can use unlabeled data to improve the generalization error obtained by natural gradient and empirically evaluate the robustness of the algorithm to the ordering of the training set compared to stochastic gradient descent. Finally we extend natural gradient to incorporate second order information alongside the manifold information and provide a benchmark of the new algorithm using a truncated Newton approach for inverting the metric matrix instead of using a diagonal approximation of it.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1301.3584v7</bib_extra>
			<bib_extra key="File">1301.3584v7.pdf</bib_extra>
			<bib_extra key="Month">Jan</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1301.3584v7</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2006.07781v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Non-local Policy Optimization via Diversity-regularized Collaborative Exploration</bib_title>
			<bib_authors>Peng, Zhenghao and Sun, Hao and Zhou, Bolei</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Conventional Reinforcement Learning (RL) algorithms usually have one single agent learning to solve the task independently. As a result, the agent can only explore a limited part of the state-action space while the learned behavior is highly correlated to the agent’s previous experience, making the training prone to a local minimum. In this work, we empower RL with the capability of teamwork and propose a novel non-local policy optimization framework called Diversity-regularized Collaborative Exploration (DiCE). DiCE utilizes a group of heterogeneous agents to explore the environment simultaneously and share the collected experiences. A regularization mechanism is further designed to maintain the diversity of the team and modulate the exploration. We implement the framework in both on-policy and off-policy settings and the experimental results show that DiCE can achieve substantial improvement over the baselines in the MuJoCo locomotion tasks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2006.07781v1</bib_extra>
			<bib_extra key="File">2006.07781v1.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2006.07781v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1707.05300v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Reverse Curriculum Generation for Reinforcement Learning</bib_title>
			<bib_authors>Florensa, Carlos and Held, David and Wulfmeier, Markus and Zhang, Michael and Abbeel, Pieter</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">Many relevant tasks require an agent to reach a certain state, or to manipulate objects into a desired configuration. For example, we might want a robot to align and assemble a gear onto an axle or insert and turn a key in a lock. These goal-oriented tasks present a considerable challenge for reinforcement learning, since their natural reward function is sparse and prohibitive amounts of exploration are required to reach the goal and receive some learning signal. Past approaches tackle these problems by exploiting expert demonstrations or by manually designing a task-specific reward shaping function to guide the learning agent. Instead, we propose a method to learn these tasks without requiring any prior knowledge other than obtaining a single state in which the task is achieved. The robot is trained in reverse, gradually learning to reach the goal from a set of start states increasingly far from the goal. Our method automatically generates a curriculum of start states that adapts to the agent’s performance, leading to efficient training on goal-oriented tasks. We demonstrate our approach on difficult simulated navigation and fine-grained manipulation problems, not solvable by state-of-the-art reinforcement learning methods.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1707.05300v3</bib_extra>
			<bib_extra key="File">1707.05300v3.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1707.05300v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1906.01563v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Hamiltonian Neural Networks</bib_title>
			<bib_authors>Greydanus, Sam and Dzamba, Misko and Yosinski, Jason</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1906.01563v3</bib_extra>
			<bib_extra key="File">1906.01563v3.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.NE</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1906.01563v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2110.10149v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Continuous Control with Action Quantization from Demonstrations</bib_title>
			<bib_authors>Dadashi, Robert and Hussenot, Léonard and Vincent, Damien and Girgin, Sertan and Raichuk, Anton and Geist, Matthieu and Pietquin, Olivier</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">In this paper, we propose a novel Reinforcement Learning (RL) framework for problems with continuous action spaces: Action Quantization from Demonstrations (AQuaDem). The proposed approach consists in learning a discretization of continuous action spaces from human demonstrations. This discretization returns a set of plausible actions (in light of the demonstrations) for each input state, thus capturing the priors of the demonstrator and their multimodal behavior. By discretizing the action space, any discrete action deep RL technique can be readily applied to the continuous control problem. Experiments show that the proposed approach outperforms state-of-the-art methods such as SAC in the RL setup, and GAIL in the Imitation Learning setup. We provide a website with interactive videos: https://google-research.github.io/aquadem/ and make the code available: https://github.com/google-research/google-research/tree/master/aquadem.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2110.10149v2</bib_extra>
			<bib_extra key="File">2110.10149v2.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2110.10149v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1202.6258v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets</bib_title>
			<bib_authors>Roux, Nicolas Le and Schmidt, Mark and Bach, Francis</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2012</bib_year>
			<bib_extra key="Abstract">We propose a new stochastic gradient method for optimizing the sum of a finite set of smooth functions, where the sum is strongly convex. While standard stochastic gradient methods converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence rate. In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard algorithms, both in terms of optimizing the training error and reducing the test error quickly.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1202.6258v4</bib_extra>
			<bib_extra key="File">1202.6258v4.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">math.OC</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1202.6258v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2204.07049v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking</bib_title>
			<bib_authors>Chen, Kai and Cao, Rui and James, Stephen and Li, Yichuan and Liu, Yun-Hui and Abbeel, Pieter and Dou, Qi</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2022</bib_year>
			<bib_extra key="Abstract">In this paper, we propose an iterative self-training framework for sim-to-real 6D object pose estimation to facilitate cost-effective robotic grasping. Given a bin-picking scenario, we establish a photo-realistic simulator to synthesize abundant virtual data, and use this to train an initial pose estimation network. This network then takes the role of a teacher model, which generates pose predictions for unlabeled real data. With these predictions, we further design a comprehensive adaptive selection scheme to distinguish reliable results, and leverage them as pseudo labels to update a student model for pose estimation on real data. To continuously improve the quality of pseudo labels, we iterate the above steps by taking the trained student model as a new teacher and re-label real data using the refined teacher model. We evaluate our method on a public benchmark and our newly-released dataset, achieving an ADD(-S) improvement of 11.49% and 22.62% respectively. Our method is also able to improve robotic bin-picking success by 19.54%, demonstrating the potential of iterative sim-to-real solutions for robotic applications.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2204.07049v2</bib_extra>
			<bib_extra key="File">2204.07049v2.pdf</bib_extra>
			<bib_extra key="Month">Apr</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2204.07049v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1912.11032v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Towards Practical Multi-Object Manipulation using Relational Reinforcement Learning</bib_title>
			<bib_authors>Li, Richard and Jabri, Allan and Darrell, Trevor and Agrawal, Pulkit</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Learning robotic manipulation tasks using reinforcement learning with sparse rewards is currently impractical due to the outrageous data requirements. Many practical tasks require manipulation of multiple objects, and the complexity of such tasks increases with the number of objects. Learning from a curriculum of increasingly complex tasks appears to be a natural solution, but unfortunately, does not work for many scenarios. We hypothesize that the inability of the state-of-the-art algorithms to effectively utilize a task curriculum stems from the absence of inductive biases for transferring knowledge from simpler to complex tasks. We show that graph-based relational architectures overcome this limitation and enable learning of complex tasks when provided with a simple curriculum of tasks with increasing numbers of objects. We demonstrate the utility of our framework on a simulated block stacking task. Starting from scratch, our agent learns to stack six blocks into a tower. Despite using step-wise sparse rewards, our method is orders of magnitude more data-efficient and outperforms the existing state-of-the-art method that utilizes human demonstrations. Furthermore, the learned policy exhibits zero-shot generalization, successfully stacking blocks into taller towers and previously unseen configurations such as pyramids, without any further training.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1912.11032v1</bib_extra>
			<bib_extra key="File">1912.11032v1.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1912.11032v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2104.02180v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control</bib_title>
			<bib_authors>Peng, Xue Bin and Ma, Ze and Abbeel, Pieter and Levine, Sergey and Kanazawa, Angjoo</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Synthesizing graceful and life-like behaviors for physically simulated characters has been a fundamental challenge in computer animation. Data-driven methods that leverage motion tracking are a prominent class of techniques for producing high fidelity motions for a wide range of behaviors. However, the effectiveness of these tracking-based methods often hinges on carefully designed objective functions, and when applied to large and diverse motion datasets, these methods require significant additional machinery to select the appropriate motion for the character to track in a given scenario. In this work, we propose to obviate the need to manually design imitation objectives and mechanisms for motion selection by utilizing a fully automated approach based on adversarial imitation learning. High-level task objectives that the character should perform can be specified by relatively simple reward functions, while the low-level style of the character’s behaviors can be specified by a dataset of unstructured motion clips, without any explicit clip selection or sequencing. These motion clips are used to train an adversarial motion prior, which specifies style-rewards for training the character through reinforcement learning (RL). The adversarial RL procedure automatically selects which motion to perform, dynamically interpolating and generalizing from the dataset. Our system produces high-quality motions that are comparable to those achieved by state-of-the-art tracking-based techniques, while also being able to easily accommodate large datasets of unstructured motion clips. Composition of disparate skills emerges automatically from the motion prior, without requiring a high-level motion planner or other task-specific annotations of the motion clips. We demonstrate the effectiveness of our framework on a diverse cast of complex simulated characters and a challenging suite of motor control tasks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1145/3450626.3459670</bib_extra>
			<bib_extra key="Eprint">2104.02180v2</bib_extra>
			<bib_extra key="File">2104.02180v2.pdf</bib_extra>
			<bib_extra key="Month">Apr</bib_extra>
			<bib_extra key="Primaryclass">cs.GR</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2104.02180v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2010.13611v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning</bib_title>
			<bib_authors>Ajay, Anurag and Kumar, Aviral and Agrawal, Pulkit and Levine, Sergey and Nachum, Ofir</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Reinforcement learning (RL) has achieved impressive performance in a variety of online settings in which an agent’s ability to query the environment for transitions and rewards is effectively unlimited. However, in many practical applications, the situation is reversed: an agent may have access to large amounts of undirected offline experience data, while access to the online environment is severely limited. In this work, we focus on this offline setting. Our main insight is that, when presented with offline data composed of a variety of behaviors, an effective way to leverage this data is to extract a continuous space of recurring and temporally extended primitive behaviors before using these primitives for downstream task learning. Primitives extracted in this way serve two purposes: they delineate the behaviors that are supported by the data from those that are not, making them useful for avoiding distributional shift in offline RL; and they provide a degree of temporal abstraction, which reduces the effective horizon yielding better learning in theory, and improved offline RL in practice. In addition to benefiting offline policy optimization, we show that performing offline primitive learning in this way can also be leveraged for improving few-shot imitation learning as well as exploration and transfer in online RL on a variety of benchmark domains. Visualizations are available at https://sites.google.com/view/opal-iclr</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2010.13611v3</bib_extra>
			<bib_extra key="File">2010.13611v3.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2010.13611v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2005.13143v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Euclideanizing Flows: Diffeomorphic Reduction for Learning Stable Dynamical Systems</bib_title>
			<bib_authors>Rana, Muhammad Asif and Li, Anqi and Fox, Dieter and Boots, Byron and Ramos, Fabio and Ratliff, Nathan</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Robotic tasks often require motions with complex geometric structures. We present an approach to learn such motions from a limited number of human demonstrations by exploiting the regularity properties of human motions e.g. stability, smoothness, and boundedness. The complex motions are encoded as rollouts of a stable dynamical system, which, under a change of coordinates defined by a diffeomorphism, is equivalent to a simple, hand-specified dynamical system. As an immediate result of using diffeomorphisms, the stability property of the hand-specified dynamical system directly carry over to the learned dynamical system. Inspired by recent works in density estimation, we propose to represent the diffeomorphism as a composition of simple parameterized diffeomorphisms. Additional structure is imposed to provide guarantees on the smoothness of the generated motions. The efficacy of this approach is demonstrated through validation on an established benchmark as well demonstrations collected on a real-world robotic system.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2005.13143v2</bib_extra>
			<bib_extra key="File">2005.13143v2.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2005.13143v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2206.02126v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Learning Dynamics and Generalization in Reinforcement Learning</bib_title>
			<bib_authors>Lyle, Clare and Rowland, Mark and Dabney, Will and Kwiatkowska, Marta and Gal, Yarin</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2022</bib_year>
			<bib_extra key="Abstract">Solving a reinforcement learning (RL) problem poses two competing challenges: fitting a potentially discontinuous value function, and generalizing well to new observations. In this paper, we analyze the learning dynamics of temporal difference algorithms to gain novel insight into the tension between these two objectives. We show theoretically that temporal difference learning encourages agents to fit non-smooth components of the value function early in training, and at the same time induces the second-order effect of discouraging generalization. We corroborate these findings in deep RL agents trained on a range of environments, finding that neural networks trained using temporal difference algorithms on dense reward tasks exhibit weaker generalization between states than randomly initialized networks and networks trained with policy gradient methods. Finally, we investigate how post-training policy distillation may avoid this pitfall, and show that this approach improves generalization to novel environments in the ProcGen suite and improves robustness to input perturbations.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2206.02126v1</bib_extra>
			<bib_extra key="File">2206.02126v1.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2206.02126v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1907.01298v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Modified Actor-Critics</bib_title>
			<bib_authors>Merdivan, Erinc and Hanke, Sten and Geist, Matthieu</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Recent successful deep reinforcement learning algorithms, such as Trust Region Policy Optimization (TRPO) or Proximal Policy Optimization (PPO), are fundamentally variations of conservative policy iteration (CPI). These algorithms iterate policy evaluation followed by a softened policy improvement step. As so, they are naturally on-policy. In this paper, we propose to combine (any kind of) soft greediness with Modified Policy Iteration (MPI). The proposed abstract framework applies repeatedly: (i) a partial policy evaluation step that allows off-policy learning and (ii) any softened greedy step. Our contribution can be seen as a new generic tool for the deep reinforcement learning toolbox. As a proof of concept, we instantiate this framework with the PPO greediness. Comparison to the original PPO shows that our algorithm is much more sample efficient. We also show that it is competitive with the state-of-art off-policy algorithm Soft Actor Critic (SAC).</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1907.01298v2</bib_extra>
			<bib_extra key="File">1907.01298v2.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1907.01298v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1611.07507v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Variational Intrinsic Control</bib_title>
			<bib_authors>Gregor, Karol and Rezende, Danilo Jimenez and Wierstra, Daan</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">In this paper we introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. The algorithms also provide an explicit measure of empowerment in a given state that can be used by an empowerment maximizing agent. The algorithm scales well with function approximation and we demonstrate the applicability of the algorithm on a range of tasks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1611.07507v1</bib_extra>
			<bib_extra key="File">1611.07507v1.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1611.07507v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1909.12892v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Automated curricula through setter-solver interactions</bib_title>
			<bib_authors>Racaniere, Sebastien and Lampinen, Andrew K. and Santoro, Adam and Reichert, David P. and Firoiu, Vlad and Lillicrap, Timothy P.</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Reinforcement learning algorithms use correlations between policies and rewards to improve agent performance. But in dynamic or sparsely rewarding environments these correlations are often too small, or rewarding events are too infrequent to make learning feasible. Human education instead relies on curricula–the breakdown of tasks into simpler, static challenges with dense rewards–to build up to complex behaviors. While curricula are also useful for artificial agents, hand-crafting them is time consuming. This has lead researchers to explore automatic curriculum generation. Here we explore automatic curriculum generation in rich, dynamic environments. Using a setter-solver paradigm we show the importance of considering goal validity, goal feasibility, and goal coverage to construct useful curricula. We demonstrate the success of our approach in rich but sparsely rewarding 2D and 3D environments, where an agent is tasked to achieve a single goal selected from a set of possible goals that varies between episodes, and identify challenges for future work. Finally, we demonstrate the value of a novel technique that guides agents towards a desired goal distribution. Altogether, these results represent a substantial step towards applying automatic task curricula to learn complex, otherwise unlearnable goals, and to our knowledge are the first to demonstrate automated curriculum generation for goal-conditioned agents in environments where the possible goals vary between episodes.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1909.12892v2</bib_extra>
			<bib_extra key="File">1909.12892v2.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Note">International Conference on Learning Representations, 2020</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1909.12892v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1902.10186v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Attention is not Explanation</bib_title>
			<bib_authors>Jain, Sarthak and Wallace, Byron C.</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work, we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful ‘explanations’ for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code for all experiments is available at https://github.com/successar/AttentionExplanation.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1902.10186v3</bib_extra>
			<bib_extra key="File">1902.10186v3.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.CL</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1902.10186v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2011.04021v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>On the role of planning in model-based deep reinforcement learning</bib_title>
			<bib_authors>Hamrick, Jessica B. and Friesen, Abram L. and Behbahani, Feryal and Guez, Arthur and Viola, Fabio and Witherspoon, Sims and Anthony, Thomas and Buesing, Lars and Veličković, Petar and Weber, Théophane</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Model-based planning is often thought to be necessary for deep, careful reasoning and generalization in artificial agents. While recent successes of model-based reinforcement learning (MBRL) with deep function approximation have strengthened this hypothesis, the resulting diversity of model-based methods has also made it difficult to track which components drive success and why. In this paper, we seek to disentangle the contributions of recent methods by focusing on three questions: (1) How does planning benefit MBRL agents? (2) Within planning, what choices drive performance? (3) To what extent does planning improve generalization? To answer these questions, we study the performance of MuZero (Schrittwieser et al., 2019), a state-of-the-art MBRL algorithm with strong connections and overlapping components with many other MBRL algorithms. We perform a number of interventions and ablations of MuZero across a wide range of environments, including control tasks, Atari, and 9x9 Go. Our results suggest the following: (1) Planning is most useful in the learning process, both for policy updates and for providing a more useful data distribution. (2) Using shallow trees with simple Monte-Carlo rollouts is as performant as more complex methods, except in the most difficult reasoning tasks. (3) Planning alone is insufficient to drive strong generalization. These results indicate where and how to utilize planning in reinforcement learning settings, and highlight a number of open questions for future MBRL research.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2011.04021v2</bib_extra>
			<bib_extra key="File">2011.04021v2.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2011.04021v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1411.1792v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>How transferable are features in deep neural networks?</bib_title>
			<bib_authors>Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2014</bib_year>
			<bib_extra key="Abstract">Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1411.1792v1</bib_extra>
			<bib_extra key="File">1411.1792v1.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Note">Advances in Neural Information Processing Systems 27, pages 3320-3328. Dec. 2014</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1411.1792v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2002.02693v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Ready Policy One: World Building Through Active Learning</bib_title>
			<bib_authors>Ball, Philip and Parker-Holder, Jack and Pacchiano, Aldo and Choromanski, Krzysztof and Roberts, Stephen</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Model-Based Reinforcement Learning (MBRL) offers a promising direction for sample efficient learning, often achieving state of the art results for continuous control tasks. However, many existing MBRL methods rely on combining greedy policies with exploration heuristics, and even those which utilize principled exploration bonuses construct dual objectives in an ad hoc fashion. In this paper we introduce Ready Policy One (RP1), a framework that views MBRL as an active learning problem, where we aim to improve the world model in the fewest samples possible. RP1 achieves this by utilizing a hybrid objective function, which crucially adapts during optimization, allowing the algorithm to trade off reward v.s. exploration at different stages of learning. In addition, we introduce a principled mechanism to terminate sample collection once we have a rich enough trajectory batch to improve the model. We rigorously evaluate our method on a variety of continuous control tasks, and demonstrate statistically significant gains over existing approaches.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2002.02693v1</bib_extra>
			<bib_extra key="File">2002.02693v1.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2002.02693v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2112.10751v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>RvS: What is Essential for Offline RL via Supervised Learning?</bib_title>
			<bib_authors>Emmons, Scott and Eysenbach, Benjamin and Kostrikov, Ilya and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Recent work has shown that supervised learning alone, without temporal difference (TD) learning, can be remarkably effective for offline RL. When does this hold true, and which algorithmic components are necessary? Through extensive experiments, we boil supervised learning for offline RL down to its essential elements. In every environment suite we consider, simply maximizing likelihood with a two-layer feedforward MLP is competitive with state-of-the-art results of substantially more complex methods based on TD learning or sequence modeling with Transformers. Carefully choosing model capacity (e.g., via regularization or architecture) and choosing which information to condition on (e.g., goals or rewards) are critical for performance. These insights serve as a field guide for practitioners doing Reinforcement Learning via Supervised Learning (which we coin &quot;RvS learning&quot;). They also probe the limits of existing RvS methods, which are comparatively weak on random data, and suggest a number of open problems.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2112.10751v2</bib_extra>
			<bib_extra key="File">2112.10751v2.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2112.10751v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2109.00157v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>A Survey of Exploration Methods in Reinforcement Learning</bib_title>
			<bib_authors>Amin, Susan and Gomrokchi, Maziar and Satija, Harsh and Hoof, Herke van and Precup, Doina</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Exploration is an essential component of reinforcement learning algorithms, where agents need to learn how to predict and control unknown and often stochastic environments. Reinforcement learning agents depend crucially on exploration to obtain informative data for the learning process as the lack of enough information could hinder effective learning. In this article, we provide a survey of modern exploration methods in (Sequential) reinforcement learning, as well as a taxonomy of exploration methods.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2109.00157v2</bib_extra>
			<bib_extra key="File">2109.00157v2.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2109.00157v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1111.4259v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Krylov Subspace Descent for Deep Learning</bib_title>
			<bib_authors>Vinyals, Oriol and Povey, Daniel</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2011</bib_year>
			<bib_extra key="Abstract">In this paper, we propose a second order optimization method to learn models where both the dimensionality of the parameter space and the number of training samples is high. In our method, we construct on each iteration a Krylov subspace formed by the gradient and an approximation to the Hessian matrix, and then use a subset of the training data samples to optimize over this subspace. As with the Hessian Free (HF) method of [7], the Hessian matrix is never explicitly constructed, and is computed using a subset of data. In practice, as in HF, we typically use a positive definite substitute for the Hessian matrix such as the Gauss-Newton matrix. We investigate the effectiveness of our proposed method on deep neural networks, and compare its performance to widely used methods such as stochastic gradient descent, conjugate gradient descent and L-BFGS, and also to HF. Our method leads to faster convergence than either L-BFGS or HF, and generally performs better than either of them in cross-validation accuracy. It is also simpler and more general than HF, as it does not require a positive semi-definite approximation of the Hessian matrix to work well nor the setting of a damping parameter. The chief drawback versus HF is the need for memory to store a basis for the Krylov subspace.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1111.4259v1</bib_extra>
			<bib_extra key="File">1111.4259v1.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">stat.ML</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1111.4259v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1903.01973v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Learning Latent Plans from Play</bib_title>
			<bib_authors>Lynch, Corey and Khansari, Mohi and Xiao, Ted and Kumar, Vikash and Tompson, Jonathan and Levine, Sergey and Sermanet, Pierre</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Acquiring a diverse repertoire of general-purpose skills remains an open challenge for robotics. In this work, we propose self-supervising control on top of human teleoperated play data as a way to scale up skill learning. Play has two properties that make it attractive compared to conventional task demonstrations. Play is cheap, as it can be collected in large quantities quickly without task segmenting, labeling, or resetting to an initial state. Play is naturally rich, covering 4x more interaction space than task demonstrations for the same amount of collection time. To learn control from play, we introduce Play-LMP, a self-supervised method that learns to organize play behaviors in a latent space, then reuse them at test time to achieve specific goals. Combining self-supervised control with a diverse play dataset shifts the focus of skill learning from a narrow and discrete set of tasks to the full continuum of behaviors available in an environment. We find that this combination generalizes well empirically–-after self-supervising on unlabeled play, our method substantially outperforms individual expert-trained policies on 18 difficult user-specified visual manipulation tasks in a simulated robotic tabletop environment. We additionally find that play-supervised models, unlike their expert-trained counterparts, are more robust to perturbations and exhibit retrying-till-success behaviors. Finally, we find that our agent organizes its latent plan space around functional tasks, despite never being trained with task labels. Videos, code and data are available at learning-from-play.github.io</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1903.01973v2</bib_extra>
			<bib_extra key="File">1903.01973v2.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1903.01973v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1603.00748v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Continuous Deep Q-Learning with Model-based Acceleration</bib_title>
			<bib_authors>Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of model-free algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normalized adantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1603.00748v1</bib_extra>
			<bib_extra key="File">1603.00748v1.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1603.00748v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2001.06940v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Reinforcement Learning with Probabilistically Complete Exploration</bib_title>
			<bib_authors>Morere, Philippe and Francis, Gilad and Blau, Tom and Ramos, Fabio</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Balancing exploration and exploitation remains a key challenge in reinforcement learning (RL). State-of-the-art RL algorithms suffer from high sample complexity, particularly in the sparse reward case, where they can do no better than to explore in all directions until the first positive rewards are found. To mitigate this, we propose Rapidly Randomly-exploring Reinforcement Learning (R3L). We formulate exploration as a search problem and leverage widely-used planning algorithms such as Rapidly-exploring Random Tree (RRT) to find initial solutions. These solutions are used as demonstrations to initialize a policy, then refined by a generic RL algorithm, leading to faster and more stable convergence. We provide theoretical guarantees of R3L exploration finding successful solutions, as well as bounds for its sampling complexity. We experimentally demonstrate the method outperforms classic and intrinsic exploration techniques, requiring only a fraction of exploration samples and achieving better asymptotic performance.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2001.06940v1</bib_extra>
			<bib_extra key="File">2001.06940v1.pdf</bib_extra>
			<bib_extra key="Month">Jan</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2001.06940v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1811.11214v5</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Understanding the impact of entropy on policy optimization</bib_title>
			<bib_authors>Ahmed, Zafarali and Roux, Nicolas Le and Norouzi, Mohammad and Schuurmans, Dale</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Entropy regularization is commonly used to improve policy optimization in reinforcement learning. It is believed to help with exploration by encouraging the selection of more stochastic policies. In this work, we analyze this claim using new visualizations of the optimization landscape based on randomly perturbing the loss function. We first show that even with access to the exact gradient, policy optimization is difficult due to the geometry of the objective function. Then, we qualitatively show that in some environments, a policy with higher entropy can make the optimization landscape smoother, thereby connecting local optima and enabling the use of larger learning rates. This paper presents new tools for understanding the optimization landscape, shows that policy entropy serves as a regularizer, and highlights the challenge of designing general-purpose policy optimization algorithms.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1811.11214v5</bib_extra>
			<bib_extra key="File">1811.11214v5.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1811.11214v5</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1812.02648v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Deep Reinforcement Learning and the Deadly Triad</bib_title>
			<bib_authors>Hasselt, Hado van and Doron, Yotam and Strub, Florian and Hessel, Matteo and Sonnerat, Nicolas and Modayil, Joseph</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">We know from reinforcement learning theory that temporal difference learning can fail in certain cases. Sutton and Barto (2018) identify a deadly triad of function approximation, bootstrapping, and off-policy learning. When these three properties are combined, learning can diverge with the value estimates becoming unbounded. However, several algorithms successfully combine these three properties, which indicates that there is at least a partial gap in our understanding. In this work, we investigate the impact of the deadly triad in practice, in the context of a family of popular deep reinforcement learning models - deep Q-networks trained with experience replay - analysing how the components of this system play a role in the emergence of the deadly triad, and in the agent’s performance</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1812.02648v1</bib_extra>
			<bib_extra key="File">1812.02648v1.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1812.02648v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2001.00449v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Continuous-Discrete Reinforcement Learning for Hybrid Control in Robotics</bib_title>
			<bib_authors>Neunert, Michael and Abdolmaleki, Abbas and Wulfmeier, Markus and Lampe, Thomas and Springenberg, Jost Tobias and Hafner, Roland and Romano, Francesco and Buchli, Jonas and Heess, Nicolas and Riedmiller, Martin</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Many real-world control problems involve both discrete decision variables - such as the choice of control modes, gear switching or digital outputs - as well as continuous decision variables - such as velocity setpoints, control gains or analogue outputs. However, when defining the corresponding optimal control or reinforcement learning problem, it is commonly approximated with fully continuous or fully discrete action spaces. These simplifications aim at tailoring the problem to a particular algorithm or solver which may only support one type of action space. Alternatively, expert heuristics are used to remove discrete actions from an otherwise continuous space. In contrast, we propose to treat hybrid problems in their ’native’ form by solving them with hybrid reinforcement learning, which optimizes for discrete and continuous actions simultaneously. In our experiments, we first demonstrate that the proposed approach efficiently solves such natively hybrid reinforcement learning problems. We then show, both in simulation and on robotic hardware, the benefits of removing possibly imperfect expert-designed heuristics. Lastly, hybrid reinforcement learning encourages us to rethink problem definitions. We propose reformulating control problems, e.g. by adding meta actions, to improve exploration or reduce mechanical wear and tear.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2001.00449v1</bib_extra>
			<bib_extra key="File">2001.00449v1.pdf</bib_extra>
			<bib_extra key="Month">Jan</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2001.00449v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1802.08163v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>An Analysis of Categorical Distributional Reinforcement Learning</bib_title>
			<bib_authors>Rowland, Mark and Bellemare, Marc G. and Dabney, Will and Munos, Rémi and Teh, Yee Whye</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Distributional approaches to value-based reinforcement learning model the entire distribution of returns, rather than just their expected values, and have recently been shown to yield state-of-the-art empirical performance. This was demonstrated by the recently proposed C51 algorithm, based on categorical distributional reinforcement learning (CDRL) [Bellemare et al., 2017]. However, the theoretical properties of CDRL algorithms are not yet well understood. In this paper, we introduce a framework to analyse CDRL algorithms, establish the importance of the projected distributional Bellman operator in distributional RL, draw fundamental connections between CDRL and the Cramér distance, and give a proof of convergence for sample-based categorical distributional reinforcement learning algorithms.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1802.08163v1</bib_extra>
			<bib_extra key="File">1802.08163v1.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">stat.ML</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1802.08163v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1903.02219v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Training in Task Space to Speed Up and Guide Reinforcement Learning</bib_title>
			<bib_authors>Bellegarda, Guillaume and Byl, Katie</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Recent breakthroughs in the reinforcement learning (RL) community have made significant advances towards learning and deploying policies on real world robotic systems. However, even with the current state-of-the-art algorithms and computational resources, these algorithms are still plagued with high sample complexity, and thus long training times, especially for high degree of freedom (DOF) systems. There are also concerns arising from lack of perceived stability or robustness guarantees from emerging policies. This paper aims at mitigating these drawbacks by: (1) modeling a complex, high DOF system with a representative simple one, (2) making explicit use of forward and inverse kinematics without forcing the RL algorithm to &quot;learn&quot; them on its own, and (3) learning locomotion policies in Cartesian space instead of joint space. In this paper these methods are applied to JPL’s Robosimian, but can be readily used on any system with a base and end effector(s). These locomotion policies can be produced in just a few minutes, trained on a single laptop. We compare the robustness of the resulting learned policies to those of other control methods. An accompanying video for this paper can be found at https://youtu.be/xDxxSw5ahnc .</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1903.02219v1</bib_extra>
			<bib_extra key="File">1903.02219v1.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1903.02219v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2005.08290v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Longitudinal high-throughput TCR repertoire profiling reveals the dynamics of T cell memory formation after mild COVID-19 infection</bib_title>
			<bib_authors>Minervina, Anastasia A. and Komech, Ekaterina A. and Titov, Aleksei and Koraichi, Meriem Bensouda and Rosati, Elisa and Mamedov, Ilgar Z. and Franke, Andre and Efimov, Grigory A. and Chudakov, Dmitriy M. and Mora, Thierry and Walczak, Aleksandra M. and Lebedev, Yuri B. and Pogorelyy, Mikhail V.</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">COVID-19 is a global pandemic caused by the SARS-CoV-2 coronavirus. T cells play a key role in the adaptive antiviral immune response by killing infected cells and facilitating the selection of virus-specific antibodies. However neither the dynamics and cross-reactivity of the SARS-CoV-2-specific T cell response nor the diversity of resulting immune memory are well understood. In this study we use longitudinal high-throughput T cell receptor (TCR) sequencing to track changes in the T cell repertoire following two mild cases of COVID-19. In both donors we identified CD4+ and CD8+ T cell clones with transient clonal expansion after infection. The antigen specificity of CD8+ TCR sequences to SARS-CoV-2 epitopes was confirmed by both MHC tetramer binding and presence in large database of SARS-CoV-2 epitope-specific TCRs. We describe characteristic motifs in TCR sequences of COVID-19-reactive clones and show preferential occurence of these motifs in publicly available large dataset of repertoires from COVID-19 patients. We show that in both donors the majority of infection-reactive clonotypes acquire memory phenotypes. Certain T cell clones were detected in the memory fraction at the pre-infection timepoint, suggesting participation of pre-existing cross-reactive memory T cells in the immune response to SARS-CoV-2.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.7554/eLife.63502</bib_extra>
			<bib_extra key="Eprint">2005.08290v3</bib_extra>
			<bib_extra key="File">2005.08290v3.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Note">eLife 10 e63502 (2020)</bib_extra>
			<bib_extra key="Primaryclass">q-bio.GN</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2005.08290v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2008.12228v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Towards General and Autonomous Learning of Core Skills: A Case Study in Locomotion</bib_title>
			<bib_authors>Hafner, Roland and Hertweck, Tim and Klöppner, Philipp and Bloesch, Michael and Neunert, Michael and Wulfmeier, Markus and Tunyasuvunakool, Saran and Heess, Nicolas and Riedmiller, Martin</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Modern Reinforcement Learning (RL) algorithms promise to solve difficult motor control problems directly from raw sensory inputs. Their attraction is due in part to the fact that they can represent a general class of methods that allow to learn a solution with a reasonably set reward and minimal prior knowledge, even in situations where it is difficult or expensive for a human expert. For RL to truly make good on this promise, however, we need algorithms and learning setups that can work across a broad range of problems with minimal problem specific adjustments or engineering. In this paper, we study this idea of generality in the locomotion domain. We develop a learning framework that can learn sophisticated locomotion behavior for a wide spectrum of legged robots, such as bipeds, tripeds, quadrupeds and hexapods, including wheeled variants. Our learning framework relies on a data-efficient, off-policy multi-task RL algorithm and a small set of reward functions that are semantically identical across robots. To underline the general applicability of the method, we keep the hyper-parameter settings and reward definitions constant across experiments and rely exclusively on on-board sensing. For nine different types of robots, including a real-world quadruped robot, we demonstrate that the same algorithm can rapidly learn diverse and reusable locomotion skills without any platform specific adjustments or additional instrumentation of the learning setup.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2008.12228v1</bib_extra>
			<bib_extra key="File">2008.12228v1.pdf</bib_extra>
			<bib_extra key="Month">Aug</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2008.12228v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1611.01224v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Sample Efficient Actor-Critic with Experience Replay</bib_title>
			<bib_authors>Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and Freitas, Nando de</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1611.01224v2</bib_extra>
			<bib_extra key="File">1611.01224v2.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1611.01224v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2103.15309v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Robust Feedback Motion Policy Design Using Reinforcement Learning on a 3D Digit Bipedal Robot</bib_title>
			<bib_authors>Castillo, Guillermo A. and Weng, Bowen and Zhang, Wei and Hereid, Ayonga</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">In this paper, a hierarchical and robust framework for learning bipedal locomotion is presented and successfully implemented on the 3D biped robot Digit built by Agility Robotics. We propose a cascade-structure controller that combines the learning process with intuitive feedback regulations. This design allows the framework to realize robust and stable walking with a reduced-dimension state and action spaces of the policy, significantly simplifying the design and reducing the sampling efficiency of the learning method. The inclusion of feedback regulation into the framework improves the robustness of the learned walking gait and ensures the success of the sim-to-real transfer of the proposed controller with minimal tuning. We specifically present a learning pipeline that considers hardware-feasible initial poses of the robot within the learning process to ensure the initial state of the learning is replicated as close as possible to the initial state of the robot in hardware experiments. Finally, we demonstrate the feasibility of our method by successfully transferring the learned policy in simulation to the Digit robot hardware, realizing sustained walking gaits under external force disturbances and challenging terrains not included during the training process. To the best of our knowledge, this is the first time a learning-based policy is transferred successfully to the Digit robot in hardware experiments without using dynamic randomization or curriculum learning.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2103.15309v1</bib_extra>
			<bib_extra key="File">2103.15309v1.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2103.15309v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2206.07568v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Contrastive Learning as Goal-Conditioned Reinforcement Learning</bib_title>
			<bib_authors>Eysenbach, Benjamin and Zhang, Tianjun and Salakhutdinov, Ruslan and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2022</bib_year>
			<bib_extra key="Abstract">In reinforcement learning (RL), it is easier to solve a task if given a good representation. While deep RL should automatically acquire such good representations, prior work often finds that learning representations in an end-to-end fashion is unstable and instead equip RL algorithms with additional representation learning parts (e.g., auxiliary losses, data augmentation). How can we design RL algorithms that directly acquire good representations? In this paper, instead of adding representation learning parts to an existing RL algorithm, we show (contrastive) representation learning methods can be cast as RL algorithms in their own right. To do this, we build upon prior work and apply contrastive representation learning to action-labeled trajectories, in such a way that the (inner product of) learned representations exactly corresponds to a goal-conditioned value function. We use this idea to reinterpret a prior RL method as performing contrastive learning, and then use the idea to propose a much simpler method that achieves similar performance. Across a range of goal-conditioned RL tasks, we demonstrate that contrastive RL methods achieve higher success rates than prior non-contrastive methods, including in the offline RL setting. We also show that contrastive RL outperforms prior methods on image-based tasks, without using data augmentation or auxiliary objectives.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2206.07568v1</bib_extra>
			<bib_extra key="File">2206.07568v1.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2206.07568v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2005.12729v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO</bib_title>
			<bib_authors>Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Madry, Aleksander</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">We study the roots of algorithmic progress in deep policy gradient algorithms through a case study on two popular algorithms: Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO). Specifically, we investigate the consequences of &quot;code-level optimizations:&quot; algorithm augmentations found only in implementations or described as auxiliary details to the core algorithm. Seemingly of secondary importance, such optimizations turn out to have a major impact on agent behavior. Our results show that they (a) are responsible for most of PPO’s gain in cumulative reward over TRPO, and (b) fundamentally change how RL methods function. These insights show the difficulty and importance of attributing performance gains in deep reinforcement learning. Code for reproducing our results is available at https://github.com/MadryLab/implementation-matters .</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2005.12729v1</bib_extra>
			<bib_extra key="File">2005.12729v1.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2005.12729v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1705.05035v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Discrete Sequential Prediction of Continuous Actions for Deep RL</bib_title>
			<bib_authors>Metz, Luke and Ibarz, Julian and Jaitly, Navdeep and Davidson, James</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">It has long been assumed that high dimensional continuous control problems cannot be solved effectively by discretizing individual dimensions of the action space due to the exponentially large number of bins over which policies would have to be learned. In this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces. Central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that predict one dimension at a time. Specifically, we show how Q-values and policies over continuous spaces can be modeled using a next step prediction model over discretized dimensions. With this parameterization, it is possible to both leverage the compositional structure of action spaces during learning, as well as compute maxima over action spaces (approximately). On a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG. We apply the technique to off-policy (Q-learning) methods and show that our method can achieve the state-of-the-art for off-policy methods on several continuous control tasks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1705.05035v3</bib_extra>
			<bib_extra key="File">1705.05035v3.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1705.05035v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1607.00485v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Group Sparse Regularization for Deep Neural Networks</bib_title>
			<bib_authors>Scardapane, Simone and Comminiello, Danilo and Hussain, Amir and Uncini, Aurelio</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">In this paper, we consider the joint task of simultaneously optimizing (i) the weights of a deep neural network, (ii) the number of neurons for each hidden layer, and (iii) the subset of active input features (i.e., feature selection). While these problems are generally dealt with separately, we present a simple regularized formulation allowing to solve all three of them in parallel, using standard optimization routines. Specifically, we extend the group Lasso penalty (originated in the linear regression literature) in order to impose group-level sparsity on the network’s connections, where each group is defined as the set of outgoing weights from a unit. Depending on the specific case, the weights can be related to an input variable, to a hidden neuron, or to a bias unit, thus performing simultaneously all the aforementioned tasks in order to obtain a compact network. We perform an extensive experimental evaluation, by comparing with classical weight decay and Lasso penalties. We show that a sparse version of the group Lasso penalty is able to achieve competitive performances, while at the same time resulting in extremely compact networks with a smaller number of input features. We evaluate both on a toy dataset for handwritten digit recognition, and on multiple realistic large-scale classification problems.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1016/j.neucom.2017.02.029</bib_extra>
			<bib_extra key="Eprint">1607.00485v1</bib_extra>
			<bib_extra key="File">1607.00485v1.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">stat.ML</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1607.00485v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2109.13841v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Bottom-Up Skill Discovery from Unsegmented Demonstrations for Long-Horizon Robot Manipulation</bib_title>
			<bib_authors>Zhu, Yifeng and Stone, Peter and Zhu, Yuke</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">We tackle real-world long-horizon robot manipulation tasks through skill discovery. We present a bottom-up approach to learning a library of reusable skills from unsegmented demonstrations and use these skills to synthesize prolonged robot behaviors. Our method starts with constructing a hierarchical task structure from each demonstration through agglomerative clustering. From the task structures of multi-task demonstrations, we identify skills based on the recurring patterns and train goal-conditioned sensorimotor policies with hierarchical imitation learning. Finally, we train a meta controller to compose these skills to solve long-horizon manipulation tasks. The entire model can be trained on a small set of human demonstrations collected within 30 minutes without further annotations, making it amendable to real-world deployment. We systematically evaluated our method in simulation environments and on a real robot. Our method has shown superior performance over state-of-the-art imitation learning methods in multi-stage manipulation tasks. Furthermore, skills discovered from multi-task demonstrations boost the average task success by $8%$ compared to those discovered from individual tasks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2109.13841v2</bib_extra>
			<bib_extra key="File">2109.13841v2.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2109.13841v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2105.12196v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>From Motor Control to Team Play in Simulated Humanoid Football</bib_title>
			<bib_authors>Liu, Siqi and Lever, Guy and Wang, Zhe and Merel, Josh and Eslami, S. M. Ali and Hennes, Daniel and Czarnecki, Wojciech M. and Tassa, Yuval and Omidshafiei, Shayegan and Abdolmaleki, Abbas and Siegel, Noah Y. and Hasenclever, Leonard and Marris, Luke and Tunyasuvunakool, Saran and Song, H. Francis and Wulfmeier, Markus and Muller, Paul and Haarnoja, Tuomas and Tracey, Brendan D. and Tuyls, Karl and Graepel, Thore and Heess, Nicolas</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Intelligent behaviour in the physical world exhibits structure at multiple spatial and temporal scales. Although movements are ultimately executed at the level of instantaneous muscle tensions or joint torques, they must be selected to serve goals defined on much longer timescales, and in terms of relations that extend far beyond the body itself, ultimately involving coordination with other agents. Recent research in artificial intelligence has shown the promise of learning-based approaches to the respective problems of complex movement, longer-term planning and multi-agent coordination. However, there is limited research aimed at their integration. We study this problem by training teams of physically simulated humanoid avatars to play football in a realistic virtual environment. We develop a method that combines imitation learning, single- and multi-agent reinforcement learning and population-based training, and makes use of transferable representations of behaviour for decision making at different levels of abstraction. In a sequence of stages, players first learn to control a fully articulated body to perform realistic, human-like movements such as running and turning; they then acquire mid-level football skills such as dribbling and shooting; finally, they develop awareness of others and play as a team, bridging the gap between low-level motor control at a timescale of milliseconds, and coordinated goal-directed behaviour as a team at the timescale of tens of seconds. We investigate the emergence of behaviours at different levels of abstraction, as well as the representations that underlie these behaviours using several analysis techniques, including statistics from real-world sports analytics. Our work constitutes a complete demonstration of integrated decision-making at multiple scales in a physically embodied multi-agent setting. See project video at https://youtu.be/KHMwq9pv7mg.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2105.12196v1</bib_extra>
			<bib_extra key="File">2105.12196v1.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2105.12196v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1705.07874v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>A Unified Approach to Interpreting Model Predictions</bib_title>
			<bib_authors>Lundberg, Scott and Lee, Su-In</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">Understanding why a model makes a certain prediction can be as crucial as the prediction’s accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1705.07874v2</bib_extra>
			<bib_extra key="File">1705.07874v2.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1705.07874v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2109.08522v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Dynamics-Aware Quality-Diversity for Efficient Learning of Skill Repertoires</bib_title>
			<bib_authors>Lim, Bryan and Grillotti, Luca and Bernasconi, Lorenzo and Cully, Antoine</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Quality-Diversity (QD) algorithms are powerful exploration algorithms that allow robots to discover large repertoires of diverse and high-performing skills. However, QD algorithms are sample inefficient and require millions of evaluations. In this paper, we propose Dynamics-Aware Quality-Diversity (DA-QD), a framework to improve the sample efficiency of QD algorithms through the use of dynamics models. We also show how DA-QD can then be used for continual acquisition of new skill repertoires. To do so, we incrementally train a deep dynamics model from experience obtained when performing skill discovery using QD. We can then perform QD exploration in imagination with an imagined skill repertoire. We evaluate our approach on three robotic experiments. First, our experiments show DA-QD is 20 times more sample efficient than existing QD approaches for skill discovery. Second, we demonstrate learning an entirely new skill repertoire in imagination to perform zero-shot learning. Finally, we show how DA-QD is useful and effective for solving a long horizon navigation task and for damage adaptation in the real world. Videos and source code are available at: https://sites.google.com/view/da-qd.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1109/ICRA46639.2022.9811559</bib_extra>
			<bib_extra key="Eprint">2109.08522v1</bib_extra>
			<bib_extra key="File">2109.08522v1.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2109.08522v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1905.12941v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Learning Compositional Neural Programs with Recursive Tree Search and Planning</bib_title>
			<bib_authors>Pierrot, Thomas and Ligner, Guillaume and Reed, Scott and Sigaud, Olivier and Perrin, Nicolas and Laterre, Alexandre and Kas, David and Beguir, Karim and Freitas, Nando de</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">We propose a novel reinforcement learning algorithm, AlphaNPI, that incorporates the strengths of Neural Programmer-Interpreters (NPI) and AlphaZero. NPI contributes structural biases in the form of modularity, hierarchy and recursion, which are helpful to reduce sample complexity, improve generalization and increase interpretability. AlphaZero contributes powerful neural network guided search algorithms, which we augment with recursion. AlphaNPI only assumes a hierarchical program specification with sparse rewards: 1 when the program execution satisfies the specification, and 0 otherwise. Using this specification, AlphaNPI is able to train NPI models effectively with RL for the first time, completely eliminating the need for strong supervision in the form of execution traces. The experiments show that AlphaNPI can sort as well as previous strongly supervised NPI variants. The AlphaNPI agent is also trained on a Tower of Hanoi puzzle with two disks and is shown to generalize to puzzles with an arbitrary number of disk</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1905.12941v2</bib_extra>
			<bib_extra key="File">1905.12941v2.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1905.12941v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1803.06773v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Composable Deep Reinforcement Learning for Robotic Manipulation</bib_title>
			<bib_authors>Haarnoja, Tuomas and Pong, Vitchyr and Zhou, Aurick and Dalal, Murtaza and Abbeel, Pieter and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Model-free deep reinforcement learning has been shown to exhibit good performance in domains ranging from video games to simulated robotic manipulation and locomotion. However, model-free methods are known to perform poorly when the interaction time with the environment is limited, as is the case for most real-world robotic tasks. In this paper, we study how maximum entropy policies trained using soft Q-learning can be applied to real-world robotic manipulation. The application of this method to real-world manipulation is facilitated by two important features of soft Q-learning. First, soft Q-learning can learn multimodal exploration strategies by learning policies represented by expressive energy-based models. Second, we show that policies learned with soft Q-learning can be composed to create new policies, and that the optimality of the resulting policy can be bounded in terms of the divergence between the composed policies. This compositionality provides an especially valuable tool for real-world manipulation, where constructing new policies by composing existing skills can provide a large gain in efficiency over training from scratch. Our experimental evaluation demonstrates that soft Q-learning is substantially more sample efficient than prior model-free deep reinforcement learning methods, and that compositionality can be performed for both simulated and real-world tasks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1803.06773v1</bib_extra>
			<bib_extra key="File">1803.06773v1.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1803.06773v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1605.01278v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>A Bayesian Approach to Policy Recognition and State Representation Learning</bib_title>
			<bib_authors>Šošić, Adrian and Zoubir, Abdelhak M. and Koeppl, Heinz</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">Learning from demonstration (LfD) is the process of building behavioral models of a task from demonstrations provided by an expert. These models can be used e.g. for system control by generalizing the expert demonstrations to previously unencountered situations. Most LfD methods, however, make strong assumptions about the expert behavior, e.g. they assume the existence of a deterministic optimal ground truth policy or require direct monitoring of the expert’s controls, which limits their practical use as part of a general system identification framework. In this work, we consider the LfD problem in a more general setting where we allow for arbitrary stochastic expert policies, without reasoning about the optimality of the demonstrations. Following a Bayesian methodology, we model the full posterior distribution of possible expert controllers that explain the provided demonstration data. Moreover, we show that our methodology can be applied in a nonparametric context to infer the complexity of the state representation used by the expert, and to learn task-appropriate partitionings of the system state space.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1109/TPAMI.2017.2711024</bib_extra>
			<bib_extra key="Eprint">1605.01278v4</bib_extra>
			<bib_extra key="File">1605.01278v4.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">stat.ML</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1605.01278v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1912.06680v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Dota 2 with Large Scale Deep Reinforcement Learning</bib_title>
			<bib_authors>OpenAI, and :, and Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Dębiak, Przemysław and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and Józefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique P. d. O. and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1912.06680v1</bib_extra>
			<bib_extra key="File">1912.06680v1.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1912.06680v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1707.01891v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Trust-PCL: An Off-Policy Trust Region Method for Continuous Control</bib_title>
			<bib_authors>Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">Trust region methods, such as TRPO, are often used to stabilize policy optimization algorithms in reinforcement learning (RL). While current trust region strategies are effective for continuous control, they typically require a prohibitively large amount of on-policy interaction with the environment. To address this problem, we propose an off-policy trust region method, Trust-PCL. The algorithm is the result of observing that the optimal policy and state values of a maximum reward objective with a relative-entropy regularizer satisfy a set of multi-step pathwise consistencies along any path. Thus, Trust-PCL is able to maintain optimization stability while exploiting off-policy data to improve sample efficiency. When evaluated on a number of continuous control tasks, Trust-PCL improves the solution quality and sample efficiency of TRPO.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1707.01891v3</bib_extra>
			<bib_extra key="File">1707.01891v3.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1707.01891v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1906.12189v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Learning-based Model Predictive Control for Safe Exploration and Reinforcement Learning</bib_title>
			<bib_authors>Koller, Torsten and Berkenkamp, Felix and Turchetta, Matteo and Boedecker, Joschka and Krause, Andreas</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Reinforcement learning has been successfully used to solve difficult tasks in complex unknown environments. However, these methods typically do not provide any safety guarantees during the learning process. This is particularly problematic, since reinforcement learning agent actively explore their environment. This prevents their use in safety-critical, real-world applications. In this paper, we present a learning-based model predictive control scheme that provides high-probability safety guarantees throughout the learning process. Based on a reliable statistical model, we construct provably accurate confidence intervals on predicted trajectories. Unlike previous approaches, we allow for input-dependent uncertainties. Based on these reliable predictions, we guarantee that trajectories satisfy safety constraints. Moreover, we use a terminal set constraint to recursively guarantee the existence of safe control actions at every iteration. We evaluate the resulting algorithm to safely explore the dynamics of an inverted pendulum and to solve a reinforcement learning task on a cart-pole system with safety constraints.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1906.12189v1</bib_extra>
			<bib_extra key="File">1906.12189v1.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">eess.SY</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1906.12189v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1806.07366v5</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Neural Ordinary Differential Equations</bib_title>
			<bib_authors>Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1806.07366v5</bib_extra>
			<bib_extra key="File">1806.07366v5.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1806.07366v5</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2104.07749v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills</bib_title>
			<bib_authors>Chebotar, Yevgen and Hausman, Karol and Lu, Yao and Xiao, Ted and Kalashnikov, Dmitry and Varley, Jake and Irpan, Alex and Eysenbach, Benjamin and Julian, Ryan and Finn, Chelsea and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">We consider the problem of learning useful robotic skills from previously collected offline data without access to manually specified rewards or additional online exploration, a setting that is becoming increasingly important for scaling robot learning by reusing past robotic data. In particular, we propose the objective of learning a functional understanding of the environment by learning to reach any goal state in a given dataset. We employ goal-conditioned Q-learning with hindsight relabeling and develop several techniques that enable training in a particularly challenging offline setting. We find that our method can operate on high-dimensional camera images and learn a variety of skills on real robots that generalize to previously unseen scenes and objects. We also show that our method can learn to reach long-horizon goals across multiple episodes through goal chaining, and learn rich representations that can help with downstream tasks through pre-training or auxiliary objectives. The videos of our experiments can be found at https://actionable-models.github.io</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2104.07749v3</bib_extra>
			<bib_extra key="File">2104.07749v3.pdf</bib_extra>
			<bib_extra key="Month">Apr</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2104.07749v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1804.00379v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Recall Traces: Backtracking Models for Efficient Reinforcement Learning</bib_title>
			<bib_authors>Goyal, Anirudh and Brakel, Philemon and Fedus, William and Singhal, Soumye and Lillicrap, Timothy and Levine, Sergey and Larochelle, Hugo and Bengio, Yoshua</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">In many environments only a tiny subset of all states yield high reward. In these cases, few of the interactions with the environment provide a relevant learning signal. Hence, we may want to preferentially train on those high-reward states and the probable trajectories leading to them. To this end, we advocate for the use of a backtracking model that predicts the preceding states that terminate at a given high-reward state. We can train a model which, starting from a high value state (or one that is estimated to have high value), predicts and sample for which the (state, action)-tuples may have led to that high value state. These traces of (state, action) pairs, which we refer to as Recall Traces, sampled from this backtracking model starting from a high value state, are informative as they terminate in good states, and hence we can use these traces to improve a policy. We provide a variational interpretation for this idea and a practical algorithm in which the backtracking model samples from an approximate posterior distribution over trajectories which lead to large rewards. Our method improves the sample efficiency of both on- and off-policy RL algorithms across several environments and tasks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1804.00379v2</bib_extra>
			<bib_extra key="File">1804.00379v2.pdf</bib_extra>
			<bib_extra key="Month">Apr</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1804.00379v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1811.12560v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>An Introduction to Deep Reinforcement Learning</bib_title>
			<bib_authors>Francois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1561/2200000071</bib_extra>
			<bib_extra key="Eprint">1811.12560v2</bib_extra>
			<bib_extra key="File">1811.12560v2.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Note">Foundations and Trends in Machine Learning: Vol. 11, No. 3-4, 2018</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1811.12560v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1511.02540v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Speed learning on the fly</bib_title>
			<bib_authors>Massé, Pierre-Yves and Ollivier, Yann</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2015</bib_year>
			<bib_extra key="Abstract">The practical performance of online stochastic gradient descent algorithms is highly dependent on the chosen step size, which must be tediously hand-tuned in many applications. The same is true for more advanced variants of stochastic gradients, such as SAGA, SVRG, or AdaGrad. Here we propose to adapt the step size by performing a gradient descent on the step size itself, viewing the whole performance of the learning trajectory as a function of step size. Importantly, this adaptation can be computed online at little cost, without having to iterate backward passes over the full data.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1511.02540v1</bib_extra>
			<bib_extra key="File">1511.02540v1.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">math.OC</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1511.02540v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2203.01387v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems</bib_title>
			<bib_authors>Prudencio, Rafael Figueiredo and Maximo, Marcos R. O. A. and Colombini, Esther Luna</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2022</bib_year>
			<bib_extra key="Abstract">With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications such as education, healthcare, and robotics. In this work, we propose a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field, and a review of existing benchmarks’ properties and shortcomings. Finally, we provide our perspective on open problems and propose future research directions for this rapidly growing field.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2203.01387v2</bib_extra>
			<bib_extra key="File">2203.01387v2.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2203.01387v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1706.03662v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Practical Gauss-Newton Optimisation for Deep Learning</bib_title>
			<bib_authors>Botev, Aleksandar and Ritter, Hippolyt and Barber, David</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">We present an efficient block-diagonal ap- proximation to the Gauss-Newton matrix for feedforward neural networks. Our result- ing algorithm is competitive against state- of-the-art first order optimisation methods, with sometimes significant improvement in optimisation performance. Unlike first-order methods, for which hyperparameter tuning of the optimisation parameters is often a labo- rious process, our approach can provide good performance even when used with default set- tings. A side result of our work is that for piecewise linear transfer functions, the net- work objective function can have no differ- entiable local maxima, which may partially explain why such transfer functions facilitate effective optimisation.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1706.03662v2</bib_extra>
			<bib_extra key="File">1706.03662v2.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">stat.ML</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1706.03662v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2001.08116v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Q-Learning in enormous action spaces via amortized approximate maximization</bib_title>
			<bib_authors>Wiele, Tom Van de and Warde-Farley, David and Mnih, Andriy and Mnih, Volodymyr</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Applying Q-learning to high-dimensional or continuous action spaces can be difficult due to the required maximization over the set of possible actions. Motivated by techniques from amortized inference, we replace the expensive maximization over all actions with a maximization over a small subset of possible actions sampled from a learned proposal distribution. The resulting approach, which we dub Amortized Q-learning (AQL), is able to handle discrete, continuous, or hybrid action spaces while maintaining the benefits of Q-learning. Our experiments on continuous control tasks with up to 21 dimensional actions show that AQL outperforms D3PG (Barth-Maron et al, 2018) and QT-Opt (Kalashnikov et al, 2018). Experiments on structured discrete action spaces demonstrate that AQL can efficiently learn good policies in spaces with thousands of discrete actions.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2001.08116v1</bib_extra>
			<bib_extra key="File">2001.08116v1.pdf</bib_extra>
			<bib_extra key="Month">Jan</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2001.08116v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1910.01738v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>State Representation Learning from Demonstration</bib_title>
			<bib_authors>Merckling, Astrid and Coninx, Alexandre and Cressot, Loic and Doncieux, Stéphane and Perrin-Gilbert, Nicolas</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Robots could learn their own state and world representation from perception and experience without supervision. This desirable goal is the main focus of our field of interest, state representation learning (SRL). Indeed, a compact representation of such a state is beneficial to help robots grasp onto their environment for interacting. The properties of this representation have a strong impact on the adaptive capability of the agent. In this article we present an approach based on imitation learning. The idea is to train several policies that share the same representation to reproduce various demonstrations. To do so, we use a multi-head neural network with a shared state representation feeding a task-specific agent. If the demonstrations are diverse, the trained representation will eventually contain the information necessary for all tasks, while discarding irrelevant information. As such, it will potentially become a compact state representation useful for new tasks. We call this approach SRLfD (State Representation Learning from Demonstration). Our experiments confirm that when a controller takes SRLfD-based representations as input, it can achieve better performance than with other representation strategies and promote more efficient reinforcement learning (RL) than with an end-to-end RL strategy.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1007/978-3-030-64580-9_26</bib_extra>
			<bib_extra key="Eprint">1910.01738v2</bib_extra>
			<bib_extra key="File">1910.01738v2.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1910.01738v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1802.07245v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Meta-Reinforcement Learning of Structured Exploration Strategies</bib_title>
			<bib_authors>Gupta, Abhishek and Mendonca, Russell and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Exploration is a fundamental challenge in reinforcement learning (RL). Many of the current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we explore how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm – model agnostic exploration with structured noise (MAESN) – to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1802.07245v1</bib_extra>
			<bib_extra key="File">1802.07245v1.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1802.07245v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1911.06636v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Catch &amp; Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks</bib_title>
			<bib_authors>Merel, Josh and Tunyasuvunakool, Saran and Ahuja, Arun and Tassa, Yuval and Hasenclever, Leonard and Pham, Vu and Erez, Tom and Wayne, Greg and Heess, Nicolas</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">We address the longstanding challenge of producing flexible, realistic humanoid character controllers that can perform diverse whole-body tasks involving object interactions. This challenge is central to a variety of fields, from graphics and animation to robotics and motor neuroscience. Our physics-based environment uses realistic actuation and first-person perception – including touch sensors and egocentric vision – with a view to producing active-sensing behaviors (e.g. gaze direction), transferability to real robots, and comparisons to the biology. We develop an integrated neural-network based approach consisting of a motor primitive module, human demonstrations, and an instructed reinforcement learning regime with curricula and task variations. We demonstrate the utility of our approach for several tasks, including goal-conditioned box carrying and ball catching, and we characterize its behavioral robustness. The resulting controllers can be deployed in real-time on a standard PC. See overview video, https://youtu.be/2rQAW-8gQQk .</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1911.06636v2</bib_extra>
			<bib_extra key="File">1911.06636v2.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1911.06636v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1705.06366v5</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Automatic Goal Generation for Reinforcement Learning Agents</bib_title>
			<bib_authors>Florensa, Carlos and Held, David and Geng, Xinyang and Abbeel, Pieter</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing. We use a generator network to propose tasks for the agent to try to achieve, specified as goal states. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1705.06366v5</bib_extra>
			<bib_extra key="File">1705.06366v5.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1705.06366v5</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1906.09807v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Proximal Distilled Evolutionary Reinforcement Learning</bib_title>
			<bib_authors>Bodnar, Cristian and Day, Ben and Lió, Pietro</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Reinforcement Learning (RL) has achieved impressive performance in many complex environments due to the integration with Deep Neural Networks (DNNs). At the same time, Genetic Algorithms (GAs), often seen as a competing approach to RL, had limited success in scaling up to the DNNs required to solve challenging tasks. Contrary to this dichotomic view, in the physical world, evolution and learning are complementary processes that continuously interact. The recently proposed Evolutionary Reinforcement Learning (ERL) framework has demonstrated mutual benefits to performance when combining the two methods. However, ERL has not fully addressed the scalability problem of GAs. In this paper, we show that this problem is rooted in an unfortunate combination of a simple genetic encoding for DNNs and the use of traditional biologically-inspired variation operators. When applied to these encodings, the standard operators are destructive and cause catastrophic forgetting of the traits the networks acquired. We propose a novel algorithm called Proximal Distilled Evolutionary Reinforcement Learning (PDERL) that is characterised by a hierarchical integration between evolution and learning. The main innovation of PDERL is the use of learning-based variation operators that compensate for the simplicity of the genetic representation. Unlike traditional operators, our proposals meet the functional requirements of variation operators when applied on directly-encoded DNNs. We evaluate PDERL in five robot locomotion settings from the OpenAI gym. Our method outperforms ERL, as well as two state-of-the-art RL algorithms, PPO and TD3, in all tested environments.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1609/aaai.v34i04.5728</bib_extra>
			<bib_extra key="Eprint">1906.09807v4</bib_extra>
			<bib_extra key="File">1906.09807v4.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Note">Vol 34 No 04: AAAI 2020 Technical Track on Machine Learning 3283-3290</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1906.09807v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1811.11711v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Neural probabilistic motor primitives for humanoid control</bib_title>
			<bib_authors>Merel, Josh and Hasenclever, Leonard and Galashov, Alexandre and Ahuja, Arun and Pham, Vu and Wayne, Greg and Teh, Yee Whye and Heess, Nicolas</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">We focus on the problem of learning a single motor module that can flexibly express a range of behaviors for the control of high-dimensional physically simulated humanoids. To do this, we propose a motor architecture that has the general structure of an inverse model with a latent-variable bottleneck. We show that it is possible to train this model entirely offline to compress thousands of expert policies and learn a motor primitive embedding space. The trained neural probabilistic motor primitive system can perform one-shot imitation of whole-body humanoid behaviors, robustly mimicking unseen trajectories. Additionally, we demonstrate that it is also straightforward to train controllers to reuse the learned motor primitive space to solve tasks, and the resulting movements are relatively naturalistic. To support the training of our model, we compare two approaches for offline policy cloning, including an experience efficient method which we call linear feedback policy cloning. We encourage readers to view a supplementary video ( https://youtu.be/CaDEf-QcKwA ) summarizing our results.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1811.11711v2</bib_extra>
			<bib_extra key="File">1811.11711v2.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1811.11711v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1805.04874v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>GAN Q-learning</bib_title>
			<bib_authors>Doan, Thang and Mazoure, Bogdan and Lyle, Clare</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Distributional reinforcement learning (distributional RL) has seen empirical success in complex Markov Decision Processes (MDPs) in the setting of nonlinear function approximation. However, there are many different ways in which one can leverage the distributional approach to reinforcement learning. In this paper, we propose GAN Q-learning, a novel distributional RL method based on generative adversarial networks (GANs) and analyze its performance in simple tabular environments, as well as OpenAI Gym. We empirically show that our algorithm leverages the flexibility and blackbox approach of deep learning models while providing a viable alternative to traditional methods.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1805.04874v3</bib_extra>
			<bib_extra key="File">1805.04874v3.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">stat.ML</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1805.04874v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1706.04223v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Adversarially Regularized Autoencoders</bib_title>
			<bib_authors>Zhao, Jake and Kim, Yoon and Zhang, Kelly and Rush, Alexander M. and LeCun, Yann</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">Deep latent variable models, trained using variational autoencoders or generative adversarial networks, are now a key technique for representation learning of continuous structures. However, applying similar methods to discrete structures, such as text sequences or discretized images, has proven to be more challenging. In this work, we propose a flexible method for training deep latent variable models of discrete structures. Our approach is based on the recently-proposed Wasserstein autoencoder (WAE) which formalizes the adversarial autoencoder (AAE) as an optimal transport problem. We first extend this framework to model discrete sequences, and then further explore different learned priors targeting a controllable representation. This adversarially regularized autoencoder (ARAE) allows us to generate natural textual outputs as well as perform manipulations in the latent space to induce change in the output space. Finally we show that the latent representation can be trained to perform unaligned textual style transfer, giving improvements both in automatic/human evaluation compared to existing methods.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1706.04223v3</bib_extra>
			<bib_extra key="File">1706.04223v3.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1706.04223v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2102.05599v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Improving Model-Based Reinforcement Learning with Internal State Representations through Self-Supervision</bib_title>
			<bib_authors>Scholz, Julien and Weber, Cornelius and Hafez, Muhammad Burhan and Wermter, Stefan</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Using a model of the environment, reinforcement learning agents can plan their future moves and achieve superhuman performance in board games like Chess, Shogi, and Go, while remaining relatively sample-efficient. As demonstrated by the MuZero Algorithm, the environment model can even be learned dynamically, generalizing the agent to many more tasks while at the same time achieving state-of-the-art performance. Notably, MuZero uses internal state representations derived from real environment states for its predictions. In this paper, we bind the model’s predicted internal state representation to the environment state via two additional terms: a reconstruction model loss and a simpler consistency loss, both of which work independently and unsupervised, acting as constraints to stabilize the learning process. Our experiments show that this new integration of reconstruction model loss and simpler consistency loss provide a significant performance increase in OpenAI Gym environments. Our modifications also enable self-supervised pretraining for MuZero, so the algorithm can learn about environment dynamics before a goal is made available.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1109/IJCNN52387.2021.9534023</bib_extra>
			<bib_extra key="Eprint">2102.05599v1</bib_extra>
			<bib_extra key="File">2102.05599v1.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Note">Proc. Intl. Joint Conf. Neural Networks (IJCNN), 2021, forthcoming</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2102.05599v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2005.04269v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics</bib_title>
			<bib_authors>Kuznetsov, Arsenii and Shvechikov, Pavel and Grishin, Alexander and Vetrov, Dmitry</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">The overestimation bias is one of the major impediments to accurate off-policy learning. This paper investigates a novel way to alleviate the overestimation bias in a continuous control setting. Our method–-Truncated Quantile Critics, TQC,–-blends three ideas: distributional representation of a critic, truncation of critics prediction, and ensembling of multiple critics. Distributional representation and truncation allow for arbitrary granular overestimation control, while ensembling provides additional score improvements. TQC outperforms the current state of the art on all environments from the continuous control benchmark suite, demonstrating 25% improvement on the most challenging Humanoid environment.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2005.04269v1</bib_extra>
			<bib_extra key="File">2005.04269v1.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2005.04269v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1911.11679v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>The problem with DDPG: understanding failures in deterministic environments with sparse rewards</bib_title>
			<bib_authors>Matheron, Guillaume and Perrin, Nicolas and Sigaud, Olivier</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">In environments with continuous state and action spaces, state-of-the-art actor-critic reinforcement learning algorithms can solve very complex problems, yet can also fail in environments that seem trivial, but the reason for such failures is still poorly understood. In this paper, we contribute a formal explanation of these failures in the particular case of sparse reward and deterministic environments. First, using a very elementary control problem, we illustrate that the learning process can get stuck into a fixed point corresponding to a poor solution. Then, generalizing from the studied example, we provide a detailed analysis of the underlying mechanisms which results in a new understanding of one of the convergence regimes of these algorithms. The resulting perspective casts a new light on already existing solutions to the issues we have highlighted, and suggests other potential approaches.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1007/978-3-030-61616-8_25</bib_extra>
			<bib_extra key="Eprint">1911.11679v1</bib_extra>
			<bib_extra key="File">1911.11679v1.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1911.11679v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2010.11944v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Accelerating Reinforcement Learning with Learned Skill Priors</bib_title>
			<bib_authors>Pertsch, Karl and Lee, Youngwoon and Lim, Joseph J.</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Intelligent agents rely heavily on prior experience when learning a new task, yet most modern reinforcement learning (RL) approaches learn every task from scratch. One approach for leveraging prior knowledge is to transfer skills learned on prior tasks to the new task. However, as the amount of prior experience increases, the number of transferable skills grows too, making it challenging to explore the full set of available skills during downstream learning. Yet, intuitively, not all skills should be explored with equal probability; for example information about the current state can hint which skills are promising to explore. In this work, we propose to implement this intuition by learning a prior over skills. We propose a deep latent variable model that jointly learns an embedding space of skills and the skill prior from offline agent experience. We then extend common maximum-entropy RL approaches to use skill priors to guide downstream learning. We validate our approach, SPiRL (Skill-Prior RL), on complex navigation and robotic manipulation tasks and show that learned skill priors are essential for effective skill transfer from rich datasets. Videos and code are available at https://clvrai.com/spirl.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2010.11944v1</bib_extra>
			<bib_extra key="File">2010.11944v1.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2010.11944v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1708.09251v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Quality and Diversity Optimization: A Unifying Modular Framework</bib_title>
			<bib_authors>Cully, Antoine and Demiris, Yiannis</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">The optimization of functions to find the best solution according to one or several objectives has a central role in many engineering and research fields. Recently, a new family of optimization algorithms, named Quality-Diversity optimization, has been introduced, and contrasts with classic algorithms. Instead of searching for a single solution, Quality-Diversity algorithms are searching for a large collection of both diverse and high-performing solutions. The role of this collection is to cover the range of possible solution types as much as possible, and to contain the best solution for each type. The contribution of this paper is threefold. Firstly, we present a unifying framework of Quality-Diversity optimization algorithms that covers the two main algorithms of this family (Multi-dimensional Archive of Phenotypic Elites and the Novelty Search with Local Competition), and that highlights the large variety of variants that can be investigated within this family. Secondly, we propose algorithms with a new selection mechanism for Quality-Diversity algorithms that outperforms all the algorithms tested in this paper. Lastly, we present a new collection management that overcomes the erosion issues observed when using unstructured collections. These three contributions are supported by extensive experimental comparisons of Quality-Diversity algorithms on three different experimental scenarios.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1708.09251v1</bib_extra>
			<bib_extra key="File">1708.09251v1.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">cs.NE</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1708.09251v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2010.02193v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Mastering Atari with Discrete World Models</bib_title>
			<bib_authors>Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Intelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, Dreamer V2 reaches 200M frames and surpasses the final performance of the top single-GPU agents IQN and Rainbow. DreamerV2 is also applicable to tasks with continuous actions, where it learns an accurate world model of a complex humanoid robot and solves stand-up and walking from only pixel inputs.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2010.02193v4</bib_extra>
			<bib_extra key="File">2010.02193v4.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2010.02193v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1808.05832v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Importance mixing: Improving sample reuse in evolutionary policy search methods</bib_title>
			<bib_authors>Pourchot, Aloïs and Perrin, Nicolas and Sigaud, Olivier</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Deep neuroevolution, that is evolutionary policy search methods based on deep neural networks, have recently emerged as a competitor to deep reinforcement learning algorithms due to their better parallelization capabilities. However, these methods still suffer from a far worse sample efficiency. In this paper we investigate whether a mechanism known as &quot;importance mixing&quot; can significantly improve their sample efficiency. We provide a didactic presentation of importance mixing and we explain how it can be extended to reuse more samples. Then, from an empirical comparison based on a simple benchmark, we show that, though it actually provides better sample efficiency, it is still far from the sample efficiency of deep reinforcement learning, though it is more stable.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1808.05832v1</bib_extra>
			<bib_extra key="File">1808.05832v1.pdf</bib_extra>
			<bib_extra key="Month">Aug</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1808.05832v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1609.03759v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>3D Simulation for Robot Arm Control with Deep Q-Learning</bib_title>
			<bib_authors>James, Stephen and Johns, Edward</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">Recent trends in robot arm control have seen a shift towards end-to-end solutions, using deep reinforcement learning to learn a controller directly from raw sensor data, rather than relying on a hand-crafted, modular pipeline. However, the high dimensionality of the state space often means that it is impractical to generate sufficient training data with real-world experiments. As an alternative solution, we propose to learn a robot controller in simulation, with the potential of then transferring this to a real robot. Building upon the recent success of deep Q-networks, we present an approach which uses 3D simulations to train a 7-DOF robotic arm in a control task without any prior knowledge. The controller accepts images of the environment as its only input, and outputs motor actions for the task of locating and grasping a cube, over a range of initial configurations. To encourage efficient learning, a structured reward function is designed with intermediate rewards. We also present preliminary results in direct transfer of policies over to a real robot, without any further training.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1609.03759v2</bib_extra>
			<bib_extra key="File">1609.03759v2.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1609.03759v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2012.03548v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Reset-Free Lifelong Learning with Skill-Space Planning</bib_title>
			<bib_authors>Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">The objective of lifelong reinforcement learning (RL) is to optimize agents which can continuously adapt and interact in changing environments. However, current RL approaches fail drastically when environments are non-stationary and interactions are non-episodic. We propose Lifelong Skill Planning (LiSP), an algorithmic framework for non-episodic lifelong RL based on planning in an abstract space of higher-order skills. We learn the skills in an unsupervised manner using intrinsic rewards and plan over the learned skills using a learned dynamics model. Moreover, our framework permits skill discovery even from offline data, thereby reducing the need for excessive real-world interactions. We demonstrate empirically that LiSP successfully enables long-horizon planning and learns agents that can avoid catastrophic failures even in challenging non-stationary and non-episodic environments derived from gridworld and MuJoCo benchmarks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2012.03548v3</bib_extra>
			<bib_extra key="File">2012.03548v3.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2012.03548v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1703.04933v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Sharp Minima Can Generalize For Deep Nets</bib_title>
			<bib_authors>Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter &amp; Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1703.04933v2</bib_extra>
			<bib_extra key="File">1703.04933v2.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1703.04933v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2007.13363v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Learning Compositional Neural Programs for Continuous Control</bib_title>
			<bib_authors>Pierrot, Thomas and Perrin, Nicolas and Behbahani, Feryal and Laterre, Alexandre and Sigaud, Olivier and Beguir, Karim and Freitas, Nando de</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">We propose a novel solution to challenging sparse-reward, continuous control problems that require hierarchical planning at multiple levels of abstraction. Our solution, dubbed AlphaNPI-X, involves three separate stages of learning. First, we use off-policy reinforcement learning algorithms with experience replay to learn a set of atomic goal-conditioned policies, which can be easily repurposed for many tasks. Second, we learn self-models describing the effect of the atomic policies on the environment. Third, the self-models are harnessed to learn recursive compositional programs with multiple levels of abstraction. The key insight is that the self-models enable planning by imagination, obviating the need for interaction with the world when learning higher-level compositional programs. To accomplish the third stage of learning, we extend the AlphaNPI algorithm, which applies AlphaZero to learn recursive neural programmer-interpreters. We empirically show that AlphaNPI-X can effectively learn to tackle challenging sparse manipulation tasks, such as stacking multiple blocks, where powerful model-free baselines fail.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2007.13363v2</bib_extra>
			<bib_extra key="File">2007.13363v2.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2007.13363v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2003.01629v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Can Increasing Input Dimensionality Improve Deep Reinforcement Learning?</bib_title>
			<bib_authors>Ota, Kei and Oiki, Tomoaki and Jha, Devesh K. and Mariyama, Toshisada and Nikovski, Daniel</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Deep reinforcement learning (RL) algorithms have recently achieved remarkable successes in various sequential decision making tasks, leveraging advances in methods for training large deep networks. However, these methods usually require large amounts of training data, which is often a big problem for real-world applications. One natural question to ask is whether learning good representations for states and using larger networks helps in learning better policies. In this paper, we try to study if increasing input dimensionality helps improve performance and sample efficiency of model-free deep RL algorithms. To do so, we propose an online feature extractor network (OFENet) that uses neural nets to produce good representations to be used as inputs to deep RL algorithms. Even though the high dimensionality of input is usually supposed to make learning of RL agents more difficult, we show that the RL agents in fact learn more efficiently with the high-dimensional representation than with the lower-dimensional state observations. We believe that stronger feature propagation together with larger networks (and thus larger search space) allows RL agents to learn more complex functions of states and thus improves the sample efficiency. Through numerical experiments, we show that the proposed method outperforms several other state-of-the-art algorithms in terms of both sample efficiency and performance. Codes for the proposed method are available at http://www.merl.com/research/license/OFENet .</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2003.01629v2</bib_extra>
			<bib_extra key="File">2003.01629v2.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2003.01629v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2006.09359v6</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>AWAC: Accelerating Online Reinforcement Learning with Offline Datasets</bib_title>
			<bib_authors>Nair, Ashvin and Gupta, Abhishek and Dalal, Murtaza and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Reinforcement learning (RL) provides an appealing formalism for learning control policies from experience. However, the classic active formulation of RL necessitates a lengthy active exploration process for each behavior, making it difficult to apply in real-world settings such as robotic control. If we can instead allow RL algorithms to effectively use previously collected data to aid the online learning process, such applications could be made substantially more practical: the prior data would provide a starting point that mitigates challenges due to exploration and sample complexity, while the online training enables the agent to perfect the desired skill. Such prior data could either constitute expert demonstrations or sub-optimal prior data that illustrates potentially useful transitions. While a number of prior methods have either used optimal demonstrations to bootstrap RL, or have used sub-optimal data to train purely offline, it remains exceptionally difficult to train a policy with offline data and actually continue to improve it further with online RL. In this paper we analyze why this problem is so challenging, and propose an algorithm that combines sample efficient dynamic programming with maximum likelihood policy updates, providing a simple and effective framework that is able to leverage large amounts of offline data and then quickly perform online fine-tuning of RL policies. We show that our method, advantage weighted actor critic (AWAC), enables rapid learning of skills with a combination of prior demonstration data and online experience. We demonstrate these benefits on simulated and real-world robotics domains, including dexterous manipulation with a real multi-fingered hand, drawer opening with a robotic arm, and rotating a valve. Our results show that incorporating prior data can reduce the time required to learn a range of robotic skills to practical time-scales.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2006.09359v6</bib_extra>
			<bib_extra key="File">2006.09359v6.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2006.09359v6</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2202.02446v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Adversarially Trained Actor Critic for Offline Reinforcement Learning</bib_title>
			<bib_authors>Cheng, Ching-An and Xie, Tengyang and Jiang, Nan and Agarwal, Alekh</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2022</bib_year>
			<bib_extra key="Abstract">We propose Adversarially Trained Actor Critic (ATAC), a new model-free algorithm for offline reinforcement learning (RL) under insufficient data coverage, based on the concept of relative pessimism. ATAC is designed as a two-player Stackelberg game: A policy actor competes against an adversarially trained value critic, who finds data-consistent scenarios where the actor is inferior to the data-collection behavior policy. We prove that, when the actor attains no regret in the two-player game, running ATAC produces a policy that provably 1) outperforms the behavior policy over a wide range of hyperparameters that control the degree of pessimism, and 2) competes with the best policy covered by data with appropriately chosen hyperparameters. Compared with existing works, notably our framework offers both theoretical guarantees for general function approximation and a deep RL implementation scalable to complex environments and large datasets. In the D4RL benchmark, ATAC consistently outperforms state-of-the-art offline RL algorithms on a range of continuous control tasks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2202.02446v2</bib_extra>
			<bib_extra key="File">2202.02446v2.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2202.02446v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2009.13579v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Novelty Search in Representational Space for Sample Efficient Exploration</bib_title>
			<bib_authors>Tao, Ruo Yu and François-Lavet, Vincent and Pineau, Joelle</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">We present a new approach for efficient exploration which leverages a low-dimensional encoding of the environment learned with a combination of model-based and model-free objectives. Our approach uses intrinsic rewards that are based on the distance of nearest neighbors in the low dimensional representational space to gauge novelty. We then leverage these intrinsic rewards for sample-efficient exploration with planning routines in representational space for hard exploration tasks with sparse rewards. One key element of our approach is the use of information theoretic principles to shape our representations in a way so that our novelty reward goes beyond pixel similarity. We test our approach on a number of maze tasks, as well as a control problem and show that our exploration approach is more sample-efficient compared to strong baselines.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2009.13579v3</bib_extra>
			<bib_extra key="File">2009.13579v3.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2009.13579v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1804.07127v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Hierarchical Behavioral Repertoires with Unsupervised Descriptors</bib_title>
			<bib_authors>Cully, Antoine and Demiris, Yiannis</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Enabling artificial agents to automatically learn complex, versatile and high-performing behaviors is a long-lasting challenge. This paper presents a step in this direction with hierarchical behavioral repertoires that stack several behavioral repertoires to generate sophisticated behaviors. Each repertoire of this architecture uses the lower repertoires to create complex behaviors as sequences of simpler ones, while only the lowest repertoire directly controls the agent’s movements. This paper also introduces a novel approach to automatically define behavioral descriptors thanks to an unsupervised neural network that organizes the produced high-level behaviors. The experiments show that the proposed architecture enables a robot to learn how to draw digits in an unsupervised manner after having learned to draw lines and arcs. Compared to traditional behavioral repertoires, the proposed architecture reduces the dimensionality of the optimization problems by orders of magnitude and provides behaviors with a twice better fitness. More importantly, it enables the transfer of knowledge between robots: a hierarchical repertoire evolved for a robotic arm to draw digits can be transferred to a humanoid robot by simply changing the lowest layer of the hierarchy. This enables the humanoid to draw digits although it has never been trained for this task.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1145/3205455.3205571</bib_extra>
			<bib_extra key="Eprint">1804.07127v1</bib_extra>
			<bib_extra key="File">1804.07127v1.pdf</bib_extra>
			<bib_extra key="Month">Apr</bib_extra>
			<bib_extra key="Note">Genetic and Evolutionary Computation Conference 2018</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1804.07127v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1502.05477v5</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Trust Region Policy Optimization</bib_title>
			<bib_authors>Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2015</bib_year>
			<bib_extra key="Abstract">We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1502.05477v5</bib_extra>
			<bib_extra key="File">1502.05477v5.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1502.05477v5</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2006.01419v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efficient Exploration</bib_title>
			<bib_authors>Han, Seungyul and Sung, Youngchul</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">In this paper, sample-aware policy entropy regularization is proposed to enhance the conventional policy entropy regularization for better exploration. Exploiting the sample distribution obtainable from the replay buffer, the proposed sample-aware entropy regularization maximizes the entropy of the weighted sum of the policy action distribution and the sample action distribution from the replay buffer for sample-efficient exploration. A practical algorithm named diversity actor-critic (DAC) is developed by applying policy iteration to the objective function with the proposed sample-aware entropy regularization. Numerical results show that DAC significantly outperforms existing recent algorithms for reinforcement learning.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2006.01419v2</bib_extra>
			<bib_extra key="File">2006.01419v2.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2006.01419v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1703.09312v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics</bib_title>
			<bib_authors>Mahler, Jeffrey and Liang, Jacky and Niyaz, Sherdil and Laskey, Michael and Doan, Richard and Liu, Xinyu and Ojea, Juan Aparicio and Goldberg, Ken</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">To reduce data collection time for deep learning of robust robotic grasp plans, we explore training from a synthetic dataset of 6.7 million point clouds, grasps, and analytic grasp metrics generated from thousands of 3D models from Dex-Net 1.0 in randomized poses on a table. We use the resulting dataset, Dex-Net 2.0, to train a Grasp Quality Convolutional Neural Network (GQ-CNN) model that rapidly predicts the probability of success of grasps from depth images, where grasps are specified as the planar position, angle, and depth of a gripper relative to an RGB-D sensor. Experiments with over 1,000 trials on an ABB YuMi comparing grasp planning methods on singulated objects suggest that a GQ-CNN trained with only synthetic data from Dex-Net 2.0 can be used to plan grasps in 0.8sec with a success rate of 93% on eight known objects with adversarial geometry and is 3x faster than registering point clouds to a precomputed dataset of objects and indexing grasps. The Dex-Net 2.0 grasp planner also has the highest success rate on a dataset of 10 novel rigid objects and achieves 99% precision (one false positive out of 69 grasps classified as robust) on a dataset of 40 novel household objects, some of which are articulated or deformable. Code, datasets, videos, and supplementary material are available at http://berkeleyautomation.github.io/dex-net .</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1703.09312v3</bib_extra>
			<bib_extra key="File">1703.09312v3.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1703.09312v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1810.03237v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Task-Embedded Control Networks for Few-Shot Imitation Learning</bib_title>
			<bib_authors>James, Stephen and Bloesch, Michael and Davison, Andrew J.</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Much like humans, robots should have the ability to leverage knowledge from previously learned tasks in order to learn new tasks quickly in new and unfamiliar environments. Despite this, most robot learning approaches have focused on learning a single task, from scratch, with a limited notion of generalisation, and no way of leveraging the knowledge to learn other tasks more efficiently. One possible solution is meta-learning, but many of the related approaches are limited in their ability to scale to a large number of tasks and to learn further tasks without forgetting previously learned ones. With this in mind, we introduce Task-Embedded Control Networks, which employ ideas from metric learning in order to create a task embedding that can be used by a robot to learn new tasks from one or more demonstrations. In the area of visually-guided manipulation, we present simulation results in which we surpass the performance of a state-of-the-art method when using only visual information from each demonstration. Additionally, we demonstrate that our approach can also be used in conjunction with domain randomisation to train our few-shot learning ability in simulation and then deploy in the real world without any additional training. Once deployed, the robot can learn new tasks from a single real-world demonstration.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1810.03237v1</bib_extra>
			<bib_extra key="File">1810.03237v1.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1810.03237v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1412.7009v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Generative Class-conditional Autoencoders</bib_title>
			<bib_authors>Rudy, Jan and Taylor, Graham</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2014</bib_year>
			<bib_extra key="Abstract">Recent work by Bengio et al. (2013) proposes a sampling procedure for denoising autoencoders which involves learning the transition operator of a Markov chain. The transition operator is typically unimodal, which limits its capacity to model complex data. In order to perform efficient sampling from conditional distributions, we extend this work, both theoretically and algorithmically, to gated autoencoders (Memisevic, 2013), The proposed model is able to generate convincing class-conditional samples when trained on both the MNIST and TFD datasets.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1412.7009v3</bib_extra>
			<bib_extra key="File">1412.7009v3.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.NE</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1412.7009v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1707.01495v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Hindsight Experience Replay</bib_title>
			<bib_authors>Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1707.01495v3</bib_extra>
			<bib_extra key="File">1707.01495v3.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1707.01495v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2006.08505v5</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Diversity Policy Gradient for Sample Efficient Quality-Diversity Optimization</bib_title>
			<bib_authors>Pierrot, Thomas and Macé, Valentin and Chalumeau, Félix and Flajolet, Arthur and Cideron, Geoffrey and Beguir, Karim and Cully, Antoine and Sigaud, Olivier and Perrin-Gilbert, Nicolas</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">A fascinating aspect of nature lies in its ability to produce a large and diverse collection of organisms that are all high-performing in their niche. By contrast, most AI algorithms focus on finding a single efficient solution to a given problem. Aiming for diversity in addition to performance is a convenient way to deal with the exploration-exploitation trade-off that plays a central role in learning. It also allows for increased robustness when the returned collection contains several working solutions to the considered problem, making it well-suited for real applications such as robotics. Quality-Diversity (QD) methods are evolutionary algorithms designed for this purpose. This paper proposes a novel algorithm, QDPG, which combines the strength of Policy Gradient algorithms and Quality Diversity approaches to produce a collection of diverse and high-performing neural policies in continuous control environments. The main contribution of this work is the introduction of a Diversity Policy Gradient (DPG) that exploits information at the time-step level to drive policies towards more diversity in a sample-efficient manner. Specifically, QDPG selects neural controllers from a MAP-Elites grid and uses two gradient-based mutation operators to improve both quality and diversity. Our results demonstrate that QDPG is significantly more sample-efficient than its evolutionary competitors.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1145/3512290.3528845</bib_extra>
			<bib_extra key="Eprint">2006.08505v5</bib_extra>
			<bib_extra key="File">2006.08505v5.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2006.08505v5</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1602.04621v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Deep Exploration via Bootstrapped DQN</bib_title>
			<bib_authors>Osband, Ian and Blundell, Charles and Pritzel, Alexander and Roy, Benjamin Van</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1602.04621v3</bib_extra>
			<bib_extra key="File">1602.04621v3.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1602.04621v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2103.12656v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification</bib_title>
			<bib_authors>Eysenbach, Benjamin and Levine, Sergey and Salakhutdinov, Ruslan</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Reinforcement learning (RL) algorithms assume that users specify tasks by manually writing down a reward function. However, this process can be laborious and demands considerable technical expertise. Can we devise RL algorithms that instead enable users to specify tasks simply by providing examples of successful outcomes? In this paper, we derive a control algorithm that maximizes the future probability of these successful outcome examples. Prior work has approached similar problems with a two-stage process, first learning a reward function and then optimizing this reward function using another RL algorithm. In contrast, our method directly learns a value function from transitions and successful outcomes, without learning this intermediate reward function. Our method therefore requires fewer hyperparameters to tune and lines of code to debug. We show that our method satisfies a new data-driven Bellman equation, where examples take the place of the typical reward function term. Experiments show that our approach outperforms prior methods that learn explicit reward functions.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2103.12656v2</bib_extra>
			<bib_extra key="File">2103.12656v2.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2103.12656v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2106.02039v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Offline Reinforcement Learning as One Big Sequence Modeling Problem</bib_title>
			<bib_authors>Janner, Michael and Li, Qiyang and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models, leveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide effective solutions to the RL problem. To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence modeling problem simplifies a range of design decisions, allowing us to dispense with many of the components common in offline RL algorithms. We demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL. Further, we show that this approach can be combined with existing model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon tasks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2106.02039v4</bib_extra>
			<bib_extra key="File">2106.02039v4.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2106.02039v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1807.03748v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Representation Learning with Contrastive Predictive Coding</bib_title>
			<bib_authors>Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1807.03748v2</bib_extra>
			<bib_extra key="File">1807.03748v2.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1807.03748v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1306.3532v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Fast Marching Tree: a Fast Marching Sampling-Based Method for Optimal Motion Planning in Many Dimensions</bib_title>
			<bib_authors>Janson, Lucas and Schmerling, Edward and Clark, Ashley and Pavone, Marco</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2013</bib_year>
			<bib_extra key="Abstract">In this paper we present a novel probabilistic sampling-based motion planning algorithm called the Fast Marching Tree algorithm (FMT*). The algorithm is specifically aimed at solving complex motion planning problems in high-dimensional configuration spaces. This algorithm is proven to be asymptotically optimal and is shown to converge to an optimal solution faster than its state-of-the-art counterparts, chiefly PRM* and RRT*. The FMT* algorithm performs a &quot;lazy&quot; dynamic programming recursion on a predetermined number of probabilistically-drawn samples to grow a tree of paths, which moves steadily outward in cost-to-arrive space. As a departure from previous analysis approaches that are based on the notion of almost sure convergence, the FMT* algorithm is analyzed under the notion of convergence in probability: the extra mathematical flexibility of this approach allows for convergence rate bounds–the first in the field of optimal sampling-based motion planning. Specifically, for a certain selection of tuning parameters and configuration spaces, we obtain a convergence rate bound of order $O(n^-1/d+ρ)$, where $n$ is the number of sampled points, $d$ is the dimension of the configuration space, and ρ is an arbitrarily small constant. We go on to demonstrate asymptotic optimality for a number of variations on FMT*, namely when the configuration space is sampled non-uniformly, when the cost is not arc length, and when connections are made based on the number of nearest neighbors instead of a fixed connection radius. Numerical experiments over a range of dimensions and obstacle configurations confirm our theoretical and heuristic arguments by showing that FMT*, for a given execution time, returns substantially better solutions than either PRM* or RRT*, especially in high-dimensional configuration spaces and in scenarios where collision-checking is expensive.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1306.3532v4</bib_extra>
			<bib_extra key="File">1306.3532v4.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1306.3532v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2002.06038v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Never Give Up: Learning Directed Exploration Strategies</bib_title>
			<bib_authors>Badia, Adrià Puigdomènech and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Daniel and Piot, Bilal and Kapturowski, Steven and Tieleman, Olivier and Arjovsky, Martín and Pritzel, Alexander and Bolt, Andew and Blundell, Charles</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">We propose a reinforcement learning agent to solve hard exploration games by learning a range of directed exploratory policies. We construct an episodic memory-based intrinsic reward using k-nearest neighbors over the agent’s recent experience to train the directed exploratory policies, thereby encouraging the agent to repeatedly revisit all states in its environment. A self-supervised inverse dynamics model is used to train the embeddings of the nearest neighbour lookup, biasing the novelty signal towards what the agent can control. We employ the framework of Universal Value Function Approximators (UVFA) to simultaneously learn many directed exploration policies with the same neural network, with different trade-offs between exploration and exploitation. By using the same neural network for different degrees of exploration/exploitation, transfer is demonstrated from predominantly exploratory policies yielding effective exploitative policies. The proposed method can be incorporated to run with modern distributed RL agents that collect large amounts of experience from many actors running in parallel on separate environment instances. Our method doubles the performance of the base agent in all hard exploration in the Atari-57 suite while maintaining a very high score across the remaining games, obtaining a median human normalised score of 1344.0%. Notably, the proposed method is the first algorithm to achieve non-zero rewards (with a mean score of 8,400) in the game of Pitfall! without using demonstrations or hand-crafted features.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2002.06038v1</bib_extra>
			<bib_extra key="File">2002.06038v1.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2002.06038v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1806.01261v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Relational inductive biases, deep learning, and graph networks</bib_title>
			<bib_authors>Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one’s experiences–a hallmark of human intelligence from infancy–remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between &quot;hand-engineering&quot; and &quot;end-to-end&quot; learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias–the graph network–which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1806.01261v3</bib_extra>
			<bib_extra key="File">1806.01261v3.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1806.01261v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2010.05113v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Contrastive Representation Learning: A Framework and Review</bib_title>
			<bib_authors>Le-Khac, Phuc H. and Healy, Graham and Smeaton, Alan F.</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1109/ACCESS.2020.3031549</bib_extra>
			<bib_extra key="Eprint">2010.05113v2</bib_extra>
			<bib_extra key="File">2010.05113v2.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2010.05113v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2010.14641v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Learning to Plan Optimistically: Uncertainty-Guided Deep Exploration via Latent Model Ensembles</bib_title>
			<bib_authors>Seyde, Tim and Schwarting, Wilko and Karaman, Sertac and Rus, Daniela</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Learning complex robot behaviors through interaction requires structured exploration. Planning should target interactions with the potential to optimize long-term performance, while only reducing uncertainty where conducive to this objective. This paper presents Latent Optimistic Value Exploration (LOVE), a strategy that enables deep exploration through optimism in the face of uncertain long-term rewards. We combine latent world models with value function estimation to predict infinite-horizon returns and recover associated uncertainty via ensembling. The policy is then trained on an upper confidence bound (UCB) objective to identify and select the interactions most promising to improve long-term performance. We apply LOVE to visual robot control tasks in continuous action spaces and demonstrate on average more than 20% improved sample efficiency in comparison to state-of-the-art and other exploration objectives. In sparse and hard to explore environments we achieve an average improvement of over 30%.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2010.14641v3</bib_extra>
			<bib_extra key="File">2010.14641v3.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2010.14641v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1801.03954v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Model-Based Action Exploration for Learning Dynamic Motion Skills</bib_title>
			<bib_authors>Berseth, Glen and Panne, Michiel van de</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Deep reinforcement learning has achieved great strides in solving challenging motion control tasks. Recently, there has been significant work on methods for exploiting the data gathered during training, but there has been less work on how to best generate the data to learn from. For continuous action domains, the most common method for generating exploratory actions involves sampling from a Gaussian distribution centred around the mean action output by a policy. Although these methods can be quite capable, they do not scale well with the dimensionality of the action space, and can be dangerous to apply on hardware. We consider learning a forward dynamics model to predict the result, ($x_t+1$), of taking a particular action, ($u$), given a specific observation of the state, ($x_t$). With this model we perform internal look-ahead predictions of outcomes and seek actions we believe have a reasonable chance of success. This method alters the exploratory action space, thereby increasing learning speed and enables higher quality solutions to difficult problems, such as robotic locomotion and juggling.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1801.03954v2</bib_extra>
			<bib_extra key="File">1801.03954v2.pdf</bib_extra>
			<bib_extra key="Month">Jan</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1801.03954v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1010.3013v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Invariant Funnels around Trajectories using Sum-of-Squares Programming</bib_title>
			<bib_authors>Tobenkin, Mark M. and Manchester, Ian R. and Tedrake, Russ</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2010</bib_year>
			<bib_extra key="Abstract">This paper presents numerical methods for computing regions of finite-time invariance (funnels) around solutions of polynomial differential equations. First, we present a method which exactly certifies sufficient conditions for invariance despite relying on approximate trajectories from numerical integration. Our second method relaxes the constraints of the first by sampling in time. In applications, this can recover almost identical funnels but is much faster to compute. In both cases, funnels are verified using Sum-of-Squares programming to search over a family of time-varying polynomial Lyapunov functions. Initial candidate Lyapunov functions are constructed using the linearization about the trajectory, and associated time-varying Lyapunov and Riccati differential equations. The methods are compared on stabilized trajectories of a six-state model of a satellite.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1010.3013v1</bib_extra>
			<bib_extra key="File">1010.3013v1.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">math.DS</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1010.3013v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1910.12807v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Better Exploration with Optimistic Actor-Critic</bib_title>
			<bib_authors>Ciosek, Kamil and Vuong, Quan and Loftin, Robert and Hofmann, Katja</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Actor-critic methods, a type of model-free Reinforcement Learning, have been successfully applied to challenging tasks in continuous control, often achieving state-of-the art performance. However, wide-scale adoption of these methods in real-world domains is made difficult by their poor sample efficiency. We address this problem both theoretically and empirically. On the theoretical side, we identify two phenomena preventing efficient exploration in existing state-of-the-art algorithms such as Soft Actor Critic. First, combining a greedy actor update with a pessimistic estimate of the critic leads to the avoidance of actions that the agent does not know about, a phenomenon we call pessimistic underexploration. Second, current algorithms are directionally uninformed, sampling actions with equal probability in opposite directions from the current mean. This is wasteful, since we typically need actions taken along certain directions much more than others. To address both of these phenomena, we introduce a new algorithm, Optimistic Actor Critic, which approximates a lower and upper confidence bound on the state-action value function. This allows us to apply the principle of optimism in the face of uncertainty to perform directed exploration using the upper bound while still using the lower bound to avoid overestimation. We evaluate OAC in several challenging continuous control tasks, achieving state-of the art sample efficiency.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1910.12807v1</bib_extra>
			<bib_extra key="File">1910.12807v1.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Note">NeurIPS 2019</bib_extra>
			<bib_extra key="Primaryclass">stat.ML</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1910.12807v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2206.08332v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>BYOL-Explore: Exploration by Bootstrapped Prediction</bib_title>
			<bib_authors>Guo, Zhaohan Daniel and Thakoor, Shantanu and Pîslar, Miruna and Pires, Bernardo Avila and Altché, Florent and Tallec, Corentin and Saade, Alaa and Calandriello, Daniele and Grill, Jean-Bastien and Tang, Yunhao and Valko, Michal and Munos, Rémi and Azar, Mohammad Gheshlaghi and Piot, Bilal</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2022</bib_year>
			<bib_extra key="Abstract">We present BYOL-Explore, a conceptually simple yet general approach for curiosity-driven exploration in visually-complex environments. BYOL-Explore learns a world representation, the world dynamics, and an exploration policy all-together by optimizing a single prediction loss in the latent space with no additional auxiliary objective. We show that BYOL-Explore is effective in DM-HARD-8, a challenging partially-observable continuous-action hard-exploration benchmark with visually-rich 3-D environments. On this benchmark, we solve the majority of the tasks purely through augmenting the extrinsic reward with BYOL-Explore s intrinsic reward, whereas prior work could only get off the ground with human demonstrations. As further evidence of the generality of BYOL-Explore, we show that it achieves superhuman performance on the ten hardest exploration games in Atari while having a much simpler design than other competitive agents.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2206.08332v1</bib_extra>
			<bib_extra key="File">2206.08332v1.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2206.08332v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1906.04493v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Generative Adversarial Networks are Special Cases of Artificial Curiosity (1990) and also Closely Related to Predictability Minimization (1991)</bib_title>
			<bib_authors>Schmidhuber, Juergen</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">I review unsupervised or self-supervised neural networks playing minimax games in game-theoretic settings: (i) Artificial Curiosity (AC, 1990) is based on two such networks. One network learns to generate a probability distribution over outputs, the other learns to predict effects of the outputs. Each network minimizes the objective function maximized by the other. (ii) Generative Adversarial Networks (GANs, 2010-2014) are an application of AC where the effect of an output is 1 if the output is in a given set, and 0 otherwise. (iii) Predictability Minimization (PM, 1990s) models data distributions through a neural encoder that maximizes the objective function minimized by a neural predictor of the code components. I correct a previously published claim that PM is not based on a minimax game.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1906.04493v3</bib_extra>
			<bib_extra key="File">1906.04493v3.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Note">Neural Networks, Volume 127, July 2020, Pages 58-66</bib_extra>
			<bib_extra key="Primaryclass">cs.NE</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1906.04493v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1711.09846v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Population Based Training of Neural Networks</bib_title>
			<bib_authors>Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present Population Based Training (PBT), a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1711.09846v2</bib_extra>
			<bib_extra key="File">1711.09846v2.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1711.09846v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2006.04678v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Primal Wasserstein Imitation Learning</bib_title>
			<bib_authors>Dadashi, Robert and Hussenot, Léonard and Geist, Matthieu and Pietquin, Olivier</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Imitation Learning (IL) methods seek to match the behavior of an agent with that of an expert. In the present work, we propose a new IL method based on a conceptually simple algorithm: Primal Wasserstein Imitation Learning (PWIL), which ties to the primal form of the Wasserstein distance between the expert and the agent state-action distributions. We present a reward function which is derived offline, as opposed to recent adversarial IL algorithms that learn a reward function through interactions with the environment, and which requires little fine-tuning. We show that we can recover expert behavior on a variety of continuous control tasks of the MuJoCo domain in a sample efficient manner in terms of agent interactions and of expert interactions with the environment. Finally, we show that the behavior of the agent we train matches the behavior of the expert with the Wasserstein distance, rather than the commonly used proxy of performance.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2006.04678v2</bib_extra>
			<bib_extra key="File">2006.04678v2.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2006.04678v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1412.1897v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images</bib_title>
			<bib_authors>Nguyen, Anh and Yosinski, Jason and Clune, Jeff</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2014</bib_year>
			<bib_extra key="Abstract">Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call &quot;fooling images&quot; (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1412.1897v4</bib_extra>
			<bib_extra key="File">1412.1897v4.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.CV</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1412.1897v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1311.1839v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>An Efficiently Solvable Quadratic Program for Stabilizing Dynamic Locomotion</bib_title>
			<bib_authors>Kuindersma, Scott and Permenter, Frank and Tedrake, Russ</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2013</bib_year>
			<bib_extra key="Abstract">We describe a whole-body dynamic walking controller implemented as a convex quadratic program. The controller solves an optimal control problem using an approximate value function derived from a simple walking model while respecting the dynamic, input, and contact constraints of the full robot dynamics. By exploiting sparsity and temporal structure in the optimization with a custom active-set algorithm, we surpass the performance of the best available off-the-shelf solvers and achieve 1kHz control rates for a 34-DOF humanoid. We describe applications to balancing and walking tasks using the simulated Atlas robot in the DARPA Virtual Robotics Challenge.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1109/ICRA.2014.6907230</bib_extra>
			<bib_extra key="Eprint">1311.1839v2</bib_extra>
			<bib_extra key="File">1311.1839v2.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Note">In Proceedings of the International Conference on Robotics and Automation (ICRA), 2589-2594 (2014)</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1311.1839v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1912.06088v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Learning to Reach Goals via Iterated Supervised Learning</bib_title>
			<bib_authors>Ghosh, Dibya and Gupta, Abhishek and Reddy, Ashwin and Fu, Justin and Devin, Coline and Eysenbach, Benjamin and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Current reinforcement learning (RL) algorithms can be brittle and difficult to use, especially when learning goal-reaching behaviors from sparse rewards. Although supervised imitation learning provides a simple and stable alternative, it requires access to demonstrations from a human supervisor. In this paper, we study RL algorithms that use imitation learning to acquire goal reaching policies from scratch, without the need for expert demonstrations or a value function. In lieu of demonstrations, we leverage the property that any trajectory is a successful demonstration for reaching the final state in that same trajectory. We propose a simple algorithm in which an agent continually relabels and imitates the trajectories it generates to progressively learn goal-reaching behaviors from scratch. Each iteration, the agent collects new trajectories using the latest policy, and maximizes the likelihood of the actions along these trajectories under the goal that was actually reached, so as to improve the policy. We formally show that this iterated supervised learning procedure optimizes a bound on the RL objective, derive performance bounds of the learned policy, and empirically demonstrate improved goal-reaching performance and robustness over current RL algorithms in several benchmark tasks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1912.06088v4</bib_extra>
			<bib_extra key="File">1912.06088v4.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1912.06088v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2202.00161v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery</bib_title>
			<bib_authors>Laskin, Michael and Liu, Hao and Peng, Xue Bin and Yarats, Denis and Rajeswaran, Aravind and Abbeel, Pieter</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2022</bib_year>
			<bib_extra key="Abstract">We introduce Contrastive Intrinsic Control (CIC), an algorithm for unsupervised skill discovery that maximizes the mutual information between state-transitions and latent skill vectors. CIC utilizes contrastive learning between state-transitions and skills to learn behavior embeddings and maximizes the entropy of these embeddings as an intrinsic reward to encourage behavioral diversity. We evaluate our algorithm on the Unsupervised Reinforcement Learning Benchmark, which consists of a long reward-free pre-training phase followed by a short adaptation phase to downstream tasks with extrinsic rewards. CIC substantially improves over prior methods in terms of adaptation efficiency, outperforming prior unsupervised skill discovery methods by 1.79x and the next leading overall exploration algorithm by 1.18x.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2202.00161v3</bib_extra>
			<bib_extra key="File">2202.00161v3.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2202.00161v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1806.04613v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Improving Regression Performance with Distributional Losses</bib_title>
			<bib_authors>Imani, Ehsan and White, Martha</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">There is growing evidence that converting targets to soft targets in supervised learning can provide considerable gains in performance. Much of this work has considered classification, converting hard zero-one values to soft labels–-such as by adding label noise, incorporating label ambiguity or using distillation. In parallel, there is some evidence from a regression setting in reinforcement learning that learning distributions can improve performance. In this work, we investigate the reasons for this improvement, in a regression setting. We introduce a novel distributional regression loss, and similarly find it significantly improves prediction accuracy. We investigate several common hypotheses, around reducing overfitting and improved representations. We instead find evidence for an alternative hypothesis: this loss is easier to optimize, with better behaved gradients, resulting in improved generalization. We provide theoretical support for this alternative hypothesis, by characterizing the norm of the gradients of this loss.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1806.04613v1</bib_extra>
			<bib_extra key="File">1806.04613v1.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">stat.ML</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1806.04613v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1906.04556v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Exploiting the Sign of the Advantage Function to Learn Deterministic Policies in Continuous Domains</bib_title>
			<bib_authors>Zimmer, Matthieu and Weng, Paul</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">In the context of learning deterministic policies in continuous domains, we revisit an approach, which was first proposed in Continuous Actor Critic Learning Automaton (CACLA) and later extended in Neural Fitted Actor Critic (NFAC). This approach is based on a policy update different from that of deterministic policy gradient (DPG). Previous work has observed its excellent performance empirically, but a theoretical justification is lacking. To fill this gap, we provide a theoretical explanation to motivate this unorthodox policy update by relating it to another update and making explicit the objective function of the latter. We furthermore discuss in depth the properties of these updates to get a deeper understanding of the overall approach. In addition, we extend it and propose a new trust region algorithm, Penalized NFAC (PeNFAC). Finally, we experimentally demonstrate in several classic control problems that it surpasses the state-of-the-art algorithms to learn deterministic policies.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.24963/ijcai.2019/625</bib_extra>
			<bib_extra key="Eprint">1906.04556v2</bib_extra>
			<bib_extra key="File">1906.04556v2.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1906.04556v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1609.05140v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>The Option-Critic Architecture</bib_title>
			<bib_authors>Bacon, Pierre-Luc and Harb, Jean and Precup, Doina</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup &amp; Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1609.05140v2</bib_extra>
			<bib_extra key="File">1609.05140v2.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1609.05140v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1810.12281v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Three Mechanisms of Weight Decay Regularization</bib_title>
			<bib_authors>Zhang, Guodong and Wang, Chaoqi and Xu, Bowen and Grosse, Roger</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization. Literal weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. We empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. Our results provide insight into how to improve the regularization of neural networks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1810.12281v1</bib_extra>
			<bib_extra key="File">1810.12281v1.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1810.12281v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1612.07139v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement to Imitation</bib_title>
			<bib_authors>Tai, Lei and Zhang, Jingwei and Liu, Ming and Boedecker, Joschka and Burgard, Wolfram</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">Deep learning techniques have been widely applied, achieving state-of-the-art results in various fields of study. This survey focuses on deep learning solutions that target learning control policies for robotics applications. We carry out our discussions on the two main paradigms for learning control with deep networks: deep reinforcement learning and imitation learning. For deep reinforcement learning (DRL), we begin from traditional reinforcement learning algorithms, showing how they are extended to the deep context and effective mechanisms that could be added on top of the DRL algorithms. We then introduce representative works that utilize DRL to solve navigation and manipulation tasks in robotics. We continue our discussion on methods addressing the challenge of the reality gap for transferring DRL policies trained in simulation to real-world scenarios, and summarize robotics simulation platforms for conducting DRL research. For imitation leaning, we go through its three main categories, behavior cloning, inverse reinforcement learning and generative adversarial imitation learning, by introducing their formulations and their corresponding robotics applications. Finally, we discuss the open challenges and research frontiers.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1612.07139v4</bib_extra>
			<bib_extra key="File">1612.07139v4.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1612.07139v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1810.01257v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Near-Optimal Representation Learning for Hierarchical Reinforcement Learning</bib_title>
			<bib_authors>Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">We study the problem of representation learning in goal-conditioned hierarchical reinforcement learning. In such hierarchical structures, a higher-level controller solves tasks by iteratively communicating goals which a lower-level policy is trained to reach. Accordingly, the choice of representation – the mapping of observation space to goal space – is crucial. To study this problem, we develop a notion of sub-optimality of a representation, defined in terms of expected reward of the optimal hierarchical policy using this representation. We derive expressions which bound the sub-optimality and show how these expressions can be translated to representation learning objectives which may be optimized in practice. Results on a number of difficult continuous-control tasks show that our approach to representation learning yields qualitatively better representations as well as quantitatively better hierarchical policies, compared to existing methods (see videos at https://sites.google.com/view/representation-hrl).</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1810.01257v2</bib_extra>
			<bib_extra key="File">1810.01257v2.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1810.01257v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2011.10024v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Parrot: Data-Driven Behavioral Priors for Reinforcement Learning</bib_title>
			<bib_authors>Singh, Avi and Liu, Huihan and Zhou, Gaoyue and Yu, Albert and Rhinehart, Nicholas and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Reinforcement learning provides a general framework for flexible decision making and control, but requires extensive data collection for each new task that an agent needs to learn. In other machine learning fields, such as natural language processing or computer vision, pre-training on large, previously collected datasets to bootstrap learning for new tasks has emerged as a powerful paradigm to reduce data requirements when learning a new task. In this paper, we ask the following question: how can we enable similarly useful pre-training for RL agents? We propose a method for pre-training behavioral priors that can capture complex input-output relationships observed in successful trials from a wide range of previously seen tasks, and we show how this learned prior can be used for rapidly learning new tasks without impeding the RL agent’s ability to try out novel behaviors. We demonstrate the effectiveness of our approach in challenging robotic manipulation domains involving image observations and sparse reward functions, where our method outperforms prior works by a substantial margin.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2011.10024v1</bib_extra>
			<bib_extra key="File">2011.10024v1.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2011.10024v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1507.00210v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Natural Neural Networks</bib_title>
			<bib_authors>Desjardins, Guillaume and Simonyan, Karen and Pascanu, Razvan and Kavukcuoglu, Koray</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2015</bib_year>
			<bib_extra key="Abstract">We introduce Natural Neural Networks, a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the Fisher matrix. In particular, we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer, while preserving the feed-forward computation of the network. Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm (PRONG), which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm. We highlight the benefits of our method on both unsupervised and supervised learning tasks, and showcase its scalability by training on the large-scale ImageNet Challenge dataset.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1507.00210v1</bib_extra>
			<bib_extra key="File">1507.00210v1.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">stat.ML</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1507.00210v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1106.3708v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles</bib_title>
			<bib_authors>Ollivier, Yann and Arnold, Ludovic and Auger, Anne and Hansen, Nikolaus</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2011</bib_year>
			<bib_extra key="Abstract">We present a canonical way to turn any smooth parametric family of probability distributions on an arbitrary search space $X$ into a continuous-time black-box optimization method on $X$, the information-geometric optimization (IGO) method. Invariance as a design principle minimizes the number of arbitrary choices. The resulting IGO flow conducts the natural gradient ascent of an adaptive, time-dependent, quantile-based transformation of the objective function. It makes no assumptions on the objective function to be optimized. The IGO method produces explicit IGO algorithms through time discretization. It naturally recovers versions of known algorithms and offers a systematic way to derive new ones. The cross-entropy method is recovered in a particular case, and can be extended into a smoothed, parametrization-independent maximum likelihood update (IGO-ML). For Gaussian distributions on $\mathbbR^d$, IGO is related to natural evolution strategies (NES) and recovers a version of the CMA-ES algorithm. For Bernoulli distributions on $0,1^d$, we recover the PBIL algorithm. From restricted Boltzmann machines, we obtain a novel algorithm for optimization on $0,1^d$. All these algorithms are unified under a single information-geometric optimization framework. Thanks to its intrinsic formulation, the IGO method achieves invariance under reparametrization of the search space $X$, under a change of parameters of the probability distributions, and under increasing transformations of the objective function. Theory strongly suggests that IGO algorithms have minimal loss in diversity during optimization, provided the initial diversity is high. First experiments using restricted Boltzmann machines confirm this insight. Thus IGO seems to provide, from information theory, an elegant way to spontaneously explore several valleys of a fitness landscape in a single run.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1106.3708v4</bib_extra>
			<bib_extra key="File">1106.3708v4.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Note">J. Machine Learning Research 18 (2017), no 18, 1-65</bib_extra>
			<bib_extra key="Primaryclass">math.OC</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1106.3708v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1708.05866v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>A Brief Survey of Deep Reinforcement Learning</bib_title>
			<bib_authors>Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep $Q$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1109/MSP.2017.2743240</bib_extra>
			<bib_extra key="Eprint">1708.05866v2</bib_extra>
			<bib_extra key="File">1708.05866v2.pdf</bib_extra>
			<bib_extra key="Month">Aug</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1708.05866v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1806.01242v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Graph networks as learnable physics engines for inference and control</bib_title>
			<bib_authors>Sanchez-Gonzalez, Alvaro and Heess, Nicolas and Springenberg, Jost Tobias and Merel, Josh and Riedmiller, Martin and Hadsell, Raia and Battaglia, Peter</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Understanding and interacting with everyday physical scenes requires rich knowledge about the structure of the world, represented either implicitly in a value or policy function, or explicitly in a transition model. Here we introduce a new class of learnable models–based on graph networks–which implement an inductive bias for object- and relation-centric representations of complex, dynamical systems. Our results show that as a forward model, our approach supports accurate predictions from real and simulated data, and surprisingly strong and efficient generalization, across eight distinct physical systems which we varied parametrically and structurally. We also found that our inference model can perform system identification. Our models are also differentiable, and support online planning via gradient-based trajectory optimization, as well as offline policy optimization. Our framework offers new opportunities for harnessing and exploiting rich knowledge about the world, and takes a key step toward building machines with more human-like representations of the world.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1806.01242v1</bib_extra>
			<bib_extra key="File">1806.01242v1.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1806.01242v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2004.11362v5</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Supervised Contrastive Learning</bib_title>
			<bib_authors>Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4% on the ImageNet dataset, which is 0.8% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement, and reference TensorFlow code is released at https://t.ly/supcon.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2004.11362v5</bib_extra>
			<bib_extra key="File">2004.11362v5.pdf</bib_extra>
			<bib_extra key="Month">Apr</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2004.11362v5</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1803.02348v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Smoothed Action Value Functions for Learning Gaussian Policies</bib_title>
			<bib_authors>Nachum, Ofir and Norouzi, Mohammad and Tucker, George and Schuurmans, Dale</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value. We show that such smoothed Q-values still satisfy a Bellman equation, making them learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships, we develop new algorithms for training a Gaussian policy directly from a learned smoothed Q-value approximator. The approach is additionally amenable to proximal optimization by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training leads to significantly improved results on standard continuous control benchmarks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1803.02348v3</bib_extra>
			<bib_extra key="File">1803.02348v3.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1803.02348v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1911.01417v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards</bib_title>
			<bib_authors>Trott, Alexander and Zheng, Stephan and Xiong, Caiming and Socher, Richard</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">While using shaped rewards can be beneficial when solving sparse reward tasks, their successful application often requires careful engineering and is problem specific. For instance, in tasks where the agent must achieve some goal state, simple distance-to-goal reward shaping often fails, as it renders learning vulnerable to local optima. We introduce a simple and effective model-free method to learn from shaped distance-to-goal rewards on tasks where success depends on reaching a goal state. Our method introduces an auxiliary distance-based reward based on pairs of rollouts to encourage diverse exploration. This approach effectively prevents learning dynamics from stabilizing around local optima induced by the naive distance-to-goal reward shaping and enables policies to efficiently solve sparse reward tasks. Our augmented objective does not require any additional reward engineering or domain expertise to implement and converges to the original sparse objective as the agent learns to solve the task. We demonstrate that our method successfully solves a variety of hard-exploration tasks (including maze navigation and 3D construction in a Minecraft environment), where naive distance-based reward shaping otherwise fails, and intrinsic curiosity and reward relabeling strategies exhibit poor performance.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1911.01417v1</bib_extra>
			<bib_extra key="File">1911.01417v1.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1911.01417v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2202.03057v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Multi-Objective Quality Diversity Optimization</bib_title>
			<bib_authors>Pierrot, Thomas and Richard, Guillaume and Beguir, Karim and Cully, Antoine</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2022</bib_year>
			<bib_extra key="Abstract">In this work, we consider the problem of Quality-Diversity (QD) optimization with multiple objectives. QD algorithms have been proposed to search for a large collection of both diverse and high-performing solutions instead of a single set of local optima. Thriving for diversity was shown to be useful in many industrial and robotics applications. On the other hand, most real-life problems exhibit several potentially antagonist objectives to be optimized. Hence being able to optimize for multiple objectives with an appropriate technique while thriving for diversity is important to many fields. Here, we propose an extension of the MAP-Elites algorithm in the multi-objective setting: Multi-Objective MAP-Elites (MOME). Namely, it combines the diversity inherited from the MAP-Elites grid algorithm with the strength of multi-objective optimizations by filling each cell with a Pareto Front. As such, it allows to extract diverse solutions in the descriptor space while exploring different compromises between objectives. We evaluate our method on several tasks, from standard optimization problems to robotics simulations. Our experimental evaluation shows the ability of MOME to provide diverse solutions while providing global performances similar to standard multi-objective algorithms.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1145/3512290.3528823</bib_extra>
			<bib_extra key="Eprint">2202.03057v2</bib_extra>
			<bib_extra key="File">2202.03057v2.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2202.03057v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1806.01240v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Diffeomorphic Learning</bib_title>
			<bib_authors>Younes, Laurent</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">We introduce in this paper a learning paradigm in which the training data is transformed by a diffeomorphic transformation before prediction. The learning algorithm minimizes a cost function evaluating the prediction error on the training set penalized by the distance between the diffeomorphism and the identity. The approach borrows ideas from shape analysis where diffeomorphisms are estimated for shape and image alignment, and brings them in a previously unexplored setting, estimating, in particular diffeomorphisms in much larger dimensions. After introducing the concept and describing a learning algorithm, we present diverse applications, mostly with synthetic examples, demonstrating the potential of the approach, as well as some insight on how it can be improved.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1806.01240v3</bib_extra>
			<bib_extra key="File">1806.01240v3.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">stat.ML</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1806.01240v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1910.01215v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>ES-MAML: Simple Hessian-Free Meta Learning</bib_title>
			<bib_authors>Song, Xingyou and Gao, Wenbo and Yang, Yuxiang and Choromanski, Krzysztof and Pacchiano, Aldo and Tang, Yunhao</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">We introduce ES-MAML, a new framework for solving the model agnostic meta learning (MAML) problem based on Evolution Strategies (ES). Existing algorithms for MAML are based on policy gradients, and incur significant difficulties when attempting to estimate second derivatives using backpropagation on stochastic policies. We show how ES can be applied to MAML to obtain an algorithm which avoids the problem of estimating second derivatives, and is also conceptually simple and easy to implement. Moreover, ES-MAML can handle new types of nonsmooth adaptation operators, and other techniques for improving performance and estimation of ES methods become applicable. We show empirically that ES-MAML is competitive with existing methods and often yields better adaptation with fewer queries.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1910.01215v4</bib_extra>
			<bib_extra key="File">1910.01215v4.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1910.01215v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2103.06257v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Maximum Entropy RL (Provably) Solves Some Robust RL Problems</bib_title>
			<bib_authors>Eysenbach, Benjamin and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Many potential applications of reinforcement learning (RL) require guarantees that the agent will perform well in the face of disturbances to the dynamics or reward function. In this paper, we prove theoretically that maximum entropy (MaxEnt) RL maximizes a lower bound on a robust RL objective, and thus can be used to learn policies that are robust to some disturbances in the dynamics and the reward function. While this capability of MaxEnt RL has been observed empirically in prior work, to the best of our knowledge our work provides the first rigorous proof and theoretical characterization of the MaxEnt RL robust set. While a number of prior robust RL algorithms have been designed to handle similar disturbances to the reward function or dynamics, these methods typically require additional moving parts and hyperparameters on top of a base RL algorithm. In contrast, our results suggest that MaxEnt RL by itself is robust to certain disturbances, without requiring any additional modifications. While this does not imply that MaxEnt RL is the best available robust RL method, MaxEnt RL is a simple robust RL method with appealing formal guarantees.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2103.06257v2</bib_extra>
			<bib_extra key="File">2103.06257v2.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2103.06257v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2201.00129v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>A Surrogate-Assisted Controller for Expensive Evolutionary Reinforcement Learning</bib_title>
			<bib_authors>Wang, Yuxing and Zhang, Tiantian and Chang, Yongzhe and Liang, Bin and Wang, Xueqian and Yuan, Bo</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2022</bib_year>
			<bib_extra key="Abstract">The integration of Reinforcement Learning (RL) and Evolutionary Algorithms (EAs) aims at simultaneously exploiting the sample efficiency as well as the diversity and robustness of the two paradigms. Recently, hybrid learning frameworks based on this principle have achieved great success in various challenging robot control tasks. However, in these methods, policies from the genetic population are evaluated via interactions with the real environments, limiting their applicability in computationally expensive problems. In this work, we propose Surrogate-assisted Controller (SC), a novel and efficient module that can be integrated into existing frameworks to alleviate the computational burden of EAs by partially replacing the expensive policy evaluation. The key challenge in applying this module is to prevent the optimization process from being misled by the possible false minima introduced by the surrogate. To address this issue, we present two strategies for SC to control the workflow of hybrid frameworks. Experiments on six continuous control tasks from the OpenAI Gym platform show that SC can not only significantly reduce the cost of fitness evaluations, but also boost the performance of the original hybrid frameworks with collaborative learning and evolutionary processes.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2201.00129v2</bib_extra>
			<bib_extra key="File">2201.00129v2.pdf</bib_extra>
			<bib_extra key="Month">Jan</bib_extra>
			<bib_extra key="Primaryclass">cs.NE</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2201.00129v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1801.08093v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Learning Symmetric and Low-energy Locomotion</bib_title>
			<bib_authors>Yu, Wenhao and Turk, Greg and Liu, C. Karen</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Learning locomotion skills is a challenging problem. To generate realistic and smooth locomotion, existing methods use motion capture, finite state machines or morphology-specific knowledge to guide the motion generation algorithms. Deep reinforcement learning (DRL) is a promising approach for the automatic creation of locomotion control. Indeed, a standard benchmark for DRL is to automatically create a running controller for a biped character from a simple reward function. Although several different DRL algorithms can successfully create a running controller, the resulting motions usually look nothing like a real runner. This paper takes a minimalist learning approach to the locomotion problem, without the use of motion examples, finite state machines, or morphology-specific knowledge. We introduce two modifications to the DRL approach that, when used together, produce locomotion behaviors that are symmetric, low-energy, and much closer to that of a real person. First, we introduce a new term to the loss function (not the reward function) that encourages symmetric actions. Second, we introduce a new curriculum learning method that provides modulated physical assistance to help the character with left/right balance and forward movement. The algorithm automatically computes appropriate assistance to the character and gradually relaxes this assistance, so that eventually the character learns to move entirely without help. Because our method does not make use of motion capture data, it can be applied to a variety of character morphologies. We demonstrate locomotion controllers for the lower half of a biped, a full humanoid, a quadruped, and a hexapod. Our results show that learned policies are able to produce symmetric, low-energy gaits. In addition, speed-appropriate gait patterns emerge without any guidance from motion examples or contact planning.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1145/3197517.3201397</bib_extra>
			<bib_extra key="Eprint">1801.08093v3</bib_extra>
			<bib_extra key="File">1801.08093v3.pdf</bib_extra>
			<bib_extra key="Month">Jan</bib_extra>
			<bib_extra key="Note">ACM Transactions on Graphics 37(4), August 2018</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1801.08093v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1902.04706v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Simultaneously Learning Vision and Feature-based Control Policies for Real-world Ball-in-a-Cup</bib_title>
			<bib_authors>Schwab, Devin and Springenberg, Tobias and Martins, Murilo F. and Lampe, Thomas and Neunert, Michael and Abdolmaleki, Abbas and Hertweck, Tim and Hafner, Roland and Nori, Francesco and Riedmiller, Martin</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">We present a method for fast training of vision based control policies on real robots. The key idea behind our method is to perform multi-task Reinforcement Learning with auxiliary tasks that differ not only in the reward to be optimized but also in the state-space in which they operate. In particular, we allow auxiliary task policies to utilize task features that are available only at training-time. This allows for fast learning of auxiliary policies, which subsequently generate good data for training the main, vision-based control policies. This method can be seen as an extension of the Scheduled Auxiliary Control (SAC-X) framework. We demonstrate the efficacy of our method by using both a simulated and real-world Ball-in-a-Cup game controlled by a robot arm. In simulation, our approach leads to significant learning speed-ups when compared to standard SAC-X. On the real robot we show that the task can be learned from-scratch, i.e., with no transfer from simulation and no imitation learning. Videos of our learned policies running on the real robot can be found at https://sites.google.com/view/rss-2019-sawyer-bic/.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1902.04706v2</bib_extra>
			<bib_extra key="File">1902.04706v2.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1902.04706v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1802.04181v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>State Representation Learning for Control: An Overview</bib_title>
			<bib_authors>Lesort, Timothée and Díaz-Rodríguez, Natalia and Goudou, Jean-François and Filliat, David</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Representation learning algorithms are designed to learn abstract features that characterize data. State representation learning (SRL) focuses on a particular kind of representation learning where learned features are in low dimension, evolve through time, and are influenced by actions of an agent. The representation is learned to capture the variation in the environment generated by the agent’s actions; this kind of representation is particularly suitable for robotics and control scenarios. In particular, the low dimension characteristic of the representation helps to overcome the curse of dimensionality, provides easier interpretation and utilization by humans and can help improve performance and speed in policy learning algorithms such as reinforcement learning. This survey aims at covering the state-of-the-art on state representation learning in the most recent years. It reviews different SRL methods that involve interaction with the environment, their implementations and their applications in robotics control tasks (simulated or real). In particular, it highlights how generic learning objectives are differently exploited in the reviewed algorithms. Finally, it discusses evaluation methods to assess the representation learned and summarizes current and future lines of research.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1016/j.neunet.2018.07.006</bib_extra>
			<bib_extra key="Eprint">1802.04181v2</bib_extra>
			<bib_extra key="File">1802.04181v2.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1802.04181v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1206.5538v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Representation Learning: A Review and New Perspectives</bib_title>
			<bib_authors>Bengio, Yoshua and Courville, Aaron and Vincent, Pascal</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2012</bib_year>
			<bib_extra key="Abstract">The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1206.5538v3</bib_extra>
			<bib_extra key="File">1206.5538v3.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1206.5538v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2101.04736v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Bootstrapping Motor Skill Learning with Motion Planning</bib_title>
			<bib_authors>Abbatematteo, Ben and Rosen, Eric and Tellex, Stefanie and Konidaris, George</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Learning a robot motor skill from scratch is impractically slow; so much so that in practice, learning must be bootstrapped using a good skill policy obtained from human demonstration. However, relying on human demonstration necessarily degrades the autonomy of robots that must learn a wide variety of skills over their operational lifetimes. We propose using kinematic motion planning as a completely autonomous, sample efficient way to bootstrap motor skill learning for object manipulation. We demonstrate the use of motion planners to bootstrap motor skills in two complex object manipulation scenarios with different policy representations: opening a drawer with a dynamic movement primitive representation, and closing a microwave door with a deep neural network policy. We also show how our method can bootstrap a motor skill for the challenging dynamic task of learning to hit a ball off a tee, where a kinematic plan based on treating the scene as static is insufficient to solve the task, but sufficient to bootstrap a more dynamic policy. In all three cases, our method is competitive with human-demonstrated initialization, and significantly outperforms starting with a random policy. This approach enables robots to to efficiently and autonomously learn motor policies for dynamic tasks without human demonstration.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2101.04736v1</bib_extra>
			<bib_extra key="File">2101.04736v1.pdf</bib_extra>
			<bib_extra key="Month">Jan</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2101.04736v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1709.02878v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>TensorFlow Agents: Efficient Batched Reinforcement Learning in TensorFlow</bib_title>
			<bib_authors>Hafner, Danijar and Davidson, James and Vanhoucke, Vincent</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">We introduce TensorFlow Agents, an efficient infrastructure paradigm for building parallel reinforcement learning algorithms in TensorFlow. We simulate multiple environments in parallel, and group them to perform the neural network computation on a batch rather than individual observations. This allows the TensorFlow execution engine to parallelize computation, without the need for manual synchronization. Environments are stepped in separate Python processes to progress them in parallel without interference of the global interpreter lock. As part of this project, we introduce BatchPPO, an efficient implementation of the proximal policy optimization algorithm. By open sourcing TensorFlow Agents, we hope to provide a flexible starting point for future projects that accelerates future research in the field.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1709.02878v2</bib_extra>
			<bib_extra key="File">1709.02878v2.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1709.02878v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1606.04934v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Improving Variational Inference with Inverse Autoregressive Flow</bib_title>
			<bib_authors>Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1606.04934v2</bib_extra>
			<bib_extra key="File">1606.04934v2.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1606.04934v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2107.08981v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Hierarchical Few-Shot Imitation with Skill Transition Models</bib_title>
			<bib_authors>Hakhamaneshi, Kourosh and Zhao, Ruihan and Zhan, Albert and Abbeel, Pieter and Laskin, Michael</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">A desirable property of autonomous agents is the ability to both solve long-horizon problems and generalize to unseen tasks. Recent advances in data-driven skill learning have shown that extracting behavioral priors from offline data can enable agents to solve challenging long-horizon tasks with reinforcement learning. However, generalization to tasks unseen during behavioral prior training remains an outstanding challenge. To this end, we present Few-shot Imitation with Skill Transition Models (FIST), an algorithm that extracts skills from offline data and utilizes them to generalize to unseen tasks given a few downstream demonstrations. FIST learns an inverse skill dynamics model, a distance function, and utilizes a semi-parametric approach for imitation. We show that FIST is capable of generalizing to new tasks and substantially outperforms prior baselines in navigation experiments requiring traversing unseen parts of a large maze and 7-DoF robotic arm experiments requiring manipulating previously unseen objects in a kitchen.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2107.08981v2</bib_extra>
			<bib_extra key="File">2107.08981v2.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2107.08981v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1801.06159v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>When Does Stochastic Gradient Algorithm Work Well?</bib_title>
			<bib_authors>Nguyen, Lam M. and Nguyen, Nam H. and Phan, Dzung T. and Kalagnanam, Jayant R. and Scheinberg, Katya</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">In this paper, we consider a general stochastic optimization problem which is often at the core of supervised learning, such as deep learning and linear classification. We consider a standard stochastic gradient descent (SGD) method with a fixed, large step size and propose a novel assumption on the objective function, under which this method has the improved convergence rates (to a neighborhood of the optimal solutions). We then empirically demonstrate that these assumptions hold for logistic regression and standard deep neural networks on classical data sets. Thus our analysis helps to explain when efficient behavior can be expected from the SGD method in training classification models and deep neural networks.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1801.06159v2</bib_extra>
			<bib_extra key="File">1801.06159v2.pdf</bib_extra>
			<bib_extra key="Month">Jan</bib_extra>
			<bib_extra key="Primaryclass">stat.ML</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1801.06159v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1803.02999v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>On First-Order Meta-Learning Algorithms</bib_title>
			<bib_authors>Nichol, Alex and Achiam, Joshua and Schulman, John</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1803.02999v3</bib_extra>
			<bib_extra key="File">1803.02999v3.pdf</bib_extra>
			<bib_extra key="Month">Mar</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1803.02999v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1504.00702v5</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>End-to-End Training of Deep Visuomotor Policies</bib_title>
			<bib_authors>Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2015</bib_year>
			<bib_extra key="Abstract">Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot’s motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a partially observed guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1504.00702v5</bib_extra>
			<bib_extra key="File">1504.00702v5.pdf</bib_extra>
			<bib_extra key="Month">Apr</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1504.00702v5</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1906.08253v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>When to Trust Your Model: Model-Based Policy Optimization</bib_title>
			<bib_authors>Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Designing effective model-based reinforcement learning algorithms is difficult because the ease of data generation must be weighed against the bias of model-generated data. In this paper, we study the role of model usage in policy optimization both theoretically and empirically. We first formulate and analyze a model-based reinforcement learning algorithm with a guarantee of monotonic improvement at each step. In practice, this analysis is overly pessimistic and suggests that real off-policy data is always preferable to model-generated on-policy data, but we show that an empirical estimate of model generalization can be incorporated into such analysis to justify model usage. Motivated by this analysis, we then demonstrate that a simple procedure of using short model-generated rollouts branched from real data has the benefits of more complicated model-based algorithms without the usual pitfalls. In particular, this approach surpasses the sample efficiency of prior model-based methods, matches the asymptotic performance of the best model-free algorithms, and scales to horizons that cause other model-based methods to fail entirely.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1906.08253v3</bib_extra>
			<bib_extra key="File">1906.08253v3.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1906.08253v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1912.01603v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Dream to Control: Learning Behaviors by Latent Imagination</bib_title>
			<bib_authors>Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Learned world models summarize an agent’s experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1912.01603v3</bib_extra>
			<bib_extra key="File">1912.01603v3.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1912.01603v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1707.08817v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards</bib_title>
			<bib_authors>Vecerik, Mel and Hester, Todd and Scholz, Jonathan and Wang, Fumin and Pietquin, Olivier and Piot, Bilal and Heess, Nicolas and Rothörl, Thomas and Lampe, Thomas and Riedmiller, Martin</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">We propose a general and model-free approach for Reinforcement Learning (RL) on real robotics with sparse rewards. We build upon the Deep Deterministic Policy Gradient (DDPG) algorithm to use demonstrations. Both demonstrations and actual interactions are used to fill a replay buffer and the sampling ratio between demonstrations and transitions is automatically tuned via a prioritized replay mechanism. Typically, carefully engineered shaping rewards are required to enable the agents to efficiently explore on high dimensional control problems such as robotics. They are also required for model-based acceleration methods relying on local solvers such as iLQG (e.g. Guided Policy Search and Normalized Advantage Function). The demonstrations replace the need for carefully engineered rewards, and reduce the exploration problem encountered by classical RL approaches in these domains. Demonstrations are collected by a robot kinesthetically force-controlled by a human demonstrator. Results on four simulated insertion tasks show that DDPG from demonstrations out-performs DDPG, and does not require engineered rewards. Finally, we demonstrate the method on a real robotics task consisting of inserting a clip (flexible object) into a rigid object.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1707.08817v2</bib_extra>
			<bib_extra key="File">1707.08817v2.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1707.08817v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1810.12894v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Exploration by Random Network Distillation</bib_title>
			<bib_authors>Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma’s Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1810.12894v1</bib_extra>
			<bib_extra key="File">1810.12894v1.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1810.12894v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1806.03884v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis</bib_title>
			<bib_authors>George, Thomas and Laurent, César and Bouthillier, Xavier and Ballas, Nicolas and Vincent, Pascal</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Optimization algorithms that leverage gradient covariance information, such as variants of natural gradient descent (Amari, 1998), offer the prospect of yielding more effective descent directions. For models with many parameters, the covariance matrix they are based on becomes gigantic, making them inapplicable in their original form. This has motivated research into both simple diagonal approximations and more sophisticated factored approximations such as KFAC (Heskes, 2000; Martens &amp; Grosse, 2015; Grosse &amp; Martens, 2016). In the present work we draw inspiration from both to propose a novel approximation that is provably better than KFAC and amendable to cheap partial updates. It consists in tracking a diagonal variance, not in parameter coordinates, but in a Kronecker-factored eigenbasis, in which the diagonal approximation is likely to be more effective. Experiments show improvements over KFAC in optimization speed for several deep network architectures.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1806.03884v2</bib_extra>
			<bib_extra key="File">1806.03884v2.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Note">Advances in Neural Information Processing Systems 2018</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1806.03884v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2106.01946v4</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Convex optimization</bib_title>
			<bib_authors>Vorontsova, Evgeniya and Hildebrand, Roland and Gasnikov, Alexander and Stonyakin, Fedor</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">This textbook is based on lectures given by the authors at MIPT (Moscow), HSE (Moscow), FEFU (Vladivostok), V.I. Vernadsky KFU (Simferopol), ASU (Republic of Adygea), and the University of Grenoble-Alpes (Grenoble, France). First of all, the authors focused on the program of a two-semester course of lectures on convex optimization, which is given to students of MIPT. The first chapter of this book contains the materials of the first semester (&quot;Fundamentals of convex analysis and optimization&quot;), the second and third chapters contain the materials of the second semester (&quot;Numerical methods of convex optimization&quot;). The textbook has a number of features. First, in contrast to the classic manuals, this book does not provide proofs of all the theorems mentioned. This allowed, on one side, to describe more themes, but on the other side, made the presentation less self-sufficient. The second important point is that part of the material is advanced and is published in the Russian educational literature, apparently for the first time. Third, the accents that are given do not always coincide with the generally accepted accents in the textbooks that are now popular. First of all, we talk about a sufficiently advanced presentation of conic optimization, including robust optimization, as a vivid demonstration of the capabilities of modern convex analysis.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2106.01946v4</bib_extra>
			<bib_extra key="File">2106.01946v4.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">math.OC</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2106.01946v4</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2002.08484v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Estimating Training Data Influence by Tracing Gradient Descent</bib_title>
			<bib_authors>Pruthi, Garima and Liu, Frederick and Sundararajan, Mukund and Kale, Satyen</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">We introduce a method called TracIn that computes the influence of a training example on a prediction made by the model. The idea is to trace how the loss on the test point changes during the training process whenever the training example of interest was utilized. We provide a scalable implementation of TracIn via: (a) a first-order gradient approximation to the exact computation, (b) saved checkpoints of standard training procedures, and (c) cherry-picking layers of a deep neural network. In contrast with previously proposed methods, TracIn is simple to implement; all it needs is the ability to work with gradients, checkpoints, and loss functions. The method is general. It applies to any machine learning model trained using stochastic gradient descent or a variant of it, agnostic of architecture, domain and task. We expect the method to be widely useful within processes that study and improve training data.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2002.08484v3</bib_extra>
			<bib_extra key="File">2002.08484v3.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2002.08484v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1911.08265v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model</bib_title>
			<bib_authors>Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Doi">10.1038/s41586-020-03051-4</bib_extra>
			<bib_extra key="Eprint">1911.08265v2</bib_extra>
			<bib_extra key="File">1911.08265v2.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1911.08265v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2111.07999v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Adversarial Skill Chaining for Long-Horizon Robot Manipulation via Terminal State Regularization</bib_title>
			<bib_authors>Lee, Youngwoon and Lim, Joseph J. and Anandkumar, Anima and Zhu, Yuke</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">Skill chaining is a promising approach for synthesizing complex behaviors by sequentially combining previously learned skills. Yet, a naive composition of skills fails when a policy encounters a starting state never seen during its training. For successful skill chaining, prior approaches attempt to widen the policy’s starting state distribution. However, these approaches require larger state distributions to be covered as more policies are sequenced, and thus are limited to short skill sequences. In this paper, we propose to chain multiple policies without excessively large initial state distributions by regularizing the terminal state distributions in an adversarial learning framework. We evaluate our approach on two complex long-horizon manipulation tasks of furniture assembly. Our results have shown that our method establishes the first model-free reinforcement learning algorithm to solve these tasks; whereas prior skill chaining approaches fail. The code and videos are available at https://clvrai.com/skill-chaining</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2111.07999v1</bib_extra>
			<bib_extra key="File">2111.07999v1.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2111.07999v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1906.07987v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Adaptive Temporal-Difference Learning for Policy Evaluation with Per-State Uncertainty Estimates</bib_title>
			<bib_authors>Penedones, Hugo and Riquelme, Carlos and Vincent, Damien and Maennel, Hartmut and Mann, Timothy and Barreto, Andre and Gelly, Sylvain and Neu, Gergely</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2019</bib_year>
			<bib_extra key="Abstract">We consider the core reinforcement-learning problem of on-policy value function approximation from a batch of trajectory data, and focus on various issues of Temporal Difference (TD) learning and Monte Carlo (MC) policy evaluation. The two methods are known to achieve complementary bias-variance trade-off properties, with TD tending to achieve lower variance but potentially higher bias. In this paper, we argue that the larger bias of TD can be a result of the amplification of local approximation errors. We address this by proposing an algorithm that adaptively switches between TD and MC in each state, thus mitigating the propagation of errors. Our method is based on learned confidence intervals that detect biases of TD estimates. We demonstrate in a variety of policy evaluation tasks that this simple adaptive algorithm performs competitively with the best approach in hindsight, suggesting that learned confidence intervals are a powerful technique for adapting policy evaluation to use TD or MC returns in a data-driven way.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1906.07987v1</bib_extra>
			<bib_extra key="File">1906.07987v1.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1906.07987v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1607.07611v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Learning Null Space Projections in Operational Space Formulation</bib_title>
			<bib_authors>Lin, Hsiu-Chin and Howard, Matthew</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2016</bib_year>
			<bib_extra key="Abstract">In recent years, a number of tools have become available that recover the underlying control policy from constrained movements. However, few have explicitly considered learning the constraints of the motion and ways to cope with unknown environment. In this paper, we consider learning the null space projection matrix of a kinematically constrained system in the absence of any prior knowledge either on the underlying policy, the geometry, or dimensionality of the constraints. Our evaluations have demonstrated the effectiveness of the proposed approach on problems of differing dimensionality, and with different degrees of non-linearity.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1607.07611v1</bib_extra>
			<bib_extra key="File">1607.07611v1.pdf</bib_extra>
			<bib_extra key="Month">Jul</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1607.07611v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2204.07404v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Divide &amp; Conquer Imitation Learning</bib_title>
			<bib_authors>Chenu, Alexandre and Perrin-Gilbert, Nicolas and Sigaud, Olivier</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2022</bib_year>
			<bib_extra key="Abstract">When cast into the Deep Reinforcement Learning framework, many robotics tasks require solving a long horizon and sparse reward problem, where learning algorithms struggle. In such context, Imitation Learning (IL) can be a powerful approach to bootstrap the learning process. However, most IL methods require several expert demonstrations which can be prohibitively difficult to acquire. Only a handful of IL algorithms have shown efficiency in the context of an extreme low expert data regime where a single expert demonstration is available. In this paper, we present a novel algorithm designed to imitate complex robotic tasks from the states of an expert trajectory. Based on a sequential inductive bias, our method divides the complex task into smaller skills. The skills are learned into a goal-conditioned policy that is able to solve each skill individually and chain skills to solve the entire task. We show that our method imitates a non-holonomic navigation task and scales to a complex simulated robotic manipulation task with very high sample efficiency.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2204.07404v1</bib_extra>
			<bib_extra key="File">2204.07404v1.pdf</bib_extra>
			<bib_extra key="Month">Apr</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2204.07404v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1212.1524v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Layer-wise learning of deep generative models</bib_title>
			<bib_authors>Arnold, Ludovic and Ollivier, Yann</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2012</bib_year>
			<bib_extra key="Abstract">When using deep, multi-layered architectures to build generative models of data, it is difficult to train all layers at once. We propose a layer-wise training procedure admitting a performance guarantee compared to the global optimum. It is based on an optimistic proxy of future performance, the best latent marginal. We interpret auto-encoders in this setting as generative models, by showing that they train a lower bound of this criterion. We test the new learning procedure against a state of the art method (stacked RBMs), and find it to improve performance. Both theory and experiments highlight the importance, when training deep architectures, of using an inference model (from data to hidden variables) richer than the generative model (from hidden variables to data).</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1212.1524v2</bib_extra>
			<bib_extra key="File">1212.1524v2.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.NE</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1212.1524v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1811.11359v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Unsupervised Control Through Non-Parametric Discriminative Rewards</bib_title>
			<bib_authors>Warde-Farley, David and Wiele, Tom Van de and Kulkarni, Tejas and Ionescu, Catalin and Hansen, Steven and Mnih, Volodymyr</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">Learning to control an environment without hand-crafted rewards or expert data remains challenging and is at the frontier of reinforcement learning research. We present an unsupervised learning algorithm to train agents to achieve perceptually-specified goals using only a stream of observations and actions. Our agent simultaneously learns a goal-conditioned policy and a goal achievement reward function that measures how similar a state is to the goal state. This dual optimization leads to a co-operative game, giving rise to a learned reward function that reflects similarity in controllable aspects of the environment instead of distance in the space of observations. We demonstrate the efficacy of our agent to learn, in an unsupervised manner, to reach a diverse set of goals on three domains – Atari, the DeepMind Control Suite and DeepMind Lab.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1811.11359v1</bib_extra>
			<bib_extra key="File">1811.11359v1.pdf</bib_extra>
			<bib_extra key="Month">Nov</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1811.11359v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2102.12086v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Modern Koopman Theory for Dynamical Systems</bib_title>
			<bib_authors>Brunton, Steven L. and Budišić, Marko and Kaiser, Eurika and Kutz, J. Nathan</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2021</bib_year>
			<bib_extra key="Abstract">The field of dynamical systems is being transformed by the mathematical tools and algorithms emerging from modern computing and data science. First-principles derivations and asymptotic reductions are giving way to data-driven approaches that formulate models in operator theoretic or probabilistic frameworks. Koopman spectral theory has emerged as a dominant perspective over the past decade, in which nonlinear dynamics are represented in terms of an infinite-dimensional linear operator acting on the space of all possible measurement functions of the system. This linear representation of nonlinear dynamics has tremendous potential to enable the prediction, estimation, and control of nonlinear systems with standard textbook methods developed for linear systems. However, obtaining finite-dimensional coordinate systems and embeddings in which the dynamics appear approximately linear remains a central open challenge. The success of Koopman analysis is due primarily to three key factors: 1) there exists rigorous theory connecting it to classical geometric approaches for dynamical systems, 2) the approach is formulated in terms of measurements, making it ideal for leveraging big-data and machine learning techniques, and 3) simple, yet powerful numerical algorithms, such as the dynamic mode decomposition (DMD), have been developed and extended to reduce Koopman theory to practice in real-world applications. In this review, we provide an overview of modern Koopman operator theory, describing recent theoretical and algorithmic developments and highlighting these methods with a diverse range of applications. We also discuss key advances and challenges in the rapidly growing field of machine learning that are likely to drive future developments and significantly transform the theoretical landscape of dynamical systems.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2102.12086v2</bib_extra>
			<bib_extra key="File">2102.12086v2.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">math.DS</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2102.12086v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1709.10087v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations</bib_title>
			<bib_authors>Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">Dexterous multi-fingered hands are extremely versatile and provide a generic way to perform a multitude of tasks in human-centric environments. However, effectively controlling them remains challenging due to their high dimensionality and large number of potential contacts. Deep reinforcement learning (DRL) provides a model-agnostic approach to control complex dynamical systems, but has not been shown to scale to high-dimensional dexterous manipulation. Furthermore, deployment of DRL on physical systems remains challenging due to sample inefficiency. Consequently, the success of DRL in robotics has thus far been limited to simpler manipulators and tasks. In this work, we show that model-free DRL can effectively scale up to complex manipulation tasks with a high-dimensional 24-DoF hand, and solve them from scratch in simulated experiments. Furthermore, with the use of a small number of human demonstrations, the sample complexity can be significantly reduced, which enables learning with sample sizes equivalent to a few hours of robot experience. The use of demonstrations result in policies that exhibit very natural movements and, surprisingly, are also substantially more robust.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1709.10087v2</bib_extra>
			<bib_extra key="File">1709.10087v2.pdf</bib_extra>
			<bib_extra key="Month">Sep</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1709.10087v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2012.13658v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Locally Persistent Exploration in Continuous Control Tasks with Sparse Rewards</bib_title>
			<bib_authors>Amin, Susan and Gomrokchi, Maziar and Aboutalebi, Hossein and Satija, Harsh and Precup, Doina</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">A major challenge in reinforcement learning is the design of exploration strategies, especially for environments with sparse reward structures and continuous state and action spaces. Intuitively, if the reinforcement signal is very scarce, the agent should rely on some form of short-term memory in order to cover its environment efficiently. We propose a new exploration method, based on two intuitions: (1) the choice of the next exploratory action should depend not only on the (Markovian) state of the environment, but also on the agent’s trajectory so far, and (2) the agent should utilize a measure of spread in the state space to avoid getting stuck in a small region. Our method leverages concepts often used in statistical physics to provide explanations for the behavior of simplified (polymer) chains in order to generate persistent (locally self-avoiding) trajectories in state space. We discuss the theoretical properties of locally self-avoiding walks and their ability to provide a kind of short-term memory through a decaying temporal correlation within the trajectory. We provide empirical evaluations of our approach in a simulated 2D navigation task, as well as higher-dimensional MuJoCo continuous control locomotion tasks with sparse rewards.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2012.13658v2</bib_extra>
			<bib_extra key="File">2012.13658v2.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2012.13658v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1804.02808v2</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Latent Space Policies for Hierarchical Reinforcement Learning</bib_title>
			<bib_authors>Haarnoja, Tuomas and Hartikainen, Kristian and Abbeel, Pieter and Levine, Sergey</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2018</bib_year>
			<bib_extra key="Abstract">We address the problem of learning hierarchical deep neural network policies for reinforcement learning. In contrast to methods that explicitly restrict or cripple lower layers of a hierarchy to force them to use higher-level modulating signals, each layer in our framework is trained to directly solve the task, but acquires a range of diverse strategies via a maximum entropy reinforcement learning objective. Each layer is also augmented with latent random variables, which are sampled from a prior distribution during the training of that layer. The maximum entropy objective causes these latent variables to be incorporated into the layer’s policy, and the higher level layer can directly control the behavior of the lower layer through this latent space. Furthermore, by constraining the mapping from latent variables to actions to be invertible, higher layers retain full expressivity: neither the higher layers nor the lower layers are constrained in their behavior. Our experimental evaluation demonstrates that we can improve on the performance of single-layer policies on standard benchmark tasks simply by adding additional layers, and that our method can solve more complex sparse-reward tasks by learning higher-level policies on top of high-entropy skills optimized for simple low-level objectives.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1804.02808v2</bib_extra>
			<bib_extra key="File">1804.02808v2.pdf</bib_extra>
			<bib_extra key="Month">Apr</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1804.02808v2</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2202.00817v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Do Differentiable Simulators Give Better Policy Gradients?</bib_title>
			<bib_authors>Suh, H. J. Terry and Simchowitz, Max and Zhang, Kaiqing and Tedrake, Russ</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2022</bib_year>
			<bib_extra key="Abstract">Differentiable simulators promise faster computation time for reinforcement learning by replacing zeroth-order gradient estimates of a stochastic objective with an estimate based on first-order gradients. However, it is yet unclear what factors decide the performance of the two estimators on complex landscapes that involve long-horizon planning and control on physical systems, despite the crucial relevance of this question for the utility of differentiable simulators. We show that characteristics of certain physical systems, such as stiffness or discontinuities, may compromise the efficacy of the first-order estimator, and analyze this phenomenon through the lens of bias and variance. We additionally propose an α-order gradient estimator, with $α \in [0,1]$, which correctly utilizes exact gradients to combine the efficiency of first-order estimates with the robustness of zero-order methods. We demonstrate the pitfalls of traditional estimators and the advantages of the α-order estimator on some numerical examples.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2202.00817v1</bib_extra>
			<bib_extra key="File">2202.00817v1.pdf</bib_extra>
			<bib_extra key="Month">Feb</bib_extra>
			<bib_extra key="Primaryclass">cs.LG</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2202.00817v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2005.10934v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>LEAF: Latent Exploration Along the Frontier</bib_title>
			<bib_authors>Bharadhwaj, Homanga and Garg, Animesh and Shkurti, Florian</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">Self-supervised goal proposal and reaching is a key component for exploration and efficient policy learning algorithms. Such a self-supervised approach without access to any oracle goal sampling distribution requires deep exploration and commitment so that long horizon plans can be efficiently discovered. In this paper, we propose an exploration framework, which learns a dynamics-aware manifold of reachable states. For a goal, our proposed method deterministically visits a state at the current frontier of reachable states (commitment/reaching) and then stochastically explores to reach the goal (exploration). This allocates exploration budget near the frontier of the reachable region instead of its interior. We target the challenging problem of policy learning from initial and goal states specified as images, and do not assume any access to the underlying ground-truth states of the robot and the environment. To keep track of reachable latent states, we propose a distance-conditioned reachability network that is trained to infer whether one state is reachable from another within the specified latent space distance. Given an initial state, we obtain a frontier of reachable states from that state. By incorporating a curriculum for sampling easier goals (closer to the start state) before more difficult goals, we demonstrate that the proposed self-supervised exploration algorithm, superior performance compared to existing baselines on a set of challenging robotic environments.https://sites.google.com/view/leaf-exploration</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2005.10934v3</bib_extra>
			<bib_extra key="File">2005.10934v3.pdf</bib_extra>
			<bib_extra key="Month">May</bib_extra>
			<bib_extra key="Primaryclass">cs.RO</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2005.10934v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2010.14274v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Behavior Priors for Efficient Reinforcement Learning</bib_title>
			<bib_authors>Tirumala, Dhruva and Galashov, Alexandre and Noh, Hyeonwoo and Hasenclever, Leonard and Pascanu, Razvan and Schwarz, Jonathan and Desjardins, Guillaume and Czarnecki, Wojciech Marian and Ahuja, Arun and Teh, Yee Whye and Heess, Nicolas</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2020</bib_year>
			<bib_extra key="Abstract">As we deploy reinforcement learning agents to solve increasingly challenging problems, methods that allow us to inject prior knowledge about the structure of the world and effective solution strategies becomes increasingly important. In this work we consider how information and architectural constraints can be combined with ideas from the probabilistic modeling literature to learn behavior priors that capture the common movement and interaction patterns that are shared across a set of related tasks or contexts. For example the day-to day behavior of humans comprises distinctive locomotion and manipulation patterns that recur across many different situations and goals. We discuss how such behavior patterns can be captured using probabilistic trajectory models and how these can be integrated effectively into reinforcement learning schemes, e.g. to facilitate multi-task and transfer learning. We then extend these ideas to latent variable models and consider a formulation to learn hierarchical priors that capture different aspects of the behavior in reusable modules. We discuss how such latent variable formulations connect to related work on hierarchical reinforcement learning (HRL) and mutual information and curiosity based objectives, thereby offering an alternative perspective on existing ideas. We demonstrate the effectiveness of our framework by applying it to a range of simulated continuous control domains.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2010.14274v1</bib_extra>
			<bib_extra key="File">2010.14274v1.pdf</bib_extra>
			<bib_extra key="Month">Oct</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2010.14274v1</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>1712.06563v3</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients</bib_title>
			<bib_authors>Lehman, Joel and Chen, Jay and Clune, Jeff and Stanley, Kenneth O.</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2017</bib_year>
			<bib_extra key="Abstract">While neuroevolution (evolving neural networks) has a successful track record across a variety of domains from reinforcement learning to artificial life, it is rarely applied to large, deep neural networks. A central reason is that while random mutation generally works in low dimensions, a random perturbation of thousands or millions of weights is likely to break existing functionality, providing no learning signal even if some individual weight changes were beneficial. This paper proposes a solution by introducing a family of safe mutation (SM) operators that aim within the mutation operator itself to find a degree of change that does not alter network behavior too much, but still facilitates exploration. Importantly, these SM operators do not require any additional interactions with the environment. The most effective SM variant capitalizes on the intriguing opportunity to scale the degree of mutation of each individual weight according to the sensitivity of the network’s outputs to that weight, which requires computing the gradient of outputs with respect to the weights (instead of the gradient of error, as in conventional deep learning). This safe mutation through gradients (SM-G) operator dramatically increases the ability of a simple genetic algorithm-based neuroevolution method to find solutions in high-dimensional domains that require deep and/or recurrent neural networks (which tend to be particularly brittle to mutation), including domains that require processing raw pixels. By improving our ability to evolve deep neural networks, this new safer approach to mutation expands the scope of domains amenable to neuroevolution.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">1712.06563v3</bib_extra>
			<bib_extra key="File">1712.06563v3.pdf</bib_extra>
			<bib_extra key="Month">Dec</bib_extra>
			<bib_extra key="Primaryclass">cs.NE</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/1712.06563v3</bib_extra>
		</doc>
		<doc>
			<filename></filename>
			<key>2206.04114v1</key>
			<notes></notes>
			<bib_type>Article</bib_type>
			<bib_doi></bib_doi>
			<bib_title>Deep Hierarchical Planning from Pixels</bib_title>
			<bib_authors>Hafner, Danijar and Lee, Kuang-Huei and Fischer, Ian and Abbeel, Pieter</bib_authors>
			<bib_journal></bib_journal>
			<bib_volume></bib_volume>
			<bib_number></bib_number>
			<bib_pages></bib_pages>
			<bib_year>2022</bib_year>
			<bib_extra key="Abstract">Intelligent agents need to select long sequences of actions to solve complex tasks. While humans easily break down tasks into subgoals and reach them through millions of muscle commands, current artificial intelligence is limited to tasks with horizons of a few hundred decisions, despite large compute budgets. Research on hierarchical reinforcement learning aims to overcome this limitation but has proven to be challenging, current methods rely on manually specified goal spaces or subtasks, and no general solution exists. We introduce Director, a practical method for learning hierarchical behaviors directly from pixels by planning inside the latent space of a learned world model. The high-level policy maximizes task and exploration rewards by selecting latent goals and the low-level policy learns to achieve the goals. Despite operating in latent space, the decisions are interpretable because the world model can decode goals into images for visualization. Director outperforms exploration methods on tasks with sparse rewards, including 3D maze traversal with a quadruped robot from an egocentric camera and proprioception, without access to the global position or top-down view that was used by prior work. Director also learns successful behaviors across a wide range of environments, including visual control, Atari games, and DMLab levels.</bib_extra>
			<bib_extra key="Archiveprefix">arXiv</bib_extra>
			<bib_extra key="Eprint">2206.04114v1</bib_extra>
			<bib_extra key="File">2206.04114v1.pdf</bib_extra>
			<bib_extra key="Month">Jun</bib_extra>
			<bib_extra key="Primaryclass">cs.AI</bib_extra>
			<bib_extra key="Url">http://arxiv.org/abs/2206.04114v1</bib_extra>
		</doc>
	</doclist>
</library>
