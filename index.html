<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Scientific documents</title>

  <meta name="Author" content="Nicolas Perrin-Gilbert">



  <meta name="Description" content="Nicolas Perrin-Gilbert">



  <meta name="Copyright" content="This file may be redistributed and/or modified without limitation">



  <meta name="Language" content="en">



  <meta name="Generator" content="">



  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">



  <meta http-equiv="Content-Language" content="en">



  <link rev="made" href="mailto:nicolas.perrin@sorbonne-universite.fr">



  <link rel="StyleSheet" href="index_files/screenfsos.css" type="text/css" media="screen">



  <link rel="StyleSheet" href="index_files/print.css" type="text/css" media="print"></head>



<script type="text/javascript">

<!--

// QuickSearch script for JabRef HTML export 

// Version: 3.0

//

// Copyright (c) 2006-2011, Mark Schenk

//

// This software is distributed under a Creative Commons Attribution 3.0 License

// http://creativecommons.org/licenses/by/3.0/

//

// Features:

// - intuitive find-as-you-type searching

//    ~ case insensitive

//    ~ ignore diacritics (optional)

//

// - search with/without Regular Expressions

// - match BibTeX key

//



// Search settings

var searchAbstract = true;	// search in abstract

var searchReview = true;	// search in review



var noSquiggles = true; 	// ignore diacritics when searching

var searchRegExp = false; 	// enable RegExp searches





if (window.addEventListener) {

	window.addEventListener("load",initSearch,false); }

else if (window.attachEvent) {

	window.attachEvent("onload", initSearch); }



function initSearch() {

	// check for quick search table and searchfield

	if (!document.getElementById('qs_table')||!document.getElementById('quicksearch')) { return; }



	// load all the rows and sort into arrays

	loadTableData();

	

	//find the query field

	qsfield = document.getElementById('qs_field');



	// previous search term; used for speed optimisation

	prevSearch = '';



	//find statistics location

	stats = document.getElementById('stat');

	setStatistics(-1);

	

	// set up preferences

	initPreferences();



	// shows the searchfield

	document.getElementById('quicksearch').style.display = 'block';

        document.getElementById('qs_field').value = querySt("search");

	document.getElementById('qs_field').onkeyup = quickSearch;

 	quickSearch(qs_field);

}



function loadTableData() {

	// find table and appropriate rows

	searchTable = document.getElementById('qs_table');

	var allRows = searchTable.getElementsByTagName('tbody')[0].getElementsByTagName('tr');



	// split all rows into entryRows and infoRows (e.g. abstract, review, bibtex)

	entryRows = new Array(); infoRows = new Array(); absRows = new Array(); revRows = new Array();



	// get data from each row

	entryRowsData = new Array(); absRowsData = new Array(); revRowsData = new Array(); 

	

	BibTeXKeys = new Array();

	

	for (var i=0, k=0, j=0; i<allRows.length;i++) {

		if (allRows[i].className.match(/entry/)) {

			entryRows[j] = allRows[i];

			entryRowsData[j] = stripDiacritics(getTextContent(allRows[i]));

			allRows[i].id ? BibTeXKeys[j] = allRows[i].id : allRows[i].id = 'autokey_'+j;

			j ++;

		} else {

			infoRows[k++] = allRows[i];

			// check for abstract/review

			if (allRows[i].className.match(/abstract/)) {

				absRows.push(allRows[i]);

				absRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));

			} else if (allRows[i].className.match(/review/)) {

				revRows.push(allRows[i]);

				revRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));

			}

		}

	}

	//number of entries and rows

	numEntries = entryRows.length;

	numInfo = infoRows.length;

	numAbs = absRows.length;

	numRev = revRows.length;

}



function querySt(ji) {



    hu = window.location.search.substring(1);

    gy = hu.split("&");



    for (i=0;i<gy.length;i++) {

        ft = gy[i].split("=");

        if (ft[0] == ji) {

            return decodeURIComponent(ft[1]).replace(/["]/g,'');

        }

    }

    return '';

}



function quickSearch(){

	

	tInput = qsfield;



	if (tInput.value.length == 0) {

		showAll();

		setStatistics(-1);

		qsfield.className = '';

		return;

	} else {

		t = stripDiacritics(tInput.value);



		if(!searchRegExp) { t = escapeRegExp(t); }

			

		// only search for valid RegExp

		try {

			textRegExp = new RegExp(t,"im");

			closeAllInfo();

			qsfield.className = '';

		}

			catch(err) {

			prevSearch = tInput.value;

			qsfield.className = 'invalidsearch';

			return;

		}

	}

	

	// count number of hits

	var hits = 0;



	// start looping through all entry rows

	for (var i = 0; cRow = entryRows[i]; i++){



		// only show search the cells if it isn't already hidden OR if the search term is getting shorter, then search all

		if(cRow.className.indexOf('noshow')==-1 || tInput.value.length <= prevSearch.length){

			var found = false; 



			if (entryRowsData[i].search(textRegExp) != -1 || BibTeXKeys[i].search(textRegExp) != -1){ 

				found = true;

			} else {

				if(searchAbstract && absRowsData[i]!=undefined) {

					if (absRowsData[i].search(textRegExp) != -1){ found=true; } 

				}

				if(searchReview && revRowsData[i]!=undefined) {

					if (revRowsData[i].search(textRegExp) != -1){ found=true; } 

				}

			}

			

			if (found){

				cRow.className = 'entry show';

				hits++;

			} else {

				cRow.className = 'entry noshow';

			}

		}

	}



	// update statistics

	setStatistics(hits)

	

	// set previous search value

	prevSearch = tInput.value;

}





// Strip Diacritics from text

// http://stackoverflow.com/questions/990904/javascript-remove-accents-in-strings



// String containing replacement characters for stripping accents 

var stripstring = 

    'AAAAAAACEEEEIIII'+

    'DNOOOOO.OUUUUY..'+

    'aaaaaaaceeeeiiii'+

    'dnooooo.ouuuuy.y'+

    'AaAaAaCcCcCcCcDd'+

    'DdEeEeEeEeEeGgGg'+

    'GgGgHhHhIiIiIiIi'+

    'IiIiJjKkkLlLlLlL'+

    'lJlNnNnNnnNnOoOo'+

    'OoOoRrRrRrSsSsSs'+

    'SsTtTtTtUuUuUuUu'+

    'UuUuWwYyYZzZzZz.';



function stripDiacritics(str){



    if(noSquiggles==false){

        return str;

    }



    var answer='';

    for(var i=0;i<str.length;i++){

        var ch=str[i];

        var chindex=ch.charCodeAt(0)-192;   // Index of character code in the strip string

        if(chindex>=0 && chindex<stripstring.length){

            // Character is within our table, so we can strip the accent...

            var outch=stripstring.charAt(chindex);

            // ...unless it was shown as a '.'

            if(outch!='.')ch=outch;

        }

        answer+=ch;

    }

    return answer;

}



// http://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex

// NOTE: must escape every \ in the export code because of the JabRef Export...

function escapeRegExp(str) {

  return str.replace(/[-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");

}



function toggleInfo(articleid,info) {



	var entry = document.getElementById(articleid);

	var abs = document.getElementById('abs_'+articleid);

	var rev = document.getElementById('rev_'+articleid);

	var bib = document.getElementById('bib_'+articleid);

	

	if (abs && info == 'abstract') {

		abs.className.indexOf('noshow') == -1?abs.className = 'abstract noshow':abs.className = 'abstract show';

	} else if (rev && info == 'review') {

		rev.className.indexOf('noshow') == -1?rev.className = 'review noshow':rev.className = 'review show';

	} else if (bib && info == 'bibtex') {

		bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex show';

	} else { 

		return;

	}



	// check if one or the other is available

	var revshow; var absshow; var bibshow;

	(abs && abs.className.indexOf('noshow') == -1)? absshow = true: absshow = false;

	(rev && rev.className.indexOf('noshow') == -1)? revshow = true: revshow = false;	

	(bib && bib.className.indexOf('noshow') == -1)? bibshow = true: bibshow = false;

	

	// highlight original entry

	if(entry) {

		if (revshow || absshow || bibshow) {

		entry.className = 'entry highlight show';

		} else {

		entry.className = 'entry show';

		}

	}

	

	// When there's a combination of abstract/review/bibtex showing, need to add class for correct styling

	if(absshow) {

		(revshow||bibshow)?abs.className = 'abstract nextshow':abs.className = 'abstract';

	} 

	if (revshow) {

		bibshow?rev.className = 'review nextshow': rev.className = 'review';

	}	

	

}



function setStatistics (hits) {

	if(hits < 0) { hits=numEntries; }

	if(stats) { stats.firstChild.data = hits + '/' + numEntries}

}



function getTextContent(node) {

	// Function written by Arve Bersvendsen

	// http://www.virtuelvis.com

	

	if (node.nodeType == 3) {

	return node.nodeValue;

	} // text node

	if (node.nodeType == 1 && node.className != "infolinks") { // element node

	var text = [];

	for (var chld = node.firstChild;chld;chld=chld.nextSibling) {

		text.push(getTextContent(chld));

	}

	return text.join("");

	} return ""; // some other node, won't contain text nodes.

}



function showAll(){

	closeAllInfo();

	for (var i = 0; i < numEntries; i++){ entryRows[i].className = 'entry show'; }

}



function closeAllInfo(){

	for (var i=0; i < numInfo; i++){

		if (infoRows[i].className.indexOf('noshow') ==-1) {

			infoRows[i].className = infoRows[i].className + ' noshow';

		}

	}

}



function clearQS() {

	qsfield.value = '';

	showAll();

}



function redoQS(){

	showAll();

	quickSearch(qsfield);

}



function updateSetting(obj){

	var option = obj.id;

	var checked = obj.value;



	switch(option)

	 {

	 case "opt_searchAbs":

	   searchAbstract=!searchAbstract;

	   redoQS();

	   break;

	 case "opt_searchRev":

	   searchReview=!searchReview;

	   redoQS();

	   break;

	 case "opt_useRegExp":

	   searchRegExp=!searchRegExp;

	   redoQS();

	   break;

	 case "opt_noAccents":

	   noSquiggles=!noSquiggles;

	   loadTableData();

	   redoQS();

	   break;

	 }

}



function initPreferences(){

	if(searchAbstract){document.getElementById("opt_searchAbs").checked = true;}

	if(searchReview){document.getElementById("opt_searchRev").checked = true;}

	if(noSquiggles){document.getElementById("opt_noAccents").checked = true;}

	if(searchRegExp){document.getElementById("opt_useRegExp").checked = true;}

	

	if(numAbs==0) {document.getElementById("opt_searchAbs").parentNode.style.display = 'none';}

	if(numRev==0) {document.getElementById("opt_searchRev").parentNode.style.display = 'none';}

}



function toggleSettings(){

	var togglebutton = document.getElementById('showsettings');

	var settings = document.getElementById('settings');

	

	if(settings.className == "hidden"){

		settings.className = "show";

		togglebutton.innerText = "close settings";

		togglebutton.textContent = "close settings";

	}else{

		settings.className = "hidden";

		togglebutton.innerText = "settings...";		

		togglebutton.textContent = "settings...";

	}

}



-->

</script>

<style type="text/css">



html {

    font-size: max(1.4vh, 14px);

}



html a:link { color: black; }





form#quicksearch { width: auto; border-style: solid; border-color: gray; border-width: 1px 0px; padding: 0.7em 0.5em; display:none; position:relative; }

span#searchstat {padding-left: 1em;}



div#settings { margin-top:0.7em; /* border-bottom: 1px transparent solid; background-color: #efefef; border: 1px grey solid; */ }

div#settings ul {margin: 0; padding: 0; }

div#settings li {margin: 0; padding: 0 1em 0 0; display: inline; list-style: none; }

div#settings li + li { border-left: 2px #efefef solid; padding-left: 0.5em;}

div#settings input { margin-bottom: 0px;}



div#settings.hidden {display:none;}



#showsettings { border: 1px grey solid; padding: 0 0.5em; float:right; line-height: 1.6em; text-align: right; }

#showsettings:hover { cursor: pointer; }



.invalidsearch { background-color: red; }



table { border: 0px gray solid; width: 100%; empty-cells: show; }

th, td { border: 0px gray solid; padding: 0.1em; vertical-align: top;  }

td { text-align: left; vertical-align: top; }

th { background-color: #EFEFEF; }



td a, td a:hover { color: navy; font-weight: normal; }



tr.noshow { display: none;}



tr.highlight td { background-color: #787878; border-top: 2px black solid; font-weight: normal; }

tr.abstract td, tr.review td, tr.bibtex td { background-color: #F1F1F1; border-bottom: 2px black solid; }

tr.nextshow td { border-bottom: 1px gray solid; }



tr.bibtex pre { width: 100%; overflow: auto;}



p.infolinks { margin: 0.5em 0em 0em 0em; padding: 0px; }



#qssettings { padding: 0.5em; position: absolute; top: 0.2em; right: 0.2em; border: 1px gray solid; background-color: white; display: block; }

#qssettings p { font-weight: normal; cursor: pointer; }

#qssettings ul { display: none; list-style-type: none; padding-left: 0; margin: 0; }

#qssettings.active ul { display: block; }

</style>

</head>

<body>



<!-- For non-visual or non-stylesheet-capable user agents -->

<div id="mainlink"><a href="#main">Skip to

main content.</a></div>



<!-- ======== Header ======== -->

<div id="header">



</div>



<!-- ======== Main Content ======== -->



<div id="main">



<!-- ======== The body ================================================================================================ -->



<thead></thead>

<tbody>





<script type="text/javascript">
function toggleVisibility(x) { var e = document.getElementById(x); if(e.style.display == 'block') e.style.display = 'none'; else e.style.display = 'block';}
</script>
<div id="content" style="font-size:1rem">[Click on title to open arXiv / click on authors to open ar5iv] <a href="#" onclick="toggleVisibility('links')">[TAGS on/off]</a></div>
<div id="links" style="display:none;">
<table style="font-size:1rem"><td width="100\%"><i>
<a href="index.html?search=&quot;contrastive&quot;">contrastive /</a>
<a href="index.html?search=&quot;control&quot;">control /</a>
<a href="index.html?search=&quot;deep&quot;">deep /</a>
<a href="index.html?search=&quot;distill&quot;">distill /</a>
<a href="index.html?search=&quot;diversity&quot;">diversity /</a>
<a href="index.html?search=&quot;entropy&quot;">entropy /</a>
<a href="index.html?search=&quot;evolution&quot;">evolution /</a>
<a href="index.html?search=&quot;explainable&quot;">explainable /</a>
<a href="index.html?search=&quot;exploration&quot;">exploration /</a>
<a href="index.html?search=&quot;few-shot&quot;">few-shot /</a>
<a href="index.html?search=&quot;goal-conditioned&quot;">goal-conditioned /</a>
<a href="index.html?search=&quot;gradient&quot;">gradient /</a>
<a href="index.html?search=&quot;hierarchical&quot;">hierarchical /</a>
<a href="index.html?search=&quot;humanoid&quot;">humanoid /</a>
<a href="index.html?search=&quot;imitation&quot;">imitation /</a>
<a href="index.html?search=&quot;intrinsic&quot;">intrinsic /</a>
<a href="index.html?search=&quot;kinematic&quot;">kinematic /</a>
<a href="index.html?search=&quot;lifelong learning&quot;">lifelong learning /</a>
<a href="index.html?search=&quot;meta&quot;">meta /</a>
<a href="index.html?search=&quot;model-based&quot;">model-based /</a>
<a href="index.html?search=&quot;motion planning&quot;">motion planning /</a>
<a href="index.html?search=&quot;multi-task&quot;">multi-task /</a>
<a href="index.html?search=&quot;off-policy&quot;">off-policy /</a>
<a href="index.html?search=&quot;offline&quot;">offline /</a>
<a href="index.html?search=&quot;on-policy&quot;">on-policy /</a>
<a href="index.html?search=&quot;open-ended&quot;">open-ended /</a>
<a href="index.html?search=&quot;optimal&quot;">optimal /</a>
<a href="index.html?search=&quot;population&quot;">population /</a>
<a href="index.html?search=&quot;reinforcement&quot;">reinforcement /</a>
<a href="index.html?search=&quot;replay&quot;">replay /</a>
<a href="index.html?search=&quot;robot&quot;">robot /</a>
<a href="index.html?search=&quot;self-supervised&quot;">self-supervised /</a>
<a href="index.html?search=&quot;skill&quot;">skill /</a>
<a href="index.html?search=&quot;sparse&quot;">sparse /</a>
<a href="index.html?search=&quot;transfer&quot;">transfer /</a>
<a href="index.html?search=&quot;unsupervised&quot;">unsupervised /</a>
</i></td></table>
</div>


<table style="font-size:1rem" WIDTH="300" border="0" cellspacing="0" cellpadding="0">

<tr>

<td width="100%">

<form action="" id="quicksearch">

<input style="font-size:1rem; text-align:right" size="30" type="text" id="qs_field" autocomplete="off" placeholder="Type to search..." /> <input style="font-size:1rem" type="button" onclick="clearQS()" value="clear" /><br>

<span style="font-size:1rem" id="searchstat"> <span id="stat">0</span></span>

<div style="font-size:1rem" id="showsettings" onclick="toggleSettings()">...</div>

<div id="settings" class="hidden">

<ul>

<li><input type="checkbox" class="search_setting" id="opt_useRegExp" onchange="updateSetting(this)"><label style="font-size:1rem" for="opt_useRegExp"> use RegExp</label></li>

<li><input type="checkbox" class="search_setting" id="opt_noAccents" onchange="updateSetting(this)"><label style="font-size:1rem" for="opt_noAccents"> ignore accents</label></li>

<li><input type="checkbox" class="search_setting" id="opt_searchAbs" onchange="updateSetting(this)"><label style="font-size:1rem" for="opt_searchAbs"> include abstract</label></li>

<li><input type="checkbox" class="search_setting" id="opt_searchRev" onchange="updateSetting(this)"><label style="font-size:1rem" for="opt_searchRev"> include review</label></li>

</ul>

</div>

</form>

</td>

</tr>



</table>



<table style="font-size:1rem" bgcolor="#F5F6CE" WIDTH="300" id="qs_table" border="0" cellspacing="0" cellpadding="0">



<tr id=" 1 " class="entry"><td>
<b/> Bootstrapping Motor Skill Learning with Motion Planning (2021)</b>
</td></tr>
<tr id=" 2 " class="entry"><td>
<b/> Relative Entropy Regularized Policy Iteration (2018)</b>
</td></tr>
<tr id=" 3 " class="entry"><td>
<b/> Towards Characterizing Divergence in Deep Q-Learning (2019)</b>
</td></tr>
<tr id=" 4 " class="entry"><td>
<b/> Legged Locomotion in Challenging Terrains using Egocentric Vision (2022)</b>
</td></tr>
<tr id=" 5 " class="entry"><td>
<b/> Understanding the impact of entropy on policy optimization (2018)</b>
</td></tr>
<tr id=" 6 " class="entry"><td>
<b/> OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning (2020)</b>
</td></tr>
<tr id=" 7 " class="entry"><td>
<b/> Locally Persistent Exploration in Continuous Control Tasks with Sparse Rewards (2020)</b>
</td></tr>
<tr id=" 8 " class="entry"><td>
<b/> A Survey of Exploration Methods in Reinforcement Learning (2021)</b>
</td></tr>
<tr id=" 9 " class="entry"><td>
<b/> Input Convex Neural Networks (2016)</b>
</td></tr>
<tr id=" 10 " class="entry"><td>
<b/> Hindsight Experience Replay (2017)</b>
</td></tr>
<tr id=" 11 " class="entry"><td>
<b/> Layer-wise learning of deep generative models (2012)</b>
</td></tr>
<tr id=" 12 " class="entry"><td>
<b/> A Brief Survey of Deep Reinforcement Learning (2017)</b>
</td></tr>
<tr id=" 13 " class="entry"><td>
<b/> Breaking the Curse of Dimensionality with Convex Neural Networks (2014)</b>
</td></tr>
<tr id=" 14 " class="entry"><td>
<b/> The Option-Critic Architecture (2016)</b>
</td></tr>
<tr id=" 15 " class="entry"><td>
<b/> Never Give Up: Learning Directed Exploration Strategies (2020)</b>
</td></tr>
<tr id=" 16 " class="entry"><td>
<b/> Compatible Value Gradients for Reinforcement Learning of Continuous Deep Policies (2015)</b>
</td></tr>
<tr id=" 17 " class="entry"><td>
<b/> Efficient Online Reinforcement Learning with Offline Data (2023)</b>
</td></tr>
<tr id=" 18 " class="entry"><td>
<b/> Ready Policy One: World Building Through Active Learning (2020)</b>
</td></tr>
<tr id=" 19 " class="entry"><td>
<b/> Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization (2020)</b>
</td></tr>
<tr id=" 20 " class="entry"><td>
<b/> Rearrangement: A Challenge for Embodied AI (2020)</b>
</td></tr>
<tr id=" 21 " class="entry"><td>
<b/> Relational inductive biases, deep learning, and graph networks (2018)</b>
</td></tr>
<tr id=" 22 " class="entry"><td>
<b/> Learning to Continually Learn (2020)</b>
</td></tr>
<tr id=" 23 " class="entry"><td>
<b/> Training in Task Space to Speed Up and Guide Reinforcement Learning (2019)</b>
</td></tr>
<tr id=" 24 " class="entry"><td>
<b/> A Geometric Perspective on Optimal Representations for Reinforcement Learning (2019)</b>
</td></tr>
<tr id=" 25 " class="entry"><td>
<b/> A Distributional Perspective on Reinforcement Learning (2017)</b>
</td></tr>
<tr id=" 26 " class="entry"><td>
<b/> The Cramer Distance as a Solution to Biased Wasserstein Gradients (2017)</b>
</td></tr>
<tr id=" 27 " class="entry"><td>
<b/> Unifying Count-Based Exploration and Intrinsic Motivation (2016)</b>
</td></tr>
<tr id=" 28 " class="entry"><td>
<b/> Representation Learning: A Review and New Perspectives (2012)</b>
</td></tr>
<tr id=" 29 " class="entry"><td>
<b/> Model-Based Action Exploration for Learning Dynamic Motion Skills (2018)</b>
</td></tr>
<tr id=" 30 " class="entry"><td>
<b/> LEAF: Latent Exploration Along the Frontier (2020)</b>
</td></tr>
<tr id=" 31 " class="entry"><td>
<b/> Proximal Distilled Evolutionary Reinforcement Learning (2019)</b>
</td></tr>
<tr id=" 32 " class="entry"><td>
<b/> Universal Successor Features Approximators (2018)</b>
</td></tr>
<tr id=" 33 " class="entry"><td>
<b/> Practical Gauss-Newton Optimisation for Deep Learning (2017)</b>
</td></tr>
<tr id=" 34 " class="entry"><td>
<b/> A Theory of Universal Learning (2020)</b>
</td></tr>
<tr id=" 35 " class="entry"><td>
<b/> On Identifiability in Transformers (2019)</b>
</td></tr>
<tr id=" 36 " class="entry"><td>
<b/> Modern Koopman Theory for Dynamical Systems (2021)</b>
</td></tr>
<tr id=" 37 " class="entry"><td>
<b/> Exploration by Random Network Distillation (2018)</b>
</td></tr>
<tr id=" 38 " class="entry"><td>
<b/> Offline Reinforcement Learning at Multiple Frequencies (2022)</b>
</td></tr>
<tr id=" 39 " class="entry"><td>
<b/> Robust Feedback Motion Policy Design Using Reinforcement Learning on a 3D Digit Bipedal Robot (2021)</b>
</td></tr>
<tr id=" 40 " class="entry"><td>
<b/> Learning Action Representations for Reinforcement Learning (2019)</b>
</td></tr>
<tr id=" 41 " class="entry"><td>
<b/> Goal-Conditioned Reinforcement Learning with Imagined Subgoals (2021)</b>
</td></tr>
<tr id=" 42 " class="entry"><td>
<b/> Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills (2021)</b>
</td></tr>
<tr id=" 43 " class="entry"><td>
<b/> Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking (2022)</b>
</td></tr>
<tr id=" 44 " class="entry"><td>
<b/> Decision Transformer: Reinforcement Learning via Sequence Modeling (2021)</b>
</td></tr>
<tr id=" 45 " class="entry"><td>
<b/> Neural Ordinary Differential Equations (2018)</b>
</td></tr>
<tr id=" 46 " class="entry"><td>
<b/> InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets (2016)</b>
</td></tr>
<tr id=" 47 " class="entry"><td>
<b/> Symbolic Discovery of Optimization Algorithms (2023)</b>
</td></tr>
<tr id=" 48 " class="entry"><td>
<b/> Optimal transport natural gradient for statistical manifolds with continuous sample space (2018)</b>
</td></tr>
<tr id=" 49 " class="entry"><td>
<b/> Adversarially Trained Actor Critic for Offline Reinforcement Learning (2022)</b>
</td></tr>
<tr id=" 50 " class="entry"><td>
<b/> Divide & Conquer Imitation Learning (2022)</b>
</td></tr>
<tr id=" 51 " class="entry"><td>
<b/> Probability Functional Descent: A Unifying Perspective on GANs, Variational Inference, and Reinforcement Learning (2019)</b>
</td></tr>
<tr id=" 52 " class="entry"><td>
<b/> Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models (2018)</b>
</td></tr>
<tr id=" 53 " class="entry"><td>
<b/> Better Exploration with Optimistic Actor-Critic (2019)</b>
</td></tr>
<tr id=" 54 " class="entry"><td>
<b/> Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents (2017)</b>
</td></tr>
<tr id=" 55 " class="entry"><td>
<b/> Hierarchical Behavioral Repertoires with Unsupervised Descriptors (2018)</b>
</td></tr>
<tr id=" 56 " class="entry"><td>
<b/> Quality and Diversity Optimization: A Unifying Modular Framework (2017)</b>
</td></tr>
<tr id=" 57 " class="entry"><td>
<b/> Implicit Quantile Networks for Distributional Reinforcement Learning (2018)</b>
</td></tr>
<tr id=" 58 " class="entry"><td>
<b/> Distributional Reinforcement Learning with Quantile Regression (2017)</b>
</td></tr>
<tr id=" 59 " class="entry"><td>
<b/> Primal Wasserstein Imitation Learning (2020)</b>
</td></tr>
<tr id=" 60 " class="entry"><td>
<b/> Continuous Control with Action Quantization from Demonstrations (2021)</b>
</td></tr>
<tr id=" 61 " class="entry"><td>
<b/> Deep Gaussian Processes (2012)</b>
</td></tr>
<tr id=" 62 " class="entry"><td>
<b/> Natural Neural Networks (2015)</b>
</td></tr>
<tr id=" 63 " class="entry"><td>
<b/> Sharp Minima Can Generalize For Deep Nets (2017)</b>
</td></tr>
<tr id=" 64 " class="entry"><td>
<b/> Attraction-Repulsion Actor-Critic for Continuous Control Reinforcement Learning (2019)</b>
</td></tr>
<tr id=" 65 " class="entry"><td>
<b/> GAN Q-learning (2018)</b>
</td></tr>
<tr id=" 66 " class="entry"><td>
<b/> Tutorial on Variational Autoencoders (2016)</b>
</td></tr>
<tr id=" 67 " class="entry"><td>
<b/> Adapting Auxiliary Losses Using Gradient Similarity (2018)</b>
</td></tr>
<tr id=" 68 " class="entry"><td>
<b/> Online Trajectory Planning Through Combined Trajectory Optimization and Function Approximation: Application to the Exoskeleton Atalante (2019)</b>
</td></tr>
<tr id=" 69 " class="entry"><td>
<b/> Reactive Stepping for Humanoid Robots using Reinforcement Learning: Application to Standing Push Recovery on the Exoskeleton Atalante (2022)</b>
</td></tr>
<tr id=" 70 " class="entry"><td>
<b/> First return, then explore (2020)</b>
</td></tr>
<tr id=" 71 " class="entry"><td>
<b/> Go-Explore: a New Approach for Hard-Exploration Problems (2019)</b>
</td></tr>
<tr id=" 72 " class="entry"><td>
<b/> RvS: What is Essential for Offline RL via Supervised Learning? (2021)</b>
</td></tr>
<tr id=" 73 " class="entry"><td>
<b/> Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO (2020)</b>
</td></tr>
<tr id=" 74 " class="entry"><td>
<b/> IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures (2018)</b>
</td></tr>
<tr id=" 75 " class="entry"><td>
<b/> Diversity is All You Need: Learning Skills without a Reward Function (2018)</b>
</td></tr>
<tr id=" 76 " class="entry"><td>
<b/> Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification (2021)</b>
</td></tr>
<tr id=" 77 " class="entry"><td>
<b/> Maximum Entropy RL (Provably) Solves Some Robust RL Problems (2021)</b>
</td></tr>
<tr id=" 78 " class="entry"><td>
<b/> Imitating Past Successes can be Very Suboptimal (2022)</b>
</td></tr>
<tr id=" 79 " class="entry"><td>
<b/> Contrastive Learning as Goal-Conditioned Reinforcement Learning (2022)</b>
</td></tr>
<tr id=" 80 " class="entry"><td>
<b/> Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization (2016)</b>
</td></tr>
<tr id=" 81 " class="entry"><td>
<b/> Deep Spatial Autoencoders for Visuomotor Learning (2015)</b>
</td></tr>
<tr id=" 82 " class="entry"><td>
<b/> Bootstrapped Meta-Learning (2021)</b>
</td></tr>
<tr id=" 83 " class="entry"><td>
<b/> Automatic Goal Generation for Reinforcement Learning Agents (2017)</b>
</td></tr>
<tr id=" 84 " class="entry"><td>
<b/> Reverse Curriculum Generation for Reinforcement Learning (2017)</b>
</td></tr>
<tr id=" 85 " class="entry"><td>
<b/> Differentiable Quality Diversity (2021)</b>
</td></tr>
<tr id=" 86 " class="entry"><td>
<b/> Noisy Networks for Exploration (2017)</b>
</td></tr>
<tr id=" 87 " class="entry"><td>
<b/> An Introduction to Deep Reinforcement Learning (2018)</b>
</td></tr>
<tr id=" 88 " class="entry"><td>
<b/> Brax &#8211; A Differentiable Physics Engine for Large Scale Rigid Body Simulation (2021)</b>
</td></tr>
<tr id=" 89 " class="entry"><td>
<b/> D4RL: Datasets for Deep Data-Driven Reinforcement Learning (2020)</b>
</td></tr>
<tr id=" 90 " class="entry"><td>
<b/> Diagnosing Bottlenecks in Deep Q-learning Algorithms (2019)</b>
</td></tr>
<tr id=" 91 " class="entry"><td>
<b/> A Minimalist Approach to Offline Reinforcement Learning (2021)</b>
</td></tr>
<tr id=" 92 " class="entry"><td>
<b/> Addressing Function Approximation Error in Actor-Critic Methods (2018)</b>
</td></tr>
<tr id=" 93 " class="entry"><td>
<b/> Off-Policy Deep Reinforcement Learning without Exploration (2018)</b>
</td></tr>
<tr id=" 94 " class="entry"><td>
<b/> Policy Optimization by Genetic Distillation (2017)</b>
</td></tr>
<tr id=" 95 " class="entry"><td>
<b/> Hierarchical Skills for Efficient Exploration (2021)</b>
</td></tr>
<tr id=" 96 " class="entry"><td>
<b/> Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis (2018)</b>
</td></tr>
<tr id=" 97 " class="entry"><td>
<b/> Reinforcement Learning from Passive Data via Latent Intentions (2023)</b>
</td></tr>
<tr id=" 98 " class="entry"><td>
<b/> Learning to Reach Goals via Iterated Supervised Learning (2019)</b>
</td></tr>
<tr id=" 99 " class="entry"><td>
<b/> Simplifying Model-based RL: Learning Representations, Latent-space Models, and Policies with One Objective (2022)</b>
</td></tr>
<tr id=" 100 " class="entry"><td>
<b/> Recall Traces: Backtracking Models for Efficient Reinforcement Learning (2018)</b>
</td></tr>
<tr id=" 101 " class="entry"><td>
<b/> Variational Intrinsic Control (2016)</b>
</td></tr>
<tr id=" 102 " class="entry"><td>
<b/> Correlation and variable importance in random forests (2013)</b>
</td></tr>
<tr id=" 103 " class="entry"><td>
<b/> Hamiltonian Neural Networks (2019)</b>
</td></tr>
<tr id=" 104 " class="entry"><td>
<b/> Continuous Deep Q-Learning with Model-based Acceleration (2016)</b>
</td></tr>
<tr id=" 105 " class="entry"><td>
<b/> Neural Predictive Belief Representations (2018)</b>
</td></tr>
<tr id=" 106 " class="entry"><td>
<b/> BYOL-Explore: Exploration by Bootstrapped Prediction (2022)</b>
</td></tr>
<tr id=" 107 " class="entry"><td>
<b/> Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning (2017)</b>
</td></tr>
<tr id=" 108 " class="entry"><td>
<b/> Demonstration-Bootstrapped Autonomous Practicing via Multi-Task Reinforcement Learning (2022)</b>
</td></tr>
<tr id=" 109 " class="entry"><td>
<b/> Meta-Reinforcement Learning of Structured Exploration Strategies (2018)</b>
</td></tr>
<tr id=" 110 " class="entry"><td>
<b/> Towards Variable Assistance for Lower Body Exoskeletons (2019)</b>
</td></tr>
<tr id=" 111 " class="entry"><td>
<b/> World Models (2018)</b>
</td></tr>
<tr id=" 112 " class="entry"><td>
<b/> Latent Space Policies for Hierarchical Reinforcement Learning (2018)</b>
</td></tr>
<tr id=" 113 " class="entry"><td>
<b/> Composable Deep Reinforcement Learning for Robotic Manipulation (2018)</b>
</td></tr>
<tr id=" 114 " class="entry"><td>
<b/> Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor (2018)</b>
</td></tr>
<tr id=" 115 " class="entry"><td>
<b/> Soft Actor-Critic Algorithms and Applications (2018)</b>
</td></tr>
<tr id=" 116 " class="entry"><td>
<b/> TensorFlow Agents: Efficient Batched Reinforcement Learning in TensorFlow (2017)</b>
</td></tr>
<tr id=" 117 " class="entry"><td>
<b/> Deep Hierarchical Planning from Pixels (2022)</b>
</td></tr>
<tr id=" 118 " class="entry"><td>
<b/> Dream to Control: Learning Behaviors by Latent Imagination (2019)</b>
</td></tr>
<tr id=" 119 " class="entry"><td>
<b/> Mastering Atari with Discrete World Models (2020)</b>
</td></tr>
<tr id=" 120 " class="entry"><td>
<b/> Mastering Diverse Domains through World Models (2023)</b>
</td></tr>
<tr id=" 121 " class="entry"><td>
<b/> Towards General and Autonomous Learning of Core Skills: A Case Study in Locomotion (2020)</b>
</td></tr>
<tr id=" 122 " class="entry"><td>
<b/> Hierarchical Few-Shot Imitation with Skill Transition Models (2021)</b>
</td></tr>
<tr id=" 123 " class="entry"><td>
<b/> On the role of planning in model-based deep reinforcement learning (2020)</b>
</td></tr>
<tr id=" 124 " class="entry"><td>
<b/> Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efficient Exploration (2020)</b>
</td></tr>
<tr id=" 125 " class="entry"><td>
<b/> Feedback Control of an Exoskeleton for Paraplegics: Toward Robustly Stable Hands-free Dynamic Walking (2018)</b>
</td></tr>
<tr id=" 126 " class="entry"><td>
<b/> Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery (2019)</b>
</td></tr>
<tr id=" 127 " class="entry"><td>
<b/> Deep Reinforcement Learning and the Deadly Triad (2018)</b>
</td></tr>
<tr id=" 128 " class="entry"><td>
<b/> Soft Hindsight Experience Replay (2020)</b>
</td></tr>
<tr id=" 129 " class="entry"><td>
<b/> Emergence of Locomotion Behaviours in Rich Environments (2017)</b>
</td></tr>
<tr id=" 130 " class="entry"><td>
<b/> Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint Replay (2016)</b>
</td></tr>
<tr id=" 131 " class="entry"><td>
<b/> Evolved Policy Gradients (2018)</b>
</td></tr>
<tr id=" 132 " class="entry"><td>
<b/> Broadly-Exploring, Local-Policy Trees for Long-Horizon Task Planning (2020)</b>
</td></tr>
<tr id=" 133 " class="entry"><td>
<b/> A Closer Look at Deep Policy Gradients (2018)</b>
</td></tr>
<tr id=" 134 " class="entry"><td>
<b/> Improving Regression Performance with Distributional Losses (2018)</b>
</td></tr>
<tr id=" 135 " class="entry"><td>
<b/> Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (2015)</b>
</td></tr>
<tr id=" 136 " class="entry"><td>
<b/> Population Based Training of Neural Networks (2017)</b>
</td></tr>
<tr id=" 137 " class="entry"><td>
<b/> Reinforcement Learning with Unsupervised Auxiliary Tasks (2016)</b>
</td></tr>
<tr id=" 138 " class="entry"><td>
<b/> Attention is not Explanation (2019)</b>
</td></tr>
<tr id=" 139 " class="entry"><td>
<b/> Task-Embedded Control Networks for Few-Shot Imitation Learning (2018)</b>
</td></tr>
<tr id=" 140 " class="entry"><td>
<b/> 3D Simulation for Robot Arm Control with Deep Q-Learning (2016)</b>
</td></tr>
<tr id=" 141 " class="entry"><td>
<b/> When to Trust Your Model: Model-Based Policy Optimization (2019)</b>
</td></tr>
<tr id=" 142 " class="entry"><td>
<b/> Offline Reinforcement Learning as One Big Sequence Modeling Problem (2021)</b>
</td></tr>
<tr id=" 143 " class="entry"><td>
<b/> Fast Marching Tree: a Fast Marching Sampling-Based Method for Optimal Motion Planning in Many Dimensions (2013)</b>
</td></tr>
<tr id=" 144 " class="entry"><td>
<b/> gradSim: Differentiable simulation for system identification and visuomotor control (2021)</b>
</td></tr>
<tr id=" 145 " class="entry"><td>
<b/> Population-Guided Parallel Policy Search for Reinforcement Learning (2020)</b>
</td></tr>
<tr id=" 146 " class="entry"><td>
<b/> Direct then Diffuse: Incremental Unsupervised Skill Discovery for State Covering and Goal Reaching (2021)</b>
</td></tr>
<tr id=" 147 " class="entry"><td>
<b/> Supervised Contrastive Learning (2020)</b>
</td></tr>
<tr id=" 148 " class="entry"><td>
<b/> Improving Variational Inference with Inverse Autoregressive Flow (2016)</b>
</td></tr>
<tr id=" 149 " class="entry"><td>
<b/> An Introduction to Variational Autoencoders (2019)</b>
</td></tr>
<tr id=" 150 " class="entry"><td>
<b/> Learning-based Model Predictive Control for Safe Exploration and Reinforcement Learning (2019)</b>
</td></tr>
<tr id=" 151 " class="entry"><td>
<b/> Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning (2018)</b>
</td></tr>
<tr id=" 152 " class="entry"><td>
<b/> Offline Reinforcement Learning with Implicit Q-Learning (2021)</b>
</td></tr>
<tr id=" 153 " class="entry"><td>
<b/> Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels (2020)</b>
</td></tr>
<tr id=" 154 " class="entry"><td>
<b/> An Efficiently Solvable Quadratic Program for Stabilizing Dynamic Locomotion (2013)</b>
</td></tr>
<tr id=" 155 " class="entry"><td>
<b/> Deep Successor Reinforcement Learning (2016)</b>
</td></tr>
<tr id=" 156 " class="entry"><td>
<b/> RMA: Rapid Motor Adaptation for Legged Robots (2021)</b>
</td></tr>
<tr id=" 157 " class="entry"><td>
<b/> Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction (2019)</b>
</td></tr>
<tr id=" 158 " class="entry"><td>
<b/> Expanding Motor Skills through Relay Neural Networks (2017)</b>
</td></tr>
<tr id=" 159 " class="entry"><td>
<b/> Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics (2020)</b>
</td></tr>
<tr id=" 160 " class="entry"><td>
<b/> Professor Forcing: A New Algorithm for Training Recurrent Networks (2016)</b>
</td></tr>
<tr id=" 161 " class="entry"><td>
<b/> CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery (2022)</b>
</td></tr>
<tr id=" 162 " class="entry"><td>
<b/> Contrastive Representation Learning: A Framework and Review (2020)</b>
</td></tr>
<tr id=" 163 " class="entry"><td>
<b/> Robust Recovery Controller for a Quadrupedal Robot using Deep Reinforcement Learning (2019)</b>
</td></tr>
<tr id=" 164 " class="entry"><td>
<b/> Adversarial Skill Chaining for Long-Horizon Robot Manipulation via Terminal State Regularization (2021)</b>
</td></tr>
<tr id=" 165 " class="entry"><td>
<b/> Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients (2017)</b>
</td></tr>
<tr id=" 166 " class="entry"><td>
<b/> State Representation Learning for Control: An Overview (2018)</b>
</td></tr>
<tr id=" 167 " class="entry"><td>
<b/> End-to-End Training of Deep Visuomotor Policies (2015)</b>
</td></tr>
<tr id=" 168 " class="entry"><td>
<b/> Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection (2016)</b>
</td></tr>
<tr id=" 169 " class="entry"><td>
<b/> Learning Multi-Level Hierarchies with Hindsight (2017)</b>
</td></tr>
<tr id=" 170 " class="entry"><td>
<b/> Visualizing the Loss Landscape of Neural Nets (2017)</b>
</td></tr>
<tr id=" 171 " class="entry"><td>
<b/> Hierarchical Planning Through Goal-Conditioned Offline Reinforcement Learning (2022)</b>
</td></tr>
<tr id=" 172 " class="entry"><td>
<b/> Towards Practical Multi-Object Manipulation using Relational Reinforcement Learning (2019)</b>
</td></tr>
<tr id=" 173 " class="entry"><td>
<b/> Solving Compositional Reinforcement Learning Problems via Task Reduction (2021)</b>
</td></tr>
<tr id=" 174 " class="entry"><td>
<b/> Robust and Versatile Bipedal Jumping Control through Multi-Task Reinforcement Learning (2023)</b>
</td></tr>
<tr id=" 175 " class="entry"><td>
<b/> Continuous control with deep reinforcement learning (2015)</b>
</td></tr>
<tr id=" 176 " class="entry"><td>
<b/> Dynamics-Aware Quality-Diversity for Efficient Learning of Skill Repertoires (2021)</b>
</td></tr>
<tr id=" 177 " class="entry"><td>
<b/> Learning Null Space Projections in Operational Space Formulation (2016)</b>
</td></tr>
<tr id=" 178 " class="entry"><td>
<b/> From Motor Control to Team Play in Simulated Humanoid Football (2021)</b>
</td></tr>
<tr id=" 179 " class="entry"><td>
<b/> Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations (2018)</b>
</td></tr>
<tr id=" 180 " class="entry"><td>
<b/> Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control (2018)</b>
</td></tr>
<tr id=" 181 " class="entry"><td>
<b/> Reset-Free Lifelong Learning with Skill-Space Planning (2020)</b>
</td></tr>
<tr id=" 182 " class="entry"><td>
<b/> A Unified Approach to Interpreting Model Predictions (2017)</b>
</td></tr>
<tr id=" 183 " class="entry"><td>
<b/> Self-Imitation Learning by Planning (2021)</b>
</td></tr>
<tr id=" 184 " class="entry"><td>
<b/> Dynamics-Regulated Kinematic Policy for Egocentric Pose Estimation (2021)</b>
</td></tr>
<tr id=" 185 " class="entry"><td>
<b/> Embodied Scene-aware Human Pose Estimation (2022)</b>
</td></tr>
<tr id=" 186 " class="entry"><td>
<b/> From Universal Humanoid Control to Automatic Physically Valid Character Creation (2022)</b>
</td></tr>
<tr id=" 187 " class="entry"><td>
<b/> Learning Dynamics and Generalization in Reinforcement Learning (2022)</b>
</td></tr>
<tr id=" 188 " class="entry"><td>
<b/> Learning Latent Plans from Play (2019)</b>
</td></tr>
<tr id=" 189 " class="entry"><td>
<b/> Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics (2017)</b>
</td></tr>
<tr id=" 190 " class="entry"><td>
<b/> Hamilton-Jacobi formulation for reach-avoid differential games (2009)</b>
</td></tr>
<tr id=" 191 " class="entry"><td>
<b/> New insights and perspectives on the natural gradient method (2014)</b>
</td></tr>
<tr id=" 192 " class="entry"><td>
<b/> Speed learning on the fly (2015)</b>
</td></tr>
<tr id=" 193 " class="entry"><td>
<b/> PBCS : Efficient Exploration and Exploitation Using a Synergy between Reinforcement Learning and Motion Planning (2020)</b>
</td></tr>
<tr id=" 194 " class="entry"><td>
<b/> The problem with DDPG: understanding failures in deterministic environments with sparse rewards (2019)</b>
</td></tr>
<tr id=" 195 " class="entry"><td>
<b/> State Representation Learning from Demonstration (2019)</b>
</td></tr>
<tr id=" 196 " class="entry"><td>
<b/> Modified Actor-Critics (2019)</b>
</td></tr>
<tr id=" 197 " class="entry"><td>
<b/> Neural probabilistic motor primitives for humanoid control (2018)</b>
</td></tr>
<tr id=" 198 " class="entry"><td>
<b/> Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks (2019)</b>
</td></tr>
<tr id=" 199 " class="entry"><td>
<b/> Discrete Sequential Prediction of Continuous Actions for Deep RL (2017)</b>
</td></tr>
<tr id=" 200 " class="entry"><td>
<b/> Transformers are Sample-Efficient World Models (2022)</b>
</td></tr>
<tr id=" 201 " class="entry"><td>
<b/> Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt (2022)</b>
</td></tr>
<tr id=" 202 " class="entry"><td>
<b/> Longitudinal high-throughput TCR repertoire profiling reveals the dynamics of T cell memory formation after mild COVID-19 infection (2020)</b>
</td></tr>
<tr id=" 203 " class="entry"><td>
<b/> A geometrical introduction to screw theory (2012)</b>
</td></tr>
<tr id=" 204 " class="entry"><td>
<b/> Computational Geometry Column 42 (2001)</b>
</td></tr>
<tr id=" 205 " class="entry"><td>
<b/> Asynchronous Methods for Deep Reinforcement Learning (2016)</b>
</td></tr>
<tr id=" 206 " class="entry"><td>
<b/> Reinforcement Learning with Probabilistically Complete Exploration (2020)</b>
</td></tr>
<tr id=" 207 " class="entry"><td>
<b/> Convolutional neural network models for cancer type prediction based on gene expression (2019)</b>
</td></tr>
<tr id=" 208 " class="entry"><td>
<b/> Illuminating search spaces by mapping elites (2015)</b>
</td></tr>
<tr id=" 209 " class="entry"><td>
<b/> Regularizing Action Policies for Smooth Control with Reinforcement Learning (2020)</b>
</td></tr>
<tr id=" 210 " class="entry"><td>
<b/> Near-Optimal Representation Learning for Hierarchical Reinforcement Learning (2018)</b>
</td></tr>
<tr id=" 211 " class="entry"><td>
<b/> Smoothed Action Value Functions for Learning Gaussian Policies (2018)</b>
</td></tr>
<tr id=" 212 " class="entry"><td>
<b/> Trust-PCL: An Off-Policy Trust Region Method for Continuous Control (2017)</b>
</td></tr>
<tr id=" 213 " class="entry"><td>
<b/> Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning? (2019)</b>
</td></tr>
<tr id=" 214 " class="entry"><td>
<b/> AWAC: Accelerating Online Reinforcement Learning with Offline Datasets (2020)</b>
</td></tr>
<tr id=" 215 " class="entry"><td>
<b/> Overcoming Exploration in Reinforcement Learning with Demonstrations (2017)</b>
</td></tr>
<tr id=" 216 " class="entry"><td>
<b/> Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning (2023)</b>
</td></tr>
<tr id=" 217 " class="entry"><td>
<b/> Continuous-Discrete Reinforcement Learning for Hybrid Control in Robotics (2020)</b>
</td></tr>
<tr id=" 218 " class="entry"><td>
<b/> Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images (2014)</b>
</td></tr>
<tr id=" 219 " class="entry"><td>
<b/> When Does Stochastic Gradient Algorithm Work Well? (2018)</b>
</td></tr>
<tr id=" 220 " class="entry"><td>
<b/> On First-Order Meta-Learning Algorithms (2018)</b>
</td></tr>
<tr id=" 221 " class="entry"><td>
<b/> Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles (2011)</b>
</td></tr>
<tr id=" 222 " class="entry"><td>
<b/> Training recurrent networks online without backtracking (2015)</b>
</td></tr>
<tr id=" 223 " class="entry"><td>
<b/> Riemannian metrics for neural networks II: recurrent networks and learning symbolic data sequences (2013)</b>
</td></tr>
<tr id=" 224 " class="entry"><td>
<b/> True Asymptotic Natural Gradient Optimization (2017)</b>
</td></tr>
<tr id=" 225 " class="entry"><td>
<b/> Representation Learning with Contrastive Predictive Coding (2018)</b>
</td></tr>
<tr id=" 226 " class="entry"><td>
<b/> Dota 2 with Large Scale Deep Reinforcement Learning (2019)</b>
</td></tr>
<tr id=" 227 " class="entry"><td>
<b/> Deep Exploration via Bootstrapped DQN (2016)</b>
</td></tr>
<tr id=" 228 " class="entry"><td>
<b/> Can Increasing Input Dimensionality Improve Deep Reinforcement Learning? (2020)</b>
</td></tr>
<tr id=" 229 " class="entry"><td>
<b/> Vector Quantized Models for Planning (2021)</b>
</td></tr>
<tr id=" 230 " class="entry"><td>
<b/> Making Efficient Use of Demonstrations to Solve Hard Exploration Problems (2019)</b>
</td></tr>
<tr id=" 231 " class="entry"><td>
<b/> Lipschitz-constrained Unsupervised Skill Discovery (2022)</b>
</td></tr>
<tr id=" 232 " class="entry"><td>
<b/> Controllability-Aware Unsupervised Skill Discovery (2023)</b>
</td></tr>
<tr id=" 233 " class="entry"><td>
<b/> Predictable MDP Abstraction for Unsupervised Model-Based RL (2023)</b>
</td></tr>
<tr id=" 234 " class="entry"><td>
<b/> Effective Diversity in Population Based Reinforcement Learning (2020)</b>
</td></tr>
<tr id=" 235 " class="entry"><td>
<b/> Revisiting Natural Gradient for Deep Networks (2013)</b>
</td></tr>
<tr id=" 236 " class="entry"><td>
<b/> Adaptive Temporal-Difference Learning for Policy Evaluation with Per-State Uncertainty Estimates (2019)</b>
</td></tr>
<tr id=" 237 " class="entry"><td>
<b/> MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies (2019)</b>
</td></tr>
<tr id=" 238 " class="entry"><td>
<b/> Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning (2019)</b>
</td></tr>
<tr id=" 239 " class="entry"><td>
<b/> AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control (2021)</b>
</td></tr>
<tr id=" 240 " class="entry"><td>
<b/> Learning Locomotion Skills Using DeepRL: Does the Choice of Action Space Matter? (2016)</b>
</td></tr>
<tr id=" 241 " class="entry"><td>
<b/> Non-local Policy Optimization via Diversity-regularized Collaborative Exploration (2020)</b>
</td></tr>
<tr id=" 242 " class="entry"><td>
<b/> Accelerating Reinforcement Learning with Learned Skill Priors (2020)</b>
</td></tr>
<tr id=" 243 " class="entry"><td>
<b/> Demonstration-Guided Reinforcement Learning with Learned Skills (2021)</b>
</td></tr>
<tr id=" 244 " class="entry"><td>
<b/> Learning Compositional Neural Programs with Recursive Tree Search and Planning (2019)</b>
</td></tr>
<tr id=" 245 " class="entry"><td>
<b/> Diversity Policy Gradient for Sample Efficient Quality-Diversity Optimization (2020)</b>
</td></tr>
<tr id=" 246 " class="entry"><td>
<b/> Learning Compositional Neural Programs for Continuous Control (2020)</b>
</td></tr>
<tr id=" 247 " class="entry"><td>
<b/> First-order and second-order variants of the gradient descent in a unified framework (2018)</b>
</td></tr>
<tr id=" 248 " class="entry"><td>
<b/> Multi-Objective Quality Diversity Optimization (2022)</b>
</td></tr>
<tr id=" 249 " class="entry"><td>
<b/> Maximum Entropy Gain Exploration for Long Horizon Multi-goal Reinforcement Learning (2020)</b>
</td></tr>
<tr id=" 250 " class="entry"><td>
<b/> Skew-Fit: State-Covering Self-Supervised Reinforcement Learning (2019)</b>
</td></tr>
<tr id=" 251 " class="entry"><td>
<b/> Importance mixing: Improving sample reuse in evolutionary policy search methods (2018)</b>
</td></tr>
<tr id=" 252 " class="entry"><td>
<b/> Learning to Solve NP-Complete Problems - A Graph Neural Network for Decision TSP (2018)</b>
</td></tr>
<tr id=" 253 " class="entry"><td>
<b/> A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems (2022)</b>
</td></tr>
<tr id=" 254 " class="entry"><td>
<b/> Estimating Training Data Influence by Tracing Gradient Descent (2020)</b>
</td></tr>
<tr id=" 255 " class="entry"><td>
<b/> Information geometry for multiparameter models: New perspectives on the origin of simplicity (2021)</b>
</td></tr>
<tr id=" 256 " class="entry"><td>
<b/> Automated curricula through setter-solver interactions (2019)</b>
</td></tr>
<tr id=" 257 " class="entry"><td>
<b/> Learning Humanoid Locomotion with Transformers (2023)</b>
</td></tr>
<tr id=" 258 " class="entry"><td>
<b/> Smooth Exploration for Robotic Reinforcement Learning (2020)</b>
</td></tr>
<tr id=" 259 " class="entry"><td>
<b/> Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations (2017)</b>
</td></tr>
<tr id=" 260 " class="entry"><td>
<b/> Towards Generalization and Simplicity in Continuous Control (2017)</b>
</td></tr>
<tr id=" 261 " class="entry"><td>
<b/> Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables (2019)</b>
</td></tr>
<tr id=" 262 " class="entry"><td>
<b/> Euclideanizing Flows: Diffeomorphic Reduction for Learning Stable Dynamical Systems (2020)</b>
</td></tr>
<tr id=" 263 " class="entry"><td>
<b/> On the Convergence of Adam and Beyond (2019)</b>
</td></tr>
<tr id=" 264 " class="entry"><td>
<b/> SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards (2019)</b>
</td></tr>
<tr id=" 265 " class="entry"><td>
<b/> Neural Programmer-Interpreters (2015)</b>
</td></tr>
<tr id=" 266 " class="entry"><td>
<b/> Extended Tree Search for Robot Task and Motion Planning (2021)</b>
</td></tr>
<tr id=" 267 " class="entry"><td>
<b/> Backplay: "Man muss immer umkehren" (2018)</b>
</td></tr>
<tr id=" 268 " class="entry"><td>
<b/> Variational Inference with Normalizing Flows (2015)</b>
</td></tr>
<tr id=" 269 " class="entry"><td>
<b/> Learning by Playing - Solving Sparse Reward Tasks from Scratch (2018)</b>
</td></tr>
<tr id=" 270 " class="entry"><td>
<b/> A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets (2012)</b>
</td></tr>
<tr id=" 271 " class="entry"><td>
<b/> An Analysis of Categorical Distributional Reinforcement Learning (2018)</b>
</td></tr>
<tr id=" 272 " class="entry"><td>
<b/> An overview of gradient descent optimization algorithms (2016)</b>
</td></tr>
<tr id=" 273 " class="entry"><td>
<b/> Generative Class-conditional Autoencoders (2014)</b>
</td></tr>
<tr id=" 274 " class="entry"><td>
<b/> CAQL: Continuous Action Q-Learning (2019)</b>
</td></tr>
<tr id=" 275 " class="entry"><td>
<b/> Dynamic Routing Between Capsules (2017)</b>
</td></tr>
<tr id=" 276 " class="entry"><td>
<b/> Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks (2016)</b>
</td></tr>
<tr id=" 277 " class="entry"><td>
<b/> Graph networks as learnable physics engines for inference and control (2018)</b>
</td></tr>
<tr id=" 278 " class="entry"><td>
<b/> Group Sparse Regularization for Deep Neural Networks (2016)</b>
</td></tr>
<tr id=" 279 " class="entry"><td>
<b/> Prioritized Experience Replay (2015)</b>
</td></tr>
<tr id=" 280 " class="entry"><td>
<b/> Generative Adversarial Networks are Special Cases of Artificial Curiosity (1990) and also Closely Related to Predictability Minimization (1991) (2019)</b>
</td></tr>
<tr id=" 281 " class="entry"><td>
<b/> Improving Model-Based Reinforcement Learning with Internal State Representations through Self-Supervision (2021)</b>
</td></tr>
<tr id=" 282 " class="entry"><td>
<b/> Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (2019)</b>
</td></tr>
<tr id=" 283 " class="entry"><td>
<b/> Universal Value Density Estimation for Imitation Learning and Goal-Conditioned Reinforcement Learning (2020)</b>
</td></tr>
<tr id=" 284 " class="entry"><td>
<b/> Trust Region Policy Optimization (2015)</b>
</td></tr>
<tr id=" 285 " class="entry"><td>
<b/> High-Dimensional Continuous Control Using Generalized Advantage Estimation (2015)</b>
</td></tr>
<tr id=" 286 " class="entry"><td>
<b/> Proximal Policy Optimization Algorithms (2017)</b>
</td></tr>
<tr id=" 287 " class="entry"><td>
<b/> Simultaneously Learning Vision and Feature-based Control Policies for Real-world Ball-in-a-Cup (2019)</b>
</td></tr>
<tr id=" 288 " class="entry"><td>
<b/> State Entropy Maximization with Random Encoders for Efficient Exploration (2021)</b>
</td></tr>
<tr id=" 289 " class="entry"><td>
<b/> Is Bang-Bang Control All You Need?: Solving Continuous Control with Bernoulli Policies (2021)</b>
</td></tr>
<tr id=" 290 " class="entry"><td>
<b/> Learning to Plan Optimistically: Uncertainty-Guided Deep Exploration via Latent Model Ensembles (2020)</b>
</td></tr>
<tr id=" 291 " class="entry"><td>
<b/> Solving Continuous Control via Q-learning (2022)</b>
</td></tr>
<tr id=" 292 " class="entry"><td>
<b/> Emergent Real-World Robotic Skills via Unsupervised Off-Policy Reinforcement Learning (2020)</b>
</td></tr>
<tr id=" 293 " class="entry"><td>
<b/> Sequential Interpretability: Methods, Applications, and Future Direction for Understanding Deep Learning Models in the Context of Sequential Data (2020)</b>
</td></tr>
<tr id=" 294 " class="entry"><td>
<b/> Residual Policy Learning (2018)</b>
</td></tr>
<tr id=" 295 " class="entry"><td>
<b/> Parrot: Data-Driven Behavioral Priors for Reinforcement Learning (2020)</b>
</td></tr>
<tr id=" 296 " class="entry"><td>
<b/> A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning (2022)</b>
</td></tr>
<tr id=" 297 " class="entry"><td>
<b/> Eliminating all bad Local Minima from Loss Landscapes without even adding an Extra Unit (2019)</b>
</td></tr>
<tr id=" 298 " class="entry"><td>
<b/> ES-MAML: Simple Hessian-Free Meta Learning (2019)</b>
</td></tr>
<tr id=" 299 " class="entry"><td>
<b/> Local Search for Policy Iteration in Continuous Control (2020)</b>
</td></tr>
<tr id=" 300 " class="entry"><td>
<b/> Automatically Bounding the Taylor Remainder Series: Tighter Bounds and New Applications (2022)</b>
</td></tr>
<tr id=" 301 " class="entry"><td>
<b/> Do Differentiable Simulators Give Better Policy Gradients? (2022)</b>
</td></tr>
<tr id=" 302 " class="entry"><td>
<b/> A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement to Imitation (2016)</b>
</td></tr>
<tr id=" 303 " class="entry"><td>
<b/> Novelty Search in Representational Space for Sample Efficient Exploration (2020)</b>
</td></tr>
<tr id=" 304 " class="entry"><td>
<b/> DeepMind Control Suite (2018)</b>
</td></tr>
<tr id=" 305 " class="entry"><td>
<b/> Action Branching Architectures for Deep Reinforcement Learning (2017)</b>
</td></tr>
<tr id=" 306 " class="entry"><td>
<b/> On Bonus-Based Exploration Methods in the Arcade Learning Environment (2021)</b>
</td></tr>
<tr id=" 307 " class="entry"><td>
<b/> Creating Multimodal Interactive Agents with Imitation and Self-Supervised Learning (2021)</b>
</td></tr>
<tr id=" 308 " class="entry"><td>
<b/> Open-Ended Learning Leads to Generally Capable Agents (2021)</b>
</td></tr>
<tr id=" 309 " class="entry"><td>
<b/> Learning Setup Policies: Reliable Transition Between Locomotion Behaviours (2021)</b>
</td></tr>
<tr id=" 310 " class="entry"><td>
<b/> Learning When to Switch: Composing Controllers to Traverse a Sequence of Terrain Artifacts (2020)</b>
</td></tr>
<tr id=" 311 " class="entry"><td>
<b/> Behavior Priors for Efficient Reinforcement Learning (2020)</b>
</td></tr>
<tr id=" 312 " class="entry"><td>
<b/> Invariant Funnels around Trajectories using Sum-of-Squares Programming (2010)</b>
</td></tr>
<tr id=" 313 " class="entry"><td>
<b/> Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards (2019)</b>
</td></tr>
<tr id=" 314 " class="entry"><td>
<b/> Jump-Start Reinforcement Learning (2022)</b>
</td></tr>
<tr id=" 315 " class="entry"><td>
<b/> Discovering the Elite Hypervolume by Leveraging Interspecies Correlation (2018)</b>
</td></tr>
<tr id=" 316 " class="entry"><td>
<b/> Attention Is All You Need (2017)</b>
</td></tr>
<tr id=" 317 " class="entry"><td>
<b/> Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards (2017)</b>
</td></tr>
<tr id=" 318 " class="entry"><td>
<b/> Diffusion-based neuromodulation can eliminate catastrophic forgetting in simple neural networks (2017)</b>
</td></tr>
<tr id=" 319 " class="entry"><td>
<b/> SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration (2022)</b>
</td></tr>
<tr id=" 320 " class="entry"><td>
<b/> Krylov Subspace Descent for Deep Learning (2011)</b>
</td></tr>
<tr id=" 321 " class="entry"><td>
<b/> Convex optimization (2021)</b>
</td></tr>
<tr id=" 322 " class="entry"><td>
<b/> Explainable CNN-attention Networks (C-Attention Network) for Automated Detection of Alzheimer&#8217;s Disease (2020)</b>
</td></tr>
<tr id=" 323 " class="entry"><td>
<b/> Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation (2019)</b>
</td></tr>
<tr id=" 324 " class="entry"><td>
<b/> Support-weighted Adversarial Imitation Learning (2020)</b>
</td></tr>
<tr id=" 325 " class="entry"><td>
<b/> A Surrogate-Assisted Controller for Expensive Evolutionary Reinforcement Learning (2022)</b>
</td></tr>
<tr id=" 326 " class="entry"><td>
<b/> Sample Efficient Actor-Critic with Experience Replay (2016)</b>
</td></tr>
<tr id=" 327 " class="entry"><td>
<b/> Improving Exploration in Soft-Actor-Critic with Normalizing Flows Policies (2019)</b>
</td></tr>
<tr id=" 328 " class="entry"><td>
<b/> Unsupervised Control Through Non-Parametric Discriminative Rewards (2018)</b>
</td></tr>
<tr id=" 329 " class="entry"><td>
<b/> Q-Learning in enormous action spaces via amortized approximate maximization (2020)</b>
</td></tr>
<tr id=" 330 " class="entry"><td>
<b/> A Lyapunov Analysis of Momentum Methods in Optimization (2016)</b>
</td></tr>
<tr id=" 331 " class="entry"><td>
<b/> Evolving simple programs for playing Atari games (2018)</b>
</td></tr>
<tr id=" 332 " class="entry"><td>
<b/> When to Ask for Help: Proactive Interventions in Autonomous Reinforcement Learning (2022)</b>
</td></tr>
<tr id=" 333 " class="entry"><td>
<b/> VideoGPT: Video Generation using VQ-VAE and Transformers (2021)</b>
</td></tr>
<tr id=" 334 " class="entry"><td>
<b/> TRAIL: Near-Optimal Imitation Learning with Suboptimal Data (2021)</b>
</td></tr>
<tr id=" 335 " class="entry"><td>
<b/> Discovering Diverse Athletic Jumping Strategies (2021)</b>
</td></tr>
<tr id=" 336 " class="entry"><td>
<b/> How transferable are features in deep neural networks? (2014)</b>
</td></tr>
<tr id=" 337 " class="entry"><td>
<b/> Self-supervised Sequential Information Bottleneck for Robust Exploration in Deep Reinforcement Learning (2022)</b>
</td></tr>
<tr id=" 338 " class="entry"><td>
<b/> Diffeomorphic Learning (2018)</b>
</td></tr>
<tr id=" 339 " class="entry"><td>
<b/> COMBO: Conservative Offline Model-Based Policy Optimization (2021)</b>
</td></tr>
<tr id=" 340 " class="entry"><td>
<b/> Learning Symmetric and Low-energy Locomotion (2018)</b>
</td></tr>
<tr id=" 341 " class="entry"><td>
<b/> On the Importance of Hyperparameter Optimization for Model-based Reinforcement Learning (2021)</b>
</td></tr>
<tr id=" 342 " class="entry"><td>
<b/> Three Mechanisms of Weight Decay Regularization (2018)</b>
</td></tr>
<tr id=" 343 " class="entry"><td>
<b/> Understanding Hindsight Goal Relabeling from a Divergence Minimization Perspective (2022)</b>
</td></tr>
<tr id=" 344 " class="entry"><td>
<b/> C-Planning: An Automatic Curriculum for Learning Goal-Reaching Tasks (2021)</b>
</td></tr>
<tr id=" 345 " class="entry"><td>
<b/> Adversarially Regularized Autoencoders (2017)</b>
</td></tr>
<tr id=" 346 " class="entry"><td>
<b/> Bottom-Up Skill Discovery from Unsegmented Demonstrations for Long-Horizon Robot Manipulation (2021)</b>
</td></tr>
<tr id=" 347 " class="entry"><td>
<b/> Transfer Learning in Deep Reinforcement Learning: A Survey (2020)</b>
</td></tr>
<tr id=" 348 " class="entry"><td>
<b/> Exploiting the Sign of the Advantage Function to Learn Deterministic Policies in Continuous Domains (2019)</b>
</td></tr>
<tr id=" 349 " class="entry"><td>
<b/> A Bayesian Approach to Policy Recognition and State Representation Learning (2016)</b>
</td></tr>
</tbody>

</table>































<!-- ======== End of the body ================================================================================================ -->

<BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR>



</body></html>

