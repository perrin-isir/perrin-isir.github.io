<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Scientific documents</title>

  

  <meta name="Author" content="Nicolas Perrin-Gilbert">



  <meta name="Description" content="Nicolas Perrin-Gilbert">



  <meta name="Copyright" content="This file may be redistributed and/or modified without limitation">



  <meta name="Language" content="en">



  <meta name="Generator" content="">



  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">



  <meta http-equiv="Content-Language" content="en">



  <link rev="made" href="mailto:nicolas.perrin@sorbonne-universite.fr">



  <link rel="StyleSheet" href="index_files/screenfsos.css" type="text/css" media="screen">



  <link rel="StyleSheet" href="index_files/print.css" type="text/css" media="print"></head>



<script type="text/javascript">

<!--

// QuickSearch script for JabRef HTML export 

// Version: 3.0

//

// Copyright (c) 2006-2011, Mark Schenk

//

// This software is distributed under a Creative Commons Attribution 3.0 License

// http://creativecommons.org/licenses/by/3.0/

//

// Features:

// - intuitive find-as-you-type searching

//    ~ case insensitive

//    ~ ignore diacritics (optional)

//

// - search with/without Regular Expressions

// - match BibTeX key

//



// Search settings

var searchAbstract = true;	// search in abstract

var searchReview = true;	// search in review



var noSquiggles = true; 	// ignore diacritics when searching

var searchRegExp = true; 	// enable RegExp searches





if (window.addEventListener) {

	window.addEventListener("load",initSearch,false); }

else if (window.attachEvent) {

	window.attachEvent("onload", initSearch); }



function initSearch() {

	// check for quick search table and searchfield

	if (!document.getElementById('qs_table')||!document.getElementById('quicksearch')) { return; }



	// load all the rows and sort into arrays

	loadTableData();

	

	//find the query field

	qsfield = document.getElementById('qs_field');



	// previous search term; used for speed optimisation

	prevSearch = '';



	//find statistics location

	stats = document.getElementById('stat');

	setStatistics(-1);

	

	// set up preferences

	initPreferences();



	// shows the searchfield

	document.getElementById('quicksearch').style.display = 'block';

        document.getElementById('qs_field').value = querySt("search");

	document.getElementById('qs_field').onkeyup = quickSearch;

 	quickSearch(qs_field);

}



function loadTableData() {

	// find table and appropriate rows

	searchTable = document.getElementById('qs_table');

	var allRows = searchTable.getElementsByTagName('tbody')[0].getElementsByTagName('tr');



	// split all rows into entryRows and infoRows (e.g. abstract, review, bibtex)

	entryRows = new Array(); infoRows = new Array(); absRows = new Array(); revRows = new Array();



	// get data from each row

	entryRowsData = new Array(); absRowsData = new Array(); revRowsData = new Array(); 

	

	BibTeXKeys = new Array();

	

	for (var i=0, k=0, j=0; i<allRows.length;i++) {

		if (allRows[i].className.match(/entry/)) {

			entryRows[j] = allRows[i];

			entryRowsData[j] = stripDiacritics(getTextContent(allRows[i]));

			allRows[i].id ? BibTeXKeys[j] = allRows[i].id : allRows[i].id = 'autokey_'+j;

			j ++;

		} else {

			infoRows[k++] = allRows[i];

			// check for abstract/review

			if (allRows[i].className.match(/abstract/)) {

				absRows.push(allRows[i]);

				absRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));

			} else if (allRows[i].className.match(/review/)) {

				revRows.push(allRows[i]);

				revRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));

			}

		}

	}

	//number of entries and rows

	numEntries = entryRows.length;

	numInfo = infoRows.length;

	numAbs = absRows.length;

	numRev = revRows.length;

}



function querySt(ji) {



    hu = window.location.search.substring(1);

    gy = hu.split("&");



    for (i=0;i<gy.length;i++) {

        ft = gy[i].split("=");

        if (ft[0] == ji) {

            return decodeURIComponent(ft[1]).replace(/["]/g,'');

        }

    }

    return '';

}



function quickSearch(){

	

	tInput = qsfield;



	if (tInput.value.length == 0) {

		showAll();

		setStatistics(-1);

		qsfield.className = '';

		return;

	} else {

		t = stripDiacritics(tInput.value);



		if(!searchRegExp) { t = escapeRegExp(t); }

			

		// only search for valid RegExp

		try {

			textRegExp = new RegExp(t,"im");

			closeAllInfo();

			qsfield.className = '';

		}

			catch(err) {

			prevSearch = tInput.value;

			qsfield.className = 'invalidsearch';

			return;

		}

	}

	

	// count number of hits

	var hits = 0;



	// start looping through all entry rows

	for (var i = 0; cRow = entryRows[i]; i++){



		// only show search the cells if it isn't already hidden OR if the search term is getting shorter, then search all

		if(cRow.className.indexOf('noshow')==-1 || tInput.value.length <= prevSearch.length){

			var found = false; 



			if (entryRowsData[i].search(textRegExp) != -1 || BibTeXKeys[i].search(textRegExp) != -1){ 

				found = true;

			} else {

				if(searchAbstract && absRowsData[i]!=undefined) {

					if (absRowsData[i].search(textRegExp) != -1){ found=true; } 

				}

				if(searchReview && revRowsData[i]!=undefined) {

					if (revRowsData[i].search(textRegExp) != -1){ found=true; } 

				}

			}

			

			if (found){

				cRow.className = 'entry show';

				hits++;

			} else {

				cRow.className = 'entry noshow';

			}

		}

	}



	// update statistics

	setStatistics(hits)

	

	// set previous search value

	prevSearch = tInput.value;

}





// Strip Diacritics from text

// http://stackoverflow.com/questions/990904/javascript-remove-accents-in-strings



// String containing replacement characters for stripping accents 

var stripstring = 

    'AAAAAAACEEEEIIII'+

    'DNOOOOO.OUUUUY..'+

    'aaaaaaaceeeeiiii'+

    'dnooooo.ouuuuy.y'+

    'AaAaAaCcCcCcCcDd'+

    'DdEeEeEeEeEeGgGg'+

    'GgGgHhHhIiIiIiIi'+

    'IiIiJjKkkLlLlLlL'+

    'lJlNnNnNnnNnOoOo'+

    'OoOoRrRrRrSsSsSs'+

    'SsTtTtTtUuUuUuUu'+

    'UuUuWwYyYZzZzZz.';



function stripDiacritics(str){



    if(noSquiggles==false){

        return str;

    }



    var answer='';

    for(var i=0;i<str.length;i++){

        var ch=str[i];

        var chindex=ch.charCodeAt(0)-192;   // Index of character code in the strip string

        if(chindex>=0 && chindex<stripstring.length){

            // Character is within our table, so we can strip the accent...

            var outch=stripstring.charAt(chindex);

            // ...unless it was shown as a '.'

            if(outch!='.')ch=outch;

        }

        answer+=ch;

    }

    return answer;

}



// http://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex

// NOTE: must escape every \ in the export code because of the JabRef Export...

function escapeRegExp(str) {

  return str.replace(/[-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");

}



function toggleInfo(articleid,info) {



	var entry = document.getElementById(articleid);

	var abs = document.getElementById('abs_'+articleid);

	var rev = document.getElementById('rev_'+articleid);

	var bib = document.getElementById('bib_'+articleid);

	

	if (abs && info == 'abstract') {

		abs.className.indexOf('noshow') == -1?abs.className = 'abstract noshow':abs.className = 'abstract show';

	} else if (rev && info == 'review') {

		rev.className.indexOf('noshow') == -1?rev.className = 'review noshow':rev.className = 'review show';

	} else if (bib && info == 'bibtex') {

		bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex show';

	} else { 

		return;

	}



	// check if one or the other is available

	var revshow; var absshow; var bibshow;

	(abs && abs.className.indexOf('noshow') == -1)? absshow = true: absshow = false;

	(rev && rev.className.indexOf('noshow') == -1)? revshow = true: revshow = false;	

	(bib && bib.className.indexOf('noshow') == -1)? bibshow = true: bibshow = false;

	

	// highlight original entry

	if(entry) {

		if (revshow || absshow || bibshow) {

		entry.className = 'entry highlight show';

		} else {

		entry.className = 'entry show';

		}

	}

	

	// When there's a combination of abstract/review/bibtex showing, need to add class for correct styling

	if(absshow) {

		(revshow||bibshow)?abs.className = 'abstract nextshow':abs.className = 'abstract';

	} 

	if (revshow) {

		bibshow?rev.className = 'review nextshow': rev.className = 'review';

	}	

	

}



function setStatistics (hits) {

	if(hits < 0) { hits=numEntries; }

	if(stats) { stats.firstChild.data = hits + '/' + numEntries}

}



function getTextContent(node) {

	// Function written by Arve Bersvendsen

	// http://www.virtuelvis.com

	

	if (node.nodeType == 3) {

	return node.nodeValue;

	} // text node

	if (node.nodeType == 1 && node.className != "infolinks") { // element node

	var text = [];

	for (var chld = node.firstChild;chld;chld=chld.nextSibling) {

		text.push(getTextContent(chld));

	}

	return text.join("");

	} return ""; // some other node, won't contain text nodes.

}



function showAll(){

	closeAllInfo();

	for (var i = 0; i < numEntries; i++){ entryRows[i].className = 'entry show'; }

}



function closeAllInfo(){

	for (var i=0; i < numInfo; i++){

		if (infoRows[i].className.indexOf('noshow') ==-1) {

			infoRows[i].className = infoRows[i].className + ' noshow';

		}

	}

}



function clearQS() {

	qsfield.value = '';

	showAll();

}



function redoQS(){

	showAll();

	quickSearch(qsfield);

}



function updateSetting(obj){

	var option = obj.id;

	var checked = obj.value;



	switch(option)

	 {

	 case "opt_searchAbs":

	   searchAbstract=!searchAbstract;

	   redoQS();

	   break;

	 case "opt_searchRev":

	   searchReview=!searchReview;

	   redoQS();

	   break;

	 case "opt_useRegExp":

	   searchRegExp=!searchRegExp;

	   redoQS();

	   break;

	 case "opt_noAccents":

	   noSquiggles=!noSquiggles;

	   loadTableData();

	   redoQS();

	   break;

	 }

}



function initPreferences(){

	if(searchAbstract){document.getElementById("opt_searchAbs").checked = true;}

	if(searchReview){document.getElementById("opt_searchRev").checked = true;}

	if(noSquiggles){document.getElementById("opt_noAccents").checked = true;}

	if(searchRegExp){document.getElementById("opt_useRegExp").checked = true;}

	

	if(numAbs==0) {document.getElementById("opt_searchAbs").parentNode.style.display = 'none';}

	if(numRev==0) {document.getElementById("opt_searchRev").parentNode.style.display = 'none';}

}



function toggleSettings(){

	var togglebutton = document.getElementById('showsettings');

	var settings = document.getElementById('settings');

	

	if(settings.className == "hidden"){

		settings.className = "show";

		togglebutton.innerText = "close settings";

		togglebutton.textContent = "close settings";

	}else{

		settings.className = "hidden";

		togglebutton.innerText = "settings...";		

		togglebutton.textContent = "settings...";

	}

}



-->

</script>

<style type="text/css">



form#quicksearch { width: auto; border-style: solid; border-color: gray; border-width: 1px 0px; padding: 0.7em 0.5em; display:none; position:relative; }

span#searchstat {padding-left: 1em;}



div#settings { margin-top:0.7em; /* border-bottom: 1px transparent solid; background-color: #efefef; border: 1px grey solid; */ }

div#settings ul {margin: 0; padding: 0; }

div#settings li {margin: 0; padding: 0 1em 0 0; display: inline; list-style: none; }

div#settings li + li { border-left: 2px #efefef solid; padding-left: 0.5em;}

div#settings input { margin-bottom: 0px;}



div#settings.hidden {display:none;}



#showsettings { border: 1px grey solid; padding: 0 0.5em; float:right; line-height: 1.6em; text-align: right; }

#showsettings:hover { cursor: pointer; }



.invalidsearch { background-color: red; }



table { border: 0px gray solid; width: 100%; empty-cells: show; }

th, td { border: 0px gray solid; padding: 0.1em; vertical-align: top;  }

td { text-align: left; vertical-align: top; }

th { background-color: #EFEFEF; }



td a, td a:hover { color: navy; font-weight: normal; }



tr.noshow { display: none;}



tr.highlight td { background-color: #787878; border-top: 2px black solid; font-weight: normal; }

tr.abstract td, tr.review td, tr.bibtex td { background-color: #F1F1F1; border-bottom: 2px black solid; }

tr.nextshow td { border-bottom: 1px gray solid; }



tr.bibtex pre { width: 100%; overflow: auto;}



p.infolinks { margin: 0.5em 0em 0em 0em; padding: 0px; }



#qssettings { padding: 0.5em; position: absolute; top: 0.2em; right: 0.2em; border: 1px gray solid; background-color: white; display: block; }

#qssettings p { font-weight: normal; cursor: pointer; }

#qssettings ul { display: none; list-style-type: none; padding-left: 0; margin: 0; }

#qssettings.active ul { display: block; }

</style>

</head>

<body>



<!-- For non-visual or non-stylesheet-capable user agents -->

<div id="mainlink"><a href="#main">Skip to

main content.</a></div>



<!-- ======== Header ======== -->

<div id="header">



</div>



<!-- ======== Main Content ======== -->



<div id="main">



<!-- ======== The body ================================================================================================ -->



<thead></thead>

<tbody>





<script type="text/javascript">
function toggleVisibility(x) { var e = document.getElementById(x); if(e.style.display == 'block') e.style.display = 'none'; else e.style.display = 'block';}
</script>
[Content: links to ar5iv versions of some interesting articles] <a href="#" onclick="toggleVisibility('links')">[TAGS on/off]</a>
<div id="links" style="display:block">
<table style="font-size:16px"><td width="100\%"><i>
<a href="index.html?search=&quot;[^]*{control}&quot;">control /</a>
<a href="index.html?search=&quot;[^]*{deep}&quot;">deep /</a>
<a href="index.html?search=&quot;[^]*{distillation}&quot;">distillation /</a>
<a href="index.html?search=&quot;[^]*{diversity}&quot;">diversity /</a>
<a href="index.html?search=&quot;[^]*{entropy}&quot;">entropy /</a>
<a href="index.html?search=&quot;[^]*{explainab}&quot;">explainab /</a>
<a href="index.html?search=&quot;[^]*{explor}&quot;">explor /</a>
<a href="index.html?search=&quot;[^]*{few-shot}&quot;">few-shot /</a>
<a href="index.html?search=&quot;[^]*{goal-conditioned}&quot;">goal-conditioned /</a>
<a href="index.html?search=&quot;[^]*{gradient}&quot;">gradient /</a>
<a href="index.html?search=&quot;[^]*{hierarchical}&quot;">hierarchical /</a>
<a href="index.html?search=&quot;[^]*{humanoid}&quot;">humanoid /</a>
<a href="index.html?search=&quot;[^]*{hybrid}&quot;">hybrid /</a>
<a href="index.html?search=&quot;[^]*{imitation}&quot;">imitation /</a>
<a href="index.html?search=&quot;[^]*{intrinsic}&quot;">intrinsic /</a>
<a href="index.html?search=&quot;[^]*{kinematics}&quot;">kinematics /</a>
<a href="index.html?search=&quot;[^]*{meta}&quot;">meta /</a>
<a href="index.html?search=&quot;[^]*{model}&quot;">model /</a>
<a href="index.html?search=&quot;[^]*{motion planning}&quot;">motion planning /</a>
<a href="index.html?search=&quot;[^]*{multi-task}&quot;">multi-task /</a>
<a href="index.html?search=&quot;[^]*{network}&quot;">network /</a>
<a href="index.html?search=&quot;[^]*{off-policy}&quot;">off-policy /</a>
<a href="index.html?search=&quot;[^]*{on-policy}&quot;">on-policy /</a>
<a href="index.html?search=&quot;[^]*{open-ended}&quot;">open-ended /</a>
<a href="index.html?search=&quot;[^]*{optim}&quot;">optim /</a>
<a href="index.html?search=&quot;[^]*{population}&quot;">population /</a>
<a href="index.html?search=&quot;[^]*{reinforcement}&quot;">reinforcement /</a>
<a href="index.html?search=&quot;[^]*{replay}&quot;">replay /</a>
<a href="index.html?search=&quot;[^]*{robot}&quot;">robot /</a>
<a href="index.html?search=&quot;[^]*{simulation}&quot;">simulation /</a>
<a href="index.html?search=&quot;[^]*{skill}&quot;">skill /</a>
<a href="index.html?search=&quot;[^]*{sparse}&quot;">sparse /</a>
<a href="index.html?search=&quot;[^]*{theory}&quot;">theory /</a>
<a href="index.html?search=&quot;[^]*{transfer}&quot;">transfer /</a>
<a href="index.html?search=&quot;[^]*{unsupervised}&quot;">unsupervised /</a>
</i></td></table>
</div>


<table style="font-size:16px" WIDTH="300" border="0" cellspacing="0" cellpadding="0">

<tr>

<td width="100%">

<form action="" id="quicksearch">

<input style="font-size:16px; text-align:right" size="49" type="text" id="qs_field" autocomplete="off" placeholder="Type to search..." /> <input style="font-size:16px" type="button" onclick="clearQS()" value="clear" /><br>

<span style="font-size:16px" id="searchstat"> <span id="stat">0</span></span>

<div style="font-size:16px" id="showsettings" onclick="toggleSettings()">...</div>

<div id="settings" class="hidden">

<ul>

<li><input type="checkbox" class="search_setting" id="opt_useRegExp" onchange="updateSetting(this)"><label style="font-size:16px" for="opt_useRegExp"> use RegExp</label></li>

<li><input type="checkbox" class="search_setting" id="opt_noAccents" onchange="updateSetting(this)"><label style="font-size:16px" for="opt_noAccents"> ignore accents</label></li>

<li><input type="checkbox" class="search_setting" id="opt_searchAbs" onchange="updateSetting(this)"><label style="font-size:16px" for="opt_searchAbs"> include abstract</label></li>

<li><input type="checkbox" class="search_setting" id="opt_searchRev" onchange="updateSetting(this)"><label style="font-size:16px" for="opt_searchRev"> include review</label></li>

</ul>

</div>

</form>

</td>

</tr>



</table>



<table style="font-size:16px" bgcolor="#F5F6CE" WIDTH="300" id="qs_table" border="0" cellspacing="0" cellpadding="0">



<tr id=" 1 " class="entry"><td> <a href="http://ar5iv.org/abs/2101.04736v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Bootstrapping Motor Skill Learning with Motion Planning (2021)</b> &nbsp; - &nbsp; Abbatematteo, Ben and Rosen, Eric and Tellex, Stefanie and Konidaris, George &nbsp; - &nbsp; 2101.04736v1 &nbsp; - &nbsp;
{deep}
{motion planning}
{network}
{robot}
{skill}
</small></font></div></a></td></tr>
<tr id=" 2 " class="entry"><td> <a href="http://ar5iv.org/abs/1812.02256v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Relative Entropy Regularized Policy Iteration (2018)</b> &nbsp; - &nbsp; Abdolmaleki, Abbas and Springenberg, Jost Tobias and Degrave, Jonas and Bohez, Steven and Tassa, Yuval and Belov, Dan and Heess, Nicolas and Riedmiller, Martin &nbsp; - &nbsp; 1812.02256v1 &nbsp; - &nbsp;
{control}
{deep}
{gradient}
{off-policy}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 3 " class="entry"><td> <a href="http://ar5iv.org/abs/1903.08894v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Towards Characterizing Divergence in Deep Q-Learning (2019)</b> &nbsp; - &nbsp; Achiam, Joshua and Knight, Ethan and Abbeel, Pieter &nbsp; - &nbsp; 1903.08894v1 &nbsp; - &nbsp;
{control}
{deep}
{gradient}
{network}
{off-policy}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 4 " class="entry"><td> <a href="http://ar5iv.org/abs/1811.11214v5"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Understanding the impact of entropy on policy optimization (2018)</b> &nbsp; - &nbsp; Ahmed, Zafarali and Roux, Nicolas Le and Norouzi, Mohammad and Schuurmans, Dale &nbsp; - &nbsp; 1811.11214v5 &nbsp; - &nbsp;
{entropy}
{explor}
{gradient}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 5 " class="entry"><td> <a href="http://ar5iv.org/abs/2012.13658v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Locally Persistent Exploration in Continuous Control Tasks with Sparse Rewards (2020)</b> &nbsp; - &nbsp; Amin, Susan and Gomrokchi, Maziar and Aboutalebi, Hossein and Satija, Harsh and Precup, Doina &nbsp; - &nbsp; 2012.13658v2 &nbsp; - &nbsp;
{control}
{explor}
{reinforcement}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 6 " class="entry"><td> <a href="http://ar5iv.org/abs/1609.07152v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Input Convex Neural Networks (2016)</b> &nbsp; - &nbsp; Amos, Brandon and Xu, Lei and Kolter, J. Zico &nbsp; - &nbsp; 1609.07152v3 &nbsp; - &nbsp;
{deep}
{model}
{network}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 7 " class="entry"><td> <a href="http://ar5iv.org/abs/1212.1524v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Layer-wise learning of deep generative models (2012)</b> &nbsp; - &nbsp; Arnold, Ludovic and Ollivier, Yann &nbsp; - &nbsp; 1212.1524v2 &nbsp; - &nbsp;
{deep}
{model}
{optim}
{theory}
</small></font></div></a></td></tr>
<tr id=" 8 " class="entry"><td> <a href="http://ar5iv.org/abs/1708.05866v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> A Brief Survey of Deep Reinforcement Learning (2017)</b> &nbsp; - &nbsp; Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony &nbsp; - &nbsp; 1708.05866v2 &nbsp; - &nbsp;
{control}
{deep}
{network}
{on-policy}
{optim}
{reinforcement}
{robot}
</small></font></div></a></td></tr>
<tr id=" 9 " class="entry"><td> <a href="http://ar5iv.org/abs/1412.8690v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Breaking the Curse of Dimensionality with Convex Neural Networks (2014)</b> &nbsp; - &nbsp; Bach, Francis &nbsp; - &nbsp; 1412.8690v2 &nbsp; - &nbsp;
{network}
</small></font></div></a></td></tr>
<tr id=" 10 " class="entry"><td> <a href="http://ar5iv.org/abs/2002.06038v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Never Give Up: Learning Directed Exploration Strategies (2020)</b> &nbsp; - &nbsp; Badia, Adri&#224; Puigdom&#232;nech and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Daniel and Piot, Bilal and Kapturowski, Steven and Tieleman, Olivier and Arjovsky, Mart&#237;n and Pritzel, Alexander and Bolt, Andew and Blundell, Charles &nbsp; - &nbsp; 2002.06038v1 &nbsp; - &nbsp;
{control}
{explor}
{intrinsic}
{model}
{network}
{reinforcement}
{transfer}
</small></font></div></a></td></tr>
<tr id=" 11 " class="entry"><td> <a href="http://ar5iv.org/abs/1509.03005v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Compatible Value Gradients for Reinforcement Learning of Continuous Deep Policies (2015)</b> &nbsp; - &nbsp; Balduzzi, David and Ghifary, Muhammad &nbsp; - &nbsp; 1509.03005v1 &nbsp; - &nbsp;
{deep}
{gradient}
{model}
{network}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 12 " class="entry"><td> <a href="http://ar5iv.org/abs/2002.02693v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Ready Policy One: World Building Through Active Learning (2020)</b> &nbsp; - &nbsp; Ball, Philip and Parker-Holder, Jack and Pacchiano, Aldo and Choromanski, Krzysztof and Roberts, Stephen &nbsp; - &nbsp; 2002.02693v1 &nbsp; - &nbsp;
{control}
{explor}
{hybrid}
{model}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 13 " class="entry"><td> <a href="http://ar5iv.org/abs/2006.13258v6"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization (2020)</b> &nbsp; - &nbsp; Barde, Paul and Roy, Julien and Jeon, Wonseok and Pineau, Joelle and Pal, Christopher and Nowrouzezahrai, Derek &nbsp; - &nbsp; 2006.13258v6 &nbsp; - &nbsp;
{imitation}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 14 " class="entry"><td> <a href="http://ar5iv.org/abs/1806.01261v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Relational inductive biases, deep learning, and graph networks (2018)</b> &nbsp; - &nbsp; Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan &nbsp; - &nbsp; 1806.01261v3 &nbsp; - &nbsp;
{control}
{deep}
{explor}
{network}
</small></font></div></a></td></tr>
<tr id=" 15 " class="entry"><td> <a href="http://ar5iv.org/abs/2002.09571v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Learning to Continually Learn (2020)</b> &nbsp; - &nbsp; Beaulieu, Shawn and Frati, Lapo and Miconi, Thomas and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff and Cheney, Nick &nbsp; - &nbsp; 2002.09571v2 &nbsp; - &nbsp;
{control}
{deep}
{meta}
{model}
{network}
</small></font></div></a></td></tr>
<tr id=" 16 " class="entry"><td> <a href="http://ar5iv.org/abs/1903.02219v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Training in Task Space to Speed Up and Guide Reinforcement Learning (2019)</b> &nbsp; - &nbsp; Bellegarda, Guillaume and Byl, Katie &nbsp; - &nbsp; 1903.02219v1 &nbsp; - &nbsp;
{control}
{kinematics}
{model}
{reinforcement}
{robot}
</small></font></div></a></td></tr>
<tr id=" 17 " class="entry"><td> <a href="http://ar5iv.org/abs/1901.11530v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> A Geometric Perspective on Optimal Representations for Reinforcement Learning (2019)</b> &nbsp; - &nbsp; Bellemare, Marc G. and Dabney, Will and Dadashi, Robert and Taiga, Adrien Ali and Castro, Pablo Samuel and Roux, Nicolas Le and Schuurmans, Dale and Lattimore, Tor and Lyle, Clare &nbsp; - &nbsp; 1901.11530v2 &nbsp; - &nbsp;
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 18 " class="entry"><td> <a href="http://ar5iv.org/abs/1707.06887v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> A Distributional Perspective on Reinforcement Learning (2017)</b> &nbsp; - &nbsp; Bellemare, Marc G. and Dabney, Will and Munos, R&#233;mi &nbsp; - &nbsp; 1707.06887v1 &nbsp; - &nbsp;
{control}
{model}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 19 " class="entry"><td> <a href="http://ar5iv.org/abs/1705.10743v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> The Cramer Distance as a Solution to Biased Wasserstein Gradients (2017)</b> &nbsp; - &nbsp; Bellemare, Marc G. and Danihelka, Ivo and Dabney, Will and Mohamed, Shakir and Lakshminarayanan, Balaji and Hoyer, Stephan and Munos, R&#233;mi &nbsp; - &nbsp; 1705.10743v1 &nbsp; - &nbsp;
{gradient}
{model}
{network}
</small></font></div></a></td></tr>
<tr id=" 20 " class="entry"><td> <a href="http://ar5iv.org/abs/1606.01868v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Unifying Count-Based Exploration and Intrinsic Motivation (2016)</b> &nbsp; - &nbsp; Bellemare, Marc G. and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi &nbsp; - &nbsp; 1606.01868v2 &nbsp; - &nbsp;
{explor}
{intrinsic}
{model}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 21 " class="entry"><td> <a href="http://ar5iv.org/abs/1206.5538v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Representation Learning: A Review and New Perspectives (2012)</b> &nbsp; - &nbsp; Bengio, Yoshua and Courville, Aaron and Vincent, Pascal &nbsp; - &nbsp; 1206.5538v3 &nbsp; - &nbsp;
{deep}
{model}
{network}
{unsupervised}
</small></font></div></a></td></tr>
<tr id=" 22 " class="entry"><td> <a href="http://ar5iv.org/abs/1801.03954v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Model-Based Action Exploration for Learning Dynamic Motion Skills (2018)</b> &nbsp; - &nbsp; Berseth, Glen and Panne, Michiel van de &nbsp; - &nbsp; 1801.03954v2 &nbsp; - &nbsp;
{control}
{deep}
{explor}
{model}
{reinforcement}
{robot}
</small></font></div></a></td></tr>
<tr id=" 23 " class="entry"><td> <a href="http://ar5iv.org/abs/2005.10934v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> LEAF: Latent Exploration Along the Frontier (2020)</b> &nbsp; - &nbsp; Bharadhwaj, Homanga and Garg, Animesh and Shkurti, Florian &nbsp; - &nbsp; 2005.10934v3 &nbsp; - &nbsp;
{deep}
{explor}
{network}
{robot}
</small></font></div></a></td></tr>
<tr id=" 24 " class="entry"><td> <a href="http://ar5iv.org/abs/1906.09807v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Proximal Distilled Evolutionary Reinforcement Learning (2019)</b> &nbsp; - &nbsp; Bodnar, Cristian and Day, Ben and Li&#243;, Pietro &nbsp; - &nbsp; 1906.09807v4 &nbsp; - &nbsp;
{deep}
{hierarchical}
{network}
{reinforcement}
{robot}
</small></font></div></a></td></tr>
<tr id=" 25 " class="entry"><td> <a href="http://ar5iv.org/abs/1706.03662v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Practical Gauss-Newton Optimisation for Deep Learning (2017)</b> &nbsp; - &nbsp; Botev, Aleksandar and Ritter, Hippolyt and Barber, David &nbsp; - &nbsp; 1706.03662v2 &nbsp; - &nbsp;
{network}
{optim}
{transfer}
</small></font></div></a></td></tr>
<tr id=" 26 " class="entry"><td> <a href="http://ar5iv.org/abs/2011.04483v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> A Theory of Universal Learning (2020)</b> &nbsp; - &nbsp; Bousquet, Olivier and Hanneke, Steve and Moran, Shay and Handel, Ramon van and Yehudayoff, Amir &nbsp; - &nbsp; 2011.04483v1 &nbsp; - &nbsp;
{model}
{optim}
{theory}
</small></font></div></a></td></tr>
<tr id=" 27 " class="entry"><td> <a href="http://ar5iv.org/abs/1908.04211v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> On Identifiability in Transformers (2019)</b> &nbsp; - &nbsp; Brunner, Gino and Liu, Yang and Pascual, Dami&#225;n and Richter, Oliver and Ciaramita, Massimiliano and Wattenhofer, Roger &nbsp; - &nbsp; 1908.04211v4 &nbsp; - &nbsp;
{deep}
{gradient}
{model}
</small></font></div></a></td></tr>
<tr id=" 28 " class="entry"><td> <a href="http://ar5iv.org/abs/1810.12894v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Exploration by Random Network Distillation (2018)</b> &nbsp; - &nbsp; Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg &nbsp; - &nbsp; 1810.12894v1 &nbsp; - &nbsp;
{deep}
{distillation}
{explor}
{intrinsic}
{network}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 29 " class="entry"><td> <a href="http://ar5iv.org/abs/2107.00541v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Goal-Conditioned Reinforcement Learning with Imagined Subgoals (2021)</b> &nbsp; - &nbsp; Chane-Sane, Elliot and Schmid, Cordelia and Laptev, Ivan &nbsp; - &nbsp; 2107.00541v1 &nbsp; - &nbsp;
{goal-conditioned}
{reinforcement}
{robot}
{skill}
</small></font></div></a></td></tr>
<tr id=" 30 " class="entry"><td> <a href="http://ar5iv.org/abs/2104.07749v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills (2021)</b> &nbsp; - &nbsp; Chebotar, Yevgen and Hausman, Karol and Lu, Yao and Xiao, Ted and Kalashnikov, Dmitry and Varley, Jake and Irpan, Alex and Eysenbach, Benjamin and Julian, Ryan and Finn, Chelsea and Levine, Sergey &nbsp; - &nbsp; 2104.07749v3 &nbsp; - &nbsp;
{explor}
{goal-conditioned}
{model}
{robot}
{skill}
</small></font></div></a></td></tr>
<tr id=" 31 " class="entry"><td> <a href="http://ar5iv.org/abs/1806.07366v5"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Neural Ordinary Differential Equations (2018)</b> &nbsp; - &nbsp; Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David &nbsp; - &nbsp; 1806.07366v5 &nbsp; - &nbsp;
{deep}
{model}
{network}
</small></font></div></a></td></tr>
<tr id=" 32 " class="entry"><td> <a href="http://ar5iv.org/abs/1606.03657v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets (2016)</b> &nbsp; - &nbsp; Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter &nbsp; - &nbsp; 1606.03657v1 &nbsp; - &nbsp;
{network}
{optim}
{unsupervised}
</small></font></div></a></td></tr>
<tr id=" 33 " class="entry"><td> <a href="http://ar5iv.org/abs/1805.08380v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Optimal transport natural gradient for statistical manifolds with continuous sample space (2018)</b> &nbsp; - &nbsp; Chen, Yifan and Li, Wuchen &nbsp; - &nbsp; 1805.08380v4 &nbsp; - &nbsp;
{gradient}
{model}
{optim}
</small></font></div></a></td></tr>
<tr id=" 34 " class="entry"><td> <a href="http://ar5iv.org/abs/1901.10691v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Probability Functional Descent: A Unifying Perspective on GANs, Variational Inference, and Reinforcement Learning (2019)</b> &nbsp; - &nbsp; Chu, Casey and Blanchet, Jose and Glynn, Peter &nbsp; - &nbsp; 1901.10691v2 &nbsp; - &nbsp;
{network}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 35 " class="entry"><td> <a href="http://ar5iv.org/abs/1805.12114v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models (2018)</b> &nbsp; - &nbsp; Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey &nbsp; - &nbsp; 1805.12114v2 &nbsp; - &nbsp;
{deep}
{model}
{network}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 36 " class="entry"><td> <a href="http://ar5iv.org/abs/1910.12807v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Better Exploration with Optimistic Actor-Critic (2019)</b> &nbsp; - &nbsp; Ciosek, Kamil and Vuong, Quan and Loftin, Robert and Hofmann, Katja &nbsp; - &nbsp; 1910.12807v1 &nbsp; - &nbsp;
{control}
{explor}
{model}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 37 " class="entry"><td> <a href="http://ar5iv.org/abs/1712.06560v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents (2017)</b> &nbsp; - &nbsp; Conti, Edoardo and Madhavan, Vashisht and Such, Felipe Petroski and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff &nbsp; - &nbsp; 1712.06560v3 &nbsp; - &nbsp;
{deep}
{diversity}
{explor}
{gradient}
{hybrid}
{network}
{optim}
{population}
{reinforcement}
{robot}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 38 " class="entry"><td> <a href="http://ar5iv.org/abs/1708.09251v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Quality and Diversity Optimization: A Unifying Modular Framework (2017)</b> &nbsp; - &nbsp; Cully, Antoine and Demiris, Yiannis &nbsp; - &nbsp; 1708.09251v1 &nbsp; - &nbsp;
{diversity}
{optim}
</small></font></div></a></td></tr>
<tr id=" 39 " class="entry"><td> <a href="http://ar5iv.org/abs/1806.06923v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Implicit Quantile Networks for Distributional Reinforcement Learning (2018)</b> &nbsp; - &nbsp; Dabney, Will and Ostrovski, Georg and Silver, David and Munos, R&#233;mi &nbsp; - &nbsp; 1806.06923v1 &nbsp; - &nbsp;
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 40 " class="entry"><td> <a href="http://ar5iv.org/abs/1710.10044v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Distributional Reinforcement Learning with Quantile Regression (2017)</b> &nbsp; - &nbsp; Dabney, Will and Rowland, Mark and Bellemare, Marc G. and Munos, R&#233;mi &nbsp; - &nbsp; 1710.10044v1 &nbsp; - &nbsp;
{model}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 41 " class="entry"><td> <a href="http://ar5iv.org/abs/2006.04678v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Primal Wasserstein Imitation Learning (2020)</b> &nbsp; - &nbsp; Dadashi, Robert and Hussenot, L&#233;onard and Geist, Matthieu and Pietquin, Olivier &nbsp; - &nbsp; 2006.04678v2 &nbsp; - &nbsp;
{control}
{imitation}
</small></font></div></a></td></tr>
<tr id=" 42 " class="entry"><td> <a href="http://ar5iv.org/abs/1211.0358v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Deep Gaussian Processes (2012)</b> &nbsp; - &nbsp; Damianou, Andreas C. and Lawrence, Neil D. &nbsp; - &nbsp; 1211.0358v2 &nbsp; - &nbsp;
{deep}
{gradient}
{model}
{network}
{optim}
</small></font></div></a></td></tr>
<tr id=" 43 " class="entry"><td> <a href="http://ar5iv.org/abs/1507.00210v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Natural Neural Networks (2015)</b> &nbsp; - &nbsp; Desjardins, Guillaume and Simonyan, Karen and Pascanu, Razvan and Kavukcuoglu, Koray &nbsp; - &nbsp; 1507.00210v1 &nbsp; - &nbsp;
{gradient}
{network}
{unsupervised}
</small></font></div></a></td></tr>
<tr id=" 44 " class="entry"><td> <a href="http://ar5iv.org/abs/1703.04933v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Sharp Minima Can Generalize For Deep Nets (2017)</b> &nbsp; - &nbsp; Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua &nbsp; - &nbsp; 1703.04933v2 &nbsp; - &nbsp;
{deep}
{gradient}
{model}
{network}
</small></font></div></a></td></tr>
<tr id=" 45 " class="entry"><td> <a href="http://ar5iv.org/abs/1805.04874v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> GAN Q-learning (2018)</b> &nbsp; - &nbsp; Doan, Thang and Mazoure, Bogdan and Lyle, Clare &nbsp; - &nbsp; 1805.04874v3 &nbsp; - &nbsp;
{deep}
{model}
{network}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 46 " class="entry"><td> <a href="http://ar5iv.org/abs/1606.05908v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Tutorial on Variational Autoencoders (2016)</b> &nbsp; - &nbsp; Doersch, Carl &nbsp; - &nbsp; 1606.05908v3 &nbsp; - &nbsp;
{gradient}
{model}
{network}
{unsupervised}
</small></font></div></a></td></tr>
<tr id=" 47 " class="entry"><td> <a href="http://ar5iv.org/abs/2004.12919v6"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> First return, then explore (2020)</b> &nbsp; - &nbsp; Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff &nbsp; - &nbsp; 2004.12919v6 &nbsp; - &nbsp;
{explor}
{goal-conditioned}
{reinforcement}
{robot}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 48 " class="entry"><td> <a href="http://ar5iv.org/abs/1901.10995v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Go-Explore: a New Approach for Hard-Exploration Problems (2019)</b> &nbsp; - &nbsp; Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff &nbsp; - &nbsp; 1901.10995v4 &nbsp; - &nbsp;
{explor}
{imitation}
{intrinsic}
{reinforcement}
{robot}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 49 " class="entry"><td> <a href="http://ar5iv.org/abs/2005.12729v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO (2020)</b> &nbsp; - &nbsp; Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Madry, Aleksander &nbsp; - &nbsp; 2005.12729v1 &nbsp; - &nbsp;
{deep}
{gradient}
{on-policy}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 50 " class="entry"><td> <a href="http://ar5iv.org/abs/1802.01561v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures (2018)</b> &nbsp; - &nbsp; Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray &nbsp; - &nbsp; 1802.01561v3 &nbsp; - &nbsp;
{deep}
{multi-task}
{off-policy}
{reinforcement}
{transfer}
</small></font></div></a></td></tr>
<tr id=" 51 " class="entry"><td> <a href="http://ar5iv.org/abs/1802.06070v6"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Diversity is All You Need: Learning Skills without a Reward Function (2018)</b> &nbsp; - &nbsp; Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey &nbsp; - &nbsp; 1802.06070v6 &nbsp; - &nbsp;
{diversity}
{entropy}
{explor}
{hierarchical}
{reinforcement}
{robot}
{skill}
{sparse}
{unsupervised}
</small></font></div></a></td></tr>
<tr id=" 52 " class="entry"><td> <a href="http://ar5iv.org/abs/2103.12656v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification (2021)</b> &nbsp; - &nbsp; Eysenbach, Benjamin and Levine, Sergey and Salakhutdinov, Ruslan &nbsp; - &nbsp; 2103.12656v2 &nbsp; - &nbsp;
{control}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 53 " class="entry"><td> <a href="http://ar5iv.org/abs/2103.06257v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Maximum Entropy RL (Provably) Solves Some Robust RL Problems (2021)</b> &nbsp; - &nbsp; Eysenbach, Benjamin and Levine, Sergey &nbsp; - &nbsp; 2103.06257v1 &nbsp; - &nbsp;
{entropy}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 54 " class="entry"><td> <a href="http://ar5iv.org/abs/1603.00448v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization (2016)</b> &nbsp; - &nbsp; Finn, Chelsea and Levine, Sergey and Abbeel, Pieter &nbsp; - &nbsp; 1603.00448v3 &nbsp; - &nbsp;
{control}
{explor}
{network}
{optim}
{reinforcement}
{robot}
</small></font></div></a></td></tr>
<tr id=" 55 " class="entry"><td> <a href="http://ar5iv.org/abs/1509.06113v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Deep Spatial Autoencoders for Visuomotor Learning (2015)</b> &nbsp; - &nbsp; Finn, Chelsea and Tan, Xin Yu and Duan, Yan and Darrell, Trevor and Levine, Sergey and Abbeel, Pieter &nbsp; - &nbsp; 1509.06113v3 &nbsp; - &nbsp;
{control}
{deep}
{model}
{reinforcement}
{robot}
{skill}
</small></font></div></a></td></tr>
<tr id=" 56 " class="entry"><td> <a href="http://ar5iv.org/abs/1705.06366v5"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Automatic Goal Generation for Reinforcement Learning Agents (2017)</b> &nbsp; - &nbsp; Florensa, Carlos and Held, David and Geng, Xinyang and Abbeel, Pieter &nbsp; - &nbsp; 1705.06366v5 &nbsp; - &nbsp;
{network}
{optim}
{reinforcement}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 57 " class="entry"><td> <a href="http://ar5iv.org/abs/1707.05300v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Reverse Curriculum Generation for Reinforcement Learning (2017)</b> &nbsp; - &nbsp; Florensa, Carlos and Held, David and Wulfmeier, Markus and Zhang, Michael and Abbeel, Pieter &nbsp; - &nbsp; 1707.05300v3 &nbsp; - &nbsp;
{explor}
{reinforcement}
{robot}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 58 " class="entry"><td> <a href="http://ar5iv.org/abs/2106.03894v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Differentiable Quality Diversity (2021)</b> &nbsp; - &nbsp; Fontaine, Matthew C. and Nikolaidis, Stefanos &nbsp; - &nbsp; 2106.03894v3 &nbsp; - &nbsp;
{diversity}
{explor}
{gradient}
{optim}
</small></font></div></a></td></tr>
<tr id=" 59 " class="entry"><td> <a href="http://ar5iv.org/abs/1706.10295v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Noisy Networks for Exploration (2017)</b> &nbsp; - &nbsp; Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and Menick, Jacob and Osband, Ian and Graves, Alex and Mnih, Vlad and Munos, Remi and Hassabis, Demis and Pietquin, Olivier and Blundell, Charles and Legg, Shane &nbsp; - &nbsp; 1706.10295v3 &nbsp; - &nbsp;
{deep}
{entropy}
{explor}
{gradient}
{network}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 60 " class="entry"><td> <a href="http://ar5iv.org/abs/1811.12560v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> An Introduction to Deep Reinforcement Learning (2018)</b> &nbsp; - &nbsp; Francois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle &nbsp; - &nbsp; 1811.12560v2 &nbsp; - &nbsp;
{deep}
{model}
{reinforcement}
{robot}
</small></font></div></a></td></tr>
<tr id=" 61 " class="entry"><td> <a href="http://ar5iv.org/abs/2106.13281v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Brax &#8211; A Differentiable Physics Engine for Large Scale Rigid Body Simulation (2021)</b> &nbsp; - &nbsp; Freeman, C. Daniel and Frey, Erik and Raichuk, Anton and Girgin, Sertan and Mordatch, Igor and Bachem, Olivier &nbsp; - &nbsp; 2106.13281v1 &nbsp; - &nbsp;
{optim}
{reinforcement}
{simulation}
</small></font></div></a></td></tr>
<tr id=" 62 " class="entry"><td> <a href="http://ar5iv.org/abs/1902.10250v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Diagnosing Bottlenecks in Deep Q-learning Algorithms (2019)</b> &nbsp; - &nbsp; Fu, Justin and Kumar, Aviral and Soh, Matthew and Levine, Sergey &nbsp; - &nbsp; 1902.10250v1 &nbsp; - &nbsp;
{control}
{deep}
{network}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 63 " class="entry"><td> <a href="http://ar5iv.org/abs/1812.02900v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Off-Policy Deep Reinforcement Learning without Exploration (2018)</b> &nbsp; - &nbsp; Fujimoto, Scott and Meger, David and Precup, Doina &nbsp; - &nbsp; 1812.02900v3 &nbsp; - &nbsp;
{control}
{deep}
{off-policy}
{on-policy}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 64 " class="entry"><td> <a href="http://ar5iv.org/abs/1806.03884v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis (2018)</b> &nbsp; - &nbsp; George, Thomas and Laurent, C&#233;sar and Bouthillier, Xavier and Ballas, Nicolas and Vincent, Pascal &nbsp; - &nbsp; 1806.03884v2 &nbsp; - &nbsp;
{deep}
{gradient}
{model}
{network}
{optim}
</small></font></div></a></td></tr>
<tr id=" 65 " class="entry"><td> <a href="http://ar5iv.org/abs/1912.06088v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Learning to Reach Goals via Iterated Supervised Learning (2019)</b> &nbsp; - &nbsp; Ghosh, Dibya and Gupta, Abhishek and Reddy, Ashwin and Fu, Justin and Devin, Coline and Eysenbach, Benjamin and Levine, Sergey &nbsp; - &nbsp; 1912.06088v4 &nbsp; - &nbsp;
{imitation}
{optim}
{reinforcement}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 66 " class="entry"><td> <a href="http://ar5iv.org/abs/1804.00379v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Recall Traces: Backtracking Models for Efficient Reinforcement Learning (2018)</b> &nbsp; - &nbsp; Goyal, Anirudh and Brakel, Philemon and Fedus, William and Singhal, Soumye and Lillicrap, Timothy and Levine, Sergey and Larochelle, Hugo and Bengio, Yoshua &nbsp; - &nbsp; 1804.00379v2 &nbsp; - &nbsp;
{model}
{off-policy}
</small></font></div></a></td></tr>
<tr id=" 67 " class="entry"><td> <a href="http://ar5iv.org/abs/1611.07507v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Variational Intrinsic Control (2016)</b> &nbsp; - &nbsp; Gregor, Karol and Rezende, Danilo Jimenez and Wierstra, Daan &nbsp; - &nbsp; 1611.07507v1 &nbsp; - &nbsp;
{gradient}
{intrinsic}
{reinforcement}
{unsupervised}
</small></font></div></a></td></tr>
<tr id=" 68 " class="entry"><td> <a href="http://ar5iv.org/abs/1310.5726v5"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Correlation and variable importance in random forests (2013)</b> &nbsp; - &nbsp; Gregorutti, Baptiste and Michel, Bertrand and Saint-Pierre, Philippe &nbsp; - &nbsp; 1310.5726v5 &nbsp; - &nbsp;
{model}
{simulation}
</small></font></div></a></td></tr>
<tr id=" 69 " class="entry"><td> <a href="http://ar5iv.org/abs/1906.01563v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Hamiltonian Neural Networks (2019)</b> &nbsp; - &nbsp; Greydanus, Sam and Dzamba, Misko and Yosinski, Jason &nbsp; - &nbsp; 1906.01563v3 &nbsp; - &nbsp;
{model}
{network}
{unsupervised}
</small></font></div></a></td></tr>
<tr id=" 70 " class="entry"><td> <a href="http://ar5iv.org/abs/1603.00748v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Continuous Deep Q-Learning with Model-based Acceleration (2016)</b> &nbsp; - &nbsp; Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey &nbsp; - &nbsp; 1603.00748v1 &nbsp; - &nbsp;
{control}
{deep}
{explor}
{gradient}
{model}
{network}
{reinforcement}
{replay}
{robot}
</small></font></div></a></td></tr>
<tr id=" 71 " class="entry"><td> <a href="http://ar5iv.org/abs/1811.06407v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Neural Predictive Belief Representations (2018)</b> &nbsp; - &nbsp; Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Pires, Bernardo A. and Munos, R&#233;mi &nbsp; - &nbsp; 1811.06407v2 &nbsp; - &nbsp;
{optim}
{unsupervised}
</small></font></div></a></td></tr>
<tr id=" 72 " class="entry"><td> <a href="http://ar5iv.org/abs/1703.02949v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning (2017)</b> &nbsp; - &nbsp; Gupta, Abhishek and Devin, Coline and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey &nbsp; - &nbsp; 1703.02949v1 &nbsp; - &nbsp;
{reinforcement}
{robot}
{skill}
{transfer}
</small></font></div></a></td></tr>
<tr id=" 73 " class="entry"><td> <a href="http://ar5iv.org/abs/1802.07245v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Meta-Reinforcement Learning of Structured Exploration Strategies (2018)</b> &nbsp; - &nbsp; Gupta, Abhishek and Mendonca, Russell and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey &nbsp; - &nbsp; 1802.07245v1 &nbsp; - &nbsp;
{deep}
{explor}
{gradient}
{meta}
{model}
{reinforcement}
{robot}
</small></font></div></a></td></tr>
<tr id=" 74 " class="entry"><td> <a href="http://ar5iv.org/abs/1803.10122v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> World Models (2018)</b> &nbsp; - &nbsp; Ha, David and Schmidhuber, J&#252;rgen &nbsp; - &nbsp; 1803.10122v4 &nbsp; - &nbsp;
{explor}
{model}
{network}
{reinforcement}
{transfer}
{unsupervised}
</small></font></div></a></td></tr>
<tr id=" 75 " class="entry"><td> <a href="http://ar5iv.org/abs/1804.02808v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Latent Space Policies for Hierarchical Reinforcement Learning (2018)</b> &nbsp; - &nbsp; Haarnoja, Tuomas and Hartikainen, Kristian and Abbeel, Pieter and Levine, Sergey &nbsp; - &nbsp; 1804.02808v2 &nbsp; - &nbsp;
{control}
{deep}
{entropy}
{hierarchical}
{network}
{optim}
{reinforcement}
{skill}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 76 " class="entry"><td> <a href="http://ar5iv.org/abs/1803.06773v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Composable Deep Reinforcement Learning for Robotic Manipulation (2018)</b> &nbsp; - &nbsp; Haarnoja, Tuomas and Pong, Vitchyr and Zhou, Aurick and Dalal, Murtaza and Abbeel, Pieter and Levine, Sergey &nbsp; - &nbsp; 1803.06773v1 &nbsp; - &nbsp;
{deep}
{entropy}
{explor}
{model}
{optim}
{reinforcement}
{robot}
{skill}
</small></font></div></a></td></tr>
<tr id=" 77 " class="entry"><td> <a href="http://ar5iv.org/abs/1801.01290v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor (2018)</b> &nbsp; - &nbsp; Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey &nbsp; - &nbsp; 1801.01290v2 &nbsp; - &nbsp;
{control}
{deep}
{entropy}
{model}
{off-policy}
{on-policy}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 78 " class="entry"><td> <a href="http://ar5iv.org/abs/1812.05905v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Soft Actor-Critic Algorithms and Applications (2018)</b> &nbsp; - &nbsp; Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey &nbsp; - &nbsp; 1812.05905v2 &nbsp; - &nbsp;
{control}
{deep}
{entropy}
{model}
{off-policy}
{on-policy}
{reinforcement}
{robot}
</small></font></div></a></td></tr>
<tr id=" 79 " class="entry"><td> <a href="http://ar5iv.org/abs/1709.02878v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> TensorFlow Agents: Efficient Batched Reinforcement Learning in TensorFlow (2017)</b> &nbsp; - &nbsp; Hafner, Danijar and Davidson, James and Vanhoucke, Vincent &nbsp; - &nbsp; 1709.02878v2 &nbsp; - &nbsp;
{network}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 80 " class="entry"><td> <a href="http://ar5iv.org/abs/1912.01603v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Dream to Control: Learning Behaviors by Latent Imagination (2019)</b> &nbsp; - &nbsp; Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad &nbsp; - &nbsp; 1912.01603v3 &nbsp; - &nbsp;
{control}
{deep}
{gradient}
{model}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 81 " class="entry"><td> <a href="http://ar5iv.org/abs/2010.02193v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Mastering Atari with Discrete World Models (2020)</b> &nbsp; - &nbsp; Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy &nbsp; - &nbsp; 2010.02193v4 &nbsp; - &nbsp;
{humanoid}
{model}
{reinforcement}
{robot}
</small></font></div></a></td></tr>
<tr id=" 82 " class="entry"><td> <a href="http://ar5iv.org/abs/2008.12228v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Towards General and Autonomous Learning of Core Skills: A Case Study in Locomotion (2020)</b> &nbsp; - &nbsp; Hafner, Roland and Hertweck, Tim and Kl&#246;ppner, Philipp and Bloesch, Michael and Neunert, Michael and Wulfmeier, Markus and Tunyasuvunakool, Saran and Heess, Nicolas and Riedmiller, Martin &nbsp; - &nbsp; 2008.12228v1 &nbsp; - &nbsp;
{control}
{multi-task}
{off-policy}
{reinforcement}
{robot}
{skill}
</small></font></div></a></td></tr>
<tr id=" 83 " class="entry"><td> <a href="http://ar5iv.org/abs/2011.04021v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> On the role of planning in model-based deep reinforcement learning (2020)</b> &nbsp; - &nbsp; Hamrick, Jessica B. and Friesen, Abram L. and Behbahani, Feryal and Guez, Arthur and Viola, Fabio and Witherspoon, Sims and Anthony, Thomas and Buesing, Lars and Veli&#269;kovi&#263;, Petar and Weber, Th&#233;ophane &nbsp; - &nbsp; 2011.04021v2 &nbsp; - &nbsp;
{control}
{deep}
{diversity}
{model}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 84 " class="entry"><td> <a href="http://ar5iv.org/abs/2006.01419v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efficient Exploration (2020)</b> &nbsp; - &nbsp; Han, Seungyul and Sung, Youngchul &nbsp; - &nbsp; 2006.01419v2 &nbsp; - &nbsp;
{diversity}
{entropy}
{explor}
{reinforcement}
{replay}
</small></font></div></a></td></tr>
<tr id=" 85 " class="entry"><td> <a href="http://ar5iv.org/abs/1907.08225v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery (2019)</b> &nbsp; - &nbsp; Hartikainen, Kristian and Geng, Xinyang and Haarnoja, Tuomas and Levine, Sergey &nbsp; - &nbsp; 1907.08225v4 &nbsp; - &nbsp;
{gradient}
{reinforcement}
{robot}
{simulation}
{skill}
{unsupervised}
</small></font></div></a></td></tr>
<tr id=" 86 " class="entry"><td> <a href="http://ar5iv.org/abs/1812.02648v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Deep Reinforcement Learning and the Deadly Triad (2018)</b> &nbsp; - &nbsp; Hasselt, Hado van and Doron, Yotam and Strub, Florian and Hessel, Matteo and Sonnerat, Nicolas and Modayil, Joseph &nbsp; - &nbsp; 1812.02648v1 &nbsp; - &nbsp;
{deep}
{model}
{network}
{off-policy}
{reinforcement}
{replay}
{theory}
</small></font></div></a></td></tr>
<tr id=" 87 " class="entry"><td> <a href="http://ar5iv.org/abs/2002.02089v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Soft Hindsight Experience Replay (2020)</b> &nbsp; - &nbsp; He, Qiwei and Zhuang, Liansheng and Li, Houqiang &nbsp; - &nbsp; 2002.02089v1 &nbsp; - &nbsp;
{control}
{deep}
{entropy}
{model}
{reinforcement}
{replay}
{robot}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 88 " class="entry"><td> <a href="http://ar5iv.org/abs/1707.02286v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Emergence of Locomotion Behaviours in Rich Environments (2017)</b> &nbsp; - &nbsp; Heess, Nicolas and TB, Dhruva and Sriram, Srinivasan and Lemmon, Jay and Merel, Josh and Wayne, Greg and Tassa, Yuval and Erez, Tom and Wang, Ziyu and Eslami, S. M. Ali and Riedmiller, Martin and Silver, David &nbsp; - &nbsp; 1707.02286v2 &nbsp; - &nbsp;
{explor}
{gradient}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 89 " class="entry"><td> <a href="http://ar5iv.org/abs/1607.05077v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint Replay (2016)</b> &nbsp; - &nbsp; Hosu, Ionel-Alexandru and Rebedea, Traian &nbsp; - &nbsp; 1607.05077v1 &nbsp; - &nbsp;
{control}
{deep}
{explor}
{model}
{network}
{reinforcement}
{replay}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 90 " class="entry"><td> <a href="http://ar5iv.org/abs/1802.04821v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Evolved Policy Gradients (2018)</b> &nbsp; - &nbsp; Houthooft, Rein and Chen, Richard Y. and Isola, Phillip and Stadie, Bradly C. and Wolski, Filip and Ho, Jonathan and Abbeel, Pieter &nbsp; - &nbsp; 1802.04821v2 &nbsp; - &nbsp;
{gradient}
{meta}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 91 " class="entry"><td> <a href="http://ar5iv.org/abs/2010.06491v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Broadly-Exploring, Local-Policy Trees for Long-Horizon Task Planning (2020)</b> &nbsp; - &nbsp; Ichter, Brian and Sermanet, Pierre and Lynch, Corey &nbsp; - &nbsp; 2010.06491v1 &nbsp; - &nbsp;
{explor}
{goal-conditioned}
{model}
{motion planning}
</small></font></div></a></td></tr>
<tr id=" 92 " class="entry"><td> <a href="http://ar5iv.org/abs/1811.02553v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> A Closer Look at Deep Policy Gradients (2018)</b> &nbsp; - &nbsp; Ilyas, Andrew and Engstrom, Logan and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Madry, Aleksander &nbsp; - &nbsp; 1811.02553v4 &nbsp; - &nbsp;
{deep}
{gradient}
{optim}
</small></font></div></a></td></tr>
<tr id=" 93 " class="entry"><td> <a href="http://ar5iv.org/abs/1806.04613v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Improving Regression Performance with Distributional Losses (2018)</b> &nbsp; - &nbsp; Imani, Ehsan and White, Martha &nbsp; - &nbsp; 1806.04613v1 &nbsp; - &nbsp;
{distillation}
{gradient}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 94 " class="entry"><td> <a href="http://ar5iv.org/abs/1502.03167v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (2015)</b> &nbsp; - &nbsp; Ioffe, Sergey and Szegedy, Christian &nbsp; - &nbsp; 1502.03167v3 &nbsp; - &nbsp;
{deep}
{model}
{network}
</small></font></div></a></td></tr>
<tr id=" 95 " class="entry"><td> <a href="http://ar5iv.org/abs/1711.09846v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Population Based Training of Neural Networks (2017)</b> &nbsp; - &nbsp; Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray &nbsp; - &nbsp; 1711.09846v2 &nbsp; - &nbsp;
{deep}
{model}
{network}
{optim}
{population}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 96 " class="entry"><td> <a href="http://ar5iv.org/abs/1611.05397v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Reinforcement Learning with Unsupervised Auxiliary Tasks (2016)</b> &nbsp; - &nbsp; Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z. and Silver, David and Kavukcuoglu, Koray &nbsp; - &nbsp; 1611.05397v1 &nbsp; - &nbsp;
{deep}
{reinforcement}
{unsupervised}
</small></font></div></a></td></tr>
<tr id=" 97 " class="entry"><td> <a href="http://ar5iv.org/abs/1902.10186v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Attention is not Explanation (2019)</b> &nbsp; - &nbsp; Jain, Sarthak and Wallace, Byron C. &nbsp; - &nbsp; 1902.10186v3 &nbsp; - &nbsp;
{gradient}
{model}
</small></font></div></a></td></tr>
<tr id=" 98 " class="entry"><td> <a href="http://ar5iv.org/abs/1609.03759v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> 3D Simulation for Robot Arm Control with Deep Q-Learning (2016)</b> &nbsp; - &nbsp; James, Stephen and Johns, Edward &nbsp; - &nbsp; 1609.03759v2 &nbsp; - &nbsp;
{control}
{deep}
{network}
{reinforcement}
{robot}
{simulation}
{transfer}
</small></font></div></a></td></tr>
<tr id=" 99 " class="entry"><td> <a href="http://ar5iv.org/abs/1906.08253v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> When to Trust Your Model: Model-Based Policy Optimization (2019)</b> &nbsp; - &nbsp; Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey &nbsp; - &nbsp; 1906.08253v3 &nbsp; - &nbsp;
{model}
{off-policy}
{on-policy}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 100 " class="entry"><td> <a href="http://ar5iv.org/abs/1306.3532v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Fast Marching Tree: a Fast Marching Sampling-Based Method for Optimal Motion Planning in Many Dimensions (2013)</b> &nbsp; - &nbsp; Janson, Lucas and Schmerling, Edward and Clark, Ashley and Pavone, Marco &nbsp; - &nbsp; 1306.3532v4 &nbsp; - &nbsp;
{motion planning}
{optim}
</small></font></div></a></td></tr>
<tr id=" 101 " class="entry"><td> <a href="http://ar5iv.org/abs/2001.02907v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Population-Guided Parallel Policy Search for Reinforcement Learning (2020)</b> &nbsp; - &nbsp; Jung, Whiyoung and Park, Giseung and Sung, Youngchul &nbsp; - &nbsp; 2001.02907v1 &nbsp; - &nbsp;
{deep}
{gradient}
{off-policy}
{population}
{reinforcement}
{replay}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 102 " class="entry"><td> <a href="http://ar5iv.org/abs/1906.02691v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> An Introduction to Variational Autoencoders (2019)</b> &nbsp; - &nbsp; Kingma, Diederik P. and Welling, Max &nbsp; - &nbsp; 1906.02691v3 &nbsp; - &nbsp;
{deep}
{model}
</small></font></div></a></td></tr>
<tr id=" 103 " class="entry"><td> <a href="http://ar5iv.org/abs/1906.12189v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Learning-based Model Predictive Control for Safe Exploration and Reinforcement Learning (2019)</b> &nbsp; - &nbsp; Koller, Torsten and Berkenkamp, Felix and Turchetta, Matteo and Boedecker, Joschka and Krause, Andreas &nbsp; - &nbsp; 1906.12189v1 &nbsp; - &nbsp;
{control}
{explor}
{model}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 104 " class="entry"><td> <a href="http://ar5iv.org/abs/2004.13649v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels (2020)</b> &nbsp; - &nbsp; Kostrikov, Ilya and Yarats, Denis and Fergus, Rob &nbsp; - &nbsp; 2004.13649v4 &nbsp; - &nbsp;
{control}
{deep}
{model}
{network}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 105 " class="entry"><td> <a href="http://ar5iv.org/abs/1311.1839v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> An Efficiently Solvable Quadratic Program for Stabilizing Dynamic Locomotion (2013)</b> &nbsp; - &nbsp; Kuindersma, Scott and Permenter, Frank and Tedrake, Russ &nbsp; - &nbsp; 1311.1839v2 &nbsp; - &nbsp;
{control}
{humanoid}
{model}
{optim}
{robot}
</small></font></div></a></td></tr>
<tr id=" 106 " class="entry"><td> <a href="http://ar5iv.org/abs/1606.02396v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Deep Successor Reinforcement Learning (2016)</b> &nbsp; - &nbsp; Kulkarni, Tejas D. and Saeedi, Ardavan and Gautam, Simanta and Gershman, Samuel J. &nbsp; - &nbsp; 1606.02396v1 &nbsp; - &nbsp;
{deep}
{model}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 107 " class="entry"><td> <a href="http://ar5iv.org/abs/1709.07932v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Expanding Motor Skills through Relay Neural Networks (2017)</b> &nbsp; - &nbsp; Kumar, Visak C. V. and Ha, Sehoon and Liu, C. Karen &nbsp; - &nbsp; 1709.07932v3 &nbsp; - &nbsp;
{control}
{deep}
{network}
{reinforcement}
{robot}
{skill}
</small></font></div></a></td></tr>
<tr id=" 108 " class="entry"><td> <a href="http://ar5iv.org/abs/2005.04269v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics (2020)</b> &nbsp; - &nbsp; Kuznetsov, Arsenii and Shvechikov, Pavel and Grishin, Alexander and Vetrov, Dmitry &nbsp; - &nbsp; 2005.04269v1 &nbsp; - &nbsp;
{control}
{humanoid}
{off-policy}
</small></font></div></a></td></tr>
<tr id=" 109 " class="entry"><td> <a href="http://ar5iv.org/abs/1610.09038v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Professor Forcing: A New Algorithm for Training Recurrent Networks (2016)</b> &nbsp; - &nbsp; Lamb, Alex and Goyal, Anirudh and Zhang, Ying and Zhang, Saizheng and Courville, Aaron and Bengio, Yoshua &nbsp; - &nbsp; 1610.09038v1 &nbsp; - &nbsp;
{model}
{network}
</small></font></div></a></td></tr>
<tr id=" 110 " class="entry"><td> <a href="http://ar5iv.org/abs/2010.05113v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Contrastive Representation Learning: A Framework and Review (2020)</b> &nbsp; - &nbsp; Le-Khac, Phuc H. and Healy, Graham and Smeaton, Alan F. &nbsp; - &nbsp; 2010.05113v2 &nbsp; - &nbsp;
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 111 " class="entry"><td> <a href="http://ar5iv.org/abs/1802.04181v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> State Representation Learning for Control: An Overview (2018)</b> &nbsp; - &nbsp; Lesort, Timoth&#233;e and D&#237;az-Rodr&#237;guez, Natalia and Goudou, Jean-Fran&#231;ois and Filliat, David &nbsp; - &nbsp; 1802.04181v2 &nbsp; - &nbsp;
{control}
{reinforcement}
{robot}
</small></font></div></a></td></tr>
<tr id=" 112 " class="entry"><td> <a href="http://ar5iv.org/abs/1504.00702v5"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> End-to-End Training of Deep Visuomotor Policies (2015)</b> &nbsp; - &nbsp; Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter &nbsp; - &nbsp; 1504.00702v5 &nbsp; - &nbsp;
{control}
{deep}
{network}
{reinforcement}
{robot}
</small></font></div></a></td></tr>
<tr id=" 113 " class="entry"><td> <a href="http://ar5iv.org/abs/1603.02199v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection (2016)</b> &nbsp; - &nbsp; Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Quillen, Deirdre &nbsp; - &nbsp; 1603.02199v4 &nbsp; - &nbsp;
{control}
{network}
{robot}
</small></font></div></a></td></tr>
<tr id=" 114 " class="entry"><td> <a href="http://ar5iv.org/abs/1712.00948v5"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Learning Multi-Level Hierarchies with Hindsight (2017)</b> &nbsp; - &nbsp; Levy, Andrew and Konidaris, George and Platt, Robert and Saenko, Kate &nbsp; - &nbsp; 1712.00948v5 &nbsp; - &nbsp;
{hierarchical}
{optim}
{reinforcement}
{robot}
</small></font></div></a></td></tr>
<tr id=" 115 " class="entry"><td> <a href="http://ar5iv.org/abs/1712.09913v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Visualizing the Loss Landscape of Neural Nets (2017)</b> &nbsp; - &nbsp; Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom &nbsp; - &nbsp; 1712.09913v3 &nbsp; - &nbsp;
{explor}
{network}
{optim}
</small></font></div></a></td></tr>
<tr id=" 116 " class="entry"><td> <a href="http://ar5iv.org/abs/1912.11032v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Towards Practical Multi-Object Manipulation using Relational Reinforcement Learning (2019)</b> &nbsp; - &nbsp; Li, Richard and Jabri, Allan and Darrell, Trevor and Agrawal, Pulkit &nbsp; - &nbsp; 1912.11032v1 &nbsp; - &nbsp;
{imitation}
{reinforcement}
{robot}
{sparse}
{transfer}
</small></font></div></a></td></tr>
<tr id=" 117 " class="entry"><td> <a href="http://ar5iv.org/abs/1509.02971v6"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Continuous control with deep reinforcement learning (2015)</b> &nbsp; - &nbsp; Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan &nbsp; - &nbsp; 1509.02971v6 &nbsp; - &nbsp;
{deep}
{gradient}
{model}
{network}
</small></font></div></a></td></tr>
<tr id=" 118 " class="entry"><td> <a href="http://ar5iv.org/abs/2109.08522v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Dynamics-Aware Quality-Diversity for Efficient Learning of Skill Repertoires (2021)</b> &nbsp; - &nbsp; Lim, Bryan and Grillotti, Luca and Bernasconi, Lorenzo and Cully, Antoine &nbsp; - &nbsp; 2109.08522v1 &nbsp; - &nbsp;
{deep}
{diversity}
{explor}
{model}
{robot}
{skill}
</small></font></div></a></td></tr>
<tr id=" 119 " class="entry"><td> <a href="http://ar5iv.org/abs/2105.12196v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> From Motor Control to Team Play in Simulated Humanoid Football (2021)</b> &nbsp; - &nbsp; Liu, Siqi and Lever, Guy and Wang, Zhe and Merel, Josh and Eslami, S. M. Ali and Hennes, Daniel and Czarnecki, Wojciech M. and Tassa, Yuval and Omidshafiei, Shayegan and Abdolmaleki, Abbas and Siegel, Noah Y. and Hasenclever, Leonard and Marris, Luke and Tunyasuvunakool, Saran and Song, H. Francis and Wulfmeier, Markus and Muller, Paul and Haarnoja, Tuomas and Tracey, Brendan D. and Tuyls, Karl and Graepel, Thore and Heess, Nicolas &nbsp; - &nbsp; 2105.12196v1 &nbsp; - &nbsp;
{control}
{humanoid}
{imitation}
{population}
{reinforcement}
{skill}
{transfer}
</small></font></div></a></td></tr>
<tr id=" 120 " class="entry"><td> <a href="http://ar5iv.org/abs/1811.12359v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations (2018)</b> &nbsp; - &nbsp; Locatello, Francesco and Bauer, Stefan and Lucic, Mario and R&#228;tsch, Gunnar and Gelly, Sylvain and Sch&#246;lkopf, Bernhard and Bachem, Olivier &nbsp; - &nbsp; 1811.12359v4 &nbsp; - &nbsp;
{model}
{unsupervised}
</small></font></div></a></td></tr>
<tr id=" 121 " class="entry"><td> <a href="http://ar5iv.org/abs/1811.01848v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control (2018)</b> &nbsp; - &nbsp; Lowrey, Kendall and Rajeswaran, Aravind and Kakade, Sham and Todorov, Emanuel and Mordatch, Igor &nbsp; - &nbsp; 1811.01848v3 &nbsp; - &nbsp;
{control}
{explor}
{humanoid}
{model}
{optim}
</small></font></div></a></td></tr>
<tr id=" 122 " class="entry"><td> <a href="http://ar5iv.org/abs/1705.07874v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> A Unified Approach to Interpreting Model Predictions (2017)</b> &nbsp; - &nbsp; Lundberg, Scott and Lee, Su-In &nbsp; - &nbsp; 1705.07874v2 &nbsp; - &nbsp;
{deep}
{model}
</small></font></div></a></td></tr>
<tr id=" 123 " class="entry"><td> <a href="http://ar5iv.org/abs/1903.01973v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Learning Latent Plans from Play (2019)</b> &nbsp; - &nbsp; Lynch, Corey and Khansari, Mohi and Xiao, Ted and Kumar, Vikash and Tompson, Jonathan and Levine, Sergey and Sermanet, Pierre &nbsp; - &nbsp; 1903.01973v2 &nbsp; - &nbsp;
{control}
{model}
{robot}
{skill}
</small></font></div></a></td></tr>
<tr id=" 124 " class="entry"><td> <a href="http://ar5iv.org/abs/1703.09312v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics (2017)</b> &nbsp; - &nbsp; Mahler, Jeffrey and Liang, Jacky and Niyaz, Sherdil and Laskey, Michael and Doan, Richard and Liu, Xinyu and Ojea, Juan Aparicio and Goldberg, Ken &nbsp; - &nbsp; 1703.09312v3 &nbsp; - &nbsp;
{deep}
{explor}
{model}
{network}
{robot}
</small></font></div></a></td></tr>
<tr id=" 125 " class="entry"><td> <a href="http://ar5iv.org/abs/0911.4625v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Hamilton-Jacobi formulation for reach-avoid differential games (2009)</b> &nbsp; - &nbsp; Margellos, Kostas and Lygeros, John &nbsp; - &nbsp; 0911.4625v1 &nbsp; - &nbsp;
{control}
{hybrid}
{optim}
</small></font></div></a></td></tr>
<tr id=" 126 " class="entry"><td> <a href="http://ar5iv.org/abs/1412.1193v11"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> New insights and perspectives on the natural gradient method (2014)</b> &nbsp; - &nbsp; Martens, James &nbsp; - &nbsp; 1412.1193v11 &nbsp; - &nbsp;
{gradient}
{optim}
</small></font></div></a></td></tr>
<tr id=" 127 " class="entry"><td> <a href="http://ar5iv.org/abs/1511.02540v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Speed learning on the fly (2015)</b> &nbsp; - &nbsp; Mass&#233;, Pierre-Yves and Ollivier, Yann &nbsp; - &nbsp; 1511.02540v1 &nbsp; - &nbsp;
{gradient}
</small></font></div></a></td></tr>
<tr id=" 128 " class="entry"><td> <a href="http://ar5iv.org/abs/1907.01298v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Modified Actor-Critics (2019)</b> &nbsp; - &nbsp; Merdivan, Erinc and Hanke, Sten and Geist, Matthieu &nbsp; - &nbsp; 1907.01298v2 &nbsp; - &nbsp;
{deep}
{off-policy}
{on-policy}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 129 " class="entry"><td> <a href="http://ar5iv.org/abs/1811.11711v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Neural probabilistic motor primitives for humanoid control (2018)</b> &nbsp; - &nbsp; Merel, Josh and Hasenclever, Leonard and Galashov, Alexandre and Ahuja, Arun and Pham, Vu and Wayne, Greg and Teh, Yee Whye and Heess, Nicolas &nbsp; - &nbsp; 1811.11711v2 &nbsp; - &nbsp;
{control}
{humanoid}
{imitation}
{model}
</small></font></div></a></td></tr>
<tr id=" 130 " class="entry"><td> <a href="http://ar5iv.org/abs/1705.05035v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Discrete Sequential Prediction of Continuous Actions for Deep RL (2017)</b> &nbsp; - &nbsp; Metz, Luke and Ibarz, Julian and Jaitly, Navdeep and Davidson, James &nbsp; - &nbsp; 1705.05035v3 &nbsp; - &nbsp;
{control}
{model}
{network}
{off-policy}
{optim}
</small></font></div></a></td></tr>
<tr id=" 131 " class="entry"><td> <a href="http://ar5iv.org/abs/2005.08290v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Longitudinal high-throughput TCR repertoire profiling reveals the dynamics of T cell memory formation after mild COVID-19 infection (2020)</b> &nbsp; - &nbsp; Minervina, Anastasia A. and Komech, Ekaterina A. and Titov, Aleksei and Koraichi, Meriem Bensouda and Rosati, Elisa and Mamedov, Ilgar Z. and Franke, Andre and Efimov, Grigory A. and Chudakov, Dmitriy M. and Mora, Thierry and Walczak, Aleksandra M. and Lebedev, Yuri B. and Pogorelyy, Mikhail V. &nbsp; - &nbsp; 2005.08290v3 &nbsp; - &nbsp;
{diversity}
</small></font></div></a></td></tr>
<tr id=" 132 " class="entry"><td> <a href="http://ar5iv.org/abs/1201.4497v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> A geometrical introduction to screw theory (2012)</b> &nbsp; - &nbsp; Minguzzi, E. &nbsp; - &nbsp; 1201.4497v2 &nbsp; - &nbsp;
{theory}
</small></font></div></a></td></tr>
<tr id=" 133 " class="entry"><td> <a href="http://ar5iv.org/abs/cs/0108021v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Computational Geometry Column 42 (2001)</b> &nbsp; - &nbsp; Mitchell, Joseph S. B. and O&#8217;Rourke, Joseph &nbsp; - &nbsp; cs/0108021v1 &nbsp; - &nbsp;
</small></font></div></a></td></tr>
<tr id=" 134 " class="entry"><td> <a href="http://ar5iv.org/abs/1602.01783v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Asynchronous Methods for Deep Reinforcement Learning (2016)</b> &nbsp; - &nbsp; Mnih, Volodymyr and Badia, Adri&#224; Puigdom&#232;nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray &nbsp; - &nbsp; 1602.01783v2 &nbsp; - &nbsp;
{control}
{deep}
{gradient}
{network}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 135 " class="entry"><td> <a href="http://ar5iv.org/abs/2001.06940v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Reinforcement Learning with Probabilistically Complete Exploration (2020)</b> &nbsp; - &nbsp; Morere, Philippe and Francis, Gilad and Blau, Tom and Ramos, Fabio &nbsp; - &nbsp; 2001.06940v1 &nbsp; - &nbsp;
{explor}
{intrinsic}
{reinforcement}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 136 " class="entry"><td> <a href="http://ar5iv.org/abs/1906.07794v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Convolutional neural network models for cancer type prediction based on gene expression (2019)</b> &nbsp; - &nbsp; Mostavi, Milad and Chiu, Yu-Chiao and Huang, Yufei and Chen, Yidong &nbsp; - &nbsp; 1906.07794v1 &nbsp; - &nbsp;
{hybrid}
{model}
{network}
</small></font></div></a></td></tr>
<tr id=" 137 " class="entry"><td> <a href="http://ar5iv.org/abs/1504.04909v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Illuminating search spaces by mapping elites (2015)</b> &nbsp; - &nbsp; Mouret, Jean-Baptiste and Clune, Jeff &nbsp; - &nbsp; 1504.04909v1 &nbsp; - &nbsp;
{diversity}
{explor}
{model}
{network}
{robot}
</small></font></div></a></td></tr>
<tr id=" 138 " class="entry"><td> <a href="http://ar5iv.org/abs/1810.01257v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Near-Optimal Representation Learning for Hierarchical Reinforcement Learning (2018)</b> &nbsp; - &nbsp; Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey &nbsp; - &nbsp; 1810.01257v2 &nbsp; - &nbsp;
{control}
{goal-conditioned}
{hierarchical}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 139 " class="entry"><td> <a href="http://ar5iv.org/abs/1803.02348v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Smoothed Action Value Functions for Learning Gaussian Policies (2018)</b> &nbsp; - &nbsp; Nachum, Ofir and Norouzi, Mohammad and Tucker, George and Schuurmans, Dale &nbsp; - &nbsp; 1803.02348v3 &nbsp; - &nbsp;
{control}
{gradient}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 140 " class="entry"><td> <a href="http://ar5iv.org/abs/1707.01891v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Trust-PCL: An Off-Policy Trust Region Method for Continuous Control (2017)</b> &nbsp; - &nbsp; Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale &nbsp; - &nbsp; 1707.01891v3 &nbsp; - &nbsp;
{control}
{entropy}
{off-policy}
{on-policy}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 141 " class="entry"><td> <a href="http://ar5iv.org/abs/1909.10618v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning? (2019)</b> &nbsp; - &nbsp; Nachum, Ofir and Tang, Haoran and Lu, Xingyu and Gu, Shixiang and Lee, Honglak and Levine, Sergey &nbsp; - &nbsp; 1909.10618v2 &nbsp; - &nbsp;
{explor}
{hierarchical}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 142 " class="entry"><td> <a href="http://ar5iv.org/abs/1709.10089v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Overcoming Exploration in Reinforcement Learning with Demonstrations (2017)</b> &nbsp; - &nbsp; Nair, Ashvin and McGrew, Bob and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter &nbsp; - &nbsp; 1709.10089v2 &nbsp; - &nbsp;
{control}
{deep}
{explor}
{gradient}
{optim}
{reinforcement}
{replay}
{robot}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 143 " class="entry"><td> <a href="http://ar5iv.org/abs/2001.00449v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Continuous-Discrete Reinforcement Learning for Hybrid Control in Robotics (2020)</b> &nbsp; - &nbsp; Neunert, Michael and Abdolmaleki, Abbas and Wulfmeier, Markus and Lampe, Thomas and Springenberg, Jost Tobias and Hafner, Roland and Romano, Francesco and Buchli, Jonas and Heess, Nicolas and Riedmiller, Martin &nbsp; - &nbsp; 2001.00449v1 &nbsp; - &nbsp;
{control}
{explor}
{hybrid}
{meta}
{optim}
{reinforcement}
{robot}
{simulation}
</small></font></div></a></td></tr>
<tr id=" 144 " class="entry"><td> <a href="http://ar5iv.org/abs/1412.1897v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images (2014)</b> &nbsp; - &nbsp; Nguyen, Anh and Yosinski, Jason and Clune, Jeff &nbsp; - &nbsp; 1412.1897v4 &nbsp; - &nbsp;
{deep}
{gradient}
{network}
</small></font></div></a></td></tr>
<tr id=" 145 " class="entry"><td> <a href="http://ar5iv.org/abs/1801.06159v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> When Does Stochastic Gradient Algorithm Work Well? (2018)</b> &nbsp; - &nbsp; Nguyen, Lam M. and Nguyen, Nam H. and Phan, Dzung T. and Kalagnanam, Jayant R. and Scheinberg, Katya &nbsp; - &nbsp; 1801.06159v2 &nbsp; - &nbsp;
{deep}
{gradient}
{model}
{network}
{optim}
</small></font></div></a></td></tr>
<tr id=" 146 " class="entry"><td> <a href="http://ar5iv.org/abs/1803.02999v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> On First-Order Meta-Learning Algorithms (2018)</b> &nbsp; - &nbsp; Nichol, Alex and Achiam, Joshua and Schulman, John &nbsp; - &nbsp; 1803.02999v3 &nbsp; - &nbsp;
{few-shot}
{meta}
</small></font></div></a></td></tr>
<tr id=" 147 " class="entry"><td> <a href="http://ar5iv.org/abs/1106.3708v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles (2011)</b> &nbsp; - &nbsp; Ollivier, Yann and Arnold, Ludovic and Auger, Anne and Hansen, Nikolaus &nbsp; - &nbsp; 1106.3708v4 &nbsp; - &nbsp;
{diversity}
{entropy}
{explor}
{gradient}
{intrinsic}
{optim}
{theory}
</small></font></div></a></td></tr>
<tr id=" 148 " class="entry"><td> <a href="http://ar5iv.org/abs/1507.07680v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Training recurrent networks online without backtracking (2015)</b> &nbsp; - &nbsp; Ollivier, Yann and Tallec, Corentin and Charpiat, Guillaume &nbsp; - &nbsp; 1507.07680v2 &nbsp; - &nbsp;
{gradient}
{network}
</small></font></div></a></td></tr>
<tr id=" 149 " class="entry"><td> <a href="http://ar5iv.org/abs/1306.0514v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Riemannian metrics for neural networks II: recurrent networks and learning symbolic data sequences (2013)</b> &nbsp; - &nbsp; Ollivier, Yann &nbsp; - &nbsp; 1306.0514v4 &nbsp; - &nbsp;
{gradient}
{model}
{network}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 150 " class="entry"><td> <a href="http://ar5iv.org/abs/1712.08449v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> True Asymptotic Natural Gradient Optimization (2017)</b> &nbsp; - &nbsp; Ollivier, Yann &nbsp; - &nbsp; 1712.08449v1 &nbsp; - &nbsp;
{gradient}
{model}
{optim}
</small></font></div></a></td></tr>
<tr id=" 151 " class="entry"><td> <a href="http://ar5iv.org/abs/1807.03748v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Representation Learning with Contrastive Predictive Coding (2018)</b> &nbsp; - &nbsp; Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol &nbsp; - &nbsp; 1807.03748v2 &nbsp; - &nbsp;
{model}
{reinforcement}
{unsupervised}
</small></font></div></a></td></tr>
<tr id=" 152 " class="entry"><td> <a href="http://ar5iv.org/abs/1602.04621v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Deep Exploration via Bootstrapped DQN (2016)</b> &nbsp; - &nbsp; Osband, Ian and Blundell, Charles and Pritzel, Alexander and Roy, Benjamin Van &nbsp; - &nbsp; 1602.04621v3 &nbsp; - &nbsp;
{deep}
{explor}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 153 " class="entry"><td> <a href="http://ar5iv.org/abs/2003.01629v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Can Increasing Input Dimensionality Improve Deep Reinforcement Learning? (2020)</b> &nbsp; - &nbsp; Ota, Kei and Oiki, Tomoaki and Jha, Devesh K. and Mariyama, Toshisada and Nikovski, Daniel &nbsp; - &nbsp; 2003.01629v2 &nbsp; - &nbsp;
{deep}
{model}
{network}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 154 " class="entry"><td> <a href="http://ar5iv.org/abs/1909.01387v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Making Efficient Use of Demonstrations to Solve Hard Exploration Problems (2019)</b> &nbsp; - &nbsp; Paine, Tom Le and Gulcehre, Caglar and Shahriari, Bobak and Denil, Misha and Hoffman, Matt and Soyer, Hubert and Tanburn, Richard and Kapturowski, Steven and Rabinowitz, Neil and Williams, Duncan and Barth-Maron, Gabriel and Wang, Ziyu and Freitas, Nando de and Team, Worlds &nbsp; - &nbsp; 1909.01387v1 &nbsp; - &nbsp;
{explor}
</small></font></div></a></td></tr>
<tr id=" 155 " class="entry"><td> <a href="http://ar5iv.org/abs/2002.00632v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Effective Diversity in Population Based Reinforcement Learning (2020)</b> &nbsp; - &nbsp; Parker-Holder, Jack and Pacchiano, Aldo and Choromanski, Krzysztof and Roberts, Stephen &nbsp; - &nbsp; 2002.00632v3 &nbsp; - &nbsp;
{diversity}
{explor}
{gradient}
{optim}
{population}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 156 " class="entry"><td> <a href="http://ar5iv.org/abs/1301.3584v7"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Revisiting Natural Gradient for Deep Networks (2013)</b> &nbsp; - &nbsp; Pascanu, Razvan and Bengio, Yoshua &nbsp; - &nbsp; 1301.3584v7 &nbsp; - &nbsp;
{deep}
{gradient}
{model}
</small></font></div></a></td></tr>
<tr id=" 157 " class="entry"><td> <a href="http://ar5iv.org/abs/1906.07987v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Adaptive Temporal-Difference Learning for Policy Evaluation with Per-State Uncertainty Estimates (2019)</b> &nbsp; - &nbsp; Penedones, Hugo and Riquelme, Carlos and Vincent, Damien and Maennel, Hartmut and Mann, Timothy and Barreto, Andre and Gelly, Sylvain and Neu, Gergely &nbsp; - &nbsp; 1906.07987v1 &nbsp; - &nbsp;
{on-policy}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 158 " class="entry"><td> <a href="http://ar5iv.org/abs/1910.00177v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning (2019)</b> &nbsp; - &nbsp; Peng, Xue Bin and Kumar, Aviral and Zhang, Grace and Levine, Sergey &nbsp; - &nbsp; 1910.00177v3 &nbsp; - &nbsp;
{control}
{off-policy}
{reinforcement}
{replay}
</small></font></div></a></td></tr>
<tr id=" 159 " class="entry"><td> <a href="http://ar5iv.org/abs/2006.07781v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Non-local Policy Optimization via Diversity-regularized Collaborative Exploration (2020)</b> &nbsp; - &nbsp; Peng, Zhenghao and Sun, Hao and Zhou, Bolei &nbsp; - &nbsp; 2006.07781v1 &nbsp; - &nbsp;
{diversity}
{explor}
{off-policy}
{on-policy}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 160 " class="entry"><td> <a href="http://ar5iv.org/abs/2007.02832v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Maximum Entropy Gain Exploration for Long Horizon Multi-goal Reinforcement Learning (2020)</b> &nbsp; - &nbsp; Pitis, Silviu and Chan, Harris and Zhao, Stephen and Stadie, Bradly and Ba, Jimmy &nbsp; - &nbsp; 2007.02832v1 &nbsp; - &nbsp;
{entropy}
{explor}
{intrinsic}
{optim}
{reinforcement}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 161 " class="entry"><td> <a href="http://ar5iv.org/abs/1809.02721v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Learning to Solve NP-Complete Problems - A Graph Neural Network for Decision TSP (2018)</b> &nbsp; - &nbsp; Prates, Marcelo O. R. and Avelar, Pedro H. C. and Lemos, Henrique and Lamb, Luis and Vardi, Moshe &nbsp; - &nbsp; 1809.02721v3 &nbsp; - &nbsp;
{model}
{network}
{optim}
</small></font></div></a></td></tr>
<tr id=" 162 " class="entry"><td> <a href="http://ar5iv.org/abs/2002.08484v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Estimating Training Data Influence by Tracing Gradient Descent (2020)</b> &nbsp; - &nbsp; Pruthi, Garima and Liu, Frederick and Sundararajan, Mukund and Kale, Satyen &nbsp; - &nbsp; 2002.08484v3 &nbsp; - &nbsp;
{deep}
{gradient}
{model}
{network}
</small></font></div></a></td></tr>
<tr id=" 163 " class="entry"><td> <a href="http://ar5iv.org/abs/1909.12892v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Automated curricula through setter-solver interactions (2019)</b> &nbsp; - &nbsp; Racaniere, Sebastien and Lampinen, Andrew K. and Santoro, Adam and Reichert, David P. and Firoiu, Vlad and Lillicrap, Timothy P. &nbsp; - &nbsp; 1909.12892v2 &nbsp; - &nbsp;
{explor}
{goal-conditioned}
{reinforcement}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 164 " class="entry"><td> <a href="http://ar5iv.org/abs/2005.05719v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Smooth Exploration for Robotic Reinforcement Learning (2020)</b> &nbsp; - &nbsp; Raffin, Antonin and Kober, Jens and Stulp, Freek &nbsp; - &nbsp; 2005.05719v2 &nbsp; - &nbsp;
{control}
{deep}
{explor}
{reinforcement}
{robot}
{simulation}
{skill}
</small></font></div></a></td></tr>
<tr id=" 165 " class="entry"><td> <a href="http://ar5iv.org/abs/1709.10087v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations (2017)</b> &nbsp; - &nbsp; Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey &nbsp; - &nbsp; 1709.10087v2 &nbsp; - &nbsp;
{control}
{deep}
{model}
{reinforcement}
{robot}
</small></font></div></a></td></tr>
<tr id=" 166 " class="entry"><td> <a href="http://ar5iv.org/abs/1703.02660v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Towards Generalization and Simplicity in Continuous Control (2017)</b> &nbsp; - &nbsp; Rajeswaran, Aravind and Lowrey, Kendall and Todorov, Emanuel and Kakade, Sham &nbsp; - &nbsp; 1703.02660v2 &nbsp; - &nbsp;
{control}
{network}
</small></font></div></a></td></tr>
<tr id=" 167 " class="entry"><td> <a href="http://ar5iv.org/abs/1903.08254v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables (2019)</b> &nbsp; - &nbsp; Rakelly, Kate and Zhou, Aurick and Quillen, Deirdre and Finn, Chelsea and Levine, Sergey &nbsp; - &nbsp; 1903.08254v1 &nbsp; - &nbsp;
{control}
{deep}
{explor}
{meta}
{off-policy}
{on-policy}
{reinforcement}
{skill}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 168 " class="entry"><td> <a href="http://ar5iv.org/abs/2005.13143v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Euclideanizing Flows: Diffeomorphic Reduction for Learning Stable Dynamical Systems (2020)</b> &nbsp; - &nbsp; Rana, Muhammad Asif and Li, Anqi and Fox, Dieter and Boots, Byron and Ramos, Fabio and Ratliff, Nathan &nbsp; - &nbsp; 2005.13143v2 &nbsp; - &nbsp;
{robot}
</small></font></div></a></td></tr>
<tr id=" 169 " class="entry"><td> <a href="http://ar5iv.org/abs/1904.09237v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> On the Convergence of Adam and Beyond (2019)</b> &nbsp; - &nbsp; Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv &nbsp; - &nbsp; 1904.09237v1 &nbsp; - &nbsp;
{deep}
{gradient}
{network}
{optim}
</small></font></div></a></td></tr>
<tr id=" 170 " class="entry"><td> <a href="http://ar5iv.org/abs/1511.06279v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Neural Programmer-Interpreters (2015)</b> &nbsp; - &nbsp; Reed, Scott and Freitas, Nando de &nbsp; - &nbsp; 1511.06279v4 &nbsp; - &nbsp;
{model}
{network}
</small></font></div></a></td></tr>
<tr id=" 171 " class="entry"><td> <a href="http://ar5iv.org/abs/1807.06919v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Backplay: "Man muss immer umkehren" (2018)</b> &nbsp; - &nbsp; Resnick, Cinjon and Raileanu, Roberta and Kapoor, Sanyam and Peysakhovich, Alexander and Cho, Kyunghyun and Bruna, Joan &nbsp; - &nbsp; 1807.06919v4 &nbsp; - &nbsp;
{explor}
{model}
{reinforcement}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 172 " class="entry"><td> <a href="http://ar5iv.org/abs/1202.6258v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets (2012)</b> &nbsp; - &nbsp; Roux, Nicolas Le and Schmidt, Mark and Bach, Francis &nbsp; - &nbsp; 1202.6258v4 &nbsp; - &nbsp;
{gradient}
{optim}
</small></font></div></a></td></tr>
<tr id=" 173 " class="entry"><td> <a href="http://ar5iv.org/abs/1802.08163v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> An Analysis of Categorical Distributional Reinforcement Learning (2018)</b> &nbsp; - &nbsp; Rowland, Mark and Bellemare, Marc G. and Dabney, Will and Munos, R&#233;mi and Teh, Yee Whye &nbsp; - &nbsp; 1802.08163v1 &nbsp; - &nbsp;
{model}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 174 " class="entry"><td> <a href="http://ar5iv.org/abs/1609.04747v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> An overview of gradient descent optimization algorithms (2016)</b> &nbsp; - &nbsp; Ruder, Sebastian &nbsp; - &nbsp; 1609.04747v2 &nbsp; - &nbsp;
{gradient}
{optim}
</small></font></div></a></td></tr>
<tr id=" 175 " class="entry"><td> <a href="http://ar5iv.org/abs/1412.7009v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Generative Class-conditional Autoencoders (2014)</b> &nbsp; - &nbsp; Rudy, Jan and Taylor, Graham &nbsp; - &nbsp; 1412.7009v3 &nbsp; - &nbsp;
{model}
</small></font></div></a></td></tr>
<tr id=" 176 " class="entry"><td> <a href="http://ar5iv.org/abs/1909.12397v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> CAQL: Continuous Action Q-Learning (2019)</b> &nbsp; - &nbsp; Ryu, Moonkyung and Chow, Yinlam and Anderson, Ross and Tjandraatmadja, Christian and Boutilier, Craig &nbsp; - &nbsp; 1909.12397v3 &nbsp; - &nbsp;
{control}
{deep}
{entropy}
{gradient}
{network}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 177 " class="entry"><td> <a href="http://ar5iv.org/abs/1710.09829v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Dynamic Routing Between Capsules (2017)</b> &nbsp; - &nbsp; Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E. &nbsp; - &nbsp; 1710.09829v2 &nbsp; - &nbsp;
</small></font></div></a></td></tr>
<tr id=" 178 " class="entry"><td> <a href="http://ar5iv.org/abs/1602.07868v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks (2016)</b> &nbsp; - &nbsp; Salimans, Tim and Kingma, Diederik P. &nbsp; - &nbsp; 1602.07868v3 &nbsp; - &nbsp;
{deep}
{gradient}
{model}
{network}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 179 " class="entry"><td> <a href="http://ar5iv.org/abs/1806.01242v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Graph networks as learnable physics engines for inference and control (2018)</b> &nbsp; - &nbsp; Sanchez-Gonzalez, Alvaro and Heess, Nicolas and Springenberg, Jost Tobias and Merel, Josh and Riedmiller, Martin and Hadsell, Raia and Battaglia, Peter &nbsp; - &nbsp; 1806.01242v1 &nbsp; - &nbsp;
{gradient}
{model}
{network}
{optim}
</small></font></div></a></td></tr>
<tr id=" 180 " class="entry"><td> <a href="http://ar5iv.org/abs/1607.00485v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Group Sparse Regularization for Deep Neural Networks (2016)</b> &nbsp; - &nbsp; Scardapane, Simone and Comminiello, Danilo and Hussain, Amir and Uncini, Aurelio &nbsp; - &nbsp; 1607.00485v1 &nbsp; - &nbsp;
{deep}
{network}
{optim}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 181 " class="entry"><td> <a href="http://ar5iv.org/abs/1906.04493v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Generative Adversarial Networks are Special Cases of Artificial Curiosity (1990) and also Closely Related to Predictability Minimization (1991) (2019)</b> &nbsp; - &nbsp; Schmidhuber, Juergen &nbsp; - &nbsp; 1906.04493v3 &nbsp; - &nbsp;
{model}
{network}
{unsupervised}
</small></font></div></a></td></tr>
<tr id=" 182 " class="entry"><td> <a href="http://ar5iv.org/abs/2102.05599v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Improving Model-Based Reinforcement Learning with Internal State Representations through Self-Supervision (2021)</b> &nbsp; - &nbsp; Scholz, Julien and Weber, Cornelius and Hafez, Muhammad Burhan and Wermter, Stefan &nbsp; - &nbsp; 2102.05599v1 &nbsp; - &nbsp;
{model}
{reinforcement}
{unsupervised}
</small></font></div></a></td></tr>
<tr id=" 183 " class="entry"><td> <a href="http://ar5iv.org/abs/1911.08265v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (2019)</b> &nbsp; - &nbsp; Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David &nbsp; - &nbsp; 1911.08265v2 &nbsp; - &nbsp;
{model}
{on-policy}
</small></font></div></a></td></tr>
<tr id=" 184 " class="entry"><td> <a href="http://ar5iv.org/abs/2002.06473v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Universal Value Density Estimation for Imitation Learning and Goal-Conditioned Reinforcement Learning (2020)</b> &nbsp; - &nbsp; Schroecker, Yannick and Isbell, Charles &nbsp; - &nbsp; 2002.06473v1 &nbsp; - &nbsp;
{goal-conditioned}
{imitation}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 185 " class="entry"><td> <a href="http://ar5iv.org/abs/1502.05477v5"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Trust Region Policy Optimization (2015)</b> &nbsp; - &nbsp; Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter &nbsp; - &nbsp; 1502.05477v5 &nbsp; - &nbsp;
{gradient}
{network}
{on-policy}
{optim}
{robot}
{theory}
</small></font></div></a></td></tr>
<tr id=" 186 " class="entry"><td> <a href="http://ar5iv.org/abs/1506.02438v6"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> High-Dimensional Continuous Control Using Generalized Advantage Estimation (2015)</b> &nbsp; - &nbsp; Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter &nbsp; - &nbsp; 1506.02438v6 &nbsp; - &nbsp;
{gradient}
{kinematics}
{model}
{network}
{optim}
{reinforcement}
{robot}
</small></font></div></a></td></tr>
<tr id=" 187 " class="entry"><td> <a href="http://ar5iv.org/abs/1707.06347v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Proximal Policy Optimization Algorithms (2017)</b> &nbsp; - &nbsp; Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg &nbsp; - &nbsp; 1707.06347v2 &nbsp; - &nbsp;
{gradient}
{on-policy}
{optim}
{reinforcement}
{robot}
</small></font></div></a></td></tr>
<tr id=" 188 " class="entry"><td> <a href="http://ar5iv.org/abs/1902.04706v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Simultaneously Learning Vision and Feature-based Control Policies for Real-world Ball-in-a-Cup (2019)</b> &nbsp; - &nbsp; Schwab, Devin and Springenberg, Tobias and Martins, Murilo F. and Lampe, Thomas and Neunert, Michael and Abdolmaleki, Abbas and Hertweck, Tim and Hafner, Roland and Nori, Francesco and Riedmiller, Martin &nbsp; - &nbsp; 1902.04706v2 &nbsp; - &nbsp;
{control}
{imitation}
{multi-task}
{optim}
{reinforcement}
{robot}
{simulation}
{transfer}
</small></font></div></a></td></tr>
<tr id=" 189 " class="entry"><td> <a href="http://ar5iv.org/abs/2102.09430v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> State Entropy Maximization with Random Encoders for Efficient Exploration (2021)</b> &nbsp; - &nbsp; Seo, Younggyo and Chen, Lili and Shin, Jinwoo and Lee, Honglak and Abbeel, Pieter and Lee, Kimin &nbsp; - &nbsp; 2102.09430v4 &nbsp; - &nbsp;
{control}
{deep}
{entropy}
{explor}
{intrinsic}
{model}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 190 " class="entry"><td> <a href="http://ar5iv.org/abs/2010.14641v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Learning to Plan Optimistically: Uncertainty-Guided Deep Exploration via Latent Model Ensembles (2020)</b> &nbsp; - &nbsp; Seyde, Tim and Schwarting, Wilko and Karaman, Sertac and Rus, Daniela &nbsp; - &nbsp; 2010.14641v3 &nbsp; - &nbsp;
{control}
{deep}
{explor}
{model}
{optim}
{robot}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 191 " class="entry"><td> <a href="http://ar5iv.org/abs/2004.12524v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Sequential Interpretability: Methods, Applications, and Future Direction for Understanding Deep Learning Models in the Context of Sequential Data (2020)</b> &nbsp; - &nbsp; Shickel, Benjamin and Rashidi, Parisa &nbsp; - &nbsp; 2004.12524v1 &nbsp; - &nbsp;
{deep}
{explainab}
{imitation}
{model}
</small></font></div></a></td></tr>
<tr id=" 192 " class="entry"><td> <a href="http://ar5iv.org/abs/1812.06298v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Residual Policy Learning (2018)</b> &nbsp; - &nbsp; Silver, Tom and Allen, Kelsey and Tenenbaum, Josh and Kaelbling, Leslie &nbsp; - &nbsp; 1812.06298v2 &nbsp; - &nbsp;
{control}
{deep}
{model}
{reinforcement}
{robot}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 193 " class="entry"><td> <a href="http://ar5iv.org/abs/1901.03909v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Eliminating all bad Local Minima from Loss Landscapes without even adding an Extra Unit (2019)</b> &nbsp; - &nbsp; Sohl-Dickstein, Jascha and Kawaguchi, Kenji &nbsp; - &nbsp; 1901.03909v1 &nbsp; - &nbsp;
{network}
</small></font></div></a></td></tr>
<tr id=" 194 " class="entry"><td> <a href="http://ar5iv.org/abs/1910.01215v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> ES-MAML: Simple Hessian-Free Meta Learning (2019)</b> &nbsp; - &nbsp; Song, Xingyou and Gao, Wenbo and Yang, Yuxiang and Choromanski, Krzysztof and Pacchiano, Aldo and Tang, Yunhao &nbsp; - &nbsp; 1910.01215v4 &nbsp; - &nbsp;
{gradient}
{meta}
{model}
{on-policy}
</small></font></div></a></td></tr>
<tr id=" 195 " class="entry"><td> <a href="http://ar5iv.org/abs/2010.05545v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Local Search for Policy Iteration in Continuous Control (2020)</b> &nbsp; - &nbsp; Springenberg, Jost Tobias and Heess, Nicolas and Mankowitz, Daniel and Merel, Josh and Byravan, Arunkumar and Abdolmaleki, Abbas and Kay, Jackie and Degrave, Jonas and Schrittwieser, Julian and Tassa, Yuval and Buchli, Jonas and Belov, Dan and Riedmiller, Martin &nbsp; - &nbsp; 2010.05545v1 &nbsp; - &nbsp;
{control}
{model}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 196 " class="entry"><td> <a href="http://ar5iv.org/abs/1612.07139v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement to Imitation (2016)</b> &nbsp; - &nbsp; Tai, Lei and Zhang, Jingwei and Liu, Ming and Boedecker, Joschka and Burgard, Wolfram &nbsp; - &nbsp; 1612.07139v4 &nbsp; - &nbsp;
{control}
{deep}
{imitation}
{network}
{reinforcement}
{robot}
{simulation}
{transfer}
</small></font></div></a></td></tr>
<tr id=" 197 " class="entry"><td> <a href="http://ar5iv.org/abs/2009.13579v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Novelty Search in Representational Space for Sample Efficient Exploration (2020)</b> &nbsp; - &nbsp; Tao, Ruo Yu and Fran&#231;ois-Lavet, Vincent and Pineau, Joelle &nbsp; - &nbsp; 2009.13579v2 &nbsp; - &nbsp;
{control}
{explor}
{intrinsic}
{model}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 198 " class="entry"><td> <a href="http://ar5iv.org/abs/1801.00690v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> DeepMind Control Suite (2018)</b> &nbsp; - &nbsp; Tassa, Yuval and Doron, Yotam and Muldal, Alistair and Erez, Tom and Li, Yazhe and Casas, Diego de Las and Budden, David and Abdolmaleki, Abbas and Merel, Josh and Lefrancq, Andrew and Lillicrap, Timothy and Riedmiller, Martin &nbsp; - &nbsp; 1801.00690v1 &nbsp; - &nbsp;
{control}
{deep}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 199 " class="entry"><td> <a href="http://ar5iv.org/abs/1711.08946v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Action Branching Architectures for Deep Reinforcement Learning (2017)</b> &nbsp; - &nbsp; Tavakoli, Arash and Pardo, Fabio and Kormushev, Petar &nbsp; - &nbsp; 1711.08946v2 &nbsp; - &nbsp;
{control}
{deep}
{gradient}
{network}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 200 " class="entry"><td> <a href="http://ar5iv.org/abs/2107.12808v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Open-Ended Learning Leads to Generally Capable Agents (2021)</b> &nbsp; - &nbsp; Team, Open Ended Learning and Stooke, Adam and Mahajan, Anuj and Barros, Catarina and Deck, Charlie and Bauer, Jakob and Sygnowski, Jakub and Trebacz, Maja and Jaderberg, Max and Mathieu, Michael and McAleese, Nat and Bradley-Schmieg, Nathalie and Wong, Nathaniel and Porcel, Nicolas and Raileanu, Roberta and Hughes-Fitt, Steph and Dalibard, Valentin and Czarnecki, Wojciech Marian &nbsp; - &nbsp; 2107.12808v2 &nbsp; - &nbsp;
{open-ended}
{transfer}
</small></font></div></a></td></tr>
<tr id=" 201 " class="entry"><td> <a href="http://ar5iv.org/abs/2010.14274v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Behavior Priors for Efficient Reinforcement Learning (2020)</b> &nbsp; - &nbsp; Tirumala, Dhruva and Galashov, Alexandre and Noh, Hyeonwoo and Hasenclever, Leonard and Pascanu, Razvan and Schwarz, Jonathan and Desjardins, Guillaume and Czarnecki, Wojciech Marian and Ahuja, Arun and Teh, Yee Whye and Heess, Nicolas &nbsp; - &nbsp; 2010.14274v1 &nbsp; - &nbsp;
{control}
{hierarchical}
{model}
{multi-task}
{reinforcement}
{transfer}
</small></font></div></a></td></tr>
<tr id=" 202 " class="entry"><td> <a href="http://ar5iv.org/abs/1010.3013v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Invariant Funnels around Trajectories using Sum-of-Squares Programming (2010)</b> &nbsp; - &nbsp; Tobenkin, Mark M. and Manchester, Ian R. and Tedrake, Russ &nbsp; - &nbsp; 1010.3013v1 &nbsp; - &nbsp;
{model}
</small></font></div></a></td></tr>
<tr id=" 203 " class="entry"><td> <a href="http://ar5iv.org/abs/1911.01417v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards (2019)</b> &nbsp; - &nbsp; Trott, Alexander and Zheng, Stephan and Xiong, Caiming and Socher, Richard &nbsp; - &nbsp; 1911.01417v1 &nbsp; - &nbsp;
{explor}
{intrinsic}
{model}
{optim}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 204 " class="entry"><td> <a href="http://ar5iv.org/abs/1804.03906v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Discovering the Elite Hypervolume by Leveraging Interspecies Correlation (2018)</b> &nbsp; - &nbsp; Vassiliades, Vassilis and Mouret, Jean-Baptiste &nbsp; - &nbsp; 1804.03906v1 &nbsp; - &nbsp;
{diversity}
{robot}
</small></font></div></a></td></tr>
<tr id=" 205 " class="entry"><td> <a href="http://ar5iv.org/abs/1706.03762v5"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Attention Is All You Need (2017)</b> &nbsp; - &nbsp; Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia &nbsp; - &nbsp; 1706.03762v5 &nbsp; - &nbsp;
{model}
{network}
</small></font></div></a></td></tr>
<tr id=" 206 " class="entry"><td> <a href="http://ar5iv.org/abs/1707.08817v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards (2017)</b> &nbsp; - &nbsp; Vecerik, Mel and Hester, Todd and Scholz, Jonathan and Wang, Fumin and Pietquin, Olivier and Piot, Bilal and Heess, Nicolas and Roth&#246;rl, Thomas and Lampe, Thomas and Riedmiller, Martin &nbsp; - &nbsp; 1707.08817v2 &nbsp; - &nbsp;
{control}
{deep}
{explor}
{gradient}
{model}
{reinforcement}
{replay}
{robot}
{sparse}
</small></font></div></a></td></tr>
<tr id=" 207 " class="entry"><td> <a href="http://ar5iv.org/abs/1705.07241v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Diffusion-based neuromodulation can eliminate catastrophic forgetting in simple neural networks (2017)</b> &nbsp; - &nbsp; Velez, Roby and Clune, Jeff &nbsp; - &nbsp; 1705.07241v3 &nbsp; - &nbsp;
{diversity}
{network}
{skill}
{theory}
</small></font></div></a></td></tr>
<tr id=" 208 " class="entry"><td> <a href="http://ar5iv.org/abs/1111.4259v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Krylov Subspace Descent for Deep Learning (2011)</b> &nbsp; - &nbsp; Vinyals, Oriol and Povey, Daniel &nbsp; - &nbsp; 1111.4259v1 &nbsp; - &nbsp;
{deep}
{gradient}
{model}
{network}
{optim}
</small></font></div></a></td></tr>
<tr id=" 209 " class="entry"><td> <a href="http://ar5iv.org/abs/2106.01946v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Convex optimization (2021)</b> &nbsp; - &nbsp; Vorontsova, Evgeniya and Hildebrand, Roland and Gasnikov, Alexander and Stonyakin, Fedor &nbsp; - &nbsp; 2106.01946v4 &nbsp; - &nbsp;
{optim}
</small></font></div></a></td></tr>
<tr id=" 210 " class="entry"><td> <a href="http://ar5iv.org/abs/2006.14135v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Explainable CNN-attention Networks (C-Attention Network) for Automated Detection of Alzheimer&#8217;s Disease (2020)</b> &nbsp; - &nbsp; Wang, Ning and Chen, Mingxuan and Subbalakshmi, K. P. &nbsp; - &nbsp; 2006.14135v2 &nbsp; - &nbsp;
{deep}
{explainab}
{model}
{network}
</small></font></div></a></td></tr>
<tr id=" 211 " class="entry"><td> <a href="http://ar5iv.org/abs/1905.06750v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation (2019)</b> &nbsp; - &nbsp; Wang, Ruohan and Ciliberto, Carlo and Amadori, Pierluigi and Demiris, Yiannis &nbsp; - &nbsp; 1905.06750v2 &nbsp; - &nbsp;
{imitation}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 212 " class="entry"><td> <a href="http://ar5iv.org/abs/1611.01224v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Sample Efficient Actor-Critic with Experience Replay (2016)</b> &nbsp; - &nbsp; Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and Freitas, Nando de &nbsp; - &nbsp; 1611.01224v2 &nbsp; - &nbsp;
{control}
{deep}
{network}
{on-policy}
{optim}
{reinforcement}
{replay}
</small></font></div></a></td></tr>
<tr id=" 213 " class="entry"><td> <a href="http://ar5iv.org/abs/1811.11359v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Unsupervised Control Through Non-Parametric Discriminative Rewards (2018)</b> &nbsp; - &nbsp; Warde-Farley, David and Wiele, Tom Van de and Kulkarni, Tejas and Ionescu, Catalin and Hansen, Steven and Mnih, Volodymyr &nbsp; - &nbsp; 1811.11359v1 &nbsp; - &nbsp;
{control}
{deep}
{goal-conditioned}
{optim}
{reinforcement}
{unsupervised}
</small></font></div></a></td></tr>
<tr id=" 214 " class="entry"><td> <a href="http://ar5iv.org/abs/2001.08116v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Q-Learning in enormous action spaces via amortized approximate maximization (2020)</b> &nbsp; - &nbsp; Wiele, Tom Van de and Warde-Farley, David and Mnih, Andriy and Mnih, Volodymyr &nbsp; - &nbsp; 2001.08116v1 &nbsp; - &nbsp;
{control}
{hybrid}
</small></font></div></a></td></tr>
<tr id=" 215 " class="entry"><td> <a href="http://ar5iv.org/abs/1611.02635v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> A Lyapunov Analysis of Momentum Methods in Optimization (2016)</b> &nbsp; - &nbsp; Wilson, Ashia C. and Recht, Benjamin and Jordan, Michael I. &nbsp; - &nbsp; 1611.02635v4 &nbsp; - &nbsp;
{gradient}
{model}
{optim}
</small></font></div></a></td></tr>
<tr id=" 216 " class="entry"><td> <a href="http://ar5iv.org/abs/1806.05695v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Evolving simple programs for playing Atari games (2018)</b> &nbsp; - &nbsp; Wilson, Dennis G. and Cussat-Blanc, Sylvain and Luga, Herv&#233; and Miller, Julian F. &nbsp; - &nbsp; 1806.05695v1 &nbsp; - &nbsp;
{control}
</small></font></div></a></td></tr>
<tr id=" 217 " class="entry"><td> <a href="http://ar5iv.org/abs/2105.00371v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Discovering Diverse Athletic Jumping Strategies (2021)</b> &nbsp; - &nbsp; Yin, Zhiqi and Yang, Zeshi and Panne, Michiel van de and Yin, KangKang &nbsp; - &nbsp; 2105.00371v1 &nbsp; - &nbsp;
{control}
{deep}
{diversity}
{explor}
{imitation}
{optim}
{reinforcement}
{simulation}
{skill}
</small></font></div></a></td></tr>
<tr id=" 218 " class="entry"><td> <a href="http://ar5iv.org/abs/1411.1792v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> How transferable are features in deep neural networks? (2014)</b> &nbsp; - &nbsp; Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod &nbsp; - &nbsp; 1411.1792v1 &nbsp; - &nbsp;
{deep}
{network}
{optim}
{transfer}
</small></font></div></a></td></tr>
<tr id=" 219 " class="entry"><td> <a href="http://ar5iv.org/abs/1806.01240v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Diffeomorphic Learning (2018)</b> &nbsp; - &nbsp; Younes, Laurent &nbsp; - &nbsp; 1806.01240v3 &nbsp; - &nbsp;
{explor}
</small></font></div></a></td></tr>
<tr id=" 220 " class="entry"><td> <a href="http://ar5iv.org/abs/1801.08093v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Learning Symmetric and Low-energy Locomotion (2018)</b> &nbsp; - &nbsp; Yu, Wenhao and Turk, Greg and Liu, C. Karen &nbsp; - &nbsp; 1801.08093v3 &nbsp; - &nbsp;
{control}
{deep}
{humanoid}
{reinforcement}
{skill}
</small></font></div></a></td></tr>
<tr id=" 221 " class="entry"><td> <a href="http://ar5iv.org/abs/2102.13651v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> On the Importance of Hyperparameter Optimization for Model-based Reinforcement Learning (2021)</b> &nbsp; - &nbsp; Zhang, Baohe and Rajan, Raghu and Pineda, Luis and Lambert, Nathan and Biedenkapp, Andr&#233; and Chua, Kurtland and Hutter, Frank and Calandra, Roberto &nbsp; - &nbsp; 2102.13651v1 &nbsp; - &nbsp;
{control}
{model}
{optim}
{reinforcement}
</small></font></div></a></td></tr>
<tr id=" 222 " class="entry"><td> <a href="http://ar5iv.org/abs/1810.12281v1"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Three Mechanisms of Weight Decay Regularization (2018)</b> &nbsp; - &nbsp; Zhang, Guodong and Wang, Chaoqi and Xu, Bowen and Grosse, Roger &nbsp; - &nbsp; 1810.12281v1 &nbsp; - &nbsp;
{network}
{optim}
</small></font></div></a></td></tr>
<tr id=" 223 " class="entry"><td> <a href="http://ar5iv.org/abs/1706.04223v3"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Adversarially Regularized Autoencoders (2017)</b> &nbsp; - &nbsp; Zhao, Jake and Kim, Yoon and Zhang, Kelly and Rush, Alexander M. and LeCun, Yann &nbsp; - &nbsp; 1706.04223v3 &nbsp; - &nbsp;
{control}
{deep}
{explor}
{model}
{network}
{optim}
{transfer}
</small></font></div></a></td></tr>
<tr id=" 224 " class="entry"><td> <a href="http://ar5iv.org/abs/2109.13841v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Bottom-Up Skill Discovery from Unsegmented Demonstrations for Long-Horizon Robot Manipulation (2021)</b> &nbsp; - &nbsp; Zhu, Yifeng and Stone, Peter and Zhu, Yuke &nbsp; - &nbsp; 2109.13841v2 &nbsp; - &nbsp;
{control}
{goal-conditioned}
{hierarchical}
{imitation}
{meta}
{model}
{multi-task}
{robot}
{simulation}
{skill}
</small></font></div></a></td></tr>
<tr id=" 225 " class="entry"><td> <a href="http://ar5iv.org/abs/1906.04556v2"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> Exploiting the Sign of the Advantage Function to Learn Deterministic Policies in Continuous Domains (2019)</b> &nbsp; - &nbsp; Zimmer, Matthieu and Weng, Paul &nbsp; - &nbsp; 1906.04556v2 &nbsp; - &nbsp;
{control}
{deep}
{gradient}
</small></font></div></a></td></tr>
<tr id=" 226 " class="entry"><td> <a href="http://ar5iv.org/abs/1605.01278v4"><div style="height:100%;width:100%"><small>
<font color="black"> 
&nbsp; &bull;<b/> A Bayesian Approach to Policy Recognition and State Representation Learning (2016)</b> &nbsp; - &nbsp; &#352;o&#353;i&#263;, Adrian and Zoubir, Abdelhak M. and Koeppl, Heinz &nbsp; - &nbsp; 1605.01278v4 &nbsp; - &nbsp;
{control}
{model}
{optim}
</small></font></div></a></td></tr>
</tbody>

</table>































<!-- ======== End of the body ================================================================================================ -->

<BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR>



</body></html>

