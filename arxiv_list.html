<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><title>Scientific documents</title>

  <meta name="Author" content="Nicolas Perrin-Gilbert">



  <meta name="Description" content="Academic webpage of Nicolas Perrin-Gilbert">



  <meta name="HandheldFriendly" content="true" />

  <meta name="MobileOptimized" content="320" />

  <meta name="viewport" content="initial-scale=1.0, width=device-width, user-scalable=yes" />



  <meta name="Copyright" content="This file may be redistributed and/or modified without limitation">



  <meta name="Language" content="en">



  <meta name="Generator" content="">



  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">



  <meta http-equiv="Content-Language" content="en">



  <link rel="StyleSheet" href="index_files/screenfsos.css" type="text/css" media="screen">



  <link rel="StyleSheet" href="index_files/print.css" type="text/css" media="print"></head>

  

  <link rel="shortcut icon" type="image/png" href="https://raw.githubusercontent.com/perrin-isir/perrin-isir.github.io/main/images/logo_isir.png"></head>



<script type="text/javascript">

<!--

// QuickSearch script for JabRef HTML export 

// Version: 3.0

//

// Copyright (c) 2006-2011, Mark Schenk

//

// This software is distributed under a Creative Commons Attribution 3.0 License

// http://creativecommons.org/licenses/by/3.0/

//

// Features:

// - intuitive find-as-you-type searching

//    ~ case insensitive

//    ~ ignore diacritics (optional)

//

// - search with/without Regular Expressions

// - match BibTeX key

//



// Search settings

var searchAbstract = true;	// search in abstract

var searchReview = true;	// search in review



var noSquiggles = true; 	// ignore diacritics when searching

var searchRegExp = false; 	// enable RegExp searches





if (window.addEventListener) {

	window.addEventListener("load",initSearch,false); }

else if (window.attachEvent) {

	window.attachEvent("onload", initSearch); }



function initSearch() {

	// check for quick search table and searchfield

	if (!document.getElementById('qs_table')||!document.getElementById('quicksearch')) { return; }



	// load all the rows and sort into arrays

	loadTableData();

	

	//find the query field

	qsfield = document.getElementById('qs_field');



	// previous search term; used for speed optimisation

	prevSearch = '';



	//find statistics location

	stats = document.getElementById('stat');

	setStatistics(-1);

	

	// set up preferences

	initPreferences();



	// shows the searchfield

	document.getElementById('quicksearch').style.display = 'block';

        document.getElementById('qs_field').value = querySt("search");

	document.getElementById('qs_field').onkeyup = quickSearch;

	document.getElementById('qs_field').onchange = quickSearch;

 	quickSearch(qs_field);

}



function loadTableData() {

	// find table and appropriate rows

	searchTable = document.getElementById('qs_table');

	var allRows = searchTable.getElementsByTagName('tbody')[0].getElementsByTagName('tr');



	// split all rows into entryRows and infoRows (e.g. abstract, review, bibtex)

	entryRows = new Array(); infoRows = new Array(); absRows = new Array(); revRows = new Array();



	// get data from each row

	entryRowsData = new Array(); absRowsData = new Array(); revRowsData = new Array(); 

	

	BibTeXKeys = new Array();

	

	for (var i=0, k=0, j=0; i<allRows.length;i++) {

		if (allRows[i].className.match(/entry/)) {

			entryRows[j] = allRows[i];

			entryRowsData[j] = stripDiacritics(getTextContent(allRows[i]));

			allRows[i].id ? BibTeXKeys[j] = allRows[i].id : allRows[i].id = 'autokey_'+j;

			j ++;

		} else {

			infoRows[k++] = allRows[i];

			// check for abstract/review

			if (allRows[i].className.match(/abstract/)) {

				absRows.push(allRows[i]);

				absRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));

			} else if (allRows[i].className.match(/review/)) {

				revRows.push(allRows[i]);

				revRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));

			}

		}

	}

	//number of entries and rows

	numEntries = entryRows.length;

	numInfo = infoRows.length;

	numAbs = absRows.length;

	numRev = revRows.length;

}



function querySt(ji) {



    hu = window.location.search.substring(1);

    gy = hu.split("&");



    for (i=0;i<gy.length;i++) {

        ft = gy[i].split("=");

        if (ft[0] == ji) {

            return decodeURIComponent(ft[1]).replace(/["]/g,'');

        }

    }

    return '';

}



function quickSearch(){

	

	tInput = qsfield;



	if (tInput.value.length == 0) {

		showAll();

		setStatistics(-1);

		qsfield.className = '';

		return;

	} else {

		t = stripDiacritics(tInput.value);



		if(!searchRegExp) { t = escapeRegExp(t); }

			

		// only search for valid RegExp

		try {

			textRegExp = new RegExp(t,"im");

			closeAllInfo();

			qsfield.className = '';

		}

			catch(err) {

			prevSearch = tInput.value;

			qsfield.className = 'invalidsearch';

			return;

		}

	}

	

	// count number of hits

	var hits = 0;



	// start looping through all entry rows

	for (var i = 0; cRow = entryRows[i]; i++){



		// only show search the cells if it isn't already hidden OR if the search term is getting shorter, then search all

		if(cRow.className.indexOf('noshow')==-1 || tInput.value.length <= prevSearch.length){

			var found = false; 



			if (entryRowsData[i].search(textRegExp) != -1 || BibTeXKeys[i].search(textRegExp) != -1){ 

				found = true;

			} else {

				if(searchAbstract && absRowsData[i]!=undefined) {

					if (absRowsData[i].search(textRegExp) != -1){ found=true; } 

				}

				if(searchReview && revRowsData[i]!=undefined) {

					if (revRowsData[i].search(textRegExp) != -1){ found=true; } 

				}

			}

			

			if (found){

				cRow.className = 'entry show';

				hits++;

			} else {

				cRow.className = 'entry noshow';

			}

		}

	}



	// update statistics

	setStatistics(hits)

	

	// set previous search value

	prevSearch = tInput.value;

}





// Strip Diacritics from text

// http://stackoverflow.com/questions/990904/javascript-remove-accents-in-strings



// String containing replacement characters for stripping accents 

var stripstring = 

    'AAAAAAACEEEEIIII'+

    'DNOOOOO.OUUUUY..'+

    'aaaaaaaceeeeiiii'+

    'dnooooo.ouuuuy.y'+

    'AaAaAaCcCcCcCcDd'+

    'DdEeEeEeEeEeGgGg'+

    'GgGgHhHhIiIiIiIi'+

    'IiIiJjKkkLlLlLlL'+

    'lJlNnNnNnnNnOoOo'+

    'OoOoRrRrRrSsSsSs'+

    'SsTtTtTtUuUuUuUu'+

    'UuUuWwYyYZzZzZz.';



function stripDiacritics(str){



    if(noSquiggles==false){

        return str;

    }



    var answer='';

    for(var i=0;i<str.length;i++){

        var ch=str[i];

        var chindex=ch.charCodeAt(0)-192;   // Index of character code in the strip string

        if(chindex>=0 && chindex<stripstring.length){

            // Character is within our table, so we can strip the accent...

            var outch=stripstring.charAt(chindex);

            // ...unless it was shown as a '.'

            if(outch!='.')ch=outch;

        }

        answer+=ch;

    }

    return answer;

}



// http://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex

// NOTE: must escape every \ in the export code because of the JabRef Export...

function escapeRegExp(str) {

  return str.replace(/[-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");

}



function toggleInfo(articleid,info) {



	var entry = document.getElementById(articleid);

	var abs = document.getElementById('abs_'+articleid);

	var rev = document.getElementById('rev_'+articleid);

	var bib = document.getElementById('bib_'+articleid);

	

	if (abs && info == 'abstract') {

		abs.className.indexOf('noshow') == -1?abs.className = 'abstract noshow':abs.className = 'abstract show';

	} else if (rev && info == 'review') {

		rev.className.indexOf('noshow') == -1?rev.className = 'review noshow':rev.className = 'review show';

	} else if (bib && info == 'bibtex') {

		bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex show';

	} else { 

		return;

	}



	// check if one or the other is available

	var revshow; var absshow; var bibshow;

	(abs && abs.className.indexOf('noshow') == -1)? absshow = true: absshow = false;

	(rev && rev.className.indexOf('noshow') == -1)? revshow = true: revshow = false;	

	(bib && bib.className.indexOf('noshow') == -1)? bibshow = true: bibshow = false;

	

	// highlight original entry

	if(entry) {

		if (revshow || absshow || bibshow) {

		entry.className = 'entry highlight show';

		} else {

		entry.className = 'entry show';

		}

	}

	

	// When there's a combination of abstract/review/bibtex showing, need to add class for correct styling

	if(absshow) {

		(revshow||bibshow)?abs.className = 'abstract nextshow':abs.className = 'abstract';

	} 

	if (revshow) {

		bibshow?rev.className = 'review nextshow': rev.className = 'review';

	}	

	

}



function setStatistics (hits) {

	if(hits < 0) { hits=numEntries; }

	if(stats) { stats.firstChild.data = hits + '/' + numEntries}

}



function getTextContent(node) {

	// Function written by Arve Bersvendsen

	// http://www.virtuelvis.com

	

	if (node.nodeType == 3) {

	return node.nodeValue;

	} // text node

	if (node.nodeType == 1 && node.className != "infolinks") { // element node

	var text = [];

	for (var chld = node.firstChild;chld;chld=chld.nextSibling) {

		text.push(getTextContent(chld));

	}

	return text.join("");

	} return ""; // some other node, won't contain text nodes.

}



function showAll(){

	closeAllInfo();

	for (var i = 0; i < numEntries; i++){ entryRows[i].className = 'entry show'; }

}



function closeAllInfo(){

	for (var i=0; i < numInfo; i++){

		if (infoRows[i].className.indexOf('noshow') ==-1) {

			infoRows[i].className = infoRows[i].className + ' noshow';

		}

	}

}



function clearQS() {

	qsfield.value = '';

	qsfield.dispatchEvent(new Event('change'));

	qsfield.dispatchEvent(new Event('change'));

	showAll();

}



function redoQS(){

	showAll();

	quickSearch(qsfield);

}



function updateSetting(obj){

	var option = obj.id;

	var checked = obj.value;



	switch(option)

	 {

	 case "opt_useRegExp":

	   searchRegExp=!searchRegExp;

	   redoQS();

	   break;

	 case "opt_noAccents":

	   noSquiggles=!noSquiggles;

	   loadTableData();

	   redoQS();

	   break;

	 }

}



function initPreferences(){

	if(noSquiggles){document.getElementById("opt_noAccents").checked = true;}

	if(searchRegExp){document.getElementById("opt_useRegExp").checked = true;}

}



function toggleSettings(){

	var togglebutton = document.getElementById('showsettings');

	var settings = document.getElementById('settings');

	

	if(settings.className == "hidden"){

		settings.className = "show";

		togglebutton.innerText = "close settings";

		togglebutton.textContent = "close settings";

	}else{

		settings.className = "hidden";

		togglebutton.innerText = "settings...";		

		togglebutton.textContent = "settings...";

	}

}



-->

</script>

<style type="text/css">



html {

    font-size: max(1.4vh, 14px);

}



html a:link {

    color: black;

}



html a:hover {

    text-decoration: none;

}



html a:active {

    text-decoration: none;

}





form#quicksearch { width: auto; border-style: solid; border-color: gray; border-width: 1px 0px; padding: 0.7em 0.5em; display:none; position:relative; }

span#searchstat {padding-left: 1em;}



div#settings { margin-top:0.7em; /* border-bottom: 1px transparent solid; background-color: #efefef; border: 1px grey solid; */ }

div#settings ul {margin: 0; padding: 0; }

div#settings li {margin: 0; padding: 0 1em 0 0; display: inline; list-style: none; }

div#settings li + li { border-left: 2px #efefef solid; padding-left: 0.5em;}

div#settings input { margin-bottom: 0px;}



div#settings.hidden {display:none;}



#showsettings { border: 1px grey solid; padding: 0 0.5em; float:right; line-height: 1.6em; text-align: right; }

#showsettings:hover { cursor: pointer; }



.invalidsearch { background-color: red; }



table { border: 0px gray solid; width: 100%; empty-cells: show; }

th, td { border: 0px gray solid; padding: 0.1em; vertical-align: top;  }

td { text-align: left; vertical-align: top; }

th { background-color: #EFEFEF; }



td a, td a:hover { color: navy; font-weight: normal; }



tr.noshow { display: none;}



tr.highlight td { background-color: #787878; border-top: 2px black solid; font-weight: normal; }

tr.abstract td, tr.review td, tr.bibtex td { background-color: #F1F1F1; border-bottom: 2px black solid; }

tr.nextshow td { border-bottom: 1px gray solid; }



tr.bibtex pre { width: 100%; overflow: auto;}



p.infolinks { margin: 0.5em 0em 0em 0em; padding: 0px; }



#qssettings { padding: 0.5em; position: absolute; top: 0.2em; right: 0.2em; border: 1px gray solid; background-color: white; display: block; }

#qssettings p { font-weight: normal; cursor: pointer; }

#qssettings ul { display: none; list-style-type: none; padding-left: 0; margin: 0; }

#qssettings.active ul { display: block; }

</style>

</head>

<body>



<!-- For non-visual or non-stylesheet-capable user agents -->

<div id="mainlink"><a href="#main">Skip to

main content.</a></div>



<!-- ======== Header ======== -->

<div id="header">



</div>



<!-- ======== Main Content ======== -->



<div id="main">



<!-- ======== The body ================================================================================================ -->



<thead></thead>

<tbody>





<script type="text/javascript">
function toggleVisibility(x) { var e = document.getElementById(x); if(e.style.display == 'block') e.style.display = 'none'; else e.style.display = 'block';}
</script>
<script type="text/javascript">
function change(x){ elt = document.getElementById("qs_field"); elt.value = x; elt.dispatchEvent(new Event('change')); elt.dispatchEvent(new Event('change')); }
</script>
<div style="font-size:1rem"><a href="./index.html">HOME</a> - Click on [|&bull;|] to show the abstract and on the abstract to hide it - Click on the title to open on arXiv - Click on the authors to open on ar5iv</div>
<div id="links" style="display:none;">
<table style="font-size:1rem"><td width="100\%"><i>
<a onclick="change('contrastive');">contrastive /</a>
<a onclick="change('control');">control /</a>
<a onclick="change('deep');">deep /</a>
<a onclick="change('distill');">distill /</a>
<a onclick="change('diversity');">diversity /</a>
<a onclick="change('entropy');">entropy /</a>
<a onclick="change('evolution');">evolution /</a>
<a onclick="change('exoskeleton');">exoskeleton /</a>
<a onclick="change('explainable');">explainable /</a>
<a onclick="change('exploration');">exploration /</a>
<a onclick="change('few-shot');">few-shot /</a>
<a onclick="change('goal-conditioned');">goal-conditioned /</a>
<a onclick="change('gradient');">gradient /</a>
<a onclick="change('hierarchical');">hierarchical /</a>
<a onclick="change('humanoid');">humanoid /</a>
<a onclick="change('imitation');">imitation /</a>
<a onclick="change('intrinsic');">intrinsic /</a>
<a onclick="change('kinematic');">kinematic /</a>
<a onclick="change('lifelong learning');">lifelong learning /</a>
<a onclick="change('meta');">meta /</a>
<a onclick="change('model-based');">model-based /</a>
<a onclick="change('motion planning');">motion planning /</a>
<a onclick="change('multi-task');">multi-task /</a>
<a onclick="change('off-policy');">off-policy /</a>
<a onclick="change('offline');">offline /</a>
<a onclick="change('on-policy');">on-policy /</a>
<a onclick="change('open-ended');">open-ended /</a>
<a onclick="change('optimal');">optimal /</a>
<a onclick="change('population');">population /</a>
<a onclick="change('reinforcement');">reinforcement /</a>
<a onclick="change('replay');">replay /</a>
<a onclick="change('robot');">robot /</a>
<a onclick="change('self-supervised');">self-supervised /</a>
<a onclick="change('skill');">skill /</a>
<a onclick="change('sparse');">sparse /</a>
<a onclick="change('transfer');">transfer /</a>
<a onclick="change('unsupervised');">unsupervised /</a>
</i></td></table>
</div>


<table style="font-size:1rem" WIDTH="300" border="0" cellspacing="0" cellpadding="0">

<tr>

<td width="100%">

<form action="" id="quicksearch">

<input style="font-size:1rem; text-align:right" size="30" type="text" id="qs_field" autocomplete="off" placeholder="Type to search..." /> <span style="font-size:1rem" id="searchstat"> <span id="stat">0</span></span> <input style="font-size:1rem" type="button" onclick="clearQS()" value="clear" /> <input style="font-size:1rem" type="button" onclick="toggleVisibility('links')" value="tags on/off" /> <br>

<span>&nbsp;</span>

<div style="font-size:1rem" id="showsettings" onclick="toggleSettings()">...</div>

<div id="settings" class="hidden">

<ul>

<li><input type="checkbox" class="search_setting" id="opt_useRegExp" onchange="updateSetting(this)"><label style="font-size:1rem" for="opt_useRegExp"> use RegExp</label></li>

<li><input type="checkbox" class="search_setting" id="opt_noAccents" onchange="updateSetting(this)"><label style="font-size:1rem" for="opt_noAccents"> ignore accents</label></li>

</ul>

</div>

</form>

</td>

</tr>





</table><table style="font-size:1rem" bgcolor="#F5F6CE" WIDTH="300" id="qs_table" border="0" cellspacing="0" cellpadding="0">
<tr id=" 1 " class="entry"><td>
<a onclick="toggleVisibility('abstract1');">[|&bull;|]</a><a href="http://arxiv.org/abs/2406.00592v3">
<b/> Model Predictive Control and Reinforcement Learning: A Unified Framework
Based on Dynamic Programming (2024)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2406.00592v3">
Dimitri P. Bertsekas &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract1');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract1');"><div id="abstract1" style="font-size: 1rem; color:black; display: none">
<u><I> 2406.00592v3 &nbsp; - &nbsp;
{control}
{reinforcement}
</I></u><br> In this paper we describe a new conceptual framework that connects
approximate Dynamic Programming (DP), Model Predictive Control (MPC), and
Reinforcement Learning (RL). This framework centers around two algorithms,
which are designed largely independently of each other and operate in synergy
through the powerful mechanism of Newton's method. We call them the off-line
training and the on-line play algorithms. The names are borrowed from some of
the major successes of RL involving games; primary examples are the recent
(2017) AlphaZero program (which plays chess, [SHS17], [SSS17]), and the
similarly structured and earlier (1990s) TD-Gammon program (which plays
backgammon, [Tes94], [Tes95], [TeG96]). In these game contexts, the off-line
training algorithm is the method used to teach the program how to evaluate
positions and to generate good moves at any given position, while the on-line
play algorithm is the method used to play in real time against human or
computer opponents.
Significantly, the synergy between off-line training and on-line play also
underlies MPC (as well as other major classes of sequential decision problems),
and indeed the MPC design architecture is very similar to the one of AlphaZero
and TD-Gammon. This conceptual insight provides a vehicle for bridging the
cultural gap between RL and MPC, and sheds new light on some fundamental issues
in MPC. These include the enhancement of stability properties through rollout,
the treatment of uncertainty through the use of certainty equivalence, the
resilience of MPC in adaptive control settings that involve changing system
parameters, and the insights provided by the superlinear performance bounds
implied by Newton's method. <br><br></div></a>
</td></tr>
<tr id=" 2 " class="entry"><td>
<a onclick="toggleVisibility('abstract2');">[|&bull;|]</a><a href="http://arxiv.org/abs/2402.00366v1">
<b/> Legged Robot State Estimation With Invariant Extended Kalman Filter
Using Neural Measurement Network (2024)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2402.00366v1">
Donghoon Youm, Hyunsik Oh, Suyoung Choi, Hyeongjun Kim, Jemin Hwangbo &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract2');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract2');"><div id="abstract2" style="font-size: 1rem; color:black; display: none">
<u><I> 2402.00366v1 &nbsp; - &nbsp;
{deep}
{model-based}
{robot}
</I></u><br> This paper introduces a novel proprioceptive state estimator for legged
robots that combines model-based filters and deep neural networks. Recent
studies have shown that neural networks such as multi-layer perceptron or
recurrent neural networks can estimate the robot states, including contact
probability and linear velocity. Inspired by this, we develop a state
estimation framework that integrates a neural measurement network (NMN) with an
invariant extended Kalman filter. We show that our framework improves
estimation performance in various terrains. Existing studies that combine
model-based filters and learning-based approaches typically use real-world
data. However, our approach relies solely on simulation data, as it allows us
to easily obtain extensive data. This difference leads to a gap between the
learning and the inference domain, commonly referred to as a sim-to-real gap.
We address this challenge by adapting existing learning techniques and
regularization. To validate our proposed method, we conduct experiments using a
quadruped robot on four types of terrain: \textit{flat}, \textit{debris},
\textit{soft}, and \textit{slippery}. We observe that our approach
significantly reduces position drift compared to the existing model-based state
estimator. <br><br></div></a>
</td></tr>
<tr id=" 3 " class="entry"><td>
<a onclick="toggleVisibility('abstract3');">[|&bull;|]</a><a href="http://arxiv.org/abs/2405.12424v2">
<b/> Rethinking Robustness Assessment: Adversarial Attacks on Learning-based
Quadrupedal Locomotion Controllers (2024)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2405.12424v2">
Fan Shi, Chong Zhang, Takahiro Miki, Joonho Lee, Marco Hutter, Stelian Coros &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract3');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract3');"><div id="abstract3" style="font-size: 1rem; color:black; display: none">
<u><I> 2405.12424v2 &nbsp; - &nbsp;
{control}
{deep}
{reinforcement}
{robot}
</I></u><br> Legged locomotion has recently achieved remarkable success with the progress
of machine learning techniques, especially deep reinforcement learning (RL).
Controllers employing neural networks have demonstrated empirical and
qualitative robustness against real-world uncertainties, including sensor noise
and external perturbations. However, formally investigating the vulnerabilities
of these locomotion controllers remains a challenge. This difficulty arises
from the requirement to pinpoint vulnerabilities across a long-tailed
distribution within a high-dimensional, temporally sequential space. As a first
step towards quantitative verification, we propose a computational method that
leverages sequential adversarial attacks to identify weaknesses in learned
locomotion controllers. Our research demonstrates that, even state-of-the-art
robust controllers can fail significantly under well-designed, low-magnitude
adversarial sequence. Through experiments in simulation and on the real robot,
we validate our approach's effectiveness, and we illustrate how the results it
generates can be used to robustify the original policy and offer valuable
insights into the safety of these black-box policies. Project page:
https://fanshi14.github.io/me/rss24.html <br><br></div></a>
</td></tr>
<tr id=" 4 " class="entry"><td>
<a onclick="toggleVisibility('abstract4');">[|&bull;|]</a><a href="http://arxiv.org/abs/2403.13297v2">
<b/> POLICEd RL: Learning Closed-Loop Robot Control Policies with Provable
Satisfaction of Hard Constraints (2024)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2403.13297v2">
Jean-Baptiste Bouvier, Kartik Nagpal, Negar Mehr &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract4');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract4');"><div id="abstract4" style="font-size: 1rem; color:black; display: none">
<u><I> 2403.13297v2 &nbsp; - &nbsp;
{robot}
</I></u><br> In this paper, we seek to learn a robot policy guaranteed to satisfy state
constraints. To encourage constraint satisfaction, existing RL algorithms
typically rely on Constrained Markov Decision Processes and discourage
constraint violations through reward shaping. However, such soft constraints
cannot offer verifiable safety guarantees. To address this gap, we propose
POLICEd RL, a novel RL algorithm explicitly designed to enforce affine hard
constraints in closed-loop with a black-box environment. Our key insight is to
force the learned policy to be affine around the unsafe set and use this affine
region as a repulsive buffer to prevent trajectories from violating the
constraint. We prove that such policies exist and guarantee constraint
satisfaction. Our proposed framework is applicable to both systems with
continuous and discrete state and action spaces and is agnostic to the choice
of the RL training algorithm. Our results demonstrate the capacity of POLICEd
RL to enforce hard constraints in robotic tasks while significantly
outperforming existing methods. <br><br></div></a>
</td></tr>
<tr id=" 5 " class="entry"><td>
<a onclick="toggleVisibility('abstract5');">[|&bull;|]</a><a href="http://arxiv.org/abs/2402.19432v1">
<b/> Pushing the Limits of Cross-Embodiment Learning for Manipulation and
Navigation (2024)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2402.19432v1">
Jonathan Yang, Catherine Glossop, Arjun Bhorkar, Dhruv Shah, Quan Vuong, Chelsea Finn, Dorsa Sadigh, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract5');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract5');"><div id="abstract5" style="font-size: 1rem; color:black; display: none">
<u><I> 2402.19432v1 &nbsp; - &nbsp;
{control}
{goal-conditioned}
{imitation}
{robot}
{transfer}
</I></u><br> Recent years in robotics and imitation learning have shown remarkable
progress in training large-scale foundation models by leveraging data across a
multitude of embodiments. The success of such policies might lead us to wonder:
just how diverse can the robots in the training set be while still facilitating
positive transfer? In this work, we study this question in the context of
heterogeneous embodiments, examining how even seemingly very different domains,
such as robotic navigation and manipulation, can provide benefits when included
in the training data for the same model. We train a single goal-conditioned
policy that is capable of controlling robotic arms, quadcopters, quadrupeds,
and mobile bases. We then investigate the extent to which transfer can occur
across navigation and manipulation on these embodiments by framing them as a
single goal-reaching task. We find that co-training with navigation data can
enhance robustness and performance in goal-conditioned manipulation with a
wrist-mounted camera. We then deploy our policy trained only from
navigation-only and static manipulation-only data on a mobile manipulator,
showing that it can control a novel embodiment in a zero-shot manner. These
results provide evidence that large-scale robotic policies can benefit from
data collected across various embodiments. Further information and robot videos
can be found on our project website http://extreme-cross-embodiment.github.io. <br><br></div></a>
</td></tr>
<tr id=" 6 " class="entry"><td>
<a onclick="toggleVisibility('abstract6');">[|&bull;|]</a><a href="http://arxiv.org/abs/2403.05996v3">
<b/> Dissecting Deep RL with High Update Ratios: Combatting Value Divergence (2024)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2403.05996v3">
Marcel Hussing, Claas Voelcker, Igor Gilitschenski, Amir-massoud Farahmand, Eric Eaton &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract6');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract6');"><div id="abstract6" style="font-size: 1rem; color:black; display: none">
<u><I> 2403.05996v3 &nbsp; - &nbsp;
{control}
{deep}
{gradient}
{model-based}
{optimal}
{reinforcement}
</I></u><br> We show that deep reinforcement learning algorithms can retain their ability
to learn without resetting network parameters in settings where the number of
gradient updates greatly exceeds the number of environment samples by
combatting value function divergence. Under large update-to-data ratios, a
recent study by Nikishin et al. (2022) suggested the emergence of a primacy
bias, in which agents overfit early interactions and downplay later experience,
impairing their ability to learn. In this work, we investigate the phenomena
leading to the primacy bias. We inspect the early stages of training that were
conjectured to cause the failure to learn and find that one fundamental
challenge is a long-standing acquaintance: value function divergence.
Overinflated Q-values are found not only on out-of-distribution but also
in-distribution data and can be linked to overestimation on unseen action
prediction propelled by optimizer momentum. We employ a simple unit-ball
normalization that enables learning under large update ratios, show its
efficacy on the widely used dm_control suite, and obtain strong performance on
the challenging dog tasks, competitive with model-based approaches. Our results
question, in parts, the prior explanation for sub-optimal learning due to
overfitting early data. <br><br></div></a>
</td></tr>
<tr id=" 7 " class="entry"><td>
<a onclick="toggleVisibility('abstract7');">[|&bull;|]</a><a href="http://arxiv.org/abs/2402.05963v1">
<b/> Frugal Actor-Critic: Sample Efficient Off-Policy Deep Reinforcement
Learning Using Unique Experiences (2024)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2402.05963v1">
Nikhil Kumar Singh, Indranil Saha &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract7');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract7');"><div id="abstract7" style="font-size: 1rem; color:black; display: none">
<u><I> 2402.05963v1 &nbsp; - &nbsp;
{control}
{exploration}
{off-policy}
{reinforcement}
{replay}
</I></u><br> Efficient utilization of the replay buffer plays a significant role in the
off-policy actor-critic reinforcement learning (RL) algorithms used for
model-free control policy synthesis for complex dynamical systems. We propose a
method for achieving sample efficiency, which focuses on selecting unique
samples and adding them to the replay buffer during the exploration with the
goal of reducing the buffer size and maintaining the independent and
identically distributed (IID) nature of the samples. Our method is based on
selecting an important subset of the set of state variables from the
experiences encountered during the initial phase of random exploration,
partitioning the state space into a set of abstract states based on the
selected important state variables, and finally selecting the experiences with
unique state-reward combination by using a kernel density estimator. We
formally prove that the off-policy actor-critic algorithm incorporating the
proposed method for unique experience accumulation converges faster than the
vanilla off-policy actor-critic algorithm. Furthermore, we evaluate our method
by comparing it with two state-of-the-art actor-critic RL algorithms on several
continuous control benchmarks available in the Gym environment. Experimental
results demonstrate that our method achieves a significant reduction in the
size of the replay buffer for all the benchmarks while achieving either faster
convergent or better reward accumulation compared to the baseline algorithms. <br><br></div></a>
</td></tr>
<tr id=" 8 " class="entry"><td>
<a onclick="toggleVisibility('abstract8');">[|&bull;|]</a><a href="http://arxiv.org/abs/2307.06324v5">
<b/> Provably Faster Gradient Descent via Long Steps (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2307.06324v5">
Benjamin Grimmer &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract8');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract8');"><div id="abstract8" style="font-size: 1rem; color:black; display: none">
<u><I> 2307.06324v5 &nbsp; - &nbsp;
{gradient}
</I></u><br> This work establishes new convergence guarantees for gradient descent in
smooth convex optimization via a computer-assisted analysis technique. Our
theory allows nonconstant stepsize policies with frequent long steps
potentially violating descent by analyzing the overall effect of many
iterations at once rather than the typical one-iteration inductions used in
most first-order method analyses. We show that long steps, which may increase
the objective value in the short term, lead to provably faster convergence in
the long term. A conjecture towards proving a faster $O(1/T\log T)$ rate for
gradient descent is also motivated along with simple numerical validation. <br><br></div></a>
</td></tr>
<tr id=" 9 " class="entry"><td>
<a onclick="toggleVisibility('abstract9');">[|&bull;|]</a><a href="http://arxiv.org/abs/2303.02386v1">
<b/> Modular Safety-Critical Control of Legged Robots (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2303.02386v1">
Berk Tosun, Evren Samur &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract9');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract9');"><div id="abstract9" style="font-size: 1rem; color:black; display: none">
<u><I> 2303.02386v1 &nbsp; - &nbsp;
{control}
{deep}
{model-based}
{optimal}
{robot}
{transfer}
</I></u><br> Safety concerns during the operation of legged robots must be addressed to
enable their widespread use. Machine learning-based control methods that use
model-based constraints provide promising means to improve robot safety. This
study presents a modular safety filter to improve the safety of a legged robot,
i.e., reduce the chance of a fall. The prerequisite is the availability of a
robot that is capable of locomotion, i.e., a nominal controller exists. During
locomotion, terrain properties around the robot are estimated through machine
learning which uses a minimal set of proprioceptive signals. A novel
deep-learning model utilizing an efficient transformer architecture is used for
the terrain estimation. A quadratic program combines the terrain estimations
with inverse dynamics and a novel exponential control barrier function
constraint to filter and certify nominal control signals. The result is an
optimal controller that acts as a filter. The filtered control signal allows
safe locomotion of the robot. The resulting approach is generalizable, and
could be transferred with low effort to any other legged system. <br><br></div></a>
</td></tr>
<tr id=" 10 " class="entry"><td>
<a onclick="toggleVisibility('abstract10');">[|&bull;|]</a><a href="http://arxiv.org/abs/2303.04137v5">
<b/> Diffusion Policy: Visuomotor Policy Learning via Action Diffusion (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2303.04137v5">
Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, Shuran Song &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract10');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract10');"><div id="abstract10" style="font-size: 1rem; color:black; display: none">
<u><I> 2303.04137v5 &nbsp; - &nbsp;
{control}
{gradient}
{on-policy}
{robot}
</I></u><br> This paper introduces Diffusion Policy, a new way of generating robot
behavior by representing a robot's visuomotor policy as a conditional denoising
diffusion process. We benchmark Diffusion Policy across 12 different tasks from
4 different robot manipulation benchmarks and find that it consistently
outperforms existing state-of-the-art robot learning methods with an average
improvement of 46.9%. Diffusion Policy learns the gradient of the
action-distribution score function and iteratively optimizes with respect to
this gradient field during inference via a series of stochastic Langevin
dynamics steps. We find that the diffusion formulation yields powerful
advantages when used for robot policies, including gracefully handling
multimodal action distributions, being suitable for high-dimensional action
spaces, and exhibiting impressive training stability. To fully unlock the
potential of diffusion models for visuomotor policy learning on physical
robots, this paper presents a set of key technical contributions including the
incorporation of receding horizon control, visual conditioning, and the
time-series diffusion transformer. We hope this work will help motivate a new
generation of policy learning techniques that are able to leverage the powerful
generative modeling capabilities of diffusion models. Code, data, and training
details is publicly available diffusion-policy.cs.columbia.edu <br><br></div></a>
</td></tr>
<tr id=" 11 " class="entry"><td>
<a onclick="toggleVisibility('abstract11');">[|&bull;|]</a><a href="http://arxiv.org/abs/2301.04104v2">
<b/> Mastering Diverse Domains through World Models (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2301.04104v2">
Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract11');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract11');"><div id="abstract11" style="font-size: 1rem; color:black; display: none">
<u><I> 2301.04104v2 &nbsp; - &nbsp;
{control}
{reinforcement}
{sparse}
</I></u><br> Developing a general algorithm that learns to solve tasks across a wide range
of applications has been a fundamental challenge in artificial intelligence.
Although current reinforcement learning algorithms can be readily applied to
tasks similar to what they have been developed for, configuring them for new
application domains requires significant human expertise and experimentation.
We present DreamerV3, a general algorithm that outperforms specialized methods
across over 150 diverse tasks, with a single configuration. Dreamer learns a
model of the environment and improves its behavior by imagining future
scenarios. Robustness techniques based on normalization, balancing, and
transformations enable stable learning across domains. Applied out of the box,
Dreamer is the first algorithm to collect diamonds in Minecraft from scratch
without human data or curricula. This achievement has been posed as a
significant challenge in artificial intelligence that requires exploring
farsighted strategies from pixels and sparse rewards in an open world. Our work
allows solving challenging control problems without extensive experimentation,
making reinforcement learning broadly applicable. <br><br></div></a>
</td></tr>
<tr id=" 12 " class="entry"><td>
<a onclick="toggleVisibility('abstract12');">[|&bull;|]</a><a href="http://arxiv.org/abs/2306.14874v1">
<b/> ANYmal Parkour: Learning Agile Navigation for Quadrupedal Robots (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2306.14874v1">
David Hoeller, Nikita Rudin, Dhionis Sako, Marco Hutter &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract12');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract12');"><div id="abstract12" style="font-size: 1rem; color:black; display: none">
<u><I> 2306.14874v1 &nbsp; - &nbsp;
{control}
{hierarchical}
{offline}
{on-policy}
{robot}
{skill}
{transfer}
</I></u><br> Performing agile navigation with four-legged robots is a challenging task due
to the highly dynamic motions, contacts with various parts of the robot, and
the limited field of view of the perception sensors. In this paper, we propose
a fully-learned approach to train such robots and conquer scenarios that are
reminiscent of parkour challenges. The method involves training advanced
locomotion skills for several types of obstacles, such as walking, jumping,
climbing, and crouching, and then using a high-level policy to select and
control those skills across the terrain. Thanks to our hierarchical
formulation, the navigation policy is aware of the capabilities of each skill,
and it will adapt its behavior depending on the scenario at hand. Additionally,
a perception module is trained to reconstruct obstacles from highly occluded
and noisy sensory data and endows the pipeline with scene understanding.
Compared to previous attempts, our method can plan a path for challenging
scenarios without expert demonstration, offline computation, a priori knowledge
of the environment, or taking contacts explicitly into account. While these
modules are trained from simulated data only, our real-world experiments
demonstrate successful transfer on hardware, where the robot navigates and
crosses consecutive challenging obstacles with speeds of up to two meters per
second. The supplementary video can be found on the project website:
https://sites.google.com/leggedrobotics.com/agile-navigation <br><br></div></a>
</td></tr>
<tr id=" 13 " class="entry"><td>
<a onclick="toggleVisibility('abstract13');">[|&bull;|]</a><a href="http://arxiv.org/abs/2304.04782v1">
<b/> Reinforcement Learning from Passive Data via Latent Intentions (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2304.04782v1">
Dibya Ghosh, Chethan Bhateja, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract13');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract13');"><div id="abstract13" style="font-size: 1rem; color:black; display: none">
<u><I> 2304.04782v1 &nbsp; - &nbsp;
</I></u><br> Passive observational data, such as human videos, is abundant and rich in
information, yet remains largely untapped by current RL methods. Perhaps
surprisingly, we show that passive data, despite not having reward or action
labels, can still be used to learn features that accelerate downstream RL. Our
approach learns from passive data by modeling intentions: measuring how the
likelihood of future outcomes change when the agent acts to achieve a
particular task. We propose a temporal difference learning objective to learn
about intentions, resulting in an algorithm similar to conventional RL, but
which learns entirely from passive data. When optimizing this objective, our
agent simultaneously learns representations of states, of policies, and of
possible outcomes in an environment, all from raw observational data. Both
theoretically and empirically, this scheme learns features amenable for value
prediction for downstream tasks, and our experiments demonstrate the ability to
learn from many forms of passive data, including cross-embodiment video data
and YouTube videos. <br><br></div></a>
</td></tr>
<tr id=" 14 " class="entry"><td>
<a onclick="toggleVisibility('abstract14');">[|&bull;|]</a><a href="http://arxiv.org/abs/2303.13002v1">
<b/> Planning Goals for Exploration (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2303.13002v1">
Edward S. Hu, Richard Chang, Oleh Rybkin, Dinesh Jayaraman &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract14');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract14');"><div id="abstract14" style="font-size: 1rem; color:black; display: none">
<u><I> 2303.13002v1 &nbsp; - &nbsp;
{exploration}
{goal-conditioned}
{intrinsic}
{on-policy}
{reinforcement}
{robot}
</I></u><br> Dropped into an unknown environment, what should an agent do to quickly learn
about the environment and how to accomplish diverse tasks within it? We address
this question within the goal-conditioned reinforcement learning paradigm, by
identifying how the agent should set its goals at training time to maximize
exploration. We propose "Planning Exploratory Goals" (PEG), a method that sets
goals for each training episode to directly optimize an intrinsic exploration
reward. PEG first chooses goal commands such that the agent's goal-conditioned
policy, at its current level of training, will end up in states with high
exploration potential. It then launches an exploration policy starting at those
promising states. To enable this direct optimization, PEG learns world models
and adapts sampling-based planning algorithms to "plan goal commands". In
challenging simulated robotics environments including a multi-legged ant robot
in a maze, and a robot arm on a cluttered tabletop, PEG exploration enables
more efficient and effective training of goal-conditioned policies relative to
baselines and ablations. Our ant successfully navigates a long maze, and the
robot arm successfully builds a stack of three blocks upon command. Website:
https://penn-pal-lab.github.io/peg/ <br><br></div></a>
</td></tr>
<tr id=" 15 " class="entry"><td>
<a onclick="toggleVisibility('abstract15');">[|&bull;|]</a><a href="http://arxiv.org/abs/2303.15810v1">
<b/> Offline RL with No OOD Actions: In-Sample Learning via Implicit Value
Regularization (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2303.15810v1">
Haoran Xu, Li Jiang, Jianxiong Li, Zhuoran Yang, Zhaoran Wang, Victor Wai Kin Chan, Xianyuan Zhan &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract15');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract15');"><div id="abstract15" style="font-size: 1rem; color:black; display: none">
<u><I> 2303.15810v1 &nbsp; - &nbsp;
{deep}
{offline}
{optimal}
{reinforcement}
{sparse}
</I></u><br> Most offline reinforcement learning (RL) methods suffer from the trade-off
between improving the policy to surpass the behavior policy and constraining
the policy to limit the deviation from the behavior policy as computing
$Q$-values using out-of-distribution (OOD) actions will suffer from errors due
to distributional shift. The recently proposed \textit{In-sample Learning}
paradigm (i.e., IQL), which improves the policy by quantile regression using
only data samples, shows great promise because it learns an optimal policy
without querying the value function of any unseen actions. However, it remains
unclear how this type of method handles the distributional shift in learning
the value function. In this work, we make a key finding that the in-sample
learning paradigm arises under the \textit{Implicit Value Regularization} (IVR)
framework. This gives a deeper understanding of why the in-sample learning
paradigm works, i.e., it applies implicit value regularization to the policy.
Based on the IVR framework, we further propose two practical algorithms, Sparse
$Q$-learning (SQL) and Exponential $Q$-learning (EQL), which adopt the same
value regularization used in existing works, but in a complete in-sample
manner. Compared with IQL, we find that our algorithms introduce sparsity in
learning the value function, making them more robust in noisy data regimes. We
also verify the effectiveness of SQL and EQL on D4RL benchmark datasets and
show the benefits of in-sample learning by comparing them with CQL in small
data regimes. <br><br></div></a>
</td></tr>
<tr id=" 16 " class="entry"><td>
<a onclick="toggleVisibility('abstract16');">[|&bull;|]</a><a href="http://arxiv.org/abs/2303.03381v2">
<b/> Real-World Humanoid Locomotion with Reinforcement Learning (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2303.03381v2">
Ilija Radosavovic, Tete Xiao, Bike Zhang, Trevor Darrell, Jitendra Malik, Koushil Sreenath &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract16');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract16');"><div id="abstract16" style="font-size: 1rem; color:black; display: none">
<u><I> 2303.03381v2 &nbsp; - &nbsp;
{control}
{humanoid}
{reinforcement}
{robot}
</I></u><br> Humanoid robots that can autonomously operate in diverse environments have
the potential to help address labour shortages in factories, assist elderly at
homes, and colonize new planets. While classical controllers for humanoid
robots have shown impressive results in a number of settings, they are
challenging to generalize and adapt to new environments. Here, we present a
fully learning-based approach for real-world humanoid locomotion. Our
controller is a causal transformer that takes the history of proprioceptive
observations and actions as input and predicts the next action. We hypothesize
that the observation-action history contains useful information about the world
that a powerful transformer model can use to adapt its behavior in-context,
without updating its weights. We train our model with large-scale model-free
reinforcement learning on an ensemble of randomized environments in simulation
and deploy it to the real world zero-shot. Our controller can walk over various
outdoor terrains, is robust to external disturbances, and can adapt in context. <br><br></div></a>
</td></tr>
<tr id=" 17 " class="entry"><td>
<a onclick="toggleVisibility('abstract17');">[|&bull;|]</a><a href="http://arxiv.org/abs/2304.10782v1">
<b/> Contrastive Language, Action, and State Pre-training for Robot Learning (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2304.10782v1">
Krishan Rana, Andrew Melnik, Niko S&#252;nderhauf &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract17');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract17');"><div id="abstract17" style="font-size: 1rem; color:black; display: none">
<u><I> 2304.10782v1 &nbsp; - &nbsp;
{contrastive}
{reinforcement}
{robot}
</I></u><br> In this paper, we introduce a method for unifying language, action, and state
information in a shared embedding space to facilitate a range of downstream
tasks in robot learning. Our method, Contrastive Language, Action, and State
Pre-training (CLASP), extends the CLIP formulation by incorporating
distributional learning, capturing the inherent complexities and one-to-many
relationships in behaviour-text alignment. By employing distributional outputs
for both text and behaviour encoders, our model effectively associates diverse
textual commands with a single behaviour and vice-versa. We demonstrate the
utility of our method for the following downstream tasks: zero-shot
text-behaviour retrieval, captioning unseen robot behaviours, and learning a
behaviour prior for language-conditioned reinforcement learning. Our
distributional encoders exhibit superior retrieval and captioning performance
on unseen datasets, and the ability to generate meaningful exploratory
behaviours from textual commands, capturing the intricate relationships between
language, action, and state. This work represents an initial step towards
developing a unified pre-trained model for robotics, with the potential to
generalise to a broad range of downstream tasks. <br><br></div></a>
</td></tr>
<tr id=" 18 " class="entry"><td>
<a onclick="toggleVisibility('abstract18');">[|&bull;|]</a><a href="http://arxiv.org/abs/2305.00976v2">
<b/> TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion
Synthesis (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2305.00976v2">
Mathis Petrovich, Michael J. Black, G&#252;l Varol &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract18');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract18');"><div id="abstract18" style="font-size: 1rem; color:black; display: none">
<u><I> 2305.00976v2 &nbsp; - &nbsp;
{contrastive}
</I></u><br> In this paper, we present TMR, a simple yet effective approach for text to 3D
human motion retrieval. While previous work has only treated retrieval as a
proxy evaluation metric, we tackle it as a standalone task. Our method extends
the state-of-the-art text-to-motion synthesis model TEMOS, and incorporates a
contrastive loss to better structure the cross-modal latent space. We show that
maintaining the motion generation loss, along with the contrastive training, is
crucial to obtain good performance. We introduce a benchmark for evaluation and
provide an in-depth analysis by reporting results on several protocols. Our
extensive experiments on the KIT-ML and HumanML3D datasets show that TMR
outperforms the prior work by a significant margin, for example reducing the
median rank from 54 to 19. Finally, we showcase the potential of our approach
on moment retrieval. Our code and models are publicly available at
https://mathis.petrovich.fr/tmr. <br><br></div></a>
</td></tr>
<tr id=" 19 " class="entry"><td>
<a onclick="toggleVisibility('abstract19');">[|&bull;|]</a><a href="http://arxiv.org/abs/2305.19452v3">
<b/> Bigger, Better, Faster: Human-level Atari with human-level efficiency (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2305.19452v3">
Max Schwarzer, Johan Obando-Ceron, Aaron Courville, Marc Bellemare, Rishabh Agarwal, Pablo Samuel Castro &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract19');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract19');"><div id="abstract19" style="font-size: 1rem; color:black; display: none">
<u><I> 2305.19452v3 &nbsp; - &nbsp;
</I></u><br> We introduce a value-based RL agent, which we call BBF, that achieves
super-human performance in the Atari 100K benchmark. BBF relies on scaling the
neural networks used for value estimation, as well as a number of other design
choices that enable this scaling in a sample-efficient manner. We conduct
extensive analyses of these design choices and provide insights for future
work. We end with a discussion about updating the goalposts for
sample-efficient RL research on the ALE. We make our code and data publicly
available at
https://github.com/google-research/google-research/tree/master/bigger_better_faster. <br><br></div></a>
</td></tr>
<tr id=" 20 " class="entry"><td>
<a onclick="toggleVisibility('abstract20');">[|&bull;|]</a><a href="http://arxiv.org/abs/2303.05479v4">
<b/> Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online
Fine-Tuning (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2303.05479v4">
Mitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn, Aviral Kumar, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract20');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract20');"><div id="abstract20" style="font-size: 1rem; color:black; display: none">
<u><I> 2303.05479v4 &nbsp; - &nbsp;
{offline}
{optimal}
{reinforcement}
</I></u><br> A compelling use case of offline reinforcement learning (RL) is to obtain a
policy initialization from existing datasets followed by fast online
fine-tuning with limited interaction. However, existing offline RL methods tend
to behave poorly during fine-tuning. In this paper, we devise an approach for
learning an effective initialization from offline data that also enables fast
online fine-tuning capabilities. Our approach, calibrated Q-learning (Cal-QL),
accomplishes this by learning a conservative value function initialization that
underestimates the value of the learned policy from offline data, while also
being calibrated, in the sense that the learned Q-values are at a reasonable
scale. We refer to this property as calibration, and define it formally as
providing a lower bound on the true value function of the learned policy and an
upper bound on the value of some other (suboptimal) reference policy, which may
simply be the behavior policy. We show that offline RL algorithms that learn
such calibrated value functions lead to effective online fine-tuning, enabling
us to take the benefits of offline initializations in online fine-tuning. In
practice, Cal-QL can be implemented on top of the conservative Q learning (CQL)
for offline RL within a one-line code change. Empirically, Cal-QL outperforms
state-of-the-art methods on 9/11 fine-tuning benchmark tasks that we study in
this paper. Code and video are available at https://nakamotoo.github.io/Cal-QL <br><br></div></a>
</td></tr>
<tr id=" 21 " class="entry"><td>
<a onclick="toggleVisibility('abstract21');">[|&bull;|]</a><a href="http://arxiv.org/abs/2309.12566v2">
<b/> Recent Advances in Path Integral Control for Trajectory Optimization: An
Overview in Theoretical and Algorithmic Perspectives (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2309.12566v2">
Muhammad Kazim, JunGee Hong, Min-Gyeom Kim, Kwang-Ki K. Kim &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract21');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract21');"><div id="abstract21" style="font-size: 1rem; color:black; display: none">
<u><I> 2309.12566v2 &nbsp; - &nbsp;
{control}
{entropy}
{optimal}
</I></u><br> This paper presents a tutorial overview of path integral (PI) control
approaches for stochastic optimal control and trajectory optimization. We
concisely summarize the theoretical development of path integral control to
compute a solution for stochastic optimal control and provide algorithmic
descriptions of the cross-entropy (CE) method, an open-loop controller using
the receding horizon scheme known as the model predictive path integral (MPPI),
and a parameterized state feedback controller based on the path integral
control theory. We discuss policy search methods based on path integral
control, efficient and stable sampling strategies, extensions to multi-agent
decision-making, and MPPI for the trajectory optimization on manifolds. For
tutorial demonstrations, some PI-based controllers are implemented in Python,
MATLAB and ROS2/Gazebo simulations for trajectory optimization. The simulation
frameworks and source codes are publicly available at
https://github.com/INHA-Autonomous-Systems-Laboratory-ASL/An-Overview-on-Recent-Advances-in-Path-Integral-Control. <br><br></div></a>
</td></tr>
<tr id=" 22 " class="entry"><td>
<a onclick="toggleVisibility('abstract22');">[|&bull;|]</a><a href="http://arxiv.org/abs/2302.02948v4">
<b/> Efficient Online Reinforcement Learning with Offline Data (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2302.02948v4">
Philip J. Ball, Laura Smith, Ilya Kostrikov, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract22');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract22');"><div id="abstract22" style="font-size: 1rem; color:black; display: none">
<u><I> 2302.02948v4 &nbsp; - &nbsp;
{exploration}
{off-policy}
{offline}
{on-policy}
{optimal}
{reinforcement}
</I></u><br> Sample efficiency and exploration remain major challenges in online
reinforcement learning (RL). A powerful approach that can be applied to address
these issues is the inclusion of offline data, such as prior trajectories from
a human expert or a sub-optimal exploration policy. Previous methods have
relied on extensive modifications and additional complexity to ensure the
effective use of this data. Instead, we ask: can we simply apply existing
off-policy methods to leverage offline data when learning online? In this work,
we demonstrate that the answer is yes; however, a set of minimal but important
changes to existing off-policy RL algorithms are required to achieve reliable
performance. We extensively ablate these design choices, demonstrating the key
factors that most affect performance, and arrive at a set of recommendations
that practitioners can readily apply, whether their data comprise a small
number of expert demonstrations or large volumes of sub-optimal trajectories.
We see that correct application of these simple recommendations can provide a
$\mathbf{2.5\times}$ improvement over existing approaches across a diverse set
of competitive benchmarks, with no additional computational overhead. We have
released our code at https://github.com/ikostrikov/rlpd. <br><br></div></a>
</td></tr>
<tr id=" 23 " class="entry"><td>
<a onclick="toggleVisibility('abstract23');">[|&bull;|]</a><a href="http://arxiv.org/abs/2304.10573v2">
<b/> IDQL: Implicit Q-Learning as an Actor-Critic Method with Diffusion
Policies (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2304.10573v2">
Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract23');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract23');"><div id="abstract23" style="font-size: 1rem; color:black; display: none">
<u><I> 2304.10573v2 &nbsp; - &nbsp;
{offline}
</I></u><br> Effective offline RL methods require properly handling out-of-distribution
actions. Implicit Q-learning (IQL) addresses this by training a Q-function
using only dataset actions through a modified Bellman backup. However, it is
unclear which policy actually attains the values represented by this implicitly
trained Q-function. In this paper, we reinterpret IQL as an actor-critic method
by generalizing the critic objective and connecting it to a
behavior-regularized implicit actor. This generalization shows how the induced
actor balances reward maximization and divergence from the behavior policy,
with the specific loss choice determining the nature of this tradeoff. Notably,
this actor can exhibit complex and multimodal characteristics, suggesting
issues with the conditional Gaussian actor fit with advantage weighted
regression (AWR) used in prior methods. Instead, we propose using samples from
a diffusion parameterized behavior policy and weights computed from the critic
to then importance sampled our intended policy. We introduce Implicit Diffusion
Q-learning (IDQL), combining our general IQL critic with the policy extraction
method. IDQL maintains the ease of implementation of IQL while outperforming
prior offline RL methods and demonstrating robustness to hyperparameters. Code
is available at https://github.com/philippe-eecs/IDQL. <br><br></div></a>
</td></tr>
<tr id=" 24 " class="entry"><td>
<a onclick="toggleVisibility('abstract24');">[|&bull;|]</a><a href="http://arxiv.org/abs/2307.10142v1">
<b/> Benchmarking Potential Based Rewards for Learning Humanoid Locomotion (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2307.10142v1">
Se Hwan Jeon, Steve Heim, Charles Khazoom, Sangbae Kim &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract24');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract24');"><div id="abstract24" style="font-size: 1rem; color:black; display: none">
<u><I> 2307.10142v1 &nbsp; - &nbsp;
{humanoid}
{optimal}
{reinforcement}
{robot}
</I></u><br> The main challenge in developing effective reinforcement learning (RL)
pipelines is often the design and tuning the reward functions. Well-designed
shaping reward can lead to significantly faster learning. Naively formulated
rewards, however, can conflict with the desired behavior and result in
overfitting or even erratic performance if not properly tuned. In theory, the
broad class of potential based reward shaping (PBRS) can help guide the
learning process without affecting the optimal policy. Although several studies
have explored the use of potential based reward shaping to accelerate learning
convergence, most have been limited to grid-worlds and low-dimensional systems,
and RL in robotics has predominantly relied on standard forms of reward
shaping. In this paper, we benchmark standard forms of shaping with PBRS for a
humanoid robot. We find that in this high-dimensional system, PBRS has only
marginal benefits in convergence speed. However, the PBRS reward terms are
significantly more robust to scaling than typical reward shaping approaches,
and thus easier to tune. <br><br></div></a>
</td></tr>
<tr id=" 25 " class="entry"><td>
<a onclick="toggleVisibility('abstract25');">[|&bull;|]</a><a href="http://arxiv.org/abs/2302.05103v3">
<b/> Controllability-Aware Unsupervised Skill Discovery (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2302.05103v3">
Seohong Park, Kimin Lee, Youngwoon Lee, Pieter Abbeel &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract25');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract25');"><div id="abstract25" style="font-size: 1rem; color:black; display: none">
<u><I> 2302.05103v3 &nbsp; - &nbsp;
{control}
{robot}
{skill}
{unsupervised}
</I></u><br> One of the key capabilities of intelligent agents is the ability to discover
useful skills without external supervision. However, the current unsupervised
skill discovery methods are often limited to acquiring simple, easy-to-learn
skills due to the lack of incentives to discover more complex, challenging
behaviors. We introduce a novel unsupervised skill discovery method,
Controllability-aware Skill Discovery (CSD), which actively seeks complex,
hard-to-control skills without supervision. The key component of CSD is a
controllability-aware distance function, which assigns larger values to state
transitions that are harder to achieve with the current skills. Combined with
distance-maximizing skill discovery, CSD progressively learns more challenging
skills over the course of training as our jointly trained distance function
reduces rewards for easy-to-achieve skills. Our experimental results in six
robotic manipulation and locomotion environments demonstrate that CSD can
discover diverse complex skills including object manipulation and locomotion
skills with no supervision, significantly outperforming prior unsupervised
skill discovery methods. Videos and code are available at
https://seohong.me/projects/csd/ <br><br></div></a>
</td></tr>
<tr id=" 26 " class="entry"><td>
<a onclick="toggleVisibility('abstract26');">[|&bull;|]</a><a href="http://arxiv.org/abs/2302.03921v2">
<b/> Predictable MDP Abstraction for Unsupervised Model-Based RL (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2302.03921v2">
Seohong Park, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract26');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract26');"><div id="abstract26" style="font-size: 1rem; color:black; display: none">
<u><I> 2302.03921v2 &nbsp; - &nbsp;
{control}
{model-based}
{reinforcement}
{unsupervised}
</I></u><br> A key component of model-based reinforcement learning (RL) is a dynamics
model that predicts the outcomes of actions. Errors in this predictive model
can degrade the performance of model-based controllers, and complex Markov
decision processes (MDPs) can present exceptionally difficult prediction
problems. To mitigate this issue, we propose predictable MDP abstraction (PMA):
instead of training a predictive model on the original MDP, we train a model on
a transformed MDP with a learned action space that only permits predictable,
easy-to-model actions, while covering the original state-action space as much
as possible. As a result, model learning becomes easier and more accurate,
which allows robust, stable model-based planning or model-based RL. This
transformation is learned in an unsupervised manner, before any task is
specified by the user. Downstream tasks can then be solved with model-based
control in a zero-shot fashion, without additional environment interactions. We
theoretically analyze PMA and empirically demonstrate that PMA leads to
significant improvements over prior unsupervised model-based RL approaches in a
range of benchmark environments. Our code and videos are available at
https://seohong.me/projects/pma/ <br><br></div></a>
</td></tr>
<tr id=" 27 " class="entry"><td>
<a onclick="toggleVisibility('abstract27');">[|&bull;|]</a><a href="http://arxiv.org/abs/2306.14884v2">
<b/> Learning to Modulate pre-trained Models in RL (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2306.14884v2">
Thomas Schmied, Markus Hofmarcher, Fabian Paischer, Razvan Pascanu, Sepp Hochreiter &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract27');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract27');"><div id="abstract27" style="font-size: 1rem; color:black; display: none">
<u><I> 2306.14884v2 &nbsp; - &nbsp;
{control}
{meta}
{reinforcement}
{robot}
{skill}
</I></u><br> Reinforcement Learning (RL) has been successful in various domains like
robotics, game playing, and simulation. While RL agents have shown impressive
capabilities in their specific tasks, they insufficiently adapt to new tasks.
In supervised learning, this adaptation problem is addressed by large-scale
pre-training followed by fine-tuning to new down-stream tasks. Recently,
pre-training on multiple tasks has been gaining traction in RL. However,
fine-tuning a pre-trained model often suffers from catastrophic forgetting.
That is, the performance on the pre-training tasks deteriorates when
fine-tuning on new tasks. To investigate the catastrophic forgetting
phenomenon, we first jointly pre-train a model on datasets from two benchmark
suites, namely Meta-World and DMControl. Then, we evaluate and compare a
variety of fine-tuning methods prevalent in natural language processing, both
in terms of performance on new tasks, and how well performance on pre-training
tasks is retained. Our study shows that with most fine-tuning approaches, the
performance on pre-training tasks deteriorates significantly. Therefore, we
propose a novel method, Learning-to-Modulate (L2M), that avoids the degradation
of learned skills by modulating the information flow of the frozen pre-trained
model via a learnable modulation pool. Our method achieves state-of-the-art
performance on the Continual-World benchmark, while retaining performance on
the pre-training tasks. Finally, to aid future research in this area, we
release a dataset encompassing 50 Meta-World and 16 DMControl tasks. <br><br></div></a>
</td></tr>
<tr id=" 28 " class="entry"><td>
<a onclick="toggleVisibility('abstract28');">[|&bull;|]</a><a href="http://arxiv.org/abs/2306.02865v5">
<b/> Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy
Actor-Critic (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2306.02865v5">
Tianying Ji, Yu Luo, Fuchun Sun, Xianyuan Zhan, Jianwei Zhang, Huazhe Xu &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract28');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract28');"><div id="abstract28" style="font-size: 1rem; color:black; display: none">
<u><I> 2306.02865v5 &nbsp; - &nbsp;
{control}
{deep}
{exploration}
{off-policy}
{optimal}
{reinforcement}
{replay}
{robot}
</I></u><br> Learning high-quality $Q$-value functions plays a key role in the success of
many modern off-policy deep reinforcement learning (RL) algorithms. Previous
works primarily focus on addressing the value overestimation issue, an outcome
of adopting function approximators and off-policy learning. Deviating from the
common viewpoint, we observe that $Q$-values are often underestimated in the
latter stage of the RL training process, potentially hindering policy learning
and reducing sample efficiency. We find that such a long-neglected phenomenon
is often related to the use of inferior actions from the current policy in
Bellman updates as compared to the more optimal action samples in the replay
buffer. To address this issue, our insight is to incorporate sufficient
exploitation of past successes while maintaining exploration optimism. We
propose the Blended Exploitation and Exploration (BEE) operator, a simple yet
effective approach that updates $Q$-value using both historical best-performing
actions and the current policy. Based on BEE, the resulting practical algorithm
BAC outperforms state-of-the-art methods in over 50 continuous control tasks
and achieves strong performance in failure-prone scenarios and real-world robot
tasks. Benchmark results and videos are available at
https://jity16.github.io/BEE/. <br><br></div></a>
</td></tr>
<tr id=" 29 " class="entry"><td>
<a onclick="toggleVisibility('abstract29');">[|&bull;|]</a><a href="http://arxiv.org/abs/2304.13653v2">
<b/> Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement
Learning (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2304.13653v2">
Tuomas Haarnoja, Ben Moran, Guy Lever, Sandy H. Huang, Dhruva Tirumala, Jan Humplik, Markus Wulfmeier, Saran Tunyasuvunakool, Noah Y. Siegel, Roland Hafner, Michael Bloesch, Kristian Hartikainen, Arunkumar Byravan, Leonard Hasenclever, Yuval Tassa, Fereshteh Sadeghi, Nathan Batchelor, Federico Casarini, Stefano Saliceti, Charles Game, Neil Sreendra, Kushal Patel, Marlon Gwira, Andrea Huber, Nicole Hurley, Francesco Nori, Raia Hadsell, Nicolas Heess &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract29');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract29');"><div id="abstract29" style="font-size: 1rem; color:black; display: none">
<u><I> 2304.13653v2 &nbsp; - &nbsp;
{control}
{deep}
{humanoid}
{reinforcement}
{robot}
{skill}
{transfer}
</I></u><br> We investigate whether Deep Reinforcement Learning (Deep RL) is able to
synthesize sophisticated and safe movement skills for a low-cost, miniature
humanoid robot that can be composed into complex behavioral strategies in
dynamic environments. We used Deep RL to train a humanoid robot with 20
actuated joints to play a simplified one-versus-one (1v1) soccer game. The
resulting agent exhibits robust and dynamic movement skills such as rapid fall
recovery, walking, turning, kicking and more; and it transitions between them
in a smooth, stable, and efficient manner. The agent's locomotion and tactical
behavior adapts to specific game contexts in a way that would be impractical to
manually design. The agent also developed a basic strategic understanding of
the game, and learned, for instance, to anticipate ball movements and to block
opponent shots. Our agent was trained in simulation and transferred to real
robots zero-shot. We found that a combination of sufficiently high-frequency
control, targeted dynamics randomization, and perturbations during training in
simulation enabled good-quality transfer. Although the robots are inherently
fragile, basic regularization of the behavior during training led the robots to
learn safe and effective movements while still performing in a dynamic and
agile way -- well beyond what is intuitively expected from the robot. Indeed,
in experiments, they walked 181% faster, turned 302% faster, took 63% less time
to get up, and kicked a ball 34% faster than a scripted baseline, while
efficiently combining the skills to achieve the longer term objectives. <br><br></div></a>
</td></tr>
<tr id=" 30 " class="entry"><td>
<a onclick="toggleVisibility('abstract30');">[|&bull;|]</a><a href="http://arxiv.org/abs/2302.06675v4">
<b/> Symbolic Discovery of Optimization Algorithms (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2302.06675v4">
Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, Quoc V. Le &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract30');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract30');"><div id="abstract30" style="font-size: 1rem; color:black; display: none">
<u><I> 2302.06675v4 &nbsp; - &nbsp;
{contrastive}
{deep}
{imitation}
{sparse}
</I></u><br> We present a method to formulate algorithm discovery as program search, and
apply it to discover optimization algorithms for deep neural network training.
We leverage efficient search techniques to explore an infinite and sparse
program space. To bridge the large generalization gap between proxy and target
tasks, we also introduce program selection and simplification strategies. Our
method discovers a simple and effective optimization algorithm, $\textbf{Lion}$
($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$).
It is more memory-efficient than Adam as it only keeps track of the momentum.
Different from adaptive optimizers, its update has the same magnitude for each
parameter calculated through the sign operation. We compare Lion with widely
used optimizers, such as Adam and Adafactor, for training a variety of models
on different tasks. On image classification, Lion boosts the accuracy of ViT by
up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On
vision-language contrastive learning, we achieve 88.3% $\textit{zero-shot}$ and
91.1% $\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best
results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms
Adam by achieving a better FID score and reducing the training compute by up to
2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion
exhibits a similar or better performance compared to Adam. Our analysis of Lion
reveals that its performance gain grows with the training batch size. It also
requires a smaller learning rate than Adam due to the larger norm of the update
produced by the sign function. Additionally, we examine the limitations of Lion
and identify scenarios where its improvements are small or not statistically
significant. Lion is also successfully deployed in production systems such as
Google search ads CTR model. <br><br></div></a>
</td></tr>
<tr id=" 31 " class="entry"><td>
<a onclick="toggleVisibility('abstract31');">[|&bull;|]</a><a href="http://arxiv.org/abs/2310.04413v2">
<b/> Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced
Datasets (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2310.04413v2">
Zhang-Wei Hong, Aviral Kumar, Sathwik Karnik, Abhishek Bhandwaldar, Akash Srivastava, Joni Pajarinen, Romain Laroche, Abhishek Gupta, Pulkit Agrawal &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract31');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract31');"><div id="abstract31" style="font-size: 1rem; color:black; display: none">
<u><I> 2310.04413v2 &nbsp; - &nbsp;
{offline}
{optimal}
{reinforcement}
</I></u><br> Offline policy learning is aimed at learning decision-making policies using
existing datasets of trajectories without collecting additional data. The
primary motivation for using reinforcement learning (RL) instead of supervised
learning techniques such as behavior cloning is to find a policy that achieves
a higher average return than the trajectories constituting the dataset.
However, we empirically find that when a dataset is dominated by suboptimal
trajectories, state-of-the-art offline RL algorithms do not substantially
improve over the average return of trajectories in the dataset. We argue this
is due to an assumption made by current offline RL algorithms of staying close
to the trajectories in the dataset. If the dataset primarily consists of
sub-optimal trajectories, this assumption forces the policy to mimic the
suboptimal actions. We overcome this issue by proposing a sampling strategy
that enables the policy to only be constrained to ``good data" rather than all
actions in the dataset (i.e., uniform sampling). We present a realization of
the sampling strategy and an algorithm that can be used as a plug-and-play
module in standard offline RL algorithms. Our evaluation demonstrates
significant performance gains in 72 imbalanced datasets, D4RL dataset, and
across three different offline RL algorithms. Code is available at
https://github.com/Improbable-AI/dw-offline-rl. <br><br></div></a>
</td></tr>
<tr id=" 32 " class="entry"><td>
<a onclick="toggleVisibility('abstract32');">[|&bull;|]</a><a href="http://arxiv.org/abs/2302.09450v2">
<b/> Robust and Versatile Bipedal Jumping Control through Reinforcement
Learning (2023)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2302.09450v2">
Zhongyu Li, Xue Bin Peng, Pieter Abbeel, Sergey Levine, Glen Berseth, Koushil Sreenath &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract32');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract32');"><div id="abstract32" style="font-size: 1rem; color:black; display: none">
<u><I> 2302.09450v2 &nbsp; - &nbsp;
{control}
{reinforcement}
{robot}
{transfer}
</I></u><br> This work aims to push the limits of agility for bipedal robots by enabling a
torque-controlled bipedal robot to perform robust and versatile dynamic jumps
in the real world. We present a reinforcement learning framework for training a
robot to accomplish a large variety of jumping tasks, such as jumping to
different locations and directions. To improve performance on these challenging
tasks, we develop a new policy structure that encodes the robot's long-term
input/output (I/O) history while also providing direct access to a short-term
I/O history. In order to train a versatile jumping policy, we utilize a
multi-stage training scheme that includes different training stages for
different objectives. After multi-stage training, the policy can be directly
transferred to a real bipedal Cassie robot. Training on different tasks and
exploring more diverse scenarios lead to highly robust policies that can
exploit the diverse set of learned maneuvers to recover from perturbations or
poor landings during real-world deployment. Such robustness in the proposed
policy enables Cassie to succeed in completing a variety of challenging jump
tasks in the real world, such as standing long jumps, jumping onto elevated
platforms, and multi-axes jumps. <br><br></div></a>
</td></tr>
<tr id=" 33 " class="entry"><td>
<a onclick="toggleVisibility('abstract33');">[|&bull;|]</a><a href="http://arxiv.org/abs/2203.15755v1">
<b/> Demonstration-Bootstrapped Autonomous Practicing via Multi-Task
Reinforcement Learning (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2203.15755v1">
Abhishek Gupta, Corey Lynch, Brandon Kinman, Garrett Peake, Sergey Levine, Karol Hausman &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract33');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract33');"><div id="abstract33" style="font-size: 1rem; color:black; display: none">
<u><I> 2203.15755v1 &nbsp; - &nbsp;
{multi-task}
{reinforcement}
{robot}
</I></u><br> Reinforcement learning systems have the potential to enable continuous
improvement in unstructured environments, leveraging data collected
autonomously. However, in practice these systems require significant amounts of
instrumentation or human intervention to learn in the real world. In this work,
we propose a system for reinforcement learning that leverages multi-task
reinforcement learning bootstrapped with prior data to enable continuous
autonomous practicing, minimizing the number of resets needed while being able
to learn temporally extended behaviors. We show how appropriately provided
prior data can help bootstrap both low-level multi-task policies and strategies
for sequencing these tasks one after another to enable learning with minimal
resets. This mechanism enables our robotic system to practice with minimal
human intervention at training time while being able to solve long horizon
tasks at test time. We show the efficacy of the proposed system on a
challenging kitchen manipulation task both in simulation and in the real world,
demonstrating the ability to practice autonomously in order to solve temporally
extended problems. <br><br></div></a>
</td></tr>
<tr id=" 34 " class="entry"><td>
<a onclick="toggleVisibility('abstract34');">[|&bull;|]</a><a href="http://arxiv.org/abs/2204.07404v2">
<b/> Divide & Conquer Imitation Learning (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2204.07404v2">
Alexandre Chenu, Nicolas Perrin-Gilbert, Olivier Sigaud &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract34');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract34');"><div id="abstract34" style="font-size: 1rem; color:black; display: none">
<u><I> 2204.07404v2 &nbsp; - &nbsp;
{deep}
{goal-conditioned}
{imitation}
{reinforcement}
{robot}
{skill}
{sparse}
</I></u><br> When cast into the Deep Reinforcement Learning framework, many robotics tasks
require solving a long horizon and sparse reward problem, where learning
algorithms struggle. In such context, Imitation Learning (IL) can be a powerful
approach to bootstrap the learning process. However, most IL methods require
several expert demonstrations which can be prohibitively difficult to acquire.
Only a handful of IL algorithms have shown efficiency in the context of an
extreme low expert data regime where a single expert demonstration is
available. In this paper, we present a novel algorithm designed to imitate
complex robotic tasks from the states of an expert trajectory. Based on a
sequential inductive bias, our method divides the complex task into smaller
skills. The skills are learned into a goal-conditioned policy that is able to
solve each skill individually and chain skills to solve the entire task. We
show that our method imitates a non-holonomic navigation task and scales to a
complex simulated robotic manipulation task with very high sample efficiency. <br><br></div></a>
</td></tr>
<tr id=" 35 " class="entry"><td>
<a onclick="toggleVisibility('abstract35');">[|&bull;|]</a><a href="http://arxiv.org/abs/2203.01148v3">
<b/> Reactive Stepping for Humanoid Robots using Reinforcement Learning:
Application to Standing Push Recovery on the Exoskeleton Atalante (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2203.01148v3">
Alexis Duburcq, Fabian Schramm, Guilhem Bo&#233;ris, Nicolas Bredeche, Yann Chevaleyre &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract35');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract35');"><div id="abstract35" style="font-size: 1rem; color:black; display: none">
<u><I> 2203.01148v3 &nbsp; - &nbsp;
{exoskeleton}
{imitation}
{reinforcement}
{robot}
{transfer}
</I></u><br> State-of-the-art reinforcement learning is now able to learn versatile
locomotion, balancing and push-recovery capabilities for bipedal robots in
simulation. Yet, the reality gap has mostly been overlooked and the simulated
results hardly transfer to real hardware. Either it is unsuccessful in practice
because the physics is over-simplified and hardware limitations are ignored, or
regularity is not guaranteed, and unexpected hazardous motions can occur. This
paper presents a reinforcement learning framework capable of learning robust
standing push recovery for bipedal robots that smoothly transfer to reality,
providing only instantaneous proprioceptive observations. By combining original
termination conditions and policy smoothness conditioning, we achieve stable
learning, sim-to-real transfer and safety using a policy without memory nor
explicit history. Reward engineering is then used to give insights into how to
keep balance. We demonstrate its performance in reality on the lower-limb
medical exoskeleton Atalante. <br><br></div></a>
</td></tr>
<tr id=" 36 " class="entry"><td>
<a onclick="toggleVisibility('abstract36');">[|&bull;|]</a><a href="http://arxiv.org/abs/2211.07638v1">
<b/> Legged Locomotion in Challenging Terrains using Egocentric Vision (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2211.07638v1">
Ananye Agarwal, Ashish Kumar, Jitendra Malik, Deepak Pathak &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract36');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract36');"><div id="abstract36" style="font-size: 1rem; color:black; display: none">
<u><I> 2211.07638v1 &nbsp; - &nbsp;
{distill}
{reinforcement}
{robot}
{transfer}
</I></u><br> Animals are capable of precise and agile locomotion using vision. Replicating
this ability has been a long-standing goal in robotics. The traditional
approach has been to decompose this problem into elevation mapping and foothold
planning phases. The elevation mapping, however, is susceptible to failure and
large noise artifacts, requires specialized hardware, and is biologically
implausible. In this paper, we present the first end-to-end locomotion system
capable of traversing stairs, curbs, stepping stones, and gaps. We show this
result on a medium-sized quadruped robot using a single front-facing depth
camera. The small size of the robot necessitates discovering specialized gait
patterns not seen elsewhere. The egocentric camera requires the policy to
remember past information to estimate the terrain under its hind feet. We train
our policy in simulation. Training has two phases - first, we train a policy
using reinforcement learning with a cheap-to-compute variant of depth image and
then in phase 2 distill it into the final policy that uses depth using
supervised learning. The resulting policy transfers to the real world and is
able to run in real-time on the limited compute of the robot. It can traverse a
large variety of terrain while being robust to perturbations like pushes,
slippery surfaces, and rocky terrain. Videos are at
https://vision-locomotion.github.io <br><br></div></a>
</td></tr>
<tr id=" 37 " class="entry"><td>
<a onclick="toggleVisibility('abstract37');">[|&bull;|]</a><a href="http://arxiv.org/abs/2210.10765v1">
<b/> When to Ask for Help: Proactive Interventions in Autonomous
Reinforcement Learning (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2210.10765v1">
Annie Xie, Fahim Tajwar, Archit Sharma, Chelsea Finn &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract37');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract37');"><div id="abstract37" style="font-size: 1rem; color:black; display: none">
<u><I> 2210.10765v1 &nbsp; - &nbsp;
{control}
{reinforcement}
{robot}
</I></u><br> A long-term goal of reinforcement learning is to design agents that can
autonomously interact and learn in the world. A critical challenge to such
autonomy is the presence of irreversible states which require external
assistance to recover from, such as when a robot arm has pushed an object off
of a table. While standard agents require constant monitoring to decide when to
intervene, we aim to design proactive agents that can request human
intervention only when needed. To this end, we propose an algorithm that
efficiently learns to detect and avoid states that are irreversible, and
proactively asks for help in case the agent does enter them. On a suite of
continuous control environments with unknown irreversible states, we find that
our algorithm exhibits better sample- and intervention-efficiency compared to
existing methods. Our code is publicly available at
https://sites.google.com/view/proactive-interventions <br><br></div></a>
</td></tr>
<tr id=" 38 " class="entry"><td>
<a onclick="toggleVisibility('abstract38');">[|&bull;|]</a><a href="http://arxiv.org/abs/2209.05333v1">
<b/> Self-supervised Sequential Information Bottleneck for Robust Exploration
in Deep Reinforcement Learning (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2209.05333v1">
Bang You, Jingming Xie, Youping Chen, Jan Peters, Oleg Arenz &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract38');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract38');"><div id="abstract38" style="font-size: 1rem; color:black; display: none">
<u><I> 2209.05333v1 &nbsp; - &nbsp;
{control}
{entropy}
{exploration}
{intrinsic}
{reinforcement}
{sparse}
</I></u><br> Effective exploration is critical for reinforcement learning agents in
environments with sparse rewards or high-dimensional state-action spaces.
Recent works based on state-visitation counts, curiosity and
entropy-maximization generate intrinsic reward signals to motivate the agent to
visit novel states for exploration. However, the agent can get distracted by
perturbations to sensor inputs that contain novel but task-irrelevant
information, e.g. due to sensor noise or changing background. In this work, we
introduce the sequential information bottleneck objective for learning
compressed and temporally coherent representations by modelling and compressing
sequential predictive information in time-series observations. For efficient
exploration in noisy environments, we further construct intrinsic rewards that
capture task-relevant state novelty based on the learned representations. We
derive a variational upper bound of our sequential information bottleneck
objective for practical optimization and provide an information-theoretic
interpretation of the derived upper bound. Our experiments on a set of
challenging image-based simulated control tasks show that our method achieves
better sample efficiency, and robustness to both white noise and natural video
backgrounds compared to state-of-art methods based on curiosity, entropy
maximization and information-gain. <br><br></div></a>
</td></tr>
<tr id=" 39 " class="entry"><td>
<a onclick="toggleVisibility('abstract39');">[|&bull;|]</a><a href="http://arxiv.org/abs/2206.03378v2">
<b/> Imitating Past Successes can be Very Suboptimal (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2206.03378v2">
Benjamin Eysenbach, Soumith Udatha, Sergey Levine, Ruslan Salakhutdinov &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract39');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract39');"><div id="abstract39" style="font-size: 1rem; color:black; display: none">
<u><I> 2206.03378v2 &nbsp; - &nbsp;
{imitation}
{reinforcement}
</I></u><br> Prior work has proposed a simple strategy for reinforcement learning (RL):
label experience with the outcomes achieved in that experience, and then
imitate the relabeled experience. These outcome-conditioned imitation learning
methods are appealing because of their simplicity, strong performance, and
close ties with supervised learning. However, it remains unclear how these
methods relate to the standard RL objective, reward maximization. In this
paper, we formally relate outcome-conditioned imitation learning to reward
maximization, drawing a precise relationship between the learned policy and
Q-values and explaining the close connections between these methods and prior
EM-based policy search methods. This analysis shows that existing
outcome-conditioned imitation learning methods do not necessarily improve the
policy, but a simple modification results in a method that does guarantee
policy improvement, under some assumptions. <br><br></div></a>
</td></tr>
<tr id=" 40 " class="entry"><td>
<a onclick="toggleVisibility('abstract40');">[|&bull;|]</a><a href="http://arxiv.org/abs/2206.07568v2">
<b/> Contrastive Learning as Goal-Conditioned Reinforcement Learning (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2206.07568v2">
Benjamin Eysenbach, Tianjun Zhang, Ruslan Salakhutdinov, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract40');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract40');"><div id="abstract40" style="font-size: 1rem; color:black; display: none">
<u><I> 2206.07568v2 &nbsp; - &nbsp;
{contrastive}
{deep}
{goal-conditioned}
{offline}
{reinforcement}
</I></u><br> In reinforcement learning (RL), it is easier to solve a task if given a good
representation. While deep RL should automatically acquire such good
representations, prior work often finds that learning representations in an
end-to-end fashion is unstable and instead equip RL algorithms with additional
representation learning parts (e.g., auxiliary losses, data augmentation). How
can we design RL algorithms that directly acquire good representations? In this
paper, instead of adding representation learning parts to an existing RL
algorithm, we show (contrastive) representation learning methods can be cast as
RL algorithms in their own right. To do this, we build upon prior work and
apply contrastive representation learning to action-labeled trajectories, in
such a way that the (inner product of) learned representations exactly
corresponds to a goal-conditioned value function. We use this idea to
reinterpret a prior RL method as performing contrastive learning, and then use
the idea to propose a much simpler method that achieves similar performance.
Across a range of goal-conditioned RL tasks, we demonstrate that contrastive RL
methods achieve higher success rates than prior non-contrastive methods,
including in the offline RL setting. We also show that contrastive RL
outperforms prior methods on image-based tasks, without using data augmentation
or auxiliary objectives. <br><br></div></a>
</td></tr>
<tr id=" 41 " class="entry"><td>
<a onclick="toggleVisibility('abstract41');">[|&bull;|]</a><a href="http://arxiv.org/abs/2202.02446v2">
<b/> Adversarially Trained Actor Critic for Offline Reinforcement Learning (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2202.02446v2">
Ching-An Cheng, Tengyang Xie, Nan Jiang, Alekh Agarwal &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract41');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract41');"><div id="abstract41" style="font-size: 1rem; color:black; display: none">
<u><I> 2202.02446v2 &nbsp; - &nbsp;
{control}
{deep}
{offline}
{reinforcement}
</I></u><br> We propose Adversarially Trained Actor Critic (ATAC), a new model-free
algorithm for offline reinforcement learning (RL) under insufficient data
coverage, based on the concept of relative pessimism. ATAC is designed as a
two-player Stackelberg game: A policy actor competes against an adversarially
trained value critic, who finds data-consistent scenarios where the actor is
inferior to the data-collection behavior policy. We prove that, when the actor
attains no regret in the two-player game, running ATAC produces a policy that
provably 1) outperforms the behavior policy over a wide range of
hyperparameters that control the degree of pessimism, and 2) competes with the
best policy covered by data with appropriately chosen hyperparameters. Compared
with existing works, notably our framework offers both theoretical guarantees
for general function approximation and a deep RL implementation scalable to
complex environments and large datasets. In the D4RL benchmark, ATAC
consistently outperforms state-of-the-art offline RL algorithms on a range of
continuous control tasks. <br><br></div></a>
</td></tr>
<tr id=" 42 " class="entry"><td>
<a onclick="toggleVisibility('abstract42');">[|&bull;|]</a><a href="http://arxiv.org/abs/2206.02126v1">
<b/> Learning Dynamics and Generalization in Reinforcement Learning (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2206.02126v1">
Clare Lyle, Mark Rowland, Will Dabney, Marta Kwiatkowska, Yarin Gal &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract42');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract42');"><div id="abstract42" style="font-size: 1rem; color:black; display: none">
<u><I> 2206.02126v1 &nbsp; - &nbsp;
{deep}
{distill}
{gradient}
{reinforcement}
</I></u><br> Solving a reinforcement learning (RL) problem poses two competing challenges:
fitting a potentially discontinuous value function, and generalizing well to
new observations. In this paper, we analyze the learning dynamics of temporal
difference algorithms to gain novel insight into the tension between these two
objectives. We show theoretically that temporal difference learning encourages
agents to fit non-smooth components of the value function early in training,
and at the same time induces the second-order effect of discouraging
generalization. We corroborate these findings in deep RL agents trained on a
range of environments, finding that neural networks trained using temporal
difference algorithms on dense reward tasks exhibit weaker generalization
between states than randomly initialized networks and networks trained with
policy gradient methods. Finally, we investigate how post-training policy
distillation may avoid this pitfall, and show that this approach improves
generalization to novel environments in the ProcGen suite and improves
robustness to input perturbations. <br><br></div></a>
</td></tr>
<tr id=" 43 " class="entry"><td>
<a onclick="toggleVisibility('abstract43');">[|&bull;|]</a><a href="http://arxiv.org/abs/2206.04114v1">
<b/> Deep Hierarchical Planning from Pixels (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2206.04114v1">
Danijar Hafner, Kuang-Huei Lee, Ian Fischer, Pieter Abbeel &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract43');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract43');"><div id="abstract43" style="font-size: 1rem; color:black; display: none">
<u><I> 2206.04114v1 &nbsp; - &nbsp;
{control}
{exploration}
{hierarchical}
{imitation}
{reinforcement}
{robot}
{sparse}
</I></u><br> Intelligent agents need to select long sequences of actions to solve complex
tasks. While humans easily break down tasks into subgoals and reach them
through millions of muscle commands, current artificial intelligence is limited
to tasks with horizons of a few hundred decisions, despite large compute
budgets. Research on hierarchical reinforcement learning aims to overcome this
limitation but has proven to be challenging, current methods rely on manually
specified goal spaces or subtasks, and no general solution exists. We introduce
Director, a practical method for learning hierarchical behaviors directly from
pixels by planning inside the latent space of a learned world model. The
high-level policy maximizes task and exploration rewards by selecting latent
goals and the low-level policy learns to achieve the goals. Despite operating
in latent space, the decisions are interpretable because the world model can
decode goals into images for visualization. Director outperforms exploration
methods on tasks with sparse rewards, including 3D maze traversal with a
quadruped robot from an egocentric camera and proprioception, without access to
the global position or top-down view that was used by prior work. Director also
learns successful behaviors across a wide range of environments, including
visual control, Atari games, and DMLab levels. <br><br></div></a>
</td></tr>
<tr id=" 44 " class="entry"><td>
<a onclick="toggleVisibility('abstract44');">[|&bull;|]</a><a href="http://arxiv.org/abs/2202.04849v2">
<b/> SAFER: Data-Efficient and Safe Reinforcement Learning via Skill
Acquisition (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2202.04849v2">
Dylan Slack, Yinlam Chow, Bo Dai, Nevan Wichers &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract44');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract44');"><div id="abstract44" style="font-size: 1rem; color:black; display: none">
<u><I> 2202.04849v2 &nbsp; - &nbsp;
{contrastive}
{deep}
{offline}
{reinforcement}
{robot}
{skill}
</I></u><br> Methods that extract policy primitives from offline demonstrations using deep
generative models have shown promise at accelerating reinforcement learning(RL)
for new tasks. Intuitively, these methods should also help to trainsafeRLagents
because they enforce useful skills. However, we identify these techniques are
not well equipped for safe policy learning because they ignore negative
experiences(e.g., unsafe or unsuccessful), focusing only on positive
experiences, which harms their ability to generalize to new tasks safely.
Rather, we model the latentsafetycontextusing principled contrastive training
on an offline dataset of demonstrations from many tasks, including both
negative and positive experiences. Using this late variable, our RL framework,
SAFEty skill pRiors (SAFER) extracts task-specific safe primitive skills to
safely and successfully generalize to new tasks. In the inference stage,
policies trained with SAFER learn to compose safe skills into successful
policies. We theoretically characterize why SAFER can enforce safe policy
learning and demonstrate its effectiveness on several complex safety-critical
robotic grasping tasks inspired by the game Operation, in which
SAFERoutperforms state-of-the-art primitive learning methods in success and
safety. <br><br></div></a>
</td></tr>
<tr id=" 45 " class="entry"><td>
<a onclick="toggleVisibility('abstract45');">[|&bull;|]</a><a href="http://arxiv.org/abs/2205.07802v1">
<b/> The Primacy Bias in Deep Reinforcement Learning (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2205.07802v1">
Evgenii Nikishin, Max Schwarzer, Pierluca D'Oro, Pierre-Luc Bacon, Aaron Courville &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract45');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract45');"><div id="abstract45" style="font-size: 1rem; color:black; display: none">
<u><I> 2205.07802v1 &nbsp; - &nbsp;
{control}
{deep}
{reinforcement}
</I></u><br> This work identifies a common flaw of deep reinforcement learning (RL)
algorithms: a tendency to rely on early interactions and ignore useful evidence
encountered later. Because of training on progressively growing datasets, deep
RL agents incur a risk of overfitting to earlier experiences, negatively
affecting the rest of the learning process. Inspired by cognitive science, we
refer to this effect as the primacy bias. Through a series of experiments, we
dissect the algorithmic aspects of deep RL that exacerbate this bias. We then
propose a simple yet generally-applicable mechanism that tackles the primacy
bias by periodically resetting a part of the agent. We apply this mechanism to
algorithms in both discrete (Atari 100k) and continuous action (DeepMind
Control Suite) domains, consistently improving their performance. <br><br></div></a>
</td></tr>
<tr id=" 46 " class="entry"><td>
<a onclick="toggleVisibility('abstract46');">[|&bull;|]</a><a href="http://arxiv.org/abs/2202.07789v1">
<b/> Safe Reinforcement Learning by Imagining the Near Future (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2202.07789v1">
Garrett Thomas, Yuping Luo, Tengyu Ma &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract46');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract46');"><div id="abstract46" style="font-size: 1rem; color:black; display: none">
<u><I> 2202.07789v1 &nbsp; - &nbsp;
{control}
{model-based}
{optimal}
{reinforcement}
</I></u><br> Safe reinforcement learning is a promising path toward applying reinforcement
learning algorithms to real-world problems, where suboptimal behaviors may lead
to actual negative consequences. In this work, we focus on the setting where
unsafe states can be avoided by planning ahead a short time into the future. In
this setting, a model-based agent with a sufficiently accurate model can avoid
unsafe states. We devise a model-based algorithm that heavily penalizes unsafe
trajectories, and derive guarantees that our algorithm can avoid unsafe states
under certain assumptions. Experiments demonstrate that our algorithm can
achieve competitive rewards with fewer safety violations in several continuous
control tasks. <br><br></div></a>
</td></tr>
<tr id=" 47 " class="entry"><td>
<a onclick="toggleVisibility('abstract47');">[|&bull;|]</a><a href="http://arxiv.org/abs/2211.13743v3">
<b/> SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended
Exploration (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2211.13743v3">
Giulia Vezzani, Dhruva Tirumala, Markus Wulfmeier, Dushyant Rao, Abbas Abdolmaleki, Ben Moran, Tuomas Haarnoja, Jan Humplik, Roland Hafner, Michael Neunert, Claudio Fantacci, Tim Hertweck, Thomas Lampe, Fereshteh Sadeghi, Nicolas Heess, Martin Riedmiller &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract47');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract47');"><div id="abstract47" style="font-size: 1rem; color:black; display: none">
<u><I> 2211.13743v3 &nbsp; - &nbsp;
{distill}
{exploration}
{imitation}
{optimal}
{reinforcement}
{skill}
{transfer}
</I></u><br> The ability to effectively reuse prior knowledge is a key requirement when
building general and flexible Reinforcement Learning (RL) agents. Skill reuse
is one of the most common approaches, but current methods have considerable
limitations.For example, fine-tuning an existing policy frequently fails, as
the policy can degrade rapidly early in training. In a similar vein,
distillation of expert behavior can lead to poor results when given sub-optimal
experts. We compare several common approaches for skill transfer on multiple
domains including changes in task and system dynamics. We identify how existing
methods can fail and introduce an alternative approach to mitigate these
problems. Our approach learns to sequence existing temporally-extended skills
for exploration but learns the final policy directly from the raw experience.
This conceptual split enables rapid adaptation and thus efficient data
collection but without constraining the final solution.It significantly
outperforms many classical methods across a suite of evaluation tasks and we
use a broad set of ablations to highlight the importance of differentc
omponents of our method. <br><br></div></a>
</td></tr>
<tr id=" 48 " class="entry"><td>
<a onclick="toggleVisibility('abstract48');">[|&bull;|]</a><a href="http://arxiv.org/abs/2202.00817v2">
<b/> Do Differentiable Simulators Give Better Policy Gradients? (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2202.00817v2">
H. J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract48');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract48');"><div id="abstract48" style="font-size: 1rem; color:black; display: none">
<u><I> 2202.00817v2 &nbsp; - &nbsp;
{control}
{gradient}
{reinforcement}
</I></u><br> Differentiable simulators promise faster computation time for reinforcement
learning by replacing zeroth-order gradient estimates of a stochastic objective
with an estimate based on first-order gradients. However, it is yet unclear
what factors decide the performance of the two estimators on complex landscapes
that involve long-horizon planning and control on physical systems, despite the
crucial relevance of this question for the utility of differentiable
simulators. We show that characteristics of certain physical systems, such as
stiffness or discontinuities, may compromise the efficacy of the first-order
estimator, and analyze this phenomenon through the lens of bias and variance.
We additionally propose an $\alpha$-order gradient estimator, with $\alpha \in
[0,1]$, which correctly utilizes exact gradients to combine the efficiency of
first-order estimates with the robustness of zero-order methods. We demonstrate
the pitfalls of traditional estimators and the advantages of the $\alpha$-order
estimator on some numerical examples. <br><br></div></a>
</td></tr>
<tr id=" 49 " class="entry"><td>
<a onclick="toggleVisibility('abstract49');">[|&bull;|]</a><a href="http://arxiv.org/abs/2204.02372v2">
<b/> Jump-Start Reinforcement Learning (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2204.02372v2">
Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan, Jos&#233;phine Simon, Matthew Bennice, Chuyuan Fu, Cong Ma, Jiantao Jiao, Sergey Levine, Karol Hausman &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract49');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract49');"><div id="abstract49" style="font-size: 1rem; color:black; display: none">
<u><I> 2204.02372v2 &nbsp; - &nbsp;
{exploration}
{imitation}
{meta}
{offline}
{on-policy}
{reinforcement}
{robot}
</I></u><br> Reinforcement learning (RL) provides a theoretical framework for continuously
improving an agent's behavior via trial and error. However, efficiently
learning policies from scratch can be very difficult, particularly for tasks
with exploration challenges. In such settings, it might be desirable to
initialize RL with an existing policy, offline data, or demonstrations.
However, naively performing such initialization in RL often works poorly,
especially for value-based methods. In this paper, we present a meta algorithm
that can use offline data, demonstrations, or a pre-existing policy to
initialize an RL policy, and is compatible with any RL approach. In particular,
we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs
two policies to solve tasks: a guide-policy, and an exploration-policy. By
using the guide-policy to form a curriculum of starting states for the
exploration-policy, we are able to efficiently improve performance on a set of
simulated robotic tasks. We show via experiments that JSRL is able to
significantly outperform existing imitation and reinforcement learning
algorithms, particularly in the small-data regime. In addition, we provide an
upper bound on the sample complexity of JSRL and show that with the help of a
guide-policy, one can improve the sample complexity for non-optimism
exploration methods from exponential in horizon to polynomial. <br><br></div></a>
</td></tr>
<tr id=" 50 " class="entry"><td>
<a onclick="toggleVisibility('abstract50');">[|&bull;|]</a><a href="http://arxiv.org/abs/2205.11790v1">
<b/> Hierarchical Planning Through Goal-Conditioned Offline Reinforcement
Learning (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2205.11790v1">
Jinning Li, Chen Tang, Masayoshi Tomizuka, Wei Zhan &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract50');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract50');"><div id="abstract50" style="font-size: 1rem; color:black; display: none">
<u><I> 2205.11790v1 &nbsp; - &nbsp;
{exploration}
{goal-conditioned}
{hierarchical}
{model-based}
{offline}
{reinforcement}
{robot}
{skill}
</I></u><br> Offline Reinforcement learning (RL) has shown potent in many safe-critical
tasks in robotics where exploration is risky and expensive. However, it still
struggles to acquire skills in temporally extended tasks. In this paper, we
study the problem of offline RL for temporally extended tasks. We propose a
hierarchical planning framework, consisting of a low-level goal-conditioned RL
policy and a high-level goal planner. The low-level policy is trained via
offline RL. We improve the offline training to deal with out-of-distribution
goals by a perturbed goal sampling process. The high-level planner selects
intermediate sub-goals by taking advantages of model-based planning methods. It
plans over future sub-goal sequences based on the learned value function of the
low-level policy. We adopt a Conditional Variational Autoencoder to sample
meaningful high-dimensional sub-goal candidates and to solve the high-level
long-term strategy optimization problem. We evaluate our proposed method in
long-horizon driving and robot navigation tasks. Experiments show that our
method outperforms baselines with different hierarchical designs and other
regular planners without hierarchy in these complex tasks. <br><br></div></a>
</td></tr>
<tr id=" 51 " class="entry"><td>
<a onclick="toggleVisibility('abstract51');">[|&bull;|]</a><a href="http://arxiv.org/abs/2204.07049v2">
<b/> Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for
Robotic Bin Picking (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2204.07049v2">
Kai Chen, Rui Cao, Stephen James, Yichuan Li, Yun-Hui Liu, Pieter Abbeel, Qi Dou &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract51');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract51');"><div id="abstract51" style="font-size: 1rem; color:black; display: none">
<u><I> 2204.07049v2 &nbsp; - &nbsp;
{robot}
</I></u><br> In this paper, we propose an iterative self-training framework for
sim-to-real 6D object pose estimation to facilitate cost-effective robotic
grasping. Given a bin-picking scenario, we establish a photo-realistic
simulator to synthesize abundant virtual data, and use this to train an initial
pose estimation network. This network then takes the role of a teacher model,
which generates pose predictions for unlabeled real data. With these
predictions, we further design a comprehensive adaptive selection scheme to
distinguish reliable results, and leverage them as pseudo labels to update a
student model for pose estimation on real data. To continuously improve the
quality of pseudo labels, we iterate the above steps by taking the trained
student model as a new teacher and re-label real data using the refined teacher
model. We evaluate our method on a public benchmark and our newly-released
dataset, achieving an ADD(-S) improvement of 11.49% and 22.62% respectively.
Our method is also able to improve robotic bin-picking success by 19.54%,
demonstrating the potential of iterative sim-to-real solutions for robotic
applications. <br><br></div></a>
</td></tr>
<tr id=" 52 " class="entry"><td>
<a onclick="toggleVisibility('abstract52');">[|&bull;|]</a><a href="http://arxiv.org/abs/2207.13082v1">
<b/> Offline Reinforcement Learning at Multiple Frequencies (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2207.13082v1">
Kaylee Burns, Tianhe Yu, Chelsea Finn, Karol Hausman &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract52');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract52');"><div id="abstract52" style="font-size: 1rem; color:black; display: none">
<u><I> 2207.13082v1 &nbsp; - &nbsp;
{control}
{offline}
{reinforcement}
{robot}
</I></u><br> Leveraging many sources of offline robot data requires grappling with the
heterogeneity of such data. In this paper, we focus on one particular aspect of
heterogeneity: learning from offline data collected at different control
frequencies. Across labs, the discretization of controllers, sampling rates of
sensors, and demands of a task of interest may differ, giving rise to a mixture
of frequencies in an aggregated dataset. We study how well offline
reinforcement learning (RL) algorithms can accommodate data with a mixture of
frequencies during training. We observe that the $Q$-value propagates at
different rates for different discretizations, leading to a number of learning
challenges for off-the-shelf offline RL. We present a simple yet effective
solution that enforces consistency in the rate of $Q$-value updates to
stabilize learning. By scaling the value of $N$ in $N$-step returns with the
discretization size, we effectively balance $Q$-value propagation, leading to
more stable convergence. On three simulated robotic control problems, we
empirically find that this simple approach outperforms na\"ive mixing by 50% on
average. <br><br></div></a>
</td></tr>
<tr id=" 53 " class="entry"><td>
<a onclick="toggleVisibility('abstract53');">[|&bull;|]</a><a href="http://arxiv.org/abs/2211.02231v1">
<b/> Residual Skill Policies: Learning an Adaptable Skill-based Action Space
for Reinforcement Learning for Robotics (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2211.02231v1">
Krishan Rana, Ming Xu, Brendan Tidd, Michael Milford, Niko S&#252;nderhauf &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract53');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract53');"><div id="abstract53" style="font-size: 1rem; color:black; display: none">
<u><I> 2211.02231v1 &nbsp; - &nbsp;
{exploration}
{reinforcement}
{robot}
{skill}
</I></u><br> Skill-based reinforcement learning (RL) has emerged as a promising strategy
to leverage prior knowledge for accelerated robot learning. Skills are
typically extracted from expert demonstrations and are embedded into a latent
space from which they can be sampled as actions by a high-level RL agent.
However, this skill space is expansive, and not all skills are relevant for a
given robot state, making exploration difficult. Furthermore, the downstream RL
agent is limited to learning structurally similar tasks to those used to
construct the skill space. We firstly propose accelerating exploration in the
skill space using state-conditioned generative models to directly bias the
high-level agent towards only sampling skills relevant to a given state based
on prior experience. Next, we propose a low-level residual policy for
fine-grained skill adaptation enabling downstream RL agents to adapt to unseen
task variations. Finally, we validate our approach across four challenging
manipulation tasks that differ from those used to build the skill space,
demonstrating our ability to learn across task variations while significantly
accelerating exploration, outperforming prior works. Code and videos are
available on our project website: https://krishanrana.github.io/reskill. <br><br></div></a>
</td></tr>
<tr id=" 54 " class="entry"><td>
<a onclick="toggleVisibility('abstract54');">[|&bull;|]</a><a href="http://arxiv.org/abs/2208.07860v1">
<b/> A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free
Reinforcement Learning (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2208.07860v1">
Laura Smith, Ilya Kostrikov, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract54');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract54');"><div id="abstract54" style="font-size: 1rem; color:black; display: none">
<u><I> 2208.07860v1 &nbsp; - &nbsp;
{control}
{deep}
{model-based}
{reinforcement}
{robot}
</I></u><br> Deep reinforcement learning is a promising approach to learning policies in
uncontrolled environments that do not require domain knowledge. Unfortunately,
due to sample inefficiency, deep RL applications have primarily focused on
simulated environments. In this work, we demonstrate that the recent
advancements in machine learning algorithms and libraries combined with a
carefully tuned robot controller lead to learning quadruped locomotion in only
20 minutes in the real world. We evaluate our approach on several indoor and
outdoor terrains which are known to be challenging for classical model-based
controllers. We observe the robot to be able to learn walking gait consistently
on all of these terrains. Finally, we evaluate our design decisions in a
simulated environment. <br><br></div></a>
</td></tr>
<tr id=" 55 " class="entry"><td>
<a onclick="toggleVisibility('abstract55');">[|&bull;|]</a><a href="http://arxiv.org/abs/2209.13046v2">
<b/> Understanding Hindsight Goal Relabeling from a Divergence Minimization
Perspective (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2209.13046v2">
Lunjun Zhang, Bradly C. Stadie &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract55');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract55');"><div id="abstract55" style="font-size: 1rem; color:black; display: none">
<u><I> 2209.13046v2 &nbsp; - &nbsp;
{imitation}
{optimal}
{reinforcement}
</I></u><br> Hindsight goal relabeling has become a foundational technique in multi-goal
reinforcement learning (RL). The essential idea is that any trajectory can be
seen as a sub-optimal demonstration for reaching its final state. Intuitively,
learning from those arbitrary demonstrations can be seen as a form of imitation
learning (IL). However, the connection between hindsight goal relabeling and
imitation learning is not well understood. In this paper, we propose a novel
framework to understand hindsight goal relabeling from a divergence
minimization perspective. Recasting the goal reaching problem in the IL
framework not only allows us to derive several existing methods from first
principles, but also provides us with the tools from IL to improve goal
reaching algorithms. Experimentally, we find that under hindsight relabeling,
Q-learning outperforms behavioral cloning (BC). Yet, a vanilla combination of
both hurts performance. Concretely, we see that the BC loss only helps when
selectively applied to actions that get the agent closer to the goal according
to the Q-function. Our framework also explains the puzzling phenomenon wherein
a reward of (-1, 0) results in significantly better performance than a (0, 1)
reward for goal reaching. <br><br></div></a>
</td></tr>
<tr id=" 56 " class="entry"><td>
<a onclick="toggleVisibility('abstract56');">[|&bull;|]</a><a href="http://arxiv.org/abs/2212.11429v3">
<b/> Automatically Bounding the Taylor Remainder Series: Tighter Bounds and
New Applications (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2212.11429v3">
Matthew Streeter, Joshua V. Dillon &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract56');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract56');"><div id="abstract56" style="font-size: 1rem; color:black; display: none">
<u><I> 2212.11429v3 &nbsp; - &nbsp;
</I></u><br> We present a new algorithm for automatically bounding the Taylor remainder
series. In the special case of a scalar function $f: \mathbb{R} \to
\mathbb{R}$, our algorithm takes as input a reference point $x_0$, trust region
$[a, b]$, and integer $k \ge 1$, and returns an interval $I$ such that $f(x) -
\sum_{i=0}^{k-1} \frac {1} {i!} f^{(i)}(x_0) (x - x_0)^i \in I (x - x_0)^k$ for
all $x \in [a, b]$. As in automatic differentiation, the function $f$ is
provided to the algorithm in symbolic form, and must be composed of known
atomic functions.
At a high level, our algorithm has two steps. First, for a variety of
commonly-used elementary functions (e.g., $\exp$, $\log$), we use
recently-developed theory to derive sharp polynomial upper and lower bounds on
the Taylor remainder series. We then recursively combine the bounds for the
elementary functions using an interval arithmetic variant of Taylor-mode
automatic differentiation. Our algorithm can make efficient use of machine
learning hardware accelerators, and we provide an open source implementation in
JAX.
We then turn our attention to applications. Most notably, in a companion
paper we use our new machinery to create the first universal
majorization-minimization optimization algorithms: algorithms that iteratively
minimize an arbitrary loss using a majorizer that is derived automatically,
rather than by hand. We also show that our automatically-derived bounds can be
used for verified global optimization and numerical integration, and to prove
sharper versions of Jensen's inequality. <br><br></div></a>
</td></tr>
<tr id=" 57 " class="entry"><td>
<a onclick="toggleVisibility('abstract57');">[|&bull;|]</a><a href="http://arxiv.org/abs/2202.00161v3">
<b/> CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2202.00161v3">
Michael Laskin, Hao Liu, Xue Bin Peng, Denis Yarats, Aravind Rajeswaran, Pieter Abbeel &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract57');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract57');"><div id="abstract57" style="font-size: 1rem; color:black; display: none">
<u><I> 2202.00161v3 &nbsp; - &nbsp;
{contrastive}
{control}
{diversity}
{entropy}
{exploration}
{intrinsic}
{reinforcement}
{skill}
{unsupervised}
</I></u><br> We introduce Contrastive Intrinsic Control (CIC), an algorithm for
unsupervised skill discovery that maximizes the mutual information between
state-transitions and latent skill vectors. CIC utilizes contrastive learning
between state-transitions and skills to learn behavior embeddings and maximizes
the entropy of these embeddings as an intrinsic reward to encourage behavioral
diversity. We evaluate our algorithm on the Unsupervised Reinforcement Learning
Benchmark, which consists of a long reward-free pre-training phase followed by
a short adaptation phase to downstream tasks with extrinsic rewards. CIC
substantially improves over prior methods in terms of adaptation efficiency,
outperforming prior unsupervised skill discovery methods by 1.79x and the next
leading overall exploration algorithm by 1.18x. <br><br></div></a>
</td></tr>
<tr id=" 58 " class="entry"><td>
<a onclick="toggleVisibility('abstract58');">[|&bull;|]</a><a href="http://arxiv.org/abs/2203.04955v2">
<b/> Temporal Difference Learning for Model Predictive Control (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2203.04955v2">
Nicklas Hansen, Xiaolong Wang, Hao Su &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract58');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract58');"><div id="abstract58" style="font-size: 1rem; color:black; display: none">
<u><I> 2203.04955v2 &nbsp; - &nbsp;
{control}
{meta}
{model-based}
</I></u><br> Data-driven model predictive control has two key advantages over model-free
methods: a potential for improved sample efficiency through model learning, and
better performance as computational budget for planning increases. However, it
is both costly to plan over long horizons and challenging to obtain an accurate
model of the environment. In this work, we combine the strengths of model-free
and model-based methods. We use a learned task-oriented latent dynamics model
for local trajectory optimization over a short horizon, and use a learned
terminal value function to estimate long-term return, both of which are learned
jointly by temporal difference learning. Our method, TD-MPC, achieves superior
sample efficiency and asymptotic performance over prior work on both state and
image-based continuous control tasks from DMControl and Meta-World. Code and
video results are available at https://nicklashansen.github.io/td-mpc. <br><br></div></a>
</td></tr>
<tr id=" 59 " class="entry"><td>
<a onclick="toggleVisibility('abstract59');">[|&bull;|]</a><a href="http://arxiv.org/abs/2206.06719v2">
<b/> Stein Variational Goal Generation for adaptive Exploration in Multi-Goal
Reinforcement Learning (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2206.06719v2">
Nicolas Castanet, Sylvain Lamprier, Olivier Sigaud &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract59');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract59');"><div id="abstract59" style="font-size: 1rem; color:black; display: none">
<u><I> 2206.06719v2 &nbsp; - &nbsp;
{exploration}
{gradient}
{reinforcement}
{sparse}
</I></u><br> In multi-goal Reinforcement Learning, an agent can share experience between
related training tasks, resulting in better generalization for new tasks at
test time. However, when the goal space has discontinuities and the reward is
sparse, a majority of goals are difficult to reach. In this context, a
curriculum over goals helps agents learn by adapting training tasks to their
current capabilities. In this work we propose Stein Variational Goal Generation
(SVGG), which samples goals of intermediate difficulty for the agent, by
leveraging a learned predictive model of its goal reaching capabilities. The
distribution of goals is modeled with particles that are attracted in areas of
appropriate difficulty using Stein Variational Gradient Descent. We show that
SVGG outperforms state-of-the-art multi-goal Reinforcement Learning methods in
terms of success coverage in hard exploration problems, and demonstrate that it
is endowed with a useful recovery property when the environment changes. <br><br></div></a>
</td></tr>
<tr id=" 60 " class="entry"><td>
<a onclick="toggleVisibility('abstract60');">[|&bull;|]</a><a href="http://arxiv.org/abs/2211.11644v1">
<b/> Optimization-Based Control for Dynamic Legged Robots (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2211.11644v1">
Patrick M. Wensing, Michael Posa, Yue Hu, Adrien Escande, Nicolas Mansard, Andrea Del Prete &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract60');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract60');"><div id="abstract60" style="font-size: 1rem; color:black; display: none">
<u><I> 2211.11644v1 &nbsp; - &nbsp;
{control}
{humanoid}
{model-based}
{optimal}
{robot}
</I></u><br> In a world designed for legs, quadrupeds, bipeds, and humanoids have the
opportunity to impact emerging robotics applications from logistics, to
agriculture, to home assistance. The goal of this survey is to cover the recent
progress toward these applications that has been driven by model-based
optimization for the real-time generation and control of movement. The majority
of the research community has converged on the idea of generating locomotion
control laws by solving an optimal control problem (OCP) in either a
model-based or data-driven manner. However, solving the most general of these
problems online remains intractable due to complexities from intermittent
unidirectional contacts with the environment, and from the many degrees of
freedom of legged robots. This survey covers methods that have been pursued to
make these OCPs computationally tractable, with specific focus on how
environmental contacts are treated, how the model can be simplified, and how
these choices affect the numerical solution methods employed. The survey
focuses on model-based optimization, covering its recent use in a stand alone
fashion, and suggesting avenues for combination with learning-based
formulations to further accelerate progress in this growing field. <br><br></div></a>
</td></tr>
<tr id=" 61 " class="entry"><td>
<a onclick="toggleVisibility('abstract61');">[|&bull;|]</a><a href="http://arxiv.org/abs/2205.00824v1">
<b/> Exploration in Deep Reinforcement Learning: A Survey (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2205.00824v1">
Pawel Ladosz, Lilian Weng, Minwoo Kim, Hyondong Oh &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract61');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract61');"><div id="abstract61" style="font-size: 1rem; color:black; display: none">
<u><I> 2205.00824v1 &nbsp; - &nbsp;
{deep}
{exploration}
{imitation}
{reinforcement}
{sparse}
</I></u><br> This paper reviews exploration techniques in deep reinforcement learning.
Exploration techniques are of primary importance when solving sparse reward
problems. In sparse reward problems, the reward is rare, which means that the
agent will not find the reward often by acting randomly. In such a scenario, it
is challenging for reinforcement learning to learn rewards and actions
association. Thus more sophisticated exploration methods need to be devised.
This review provides a comprehensive overview of existing exploration
approaches, which are categorized based on the key contributions as follows
reward novel states, reward diverse behaviours, goal-based methods,
probabilistic methods, imitation-based methods, safe exploration and
random-based methods. Then, the unsolved challenges are discussed to provide
valuable future research directions. Finally, the approaches of different
categories are compared in terms of complexity, computational effort and
overall performance. <br><br></div></a>
</td></tr>
<tr id=" 62 " class="entry"><td>
<a onclick="toggleVisibility('abstract62');">[|&bull;|]</a><a href="http://arxiv.org/abs/2206.00484v2">
<b/> DEP-RL: Embodied Exploration for Reinforcement Learning in Overactuated
and Musculoskeletal Systems (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2206.00484v2">
Pierre Schumacher, Daniel H&#228;ufle, Dieter B&#252;chler, Syn Schmitt, Georg Martius &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract62');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract62');"><div id="abstract62" style="font-size: 1rem; color:black; display: none">
<u><I> 2206.00484v2 &nbsp; - &nbsp;
{diversity}
{exploration}
{reinforcement}
</I></u><br> Muscle-actuated organisms are capable of learning an unparalleled diversity
of dexterous movements despite their vast amount of muscles. Reinforcement
learning (RL) on large musculoskeletal models, however, has not been able to
show similar performance. We conjecture that ineffective exploration in large
overactuated action spaces is a key problem. This is supported by the finding
that common exploration noise strategies are inadequate in synthetic examples
of overactuated systems. We identify differential extrinsic plasticity (DEP), a
method from the domain of self-organization, as being able to induce
state-space covering exploration within seconds of interaction. By integrating
DEP into RL, we achieve fast learning of reaching and locomotion in
musculoskeletal systems, outperforming current approaches in all considered
tasks in sample efficiency and robustness. <br><br></div></a>
</td></tr>
<tr id=" 63 " class="entry"><td>
<a onclick="toggleVisibility('abstract63');">[|&bull;|]</a><a href="http://arxiv.org/abs/2212.12809v3">
<b/> Understanding the Complexity Gains of Single-Task RL with a Curriculum (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2212.12809v3">
Qiyang Li, Yuexiang Zhai, Yi Ma, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract63');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract63');"><div id="abstract63" style="font-size: 1rem; color:black; display: none">
<u><I> 2212.12809v3 &nbsp; - &nbsp;
{exploration}
{multi-task}
{reinforcement}
{robot}
</I></u><br> Reinforcement learning (RL) problems can be challenging without well-shaped
rewards. Prior work on provably efficient RL methods generally proposes to
address this issue with dedicated exploration strategies. However, another way
to tackle this challenge is to reformulate it as a multi-task RL problem, where
the task space contains not only the challenging task of interest but also
easier tasks that implicitly function as a curriculum. Such a reformulation
opens up the possibility of running existing multi-task RL methods as a more
efficient alternative to solving a single challenging task from scratch. In
this work, we provide a theoretical framework that reformulates a single-task
RL problem as a multi-task RL problem defined by a curriculum. Under mild
regularity conditions on the curriculum, we show that sequentially solving each
task in the multi-task RL problem is more computationally efficient than
solving the original single-task problem, without any explicit exploration
bonuses or other exploration strategies. We also show that our theoretical
insights can be translated into an effective practical learning algorithm that
can accelerate curriculum learning on simulated robotic tasks. <br><br></div></a>
</td></tr>
<tr id=" 64 " class="entry"><td>
<a onclick="toggleVisibility('abstract64');">[|&bull;|]</a><a href="http://arxiv.org/abs/2203.01387v3">
<b/> A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open
Problems (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2203.01387v3">
Rafael Figueiredo Prudencio, Marcos R. O. A. Maximo, Esther Luna Colombini &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract64');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract64');"><div id="abstract64" style="font-size: 1rem; color:black; display: none">
<u><I> 2203.01387v3 &nbsp; - &nbsp;
{control}
{deep}
{offline}
{reinforcement}
{robot}
</I></u><br> With the widespread adoption of deep learning, reinforcement learning (RL)
has experienced a dramatic increase in popularity, scaling to previously
intractable problems, such as playing complex games from pixel observations,
sustaining conversations with humans, and controlling robotic agents. However,
there is still a wide range of domains inaccessible to RL due to the high cost
and danger of interacting with the environment. Offline RL is a paradigm that
learns exclusively from static datasets of previously collected interactions,
making it feasible to extract policies from large and diverse training
datasets. Effective offline RL algorithms have a much wider range of
applications than online RL, being particularly appealing for real-world
applications, such as education, healthcare, and robotics. In this work, we
contribute with a unifying taxonomy to classify offline RL methods.
Furthermore, we provide a comprehensive review of the latest algorithmic
breakthroughs in the field using a unified notation as well as a review of
existing benchmarks' properties and shortcomings. Additionally, we provide a
figure that summarizes the performance of each method and class of methods on
different dataset properties, equipping researchers with the tools to decide
which type of algorithm is best suited for the problem at hand and identify
which classes of algorithms look the most promising. Finally, we provide our
perspective on open problems and propose future research directions for this
rapidly growing field. <br><br></div></a>
</td></tr>
<tr id=" 65 " class="entry"><td>
<a onclick="toggleVisibility('abstract65');">[|&bull;|]</a><a href="http://arxiv.org/abs/2209.08466v3">
<b/> Simplifying Model-based RL: Learning Representations, Latent-space
Models, and Policies with One Objective (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2209.08466v3">
Raj Ghugare, Homanga Bharadhwaj, Benjamin Eysenbach, Sergey Levine, Ruslan Salakhutdinov &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract65');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract65');"><div id="abstract65" style="font-size: 1rem; color:black; display: none">
<u><I> 2209.08466v3 &nbsp; - &nbsp;
{exploration}
{model-based}
{on-policy}
{reinforcement}
</I></u><br> While reinforcement learning (RL) methods that learn an internal model of the
environment have the potential to be more sample efficient than their
model-free counterparts, learning to model raw observations from high
dimensional sensors can be challenging. Prior work has addressed this challenge
by learning low-dimensional representation of observations through auxiliary
objectives, such as reconstruction or value prediction. However, the alignment
between these auxiliary objectives and the RL objective is often unclear. In
this work, we propose a single objective which jointly optimizes a latent-space
model and policy to achieve high returns while remaining self-consistent. This
objective is a lower bound on expected returns. Unlike prior bounds for
model-based RL on policy exploration or model guarantees, our bound is directly
on the overall RL objective. We demonstrate that the resulting algorithm
matches or improves the sample-efficiency of the best prior model-based and
model-free RL methods. While sample efficient methods typically are
computationally demanding, our method attains the performance of SAC in about
50% less wall-clock time. <br><br></div></a>
</td></tr>
<tr id=" 66 " class="entry"><td>
<a onclick="toggleVisibility('abstract66');">[|&bull;|]</a><a href="http://arxiv.org/abs/2202.00914v2">
<b/> Lipschitz-constrained Unsupervised Skill Discovery (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2202.00914v2">
Seohong Park, Jongwook Choi, Jaekyeom Kim, Honglak Lee, Gunhee Kim &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract66');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract66');"><div id="abstract66" style="font-size: 1rem; color:black; display: none">
<u><I> 2202.00914v2 &nbsp; - &nbsp;
{diversity}
{humanoid}
{robot}
{skill}
{unsupervised}
</I></u><br> We study the problem of unsupervised skill discovery, whose goal is to learn
a set of diverse and useful skills with no external reward. There have been a
number of skill discovery methods based on maximizing the mutual information
(MI) between skills and states. However, we point out that their MI objectives
usually prefer static skills to dynamic ones, which may hinder the application
for downstream tasks. To address this issue, we propose Lipschitz-constrained
Skill Discovery (LSD), which encourages the agent to discover more diverse,
dynamic, and far-reaching skills. Another benefit of LSD is that its learned
representation function can be utilized for solving goal-following downstream
tasks even in a zero-shot manner - i.e., without further training or complex
planning. Through experiments on various MuJoCo robotic locomotion and
manipulation environments, we demonstrate that LSD outperforms previous
approaches in terms of skill diversity, state space coverage, and performance
on seven downstream tasks including the challenging task of following multiple
goals on Humanoid. Our code and videos are available at
https://shpark.me/projects/lsd/. <br><br></div></a>
</td></tr>
<tr id=" 67 " class="entry"><td>
<a onclick="toggleVisibility('abstract67');">[|&bull;|]</a><a href="http://arxiv.org/abs/2205.10330v5">
<b/> A Review of Safe Reinforcement Learning: Methods, Theory and
Applications (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2205.10330v5">
Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, Alois Knoll &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract67');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract67');"><div id="abstract67" style="font-size: 1rem; color:black; display: none">
<u><I> 2205.10330v5 &nbsp; - &nbsp;
{control}
{reinforcement}
{robot}
</I></u><br> Reinforcement Learning (RL) has achieved tremendous success in many complex
decision-making tasks. However, safety concerns are raised during deploying RL
in real-world applications, leading to a growing demand for safe RL algorithms,
such as in autonomous driving and robotics scenarios. While safe control has a
long history, the study of safe RL algorithms is still in the early stages. To
establish a good foundation for future safe RL research, in this paper, we
provide a review of safe RL from the perspectives of methods, theories, and
applications. Firstly, we review the progress of safe RL from five dimensions
and come up with five crucial problems for safe RL being deployed in real-world
applications, coined as "2H3W". Secondly, we analyze the algorithm and theory
progress from the perspectives of answering the "2H3W" problems. Particularly,
the sample complexity of safe RL algorithms is reviewed and discussed, followed
by an introduction to the applications and benchmarks of safe RL algorithms.
Finally, we open the discussion of the challenging problems in safe RL, hoping
to inspire future research on this thread. To advance the study of safe RL
algorithms, we release an open-sourced repository containing the
implementations of major safe RL algorithms at the link:
https://github.com/chauncygu/Safe-Reinforcement-Learning-Baselines.git. <br><br></div></a>
</td></tr>
<tr id=" 68 " class="entry"><td>
<a onclick="toggleVisibility('abstract68');">[|&bull;|]</a><a href="http://arxiv.org/abs/2205.09123v1">
<b/> A2C is a special case of PPO (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2205.09123v1">
Shengyi Huang, Anssi Kanervisto, Antonin Raffin, Weixun Wang, Santiago Onta&#241;&#243;n, Rousslan Fernand Julien Dossa &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract68');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract68');"><div id="abstract68" style="font-size: 1rem; color:black; display: none">
<u><I> 2205.09123v1 &nbsp; - &nbsp;
{control}
{deep}
{reinforcement}
</I></u><br> Advantage Actor-critic (A2C) and Proximal Policy Optimization (PPO) are
popular deep reinforcement learning algorithms used for game AI in recent
years. A common understanding is that A2C and PPO are separate algorithms
because PPO's clipped objective appears significantly different than A2C's
objective. In this paper, however, we show A2C is a special case of PPO. We
present theoretical justifications and pseudocode analysis to demonstrate why.
To validate our claim, we conduct an empirical experiment using
\texttt{Stable-baselines3}, showing A2C and PPO produce the \textit{exact} same
models when other settings are controlled. <br><br></div></a>
</td></tr>
<tr id=" 69 " class="entry"><td>
<a onclick="toggleVisibility('abstract69');">[|&bull;|]</a><a href="http://arxiv.org/abs/2206.07137v3">
<b/> Prioritized Training on Points that are Learnable, Worth Learning, and
Not Yet Learnt (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2206.07137v3">
S&#246;ren Mindermann, Jan Brauner, Muhammed Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt H&#246;ltgen, Aidan N. Gomez, Adrien Morisot, Sebastian Farquhar, Yarin Gal &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract69');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract69');"><div id="abstract69" style="font-size: 1rem; color:black; display: none">
<u><I> 2206.07137v3 &nbsp; - &nbsp;
</I></u><br> Training on web-scale data can take months. But most computation and time is
wasted on redundant and noisy points that are already learnt or not learnable.
To accelerate training, we introduce Reducible Holdout Loss Selection
(RHO-LOSS), a simple but principled technique which selects approximately those
points for training that most reduce the model's generalization loss. As a
result, RHO-LOSS mitigates the weaknesses of existing data selection methods:
techniques from the optimization literature typically select 'hard' (e.g. high
loss) points, but such points are often noisy (not learnable) or less
task-relevant. Conversely, curriculum learning prioritizes 'easy' points, but
such points need not be trained on once learned. In contrast, RHO-LOSS selects
points that are learnable, worth learning, and not yet learnt. RHO-LOSS trains
in far fewer steps than prior art, improves accuracy, and speeds up training on
a wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and
BERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in
18x fewer steps and reaches 2% higher final accuracy than uniform data
shuffling. <br><br></div></a>
</td></tr>
<tr id=" 70 " class="entry"><td>
<a onclick="toggleVisibility('abstract70');">[|&bull;|]</a><a href="http://arxiv.org/abs/2212.00541v2">
<b/> Predictive Sampling: Real-time Behaviour Synthesis with MuJoCo (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2212.00541v2">
Taylor Howell, Nimrod Gileadi, Saran Tunyasuvunakool, Kevin Zakka, Tom Erez, Yuval Tassa &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract70');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract70');"><div id="abstract70" style="font-size: 1rem; color:black; display: none">
<u><I> 2212.00541v2 &nbsp; - &nbsp;
{control}
{deep}
{gradient}
{model-based}
{robot}
</I></u><br> We introduce MuJoCo MPC (MJPC), an open-source, interactive application and
software framework for real-time predictive control, based on MuJoCo physics.
MJPC allows the user to easily author and solve complex robotics tasks, and
currently supports three shooting-based planners: derivative-based iLQG and
Gradient Descent, and a simple derivative-free method we call Predictive
Sampling. Predictive Sampling was designed as an elementary baseline, mostly
for its pedagogical value, but turned out to be surprisingly competitive with
the more established algorithms. This work does not present algorithmic
advances, and instead, prioritises performant algorithms, simple code, and
accessibility of model-based methods via intuitive and interactive software.
MJPC is available at: github.com/deepmind/mujoco_mpc, a video summary can be
viewed at: dpmd.ai/mjpc. <br><br></div></a>
</td></tr>
<tr id=" 71 " class="entry"><td>
<a onclick="toggleVisibility('abstract71');">[|&bull;|]</a><a href="http://arxiv.org/abs/2202.03057v2">
<b/> Multi-Objective Quality Diversity Optimization (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2202.03057v2">
Thomas Pierrot, Guillaume Richard, Karim Beguir, Antoine Cully &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract71');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract71');"><div id="abstract71" style="font-size: 1rem; color:black; display: none">
<u><I> 2202.03057v2 &nbsp; - &nbsp;
{diversity}
{robot}
</I></u><br> In this work, we consider the problem of Quality-Diversity (QD) optimization
with multiple objectives. QD algorithms have been proposed to search for a
large collection of both diverse and high-performing solutions instead of a
single set of local optima. Thriving for diversity was shown to be useful in
many industrial and robotics applications. On the other hand, most real-life
problems exhibit several potentially antagonist objectives to be optimized.
Hence being able to optimize for multiple objectives with an appropriate
technique while thriving for diversity is important to many fields. Here, we
propose an extension of the MAP-Elites algorithm in the multi-objective
setting: Multi-Objective MAP-Elites (MOME). Namely, it combines the diversity
inherited from the MAP-Elites grid algorithm with the strength of
multi-objective optimizations by filling each cell with a Pareto Front. As
such, it allows to extract diverse solutions in the descriptor space while
exploring different compromises between objectives. We evaluate our method on
several tasks, from standard optimization problems to robotics simulations. Our
experimental evaluation shows the ability of MOME to provide diverse solutions
while providing global performances similar to standard multi-objective
algorithms. <br><br></div></a>
</td></tr>
<tr id=" 72 " class="entry"><td>
<a onclick="toggleVisibility('abstract72');">[|&bull;|]</a><a href="http://arxiv.org/abs/2210.12566v2">
<b/> Solving Continuous Control via Q-learning (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2210.12566v2">
Tim Seyde, Peter Werner, Wilko Schwarting, Igor Gilitschenski, Martin Riedmiller, Daniela Rus, Markus Wulfmeier &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract72');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract72');"><div id="abstract72" style="font-size: 1rem; color:black; display: none">
<u><I> 2210.12566v2 &nbsp; - &nbsp;
{control}
{deep}
{reinforcement}
</I></u><br> While there has been substantial success for solving continuous control with
actor-critic methods, simpler critic-only methods such as Q-learning find
limited application in the associated high-dimensional action spaces. However,
most actor-critic methods come at the cost of added complexity: heuristics for
stabilisation, compute requirements and wider hyperparameter search spaces. We
show that a simple modification of deep Q-learning largely alleviates these
issues. By combining bang-bang action discretization with value decomposition,
framing single-agent control as cooperative multi-agent reinforcement learning
(MARL), this simple critic-only approach matches performance of
state-of-the-art continuous actor-critic methods when learning from features or
pixels. We extend classical bandit examples from cooperative MARL to provide
intuition for how decoupled critics leverage state information to coordinate
joint optimization, and demonstrate surprisingly strong performance across a
variety of continuous control tasks. <br><br></div></a>
</td></tr>
<tr id=" 73 " class="entry"><td>
<a onclick="toggleVisibility('abstract73');">[|&bull;|]</a><a href="http://arxiv.org/abs/2209.00588v2">
<b/> Transformers are Sample-Efficient World Models (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2209.00588v2">
Vincent Micheli, Eloi Alonso, Fran&#231;ois Fleuret &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract73');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract73');"><div id="abstract73" style="font-size: 1rem; color:black; display: none">
<u><I> 2209.00588v2 &nbsp; - &nbsp;
{deep}
{model-based}
{reinforcement}
</I></u><br> Deep reinforcement learning agents are notoriously sample inefficient, which
considerably limits their application to real-world problems. Recently, many
model-based methods have been designed to address this issue, with learning in
the imagination of a world model being one of the most prominent approaches.
However, while virtually unlimited interaction with a simulated environment
sounds appealing, the world model has to be accurate over extended periods of
time. Motivated by the success of Transformers in sequence modeling tasks, we
introduce IRIS, a data-efficient agent that learns in a world model composed of
a discrete autoencoder and an autoregressive Transformer. With the equivalent
of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean
human normalized score of 1.046, and outperforms humans on 10 out of 26 games,
setting a new state of the art for methods without lookahead search. To foster
future research on Transformers and world models for sample-efficient
reinforcement learning, we release our code and models at
https://github.com/eloialonso/iris. <br><br></div></a>
</td></tr>
<tr id=" 74 " class="entry"><td>
<a onclick="toggleVisibility('abstract74');">[|&bull;|]</a><a href="http://arxiv.org/abs/2210.15409v2">
<b/> Constrained Differential Dynamic Programming: A primal-dual augmented
Lagrangian approach (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2210.15409v2">
Wilson Jallet, Antoine Bambade, Nicolas Mansard, Justin Carpentier &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract74');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract74');"><div id="abstract74" style="font-size: 1rem; color:black; display: none">
<u><I> 2210.15409v2 &nbsp; - &nbsp;
{control}
{optimal}
{robot}
{sparse}
</I></u><br> Trajectory optimization is an efficient approach for solving optimal control
problems for complex robotic systems. It relies on two key components: first
the transcription into a sparse nonlinear program, and second the corresponding
solver to iteratively compute its solution. On one hand, differential dynamic
programming (DDP) provides an efficient approach to transcribe the optimal
control problem into a finite-dimensional problem while optimally exploiting
the sparsity induced by time. On the other hand, augmented Lagrangian methods
make it possible to formulate efficient algorithms with advanced
constraint-satisfaction strategies. In this paper, we propose to combine these
two approaches into an efficient optimal control algorithm accepting both
equality and inequality constraints. Based on the augmented Lagrangian
literature, we first derive a generic primal-dual augmented Lagrangian strategy
for nonlinear problems with equality and inequality constraints. We then apply
it to the dynamic programming principle to solve the value-greedy optimization
problems inherent to the backward pass of DDP, which we combine with a
dedicated globalization strategy, resulting in a Newton-like algorithm for
solving constrained trajectory optimization problems. Contrary to previous
attempts of formulating an augmented Lagrangian version of DDP, our approach
exhibits adequate convergence properties without any switch in strategies. We
empirically demonstrate its interest with several case-studies from the
robotics literature. <br><br></div></a>
</td></tr>
<tr id=" 75 " class="entry"><td>
<a onclick="toggleVisibility('abstract75');">[|&bull;|]</a><a href="http://arxiv.org/abs/2201.00129v2">
<b/> A Surrogate-Assisted Controller for Expensive Evolutionary Reinforcement
Learning (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2201.00129v2">
Yuxing Wang, Tiantian Zhang, Yongzhe Chang, Bin Liang, Xueqian Wang, Bo Yuan &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract75');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract75');"><div id="abstract75" style="font-size: 1rem; color:black; display: none">
<u><I> 2201.00129v2 &nbsp; - &nbsp;
{control}
{diversity}
{evolution}
{population}
{reinforcement}
{robot}
</I></u><br> The integration of Reinforcement Learning (RL) and Evolutionary Algorithms
(EAs) aims at simultaneously exploiting the sample efficiency as well as the
diversity and robustness of the two paradigms. Recently, hybrid learning
frameworks based on this principle have achieved great success in various
challenging robot control tasks. However, in these methods, policies from the
genetic population are evaluated via interactions with the real environments,
limiting their applicability in computationally expensive problems. In this
work, we propose Surrogate-assisted Controller (SC), a novel and efficient
module that can be integrated into existing frameworks to alleviate the
computational burden of EAs by partially replacing the expensive policy
evaluation. The key challenge in applying this module is to prevent the
optimization process from being misled by the possible false minima introduced
by the surrogate. To address this issue, we present two strategies for SC to
control the workflow of hybrid frameworks. Experiments on six continuous
control tasks from the OpenAI Gym platform show that SC can not only
significantly reduce the cost of fitness evaluations, but also boost the
performance of the original hybrid frameworks with collaborative learning and
evolutionary processes. <br><br></div></a>
</td></tr>
<tr id=" 76 " class="entry"><td>
<a onclick="toggleVisibility('abstract76');">[|&bull;|]</a><a href="http://arxiv.org/abs/2206.08332v1">
<b/> BYOL-Explore: Exploration by Bootstrapped Prediction (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2206.08332v1">
Zhaohan Daniel Guo, Shantanu Thakoor, Miruna P&#238;slar, Bernardo Avila Pires, Florent Altch&#233;, Corentin Tallec, Alaa Saade, Daniele Calandriello, Jean-Bastien Grill, Yunhao Tang, Michal Valko, R&#233;mi Munos, Mohammad Gheshlaghi Azar, Bilal Piot &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract76');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract76');"><div id="abstract76" style="font-size: 1rem; color:black; display: none">
<u><I> 2206.08332v1 &nbsp; - &nbsp;
{exploration}
{intrinsic}
{on-policy}
</I></u><br> We present BYOL-Explore, a conceptually simple yet general approach for
curiosity-driven exploration in visually-complex environments. BYOL-Explore
learns a world representation, the world dynamics, and an exploration policy
all-together by optimizing a single prediction loss in the latent space with no
additional auxiliary objective. We show that BYOL-Explore is effective in
DM-HARD-8, a challenging partially-observable continuous-action
hard-exploration benchmark with visually-rich 3-D environments. On this
benchmark, we solve the majority of the tasks purely through augmenting the
extrinsic reward with BYOL-Explore s intrinsic reward, whereas prior work could
only get off the ground with human demonstrations. As further evidence of the
generality of BYOL-Explore, we show that it achieves superhuman performance on
the ten hardest exploration games in Atari while having a much simpler design
than other competitive agents. <br><br></div></a>
</td></tr>
<tr id=" 77 " class="entry"><td>
<a onclick="toggleVisibility('abstract77');">[|&bull;|]</a><a href="http://arxiv.org/abs/2206.09106v3">
<b/> Embodied Scene-aware Human Pose Estimation (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2206.09106v3">
Zhengyi Luo, Shun Iwase, Ye Yuan, Kris Kitani &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract77');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract77');"><div id="abstract77" style="font-size: 1rem; color:black; display: none">
<u><I> 2206.09106v3 &nbsp; - &nbsp;
{gradient}
</I></u><br> We propose embodied scene-aware human pose estimation where we estimate 3D
poses based on a simulated agent's proprioception and scene awareness, along
with external third-person observations. Unlike prior methods that often resort
to multistage optimization, non-causal inference, and complex contact modeling
to estimate human pose and human scene interactions, our method is one-stage,
causal, and recovers global 3D human poses in a simulated environment. Since 2D
third-person observations are coupled with the camera pose, we propose to
disentangle the camera pose and use a multi-step projection gradient defined in
the global coordinate frame as the movement cue for our embodied agent.
Leveraging a physics simulation and prescanned scenes (e.g., 3D mesh), we
simulate our agent in everyday environments (library, office, bedroom, etc.)
and equip our agent with environmental sensors to intelligently navigate and
interact with the geometries of the scene. Our method also relies only on 2D
keypoints and can be trained on synthetic datasets derived from popular human
motion databases. To evaluate, we use the popular H36M and PROX datasets and
achieve high quality pose estimation on the challenging PROX dataset without
ever using PROX motion sequences for training. Code and videos are available on
the project page. <br><br></div></a>
</td></tr>
<tr id=" 78 " class="entry"><td>
<a onclick="toggleVisibility('abstract78');">[|&bull;|]</a><a href="http://arxiv.org/abs/2206.09286v1">
<b/> From Universal Humanoid Control to Automatic Physically Valid Character
Creation (2022)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2206.09286v1">
Zhengyi Luo, Ye Yuan, Kris M. Kitani &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract78');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract78');"><div id="abstract78" style="font-size: 1rem; color:black; display: none">
<u><I> 2206.09286v1 &nbsp; - &nbsp;
{control}
{humanoid}
{robot}
</I></u><br> Automatically designing virtual humans and humanoids holds great potential in
aiding the character creation process in games, movies, and robots. In some
cases, a character creator may wish to design a humanoid body customized for
certain motions such as karate kicks and parkour jumps. In this work, we
propose a humanoid design framework to automatically generate physically valid
humanoid bodies conditioned on sequence(s) of pre-specified human motions.
First, we learn a generalized humanoid controller trained on a large-scale
human motion dataset that features diverse human motion and body shapes.
Second, we use a design-and-control framework to optimize a humanoid's physical
attributes to find body designs that can better imitate the pre-specified human
motion sequence(s). Leveraging the pre-trained humanoid controller and physics
simulation as guidance, our method is able to discover new humanoid designs
that are customized to perform pre-specified human motions. <br><br></div></a>
</td></tr>
<tr id=" 79 " class="entry"><td>
<a onclick="toggleVisibility('abstract79');">[|&bull;|]</a><a href="http://arxiv.org/abs/2112.03763v2">
<b/> Creating Multimodal Interactive Agents with Imitation and
Self-Supervised Learning (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2112.03763v2">
 DeepMind Interactive Agents Team, Josh Abramson, Arun Ahuja, Arthur Brussee, Federico Carnevale, Mary Cassin, Felix Fischer, Petko Georgiev, Alex Goldin, Mansi Gupta, Tim Harley, Felix Hill, Peter C Humphreys, Alden Hung, Jessica Landon, Timothy Lillicrap, Hamza Merzic, Alistair Muldal, Adam Santoro, Guy Scully, Tamara von Glehn, Greg Wayne, Nathaniel Wong, Chen Yan, Rui Zhu &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract79');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract79');"><div id="abstract79" style="font-size: 1rem; color:black; display: none">
<u><I> 2112.03763v2 &nbsp; - &nbsp;
{hierarchical}
{imitation}
{robot}
{self-supervised}
</I></u><br> A common vision from science fiction is that robots will one day inhabit our
physical spaces, sense the world as we do, assist our physical labours, and
communicate with us through natural language. Here we study how to design
artificial agents that can interact naturally with humans using the
simplification of a virtual environment. We show that imitation learning of
human-human interactions in a simulated world, in conjunction with
self-supervised learning, is sufficient to produce a multimodal interactive
agent, which we call MIA, that successfully interacts with non-adversarial
humans 75% of the time. We further identify architectural and algorithmic
techniques that improve performance, such as hierarchical action selection.
Altogether, our results demonstrate that imitation of multi-modal, real-time
human behaviour may provide a straightforward and surprisingly effective means
of imbuing agents with a rich behavioural prior from which agents might then be
fine-tuned for specific purposes, thus laying a foundation for training capable
agents for interactive robots or digital assistants. A video of MIA's behaviour
may be found at https://youtu.be/ZFgRhviF7mY <br><br></div></a>
</td></tr>
<tr id=" 80 " class="entry"><td>
<a onclick="toggleVisibility('abstract80');">[|&bull;|]</a><a href="http://arxiv.org/abs/2107.12808v2">
<b/> Open-Ended Learning Leads to Generally Capable Agents (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2107.12808v2">
 Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, Nat McAleese, Nathalie Bradley-Schmieg, Nathaniel Wong, Nicolas Porcel, Roberta Raileanu, Steph Hughes-Fitt, Valentin Dalibard, Wojciech Marian Czarnecki &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract80');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract80');"><div id="abstract80" style="font-size: 1rem; color:black; display: none">
<u><I> 2107.12808v2 &nbsp; - &nbsp;
{open-ended}
{transfer}
</I></u><br> In this work we create agents that can perform well beyond a single,
individual task, that exhibit much wider generalisation of behaviour to a
massive, rich space of challenges. We define a universe of tasks within an
environment domain and demonstrate the ability to train agents that are
generally capable across this vast space and beyond. The environment is
natively multi-agent, spanning the continuum of competitive, cooperative, and
independent games, which are situated within procedurally generated physical 3D
worlds. The resulting space is exceptionally diverse in terms of the challenges
posed to agents, and as such, even measuring the learning progress of an agent
is an open research problem. We propose an iterative notion of improvement
between successive generations of agents, rather than seeking to maximise a
singular objective, allowing us to quantify progress despite tasks being
incomparable in terms of achievable rewards. We show that through constructing
an open-ended learning process, which dynamically changes the training task
distributions and training objectives such that the agent never stops learning,
we achieve consistent learning of new behaviours. The resulting agent is able
to score reward in every one of our humanly solvable evaluation levels, with
behaviour generalising to many held-out points in the universe of tasks.
Examples of this zero-shot generalisation include good performance on Hide and
Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks
we characterise the behaviour of our agent, and find interesting emergent
heuristic behaviours such as trial-and-error experimentation, simple tool use,
option switching, and cooperation. Finally, we demonstrate that the general
capabilities of this agent could unlock larger scale transfer of behaviour
through cheap finetuning. <br><br></div></a>
</td></tr>
<tr id=" 81 " class="entry"><td>
<a onclick="toggleVisibility('abstract81');">[|&bull;|]</a><a href="http://arxiv.org/abs/2109.11052v1">
<b/> On Bonus-Based Exploration Methods in the Arcade Learning Environment (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2109.11052v1">
Adrien Ali Ta&#239;ga, William Fedus, Marlos C. Machado, Aaron Courville, Marc G. Bellemare &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract81');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract81');"><div id="abstract81" style="font-size: 1rem; color:black; display: none">
<u><I> 2109.11052v1 &nbsp; - &nbsp;
{exploration}
{reinforcement}
{sparse}
</I></u><br> Research on exploration in reinforcement learning, as applied to Atari 2600
game-playing, has emphasized tackling difficult exploration problems such as
Montezuma's Revenge (Bellemare et al., 2016). Recently, bonus-based exploration
methods, which explore by augmenting the environment reward, have reached
above-human average performance on such domains. In this paper we reassess
popular bonus-based exploration methods within a common evaluation framework.
We combine Rainbow (Hessel et al., 2018) with different exploration bonuses and
evaluate its performance on Montezuma's Revenge, Bellemare et al.'s set of hard
of exploration games with sparse rewards, and the whole Atari 2600 suite. We
find that while exploration bonuses lead to higher score on Montezuma's Revenge
they do not provide meaningful gains over the simpler $\epsilon$-greedy scheme.
In fact, we find that methods that perform best on that game often underperform
$\epsilon$-greedy on easy exploration Atari 2600 games. We find that our
conclusions remain valid even when hyperparameters are tuned for these
easy-exploration games. Finally, we find that none of the methods surveyed
benefit from additional training samples (1 billion frames, versus Rainbow's
200 million) on Bellemare et al.'s hard exploration games. Our results suggest
that recent gains in Montezuma's Revenge may be better attributed to
architecture change, rather than better exploration schemes; and that the real
pace of progress in exploration research for Atari 2600 games may have been
obfuscated by good results on a single domain. <br><br></div></a>
</td></tr>
<tr id=" 82 " class="entry"><td>
<a onclick="toggleVisibility('abstract82');">[|&bull;|]</a><a href="http://arxiv.org/abs/2107.04034v1">
<b/> RMA: Rapid Motor Adaptation for Legged Robots (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2107.04034v1">
Ashish Kumar, Zipeng Fu, Deepak Pathak, Jitendra Malik &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract82');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract82');"><div id="abstract82" style="font-size: 1rem; color:black; display: none">
<u><I> 2107.04034v1 &nbsp; - &nbsp;
{robot}
</I></u><br> Successful real-world deployment of legged robots would require them to adapt
in real-time to unseen scenarios like changing terrains, changing payloads,
wear and tear. This paper presents Rapid Motor Adaptation (RMA) algorithm to
solve this problem of real-time online adaptation in quadruped robots. RMA
consists of two components: a base policy and an adaptation module. The
combination of these components enables the robot to adapt to novel situations
in fractions of a second. RMA is trained completely in simulation without using
any domain knowledge like reference trajectories or predefined foot trajectory
generators and is deployed on the A1 robot without any fine-tuning. We train
RMA on a varied terrain generator using bioenergetics-inspired rewards and
deploy it on a variety of difficult terrains including rocky, slippery,
deformable surfaces in environments with grass, long vegetation, concrete,
pebbles, stairs, sand, etc. RMA shows state-of-the-art performance across
diverse real-world as well as simulation experiments. Video results at
https://ashish-kmr.github.io/rma-legged-robots/ <br><br></div></a>
</td></tr>
<tr id=" 83 " class="entry"><td>
<a onclick="toggleVisibility('abstract83');">[|&bull;|]</a><a href="http://arxiv.org/abs/2102.13651v1">
<b/> On the Importance of Hyperparameter Optimization for Model-based
Reinforcement Learning (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2102.13651v1">
Baohe Zhang, Raghu Rajan, Luis Pineda, Nathan Lambert, Andr&#233; Biedenkapp, Kurtland Chua, Frank Hutter, Roberto Calandra &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract83');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract83');"><div id="abstract83" style="font-size: 1rem; color:black; display: none">
<u><I> 2102.13651v1 &nbsp; - &nbsp;
{control}
{model-based}
{reinforcement}
</I></u><br> Model-based Reinforcement Learning (MBRL) is a promising framework for
learning control in a data-efficient manner. MBRL algorithms can be fairly
complex due to the separate dynamics modeling and the subsequent planning
algorithm, and as a result, they often possess tens of hyperparameters and
architectural choices. For this reason, MBRL typically requires significant
human expertise before it can be applied to new problems and domains. To
alleviate this problem, we propose to use automatic hyperparameter optimization
(HPO). We demonstrate that this problem can be tackled effectively with
automated HPO, which we demonstrate to yield significantly improved performance
compared to human experts. In addition, we show that tuning of several MBRL
hyperparameters dynamically, i.e. during the training itself, further improves
the performance compared to using static hyperparameters which are kept fixed
for the whole training. Finally, our experiments provide valuable insights into
the effects of several hyperparameters, such as plan horizon or learning rate
and their influence on the stability of training and resulting rewards. <br><br></div></a>
</td></tr>
<tr id=" 84 " class="entry"><td>
<a onclick="toggleVisibility('abstract84');">[|&bull;|]</a><a href="http://arxiv.org/abs/2101.04736v1">
<b/> Bootstrapping Motor Skill Learning with Motion Planning (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2101.04736v1">
Ben Abbatematteo, Eric Rosen, Stefanie Tellex, George Konidaris &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract84');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract84');"><div id="abstract84" style="font-size: 1rem; color:black; display: none">
<u><I> 2101.04736v1 &nbsp; - &nbsp;
{deep}
{kinematic}
{robot}
{skill}
</I></u><br> Learning a robot motor skill from scratch is impractically slow; so much so
that in practice, learning must be bootstrapped using a good skill policy
obtained from human demonstration. However, relying on human demonstration
necessarily degrades the autonomy of robots that must learn a wide variety of
skills over their operational lifetimes. We propose using kinematic motion
planning as a completely autonomous, sample efficient way to bootstrap motor
skill learning for object manipulation. We demonstrate the use of motion
planners to bootstrap motor skills in two complex object manipulation scenarios
with different policy representations: opening a drawer with a dynamic movement
primitive representation, and closing a microwave door with a deep neural
network policy. We also show how our method can bootstrap a motor skill for the
challenging dynamic task of learning to hit a ball off a tee, where a kinematic
plan based on treating the scene as static is insufficient to solve the task,
but sufficient to bootstrap a more dynamic policy. In all three cases, our
method is competitive with human-demonstrated initialization, and significantly
outperforms starting with a random policy. This approach enables robots to to
efficiently and autonomously learn motor policies for dynamic tasks without
human demonstration. <br><br></div></a>
</td></tr>
<tr id=" 85 " class="entry"><td>
<a onclick="toggleVisibility('abstract85');">[|&bull;|]</a><a href="http://arxiv.org/abs/2103.12656v2">
<b/> Replacing Rewards with Examples: Example-Based Policy Search via
Recursive Classification (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2103.12656v2">
Benjamin Eysenbach, Sergey Levine, Ruslan Salakhutdinov &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract85');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract85');"><div id="abstract85" style="font-size: 1rem; color:black; display: none">
<u><I> 2103.12656v2 &nbsp; - &nbsp;
{control}
{reinforcement}
</I></u><br> Reinforcement learning (RL) algorithms assume that users specify tasks by
manually writing down a reward function. However, this process can be laborious
and demands considerable technical expertise. Can we devise RL algorithms that
instead enable users to specify tasks simply by providing examples of
successful outcomes? In this paper, we derive a control algorithm that
maximizes the future probability of these successful outcome examples. Prior
work has approached similar problems with a two-stage process, first learning a
reward function and then optimizing this reward function using another RL
algorithm. In contrast, our method directly learns a value function from
transitions and successful outcomes, without learning this intermediate reward
function. Our method therefore requires fewer hyperparameters to tune and lines
of code to debug. We show that our method satisfies a new data-driven Bellman
equation, where examples take the place of the typical reward function term.
Experiments show that our approach outperforms prior methods that learn
explicit reward functions. <br><br></div></a>
</td></tr>
<tr id=" 86 " class="entry"><td>
<a onclick="toggleVisibility('abstract86');">[|&bull;|]</a><a href="http://arxiv.org/abs/2103.06257v2">
<b/> Maximum Entropy RL (Provably) Solves Some Robust RL Problems (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2103.06257v2">
Benjamin Eysenbach, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract86');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract86');"><div id="abstract86" style="font-size: 1rem; color:black; display: none">
<u><I> 2103.06257v2 &nbsp; - &nbsp;
{entropy}
{reinforcement}
</I></u><br> Many potential applications of reinforcement learning (RL) require guarantees
that the agent will perform well in the face of disturbances to the dynamics or
reward function. In this paper, we prove theoretically that maximum entropy
(MaxEnt) RL maximizes a lower bound on a robust RL objective, and thus can be
used to learn policies that are robust to some disturbances in the dynamics and
the reward function. While this capability of MaxEnt RL has been observed
empirically in prior work, to the best of our knowledge our work provides the
first rigorous proof and theoretical characterization of the MaxEnt RL robust
set. While a number of prior robust RL algorithms have been designed to handle
similar disturbances to the reward function or dynamics, these methods
typically require additional moving parts and hyperparameters on top of a base
RL algorithm. In contrast, our results suggest that MaxEnt RL by itself is
robust to certain disturbances, without requiring any additional modifications.
While this does not imply that MaxEnt RL is the best available robust RL
method, MaxEnt RL is a simple robust RL method with appealing formal
guarantees. <br><br></div></a>
</td></tr>
<tr id=" 87 " class="entry"><td>
<a onclick="toggleVisibility('abstract87');">[|&bull;|]</a><a href="http://arxiv.org/abs/2101.09391v2">
<b/> Learning Setup Policies: Reliable Transition Between Locomotion
Behaviours (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2101.09391v2">
Brendan Tidd, Nicolas Hudson, Akansel Cosgun, Jurgen Leitner &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract87');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract87');"><div id="abstract87" style="font-size: 1rem; color:black; display: none">
<u><I> 2101.09391v2 &nbsp; - &nbsp;
{control}
{deep}
{reinforcement}
</I></u><br> Dynamic platforms that operate over many unique terrain conditions typically
require many behaviours. To transition safely, there must be an overlap of
states between adjacent controllers. We develop a novel method for training
setup policies that bridge the trajectories between pre-trained Deep
Reinforcement Learning (DRL) policies. We demonstrate our method with a
simulated biped traversing a difficult jump terrain, where a single policy
fails to learn the task, and switching between pre-trained policies without
setup policies also fails. We perform an ablation of key components of our
system, and show that our method outperforms others that learn transition
policies. We demonstrate our method with several difficult and diverse terrain
types, and show that we can use setup policies as part of a modular control
suite to successfully traverse a sequence of complex terrains. We show that
using setup policies improves the success rate for traversing a single
difficult jump terrain (from 51.3% success rate with the best comparative
method to 82.2%), and traversing a random sequence of difficult obstacles (from
1.9% without setup policies to 71.2%). <br><br></div></a>
</td></tr>
<tr id=" 88 " class="entry"><td>
<a onclick="toggleVisibility('abstract88');">[|&bull;|]</a><a href="http://arxiv.org/abs/2109.08522v1">
<b/> Dynamics-Aware Quality-Diversity for Efficient Learning of Skill
Repertoires (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2109.08522v1">
Bryan Lim, Luca Grillotti, Lorenzo Bernasconi, Antoine Cully &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract88');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract88');"><div id="abstract88" style="font-size: 1rem; color:black; display: none">
<u><I> 2109.08522v1 &nbsp; - &nbsp;
{deep}
{diversity}
{exploration}
{robot}
{skill}
</I></u><br> Quality-Diversity (QD) algorithms are powerful exploration algorithms that
allow robots to discover large repertoires of diverse and high-performing
skills. However, QD algorithms are sample inefficient and require millions of
evaluations. In this paper, we propose Dynamics-Aware Quality-Diversity
(DA-QD), a framework to improve the sample efficiency of QD algorithms through
the use of dynamics models. We also show how DA-QD can then be used for
continual acquisition of new skill repertoires. To do so, we incrementally
train a deep dynamics model from experience obtained when performing skill
discovery using QD. We can then perform QD exploration in imagination with an
imagined skill repertoire. We evaluate our approach on three robotic
experiments. First, our experiments show DA-QD is 20 times more sample
efficient than existing QD approaches for skill discovery. Second, we
demonstrate learning an entirely new skill repertoire in imagination to perform
zero-shot learning. Finally, we show how DA-QD is useful and effective for
solving a long horizon navigation task and for damage adaptation in the real
world. Videos and source code are available at:
https://sites.google.com/view/da-qd. <br><br></div></a>
</td></tr>
<tr id=" 89 " class="entry"><td>
<a onclick="toggleVisibility('abstract89');">[|&bull;|]</a><a href="http://arxiv.org/abs/2106.13281v1">
<b/> Brax -- A Differentiable Physics Engine for Large Scale Rigid Body
Simulation (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2106.13281v1">
C. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, Olivier Bachem &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract89');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract89');"><div id="abstract89" style="font-size: 1rem; color:black; display: none">
<u><I> 2106.13281v1 &nbsp; - &nbsp;
{reinforcement}
</I></u><br> We present Brax, an open source library for rigid body simulation with a
focus on performance and parallelism on accelerators, written in JAX. We
present results on a suite of tasks inspired by the existing reinforcement
learning literature, but remade in our engine. Additionally, we provide
reimplementations of PPO, SAC, ES, and direct policy optimization in JAX that
compile alongside our environments, allowing the learning algorithm and the
environment processing to occur on the same device, and to scale seamlessly on
accelerators. Finally, we include notebooks that facilitate training of
performant policies on common OpenAI Gym MuJoCo-like tasks in minutes. <br><br></div></a>
</td></tr>
<tr id=" 90 " class="entry"><td>
<a onclick="toggleVisibility('abstract90');">[|&bull;|]</a><a href="http://arxiv.org/abs/2110.15701v4">
<b/> Successor Feature Representations (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2110.15701v4">
Chris Reinke, Xavier Alameda-Pineda &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract90');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract90');"><div id="abstract90" style="font-size: 1rem; color:black; display: none">
<u><I> 2110.15701v4 &nbsp; - &nbsp;
{reinforcement}
{transfer}
</I></u><br> Transfer in Reinforcement Learning aims to improve learning performance on
target tasks using knowledge from experienced source tasks. Successor
Representations (SR) and their extension Successor Features (SF) are prominent
transfer mechanisms in domains where reward functions change between tasks.
They reevaluate the expected return of previously learned policies in a new
target task to transfer their knowledge. The SF framework extended SR by
linearly decomposing rewards into successor features and a reward weight vector
allowing their application in high-dimensional tasks. But this came with the
cost of having a linear relationship between reward functions and successor
features, limiting its application to tasks where such a linear relationship
exists. We propose a novel formulation of SR based on learning the cumulative
discounted probability of successor features, called Successor Feature
Representations (SFR). Crucially, SFR allows to reevaluate the expected return
of policies for general reward functions. We introduce different SFR
variations, prove its convergence, and provide a guarantee on its transfer
performance. Experimental evaluations based on SFR with function approximation
demonstrate its advantage over SF not only for general reward functions, but
also in the case of linearly decomposable reward functions. <br><br></div></a>
</td></tr>
<tr id=" 91 " class="entry"><td>
<a onclick="toggleVisibility('abstract91');">[|&bull;|]</a><a href="http://arxiv.org/abs/2109.11361v2">
<b/> Optimal Control via Combined Inference and Numerical Optimization (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2109.11361v2">
Daniel Layeghi, Steve Tonneau, Michael Mistry &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract91');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract91');"><div id="abstract91" style="font-size: 1rem; color:black; display: none">
<u><I> 2109.11361v2 &nbsp; - &nbsp;
{control}
{optimal}
</I></u><br> Derivative based optimization methods are efficient at solving optimal
control problems near local optima. However, their ability to converge halts
when derivative information vanishes. The inference approach to optimal control
does not have strict requirements on the objective landscape. However,
sampling, the primary tool for solving such problems, tends to be much slower
in computation time. We propose a new method that combines second order methods
with inference. We utilise the Kullback Leibler (KL) control framework to
formulate an inference problem that computes the optimal controls from an
adaptive distribution approximating the solution of the second order method.
Our method allows for combining simple convex and non convex cost functions.
This simplifies the process of cost function design and leverages the strengths
of both inference and second order optimization. We compare our method to Model
Predictive Path Integral (MPPI) and iterative Linear Quadratic Regulator
(iLQG), outperforming both in sample efficiency and quality on manipulation and
obstacle avoidance tasks. <br><br></div></a>
</td></tr>
<tr id=" 92 " class="entry"><td>
<a onclick="toggleVisibility('abstract92');">[|&bull;|]</a><a href="http://arxiv.org/abs/2107.00541v1">
<b/> Goal-Conditioned Reinforcement Learning with Imagined Subgoals (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2107.00541v1">
Elliot Chane-Sane, Cordelia Schmid, Ivan Laptev &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract92');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract92');"><div id="abstract92" style="font-size: 1rem; color:black; display: none">
<u><I> 2107.00541v1 &nbsp; - &nbsp;
{goal-conditioned}
{reinforcement}
{robot}
{skill}
</I></u><br> Goal-conditioned reinforcement learning endows an agent with a large variety
of skills, but it often struggles to solve tasks that require more temporally
extended reasoning. In this work, we propose to incorporate imagined subgoals
into policy learning to facilitate learning of complex tasks. Imagined subgoals
are predicted by a separate high-level policy, which is trained simultaneously
with the policy and its critic. This high-level policy predicts intermediate
states halfway to the goal using the value function as a reachability metric.
We don't require the policy to reach these subgoals explicitly. Instead, we use
them to define a prior policy, and incorporate this prior into a KL-constrained
policy iteration scheme to speed up and regularize learning. Imagined subgoals
are used during policy learning, but not during test time, where we only apply
the learned policy. We evaluate our approach on complex robotic navigation and
manipulation tasks and show that it outperforms existing methods by a large
margin. <br><br></div></a>
</td></tr>
<tr id=" 93 " class="entry"><td>
<a onclick="toggleVisibility('abstract93');">[|&bull;|]</a><a href="http://arxiv.org/abs/2106.01946v4">
<b/> Convex optimization (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2106.01946v4">
Evgeniya Vorontsova, Roland Hildebrand, Alexander Gasnikov, Fedor Stonyakin &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract93');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract93');"><div id="abstract93" style="font-size: 1rem; color:black; display: none">
<u><I> 2106.01946v4 &nbsp; - &nbsp;
</I></u><br> This textbook is based on lectures given by the authors at MIPT (Moscow), HSE
(Moscow), FEFU (Vladivostok), V.I. Vernadsky KFU (Simferopol), ASU (Republic of
Adygea), and the University of Grenoble-Alpes (Grenoble, France). First of all,
the authors focused on the program of a two-semester course of lectures on
convex optimization, which is given to students of MIPT. The first chapter of
this book contains the materials of the first semester ("Fundamentals of convex
analysis and optimization"), the second and third chapters contain the
materials of the second semester ("Numerical methods of convex optimization").
The textbook has a number of features. First, in contrast to the classic
manuals, this book does not provide proofs of all the theorems mentioned. This
allowed, on one side, to describe more themes, but on the other side, made the
presentation less self-sufficient. The second important point is that part of
the material is advanced and is published in the Russian educational
literature, apparently for the first time. Third, the accents that are given do
not always coincide with the generally accepted accents in the textbooks that
are now popular. First of all, we talk about a sufficiently advanced
presentation of conic optimization, including robust optimization, as a vivid
demonstration of the capabilities of modern convex analysis. <br><br></div></a>
</td></tr>
<tr id=" 94 " class="entry"><td>
<a onclick="toggleVisibility('abstract94');">[|&bull;|]</a><a href="http://arxiv.org/abs/2103.15309v1">
<b/> Robust Feedback Motion Policy Design Using Reinforcement Learning on a
3D Digit Bipedal Robot (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2103.15309v1">
Guillermo A. Castillo, Bowen Weng, Wei Zhang, Ayonga Hereid &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract94');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract94');"><div id="abstract94" style="font-size: 1rem; color:black; display: none">
<u><I> 2103.15309v1 &nbsp; - &nbsp;
{control}
{hierarchical}
{robot}
{transfer}
</I></u><br> In this paper, a hierarchical and robust framework for learning bipedal
locomotion is presented and successfully implemented on the 3D biped robot
Digit built by Agility Robotics. We propose a cascade-structure controller that
combines the learning process with intuitive feedback regulations. This design
allows the framework to realize robust and stable walking with a
reduced-dimension state and action spaces of the policy, significantly
simplifying the design and reducing the sampling efficiency of the learning
method. The inclusion of feedback regulation into the framework improves the
robustness of the learned walking gait and ensures the success of the
sim-to-real transfer of the proposed controller with minimal tuning. We
specifically present a learning pipeline that considers hardware-feasible
initial poses of the robot within the learning process to ensure the initial
state of the learning is replicated as close as possible to the initial state
of the robot in hardware experiments. Finally, we demonstrate the feasibility
of our method by successfully transferring the learned policy in simulation to
the Digit robot hardware, realizing sustained walking gaits under external
force disturbances and challenging terrains not included during the training
process. To the best of our knowledge, this is the first time a learning-based
policy is transferred successfully to the Digit robot in hardware experiments
without using dynamic randomization or curriculum learning. <br><br></div></a>
</td></tr>
<tr id=" 95 " class="entry"><td>
<a onclick="toggleVisibility('abstract95');">[|&bull;|]</a><a href="http://arxiv.org/abs/2111.12953v1">
<b/> Learn Zero-Constraint-Violation Policy in Model-Free Constrained
Reinforcement Learning (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2111.12953v1">
Haitong Ma, Changliu Liu, Shengbo Eben Li, Sifa Zheng, Wenchao Sun, Jianyu Chen &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract95');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract95');"><div id="abstract95" style="font-size: 1rem; color:black; display: none">
<u><I> 2111.12953v1 &nbsp; - &nbsp;
{control}
{model-based}
{on-policy}
{reinforcement}
</I></u><br> In the trial-and-error mechanism of reinforcement learning (RL), a notorious
contradiction arises when we expect to learn a safe policy: how to learn a safe
policy without enough data and prior model about the dangerous region? Existing
methods mostly use the posterior penalty for dangerous actions, which means
that the agent is not penalized until experiencing danger. This fact causes
that the agent cannot learn a zero-violation policy even after convergence.
Otherwise, it would not receive any penalty and lose the knowledge about
danger. In this paper, we propose the safe set actor-critic (SSAC) algorithm,
which confines the policy update using safety-oriented energy functions, or the
safety indexes. The safety index is designed to increase rapidly for
potentially dangerous actions, which allows us to locate the safe set on the
action space, or the control safe set. Therefore, we can identify the dangerous
actions prior to taking them, and further obtain a zero constraint-violation
policy after convergence.We claim that we can learn the energy function in a
model-free manner similar to learning a value function. By using the energy
function transition as the constraint objective, we formulate a constrained RL
problem. We prove that our Lagrangian-based solutions make sure that the
learned policy will converge to the constrained optimum under some assumptions.
The proposed algorithm is evaluated on both the complex simulation environments
and a hardware-in-loop (HIL) experiment with a real controller from the
autonomous vehicle. Experimental results suggest that the converged policy in
all environments achieves zero constraint violation and comparable performance
with model-based baselines. <br><br></div></a>
</td></tr>
<tr id=" 96 " class="entry"><td>
<a onclick="toggleVisibility('abstract96');">[|&bull;|]</a><a href="http://arxiv.org/abs/2110.06169v1">
<b/> Offline Reinforcement Learning with Implicit Q-Learning (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2110.06169v1">
Ilya Kostrikov, Ashvin Nair, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract96');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract96');"><div id="abstract96" style="font-size: 1rem; color:black; display: none">
<u><I> 2110.06169v1 &nbsp; - &nbsp;
{offline}
{reinforcement}
</I></u><br> Offline reinforcement learning requires reconciling two conflicting aims:
learning a policy that improves over the behavior policy that collected the
dataset, while at the same time minimizing the deviation from the behavior
policy so as to avoid errors due to distributional shift. This trade-off is
critical, because most current offline reinforcement learning methods need to
query the value of unseen actions during training to improve the policy, and
therefore need to either constrain these actions to be in-distribution, or else
regularize their values. We propose an offline RL method that never needs to
evaluate actions outside of the dataset, but still enables the learned policy
to improve substantially over the best behavior in the data through
generalization. The main insight in our work is that, instead of evaluating
unseen actions from the latest policy, we can approximate the policy
improvement step implicitly by treating the state value function as a random
variable, with randomness determined by the action (while still integrating
over the dynamics to avoid excessive optimism), and then taking a state
conditional upper expectile of this random variable to estimate the value of
the best actions in that state. This leverages the generalization capacity of
the function approximator to estimate the value of the best available action at
a given state without ever directly querying a Q-function with this unseen
action. Our algorithm alternates between fitting this upper expectile value
function and backing it up into a Q-function. Then, we extract the policy via
advantage-weighted behavioral cloning. We dub our method implicit Q-learning
(IQL). IQL demonstrates the state-of-the-art performance on D4RL, a standard
benchmark for offline reinforcement learning. We also demonstrate that IQL
achieves strong performance fine-tuning using online interaction after offline
initialization. <br><br></div></a>
</td></tr>
<tr id=" 97 " class="entry"><td>
<a onclick="toggleVisibility('abstract97');">[|&bull;|]</a><a href="http://arxiv.org/abs/2110.10809v1">
<b/> Hierarchical Skills for Efficient Exploration (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2110.10809v1">
Jonas Gehring, Gabriel Synnaeve, Andreas Krause, Nicolas Usunier &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract97');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract97');"><div id="abstract97" style="font-size: 1rem; color:black; display: none">
<u><I> 2110.10809v1 &nbsp; - &nbsp;
{control}
{exploration}
{hierarchical}
{reinforcement}
{robot}
{skill}
{sparse}
{unsupervised}
</I></u><br> In reinforcement learning, pre-trained low-level skills have the potential to
greatly facilitate exploration. However, prior knowledge of the downstream task
is required to strike the right balance between generality (fine-grained
control) and specificity (faster learning) in skill design. In previous work on
continuous control, the sensitivity of methods to this trade-off has not been
addressed explicitly, as locomotion provides a suitable prior for navigation
tasks, which have been of foremost interest. In this work, we analyze this
trade-off for low-level policy pre-training with a new benchmark suite of
diverse, sparse-reward tasks for bipedal robots. We alleviate the need for
prior knowledge by proposing a hierarchical skill learning framework that
acquires skills of varying complexity in an unsupervised manner. For
utilization on downstream tasks, we present a three-layered hierarchical
learning algorithm to automatically trade off between general and specific
skills as required by the respective task. In our experiments, we show that our
approach performs this trade-off effectively and achieves better results than
current state-of-the-art methods for end- to-end hierarchical reinforcement
learning and unsupervised skill discovery. Code and videos are available at
https://facebookresearch.github.io/hsd3 . <br><br></div></a>
</td></tr>
<tr id=" 98 " class="entry"><td>
<a onclick="toggleVisibility('abstract98');">[|&bull;|]</a><a href="http://arxiv.org/abs/2102.05599v1">
<b/> Improving Model-Based Reinforcement Learning with Internal State
Representations through Self-Supervision (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2102.05599v1">
Julien Scholz, Cornelius Weber, Muhammad Burhan Hafez, Stefan Wermter &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract98');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract98');"><div id="abstract98" style="font-size: 1rem; color:black; display: none">
<u><I> 2102.05599v1 &nbsp; - &nbsp;
{reinforcement}
{self-supervised}
{unsupervised}
</I></u><br> Using a model of the environment, reinforcement learning agents can plan
their future moves and achieve superhuman performance in board games like
Chess, Shogi, and Go, while remaining relatively sample-efficient. As
demonstrated by the MuZero Algorithm, the environment model can even be learned
dynamically, generalizing the agent to many more tasks while at the same time
achieving state-of-the-art performance. Notably, MuZero uses internal state
representations derived from real environment states for its predictions. In
this paper, we bind the model's predicted internal state representation to the
environment state via two additional terms: a reconstruction model loss and a
simpler consistency loss, both of which work independently and unsupervised,
acting as constraints to stabilize the learning process. Our experiments show
that this new integration of reconstruction model loss and simpler consistency
loss provide a significant performance increase in OpenAI Gym environments. Our
modifications also enable self-supervised pretraining for MuZero, so the
algorithm can learn about environment dynamics before a goal is made available. <br><br></div></a>
</td></tr>
<tr id=" 99 " class="entry"><td>
<a onclick="toggleVisibility('abstract99');">[|&bull;|]</a><a href="http://arxiv.org/abs/2103.02503v7">
<b/> Domain Generalization: A Survey (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2103.02503v7">
Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, Chen Change Loy &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract99');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract99');"><div id="abstract99" style="font-size: 1rem; color:black; display: none">
<u><I> 2103.02503v7 &nbsp; - &nbsp;
{meta}
{reinforcement}
{transfer}
</I></u><br> Generalization to out-of-distribution (OOD) data is a capability natural to
humans yet challenging for machines to reproduce. This is because most learning
algorithms strongly rely on the i.i.d.~assumption on source/target data, which
is often violated in practice due to domain shift. Domain generalization (DG)
aims to achieve OOD generalization by using only source data for model
learning. Over the last ten years, research in DG has made great progress,
leading to a broad spectrum of methodologies, e.g., those based on domain
alignment, meta-learning, data augmentation, or ensemble learning, to name a
few; DG has also been studied in various application areas including computer
vision, speech recognition, natural language processing, medical imaging, and
reinforcement learning. In this paper, for the first time a comprehensive
literature review in DG is provided to summarize the developments over the past
decade. Specifically, we first cover the background by formally defining DG and
relating it to other relevant fields like domain adaptation and transfer
learning. Then, we conduct a thorough review into existing methods and
theories. Finally, we conclude this survey with insights and discussions on
future research directions. <br><br></div></a>
</td></tr>
<tr id=" 100 " class="entry"><td>
<a onclick="toggleVisibility('abstract100');">[|&bull;|]</a><a href="http://arxiv.org/abs/2107.10253v1">
<b/> Demonstration-Guided Reinforcement Learning with Learned Skills (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2107.10253v1">
Karl Pertsch, Youngwoon Lee, Yue Wu, Joseph J. Lim &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract100');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract100');"><div id="abstract100" style="font-size: 1rem; color:black; display: none">
<u><I> 2107.10253v1 &nbsp; - &nbsp;
{offline}
{reinforcement}
{robot}
{skill}
</I></u><br> Demonstration-guided reinforcement learning (RL) is a promising approach for
learning complex behaviors by leveraging both reward feedback and a set of
target task demonstrations. Prior approaches for demonstration-guided RL treat
every new task as an independent learning problem and attempt to follow the
provided demonstrations step-by-step, akin to a human trying to imitate a
completely unseen behavior by following the demonstrator's exact muscle
movements. Naturally, such learning will be slow, but often new behaviors are
not completely unseen: they share subtasks with behaviors we have previously
learned. In this work, we aim to exploit this shared subtask structure to
increase the efficiency of demonstration-guided RL. We first learn a set of
reusable skills from large offline datasets of prior experience collected
across many tasks. We then propose Skill-based Learning with Demonstrations
(SkiLD), an algorithm for demonstration-guided RL that efficiently leverages
the provided demonstrations by following the demonstrated skills instead of the
primitive actions, resulting in substantial performance improvements over prior
demonstration-guided RL approaches. We validate the effectiveness of our
approach on long-horizon maze navigation and complex robot manipulation tasks. <br><br></div></a>
</td></tr>
<tr id=" 101 " class="entry"><td>
<a onclick="toggleVisibility('abstract101');">[|&bull;|]</a><a href="http://arxiv.org/abs/2111.07176v2">
<b/> Information geometry for multiparameter models: New perspectives on the
origin of simplicity (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2111.07176v2">
Katherine N. Quinn, Michael C. Abbott, Mark K. Transtrum, Benjamin B. Machta, James P. Sethna &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract101');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract101');"><div id="abstract101" style="font-size: 1rem; color:black; display: none">
<u><I> 2111.07176v2 &nbsp; - &nbsp;
{control}
{deep}
{optimal}
</I></u><br> Complex models in physics, biology, economics, and engineering are often
sloppy, meaning that the model parameters are not well determined by the model
predictions for collective behavior. Many parameter combinations can vary over
decades without significant changes in the predictions. This review uses
information geometry to explore sloppiness and its deep relation to emergent
theories. We introduce the model manifold of predictions, whose coordinates are
the model parameters. Its hyperribbon structure explains why only a few
parameter combinations matter for the behavior. We review recent rigorous
results that connect the hierarchy of hyperribbon widths to approximation
theory, and to the smoothness of model predictions under changes of the control
variables. We discuss recent geodesic methods to find simpler models on nearby
boundaries of the model manifold -- emergent theories with fewer parameters
that explain the behavior equally well. We discuss a Bayesian prior which
optimizes the mutual information between model parameters and experimental
data, naturally favoring points on the emergent boundary theories and thus
simpler models. We introduce a `projected maximum likelihood' prior that
efficiently approximates this optimal prior, and contrast both to the poor
behavior of the traditional Jeffreys prior. We discuss the way the
renormalization group coarse-graining in statistical mechanics introduces a
flow of the model manifold, and connect stiff and sloppy directions along the
model manifold with relevant and irrelevant eigendirections of the
renormalization group. Finally, we discuss recently developed `intensive'
embedding methods, allowing one to visualize the predictions of arbitrary
probabilistic models as low-dimensional projections of an isometric embedding,
and illustrate our method by generating the model manifold of the Ising model. <br><br></div></a>
</td></tr>
<tr id=" 102 " class="entry"><td>
<a onclick="toggleVisibility('abstract102');">[|&bull;|]</a><a href="http://arxiv.org/abs/2107.08981v2">
<b/> Hierarchical Few-Shot Imitation with Skill Transition Models (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2107.08981v2">
Kourosh Hakhamaneshi, Ruihan Zhao, Albert Zhan, Pieter Abbeel, Michael Laskin &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract102');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract102');"><div id="abstract102" style="font-size: 1rem; color:black; display: none">
<u><I> 2107.08981v2 &nbsp; - &nbsp;
{few-shot}
{imitation}
{offline}
{reinforcement}
{robot}
{skill}
</I></u><br> A desirable property of autonomous agents is the ability to both solve
long-horizon problems and generalize to unseen tasks. Recent advances in
data-driven skill learning have shown that extracting behavioral priors from
offline data can enable agents to solve challenging long-horizon tasks with
reinforcement learning. However, generalization to tasks unseen during
behavioral prior training remains an outstanding challenge. To this end, we
present Few-shot Imitation with Skill Transition Models (FIST), an algorithm
that extracts skills from offline data and utilizes them to generalize to
unseen tasks given a few downstream demonstrations. FIST learns an inverse
skill dynamics model, a distance function, and utilizes a semi-parametric
approach for imitation. We show that FIST is capable of generalizing to new
tasks and substantially outperforms prior baselines in navigation experiments
requiring traversing unseen parts of a large maze and 7-DoF robotic arm
experiments requiring manipulating previously unseen objects in a kitchen. <br><br></div></a>
</td></tr>
<tr id=" 103 " class="entry"><td>
<a onclick="toggleVisibility('abstract103');">[|&bull;|]</a><a href="http://arxiv.org/abs/2104.02646v1">
<b/> gradSim: Differentiable simulation for system identification and
visuomotor control (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2104.02646v1">
Krishna Murthy Jatavallabhula, Miles Macklin, Florian Golemo, Vikram Voleti, Linda Petrini, Martin Weiss, Breandan Considine, Jerome Parent-Levesque, Kevin Xie, Kenny Erleben, Liam Paull, Florian Shkurti, Derek Nowrouzezahrai, Sanja Fidler &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract103');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract103');"><div id="abstract103" style="font-size: 1rem; color:black; display: none">
<u><I> 2104.02646v1 &nbsp; - &nbsp;
{control}
{evolution}
</I></u><br> We consider the problem of estimating an object's physical properties such as
mass, friction, and elasticity directly from video sequences. Such a system
identification problem is fundamentally ill-posed due to the loss of
information during image formation. Current solutions require precise 3D labels
which are labor-intensive to gather, and infeasible to create for many systems
such as deformable solids or cloth. We present gradSim, a framework that
overcomes the dependence on 3D supervision by leveraging differentiable
multiphysics simulation and differentiable rendering to jointly model the
evolution of scene dynamics and image formation. This novel combination enables
backpropagation from pixels in a video sequence through to the underlying
physical attributes that generated them. Moreover, our unified computation
graph -- spanning from the dynamics and through the rendering process --
enables learning in challenging visuomotor control tasks, without relying on
state-based (3D) supervision, while obtaining performance competitive to or
better than techniques that rely on precise 3D labels. <br><br></div></a>
</td></tr>
<tr id=" 104 " class="entry"><td>
<a onclick="toggleVisibility('abstract104');">[|&bull;|]</a><a href="http://arxiv.org/abs/2106.01345v2">
<b/> Decision Transformer: Reinforcement Learning via Sequence Modeling (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2106.01345v2">
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract104');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract104');"><div id="abstract104" style="font-size: 1rem; color:black; display: none">
<u><I> 2106.01345v2 &nbsp; - &nbsp;
{gradient}
{offline}
{optimal}
{reinforcement}
</I></u><br> We introduce a framework that abstracts Reinforcement Learning (RL) as a
sequence modeling problem. This allows us to draw upon the simplicity and
scalability of the Transformer architecture, and associated advances in
language modeling such as GPT-x and BERT. In particular, we present Decision
Transformer, an architecture that casts the problem of RL as conditional
sequence modeling. Unlike prior approaches to RL that fit value functions or
compute policy gradients, Decision Transformer simply outputs the optimal
actions by leveraging a causally masked Transformer. By conditioning an
autoregressive model on the desired return (reward), past states, and actions,
our Decision Transformer model can generate future actions that achieve the
desired return. Despite its simplicity, Decision Transformer matches or exceeds
the performance of state-of-the-art model-free offline RL baselines on Atari,
OpenAI Gym, and Key-to-Door tasks. <br><br></div></a>
</td></tr>
<tr id=" 105 " class="entry"><td>
<a onclick="toggleVisibility('abstract105');">[|&bull;|]</a><a href="http://arxiv.org/abs/2106.03894v3">
<b/> Differentiable Quality Diversity (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2106.03894v3">
Matthew C. Fontaine, Stefanos Nikolaidis &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract105');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract105');"><div id="abstract105" style="font-size: 1rem; color:black; display: none">
<u><I> 2106.03894v3 &nbsp; - &nbsp;
{diversity}
{gradient}
</I></u><br> Quality diversity (QD) is a growing branch of stochastic optimization
research that studies the problem of generating an archive of solutions that
maximize a given objective function but are also diverse with respect to a set
of specified measure functions. However, even when these functions are
differentiable, QD algorithms treat them as "black boxes", ignoring gradient
information. We present the differentiable quality diversity (DQD) problem, a
special case of QD, where both the objective and measure functions are first
order differentiable. We then present MAP-Elites via a Gradient Arborescence
(MEGA), a DQD algorithm that leverages gradient information to efficiently
explore the joint range of the objective and measure functions. Results in two
QD benchmark domains and in searching the latent space of a StyleGAN show that
MEGA significantly outperforms state-of-the-art QD algorithms, highlighting
DQD's promise for efficient quality diversity optimization when gradient
information is available. Source code is available at
https://github.com/icaros-usc/dqd. <br><br></div></a>
</td></tr>
<tr id=" 106 " class="entry"><td>
<a onclick="toggleVisibility('abstract106');">[|&bull;|]</a><a href="http://arxiv.org/abs/2110.14770v1">
<b/> TRAIL: Near-Optimal Imitation Learning with Suboptimal Data (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2110.14770v1">
Mengjiao Yang, Sergey Levine, Ofir Nachum &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract106');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract106');"><div id="abstract106" style="font-size: 1rem; color:black; display: none">
<u><I> 2110.14770v1 &nbsp; - &nbsp;
{contrastive}
{imitation}
{offline}
{optimal}
</I></u><br> The aim in imitation learning is to learn effective policies by utilizing
near-optimal expert demonstrations. However, high-quality demonstrations from
human experts can be expensive to obtain in large numbers. On the other hand,
it is often much easier to obtain large quantities of suboptimal or
task-agnostic trajectories, which are not useful for direct imitation, but can
nevertheless provide insight into the dynamical structure of the environment,
showing what could be done in the environment even if not what should be done.
We ask the question, is it possible to utilize such suboptimal offline datasets
to facilitate provably improved downstream imitation learning? In this work, we
answer this question affirmatively and present training objectives that use
offline datasets to learn a factored transition model whose structure enables
the extraction of a latent action space. Our theoretical analysis shows that
the learned latent action space can boost the sample-efficiency of downstream
imitation learning, effectively reducing the need for large near-optimal expert
datasets through the use of auxiliary non-expert data. To learn the latent
action space in practice, we propose TRAIL (Transition-Reparametrized Actions
for Imitation Learning), an algorithm that learns an energy-based transition
model contrastively, and uses the transition model to reparametrize the action
space for sample-efficient imitation learning. We evaluate the practicality of
our objective through experiments on a set of navigation and locomotion tasks.
Our results verify the benefits suggested by our theory and show that TRAIL is
able to improve baseline imitation learning by up to 4x in performance. <br><br></div></a>
</td></tr>
<tr id=" 107 " class="entry"><td>
<a onclick="toggleVisibility('abstract107');">[|&bull;|]</a><a href="http://arxiv.org/abs/2106.02039v4">
<b/> Offline Reinforcement Learning as One Big Sequence Modeling Problem (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2106.02039v4">
Michael Janner, Qiyang Li, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract107');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract107');"><div id="abstract107" style="font-size: 1rem; color:black; display: none">
<u><I> 2106.02039v4 &nbsp; - &nbsp;
{goal-conditioned}
{imitation}
{offline}
{reinforcement}
{sparse}
</I></u><br> Reinforcement learning (RL) is typically concerned with estimating stationary
policies or single-step models, leveraging the Markov property to factorize
problems in time. However, we can also view RL as a generic sequence modeling
problem, with the goal being to produce a sequence of actions that leads to a
sequence of high rewards. Viewed in this way, it is tempting to consider
whether high-capacity sequence prediction models that work well in other
domains, such as natural-language processing, can also provide effective
solutions to the RL problem. To this end, we explore how RL can be tackled with
the tools of sequence modeling, using a Transformer architecture to model
distributions over trajectories and repurposing beam search as a planning
algorithm. Framing RL as sequence modeling problem simplifies a range of design
decisions, allowing us to dispense with many of the components common in
offline RL algorithms. We demonstrate the flexibility of this approach across
long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and
offline RL. Further, we show that this approach can be combined with existing
model-free algorithms to yield a state-of-the-art planner in sparse-reward,
long-horizon tasks. <br><br></div></a>
</td></tr>
<tr id=" 108 " class="entry"><td>
<a onclick="toggleVisibility('abstract108');">[|&bull;|]</a><a href="http://arxiv.org/abs/2108.07041v2">
<b/> Implicitly Regularized RL with Implicit Q-Values (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2108.07041v2">
Nino Vieillard, Marcin Andrychowicz, Anton Raichuk, Olivier Pietquin, Matthieu Geist &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract108');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract108');"><div id="abstract108" style="font-size: 1rem; color:black; display: none">
<u><I> 2108.07041v2 &nbsp; - &nbsp;
{control}
{deep}
{entropy}
{intrinsic}
{off-policy}
{reinforcement}
</I></u><br> The $Q$-function is a central quantity in many Reinforcement Learning (RL)
algorithms for which RL agents behave following a (soft)-greedy policy w.r.t.
to $Q$. It is a powerful tool that allows action selection without a model of
the environment and even without explicitly modeling the policy. Yet, this
scheme can only be used in discrete action tasks, with small numbers of
actions, as the softmax cannot be computed exactly otherwise. Especially the
usage of function approximation, to deal with continuous action spaces in
modern actor-critic architectures, intrinsically prevents the exact computation
of a softmax. We propose to alleviate this issue by parametrizing the
$Q$-function implicitly, as the sum of a log-policy and of a value function. We
use the resulting parametrization to derive a practical off-policy deep RL
algorithm, suitable for large action spaces, and that enforces the softmax
relation between the policy and the $Q$-value. We provide a theoretical
analysis of our algorithm: from an Approximate Dynamic Programming perspective,
we show its equivalence to a regularized version of value iteration, accounting
for both entropy and Kullback-Leibler regularization, and that enjoys
beneficial error propagation results. We then evaluate our algorithm on classic
control tasks, where its results compete with state-of-the-art methods. <br><br></div></a>
</td></tr>
<tr id=" 109 " class="entry"><td>
<a onclick="toggleVisibility('abstract109');">[|&bull;|]</a><a href="http://arxiv.org/abs/2110.14457v2">
<b/> Direct then Diffuse: Incremental Unsupervised Skill Discovery for State
Covering and Goal Reaching (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2110.14457v2">
Pierre-Alexandre Kamienny, Jean Tarbouriech, Sylvain Lamprier, Alessandro Lazaric, Ludovic Denoyer &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract109');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract109');"><div id="abstract109" style="font-size: 1rem; color:black; display: none">
<u><I> 2110.14457v2 &nbsp; - &nbsp;
{control}
{reinforcement}
{skill}
{sparse}
{unsupervised}
</I></u><br> Learning meaningful behaviors in the absence of reward is a difficult problem
in reinforcement learning. A desirable and challenging unsupervised objective
is to learn a set of diverse skills that provide a thorough coverage of the
state space while being directed, i.e., reliably reaching distinct regions of
the environment. In this paper, we build on the mutual information framework
for skill discovery and introduce UPSIDE, which addresses the
coverage-directedness trade-off in the following ways: 1) We design policies
with a decoupled structure of a directed skill, trained to reach a specific
region, followed by a diffusing part that induces a local coverage. 2) We
optimize policies by maximizing their number under the constraint that each of
them reaches distinct regions of the environment (i.e., they are sufficiently
discriminable) and prove that this serves as a lower bound to the original
mutual information objective. 3) Finally, we compose the learned directed
skills into a growing tree that adaptively covers the environment. We
illustrate in several navigation and control environments how the skills
learned by UPSIDE solve sparse-reward downstream tasks better than existing
baselines. <br><br></div></a>
</td></tr>
<tr id=" 110 " class="entry"><td>
<a onclick="toggleVisibility('abstract110');">[|&bull;|]</a><a href="http://arxiv.org/abs/2108.13264v4">
<b/> Deep Reinforcement Learning at the Edge of the Statistical Precipice (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2108.13264v4">
Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, Marc G. Bellemare &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract110');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract110');"><div id="abstract110" style="font-size: 1rem; color:black; display: none">
<u><I> 2108.13264v4 &nbsp; - &nbsp;
{control}
{deep}
{reinforcement}
</I></u><br> Deep reinforcement learning (RL) algorithms are predominantly evaluated by
comparing their relative performance on a large suite of tasks. Most published
results on deep RL benchmarks compare point estimates of aggregate performance
such as mean and median scores across tasks, ignoring the statistical
uncertainty implied by the use of a finite number of training runs. Beginning
with the Arcade Learning Environment (ALE), the shift towards
computationally-demanding benchmarks has led to the practice of evaluating only
a small number of runs per task, exacerbating the statistical uncertainty in
point estimates. In this paper, we argue that reliable evaluation in the few
run deep RL regime cannot ignore the uncertainty in results without running the
risk of slowing down progress in the field. We illustrate this point using a
case study on the Atari 100k benchmark, where we find substantial discrepancies
between conclusions drawn from point estimates alone versus a more thorough
statistical analysis. With the aim of increasing the field's confidence in
reported results with a handful of runs, we advocate for reporting interval
estimates of aggregate performance and propose performance profiles to account
for the variability in results, as well as present more robust and efficient
aggregate metrics, such as interquartile mean scores, to achieve small
uncertainty in results. Using such statistical tools, we scrutinize performance
evaluations of existing algorithms on other widely used RL benchmarks including
the ALE, Procgen, and the DeepMind Control Suite, again revealing discrepancies
in prior comparisons. Our findings call for a change in how we evaluate
performance in deep RL, for which we present a more rigorous evaluation
methodology, accompanied with an open-source library rliable, to prevent
unreliable results from stagnating the field. <br><br></div></a>
</td></tr>
<tr id=" 111 " class="entry"><td>
<a onclick="toggleVisibility('abstract111');">[|&bull;|]</a><a href="http://arxiv.org/abs/2110.10149v2">
<b/> Continuous Control with Action Quantization from Demonstrations (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2110.10149v2">
Robert Dadashi, L&#233;onard Hussenot, Damien Vincent, Sertan Girgin, Anton Raichuk, Matthieu Geist, Olivier Pietquin &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract111');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract111');"><div id="abstract111" style="font-size: 1rem; color:black; display: none">
<u><I> 2110.10149v2 &nbsp; - &nbsp;
{control}
{deep}
{imitation}
{reinforcement}
</I></u><br> In this paper, we propose a novel Reinforcement Learning (RL) framework for
problems with continuous action spaces: Action Quantization from Demonstrations
(AQuaDem). The proposed approach consists in learning a discretization of
continuous action spaces from human demonstrations. This discretization returns
a set of plausible actions (in light of the demonstrations) for each input
state, thus capturing the priors of the demonstrator and their multimodal
behavior. By discretizing the action space, any discrete action deep RL
technique can be readily applied to the continuous control problem. Experiments
show that the proposed approach outperforms state-of-the-art methods such as
SAC in the RL setup, and GAIL in the Imitation Learning setup. We provide a
website with interactive videos: https://google-research.github.io/aquadem/ and
make the code available:
https://github.com/google-research/google-research/tree/master/aquadem. <br><br></div></a>
</td></tr>
<tr id=" 112 " class="entry"><td>
<a onclick="toggleVisibility('abstract112');">[|&bull;|]</a><a href="http://arxiv.org/abs/2102.10330v2">
<b/> Decoupling Value and Policy for Generalization in Reinforcement Learning (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2102.10330v2">
Roberta Raileanu, Rob Fergus &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract112');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract112');"><div id="abstract112" style="font-size: 1rem; color:black; display: none">
<u><I> 2102.10330v2 &nbsp; - &nbsp;
{control}
{deep}
{optimal}
{reinforcement}
</I></u><br> Standard deep reinforcement learning algorithms use a shared representation
for the policy and value function, especially when training directly from
images. However, we argue that more information is needed to accurately
estimate the value function than to learn the optimal policy. Consequently, the
use of a shared representation for the policy and value function can lead to
overfitting. To alleviate this problem, we propose two approaches which are
combined to create IDAAC: Invariant Decoupled Advantage Actor-Critic. First,
IDAAC decouples the optimization of the policy and value function, using
separate networks to model them. Second, it introduces an auxiliary loss which
encourages the representation to be invariant to task-irrelevant properties of
the environment. IDAAC shows good generalization to unseen environments,
achieving a new state-of-the-art on the Procgen benchmark and outperforming
popular methods on DeepMind Control tasks with distractors. Our implementation
is available at https://github.com/rraileanu/idaac. <br><br></div></a>
</td></tr>
<tr id=" 113 " class="entry"><td>
<a onclick="toggleVisibility('abstract113');">[|&bull;|]</a><a href="http://arxiv.org/abs/2112.10751v2">
<b/> RvS: What is Essential for Offline RL via Supervised Learning? (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2112.10751v2">
Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract113');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract113');"><div id="abstract113" style="font-size: 1rem; color:black; display: none">
<u><I> 2112.10751v2 &nbsp; - &nbsp;
{offline}
{reinforcement}
</I></u><br> Recent work has shown that supervised learning alone, without temporal
difference (TD) learning, can be remarkably effective for offline RL. When does
this hold true, and which algorithmic components are necessary? Through
extensive experiments, we boil supervised learning for offline RL down to its
essential elements. In every environment suite we consider, simply maximizing
likelihood with a two-layer feedforward MLP is competitive with
state-of-the-art results of substantially more complex methods based on TD
learning or sequence modeling with Transformers. Carefully choosing model
capacity (e.g., via regularization or architecture) and choosing which
information to condition on (e.g., goals or rewards) are critical for
performance. These insights serve as a field guide for practitioners doing
Reinforcement Learning via Supervised Learning (which we coin "RvS learning").
They also probe the limits of existing RvS methods, which are comparatively
weak on random data, and suggest a number of open problems. <br><br></div></a>
</td></tr>
<tr id=" 114 " class="entry"><td>
<a onclick="toggleVisibility('abstract114');">[|&bull;|]</a><a href="http://arxiv.org/abs/2106.06860v2">
<b/> A Minimalist Approach to Offline Reinforcement Learning (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2106.06860v2">
Scott Fujimoto, Shixiang Shane Gu &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract114');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract114');"><div id="abstract114" style="font-size: 1rem; color:black; display: none">
<u><I> 2106.06860v2 &nbsp; - &nbsp;
{deep}
{offline}
{reinforcement}
</I></u><br> Offline reinforcement learning (RL) defines the task of learning from a fixed
batch of data. Due to errors in value estimation from out-of-distribution
actions, most offline RL algorithms take the approach of constraining or
regularizing the policy with the actions contained in the dataset. Built on
pre-existing RL algorithms, modifications to make an RL algorithm work offline
comes at the cost of additional complexity. Offline RL algorithms introduce new
hyperparameters and often leverage secondary components such as generative
models, while adjusting the underlying RL algorithm. In this paper we aim to
make a deep RL algorithm work while making minimal changes. We find that we can
match the performance of state-of-the-art offline RL algorithms by simply
adding a behavior cloning term to the policy update of an online RL algorithm
and normalizing the data. The resulting algorithm is a simple to implement and
tune baseline, while more than halving the overall run time by removing the
additional computational overhead of previous methods. <br><br></div></a>
</td></tr>
<tr id=" 115 " class="entry"><td>
<a onclick="toggleVisibility('abstract115');">[|&bull;|]</a><a href="http://arxiv.org/abs/2109.04504v2">
<b/> Bootstrapped Meta-Learning (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2109.04504v2">
Sebastian Flennerhag, Yannick Schroecker, Tom Zahavy, Hado van Hasselt, David Silver, Satinder Singh &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract115');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract115');"><div id="abstract115" style="font-size: 1rem; color:black; display: none">
<u><I> 2109.04504v2 &nbsp; - &nbsp;
{control}
{exploration}
{gradient}
{meta}
{multi-task}
</I></u><br> Meta-learning empowers artificial intelligence to increase its efficiency by
learning how to learn. Unlocking this potential involves overcoming a
challenging meta-optimisation problem. We propose an algorithm that tackles
this problem by letting the meta-learner teach itself. The algorithm first
bootstraps a target from the meta-learner, then optimises the meta-learner by
minimising the distance to that target under a chosen (pseudo-)metric. Focusing
on meta-learning with gradients, we establish conditions that guarantee
performance improvements and show that the metric can control
meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the
effective meta-learning horizon without requiring backpropagation through all
updates. We achieve a new state-of-the art for model-free agents on the Atari
ALE benchmark and demonstrate that it yields both performance and efficiency
gains in multi-task meta-learning. Finally, we explore how bootstrapping opens
up new possibilities and find that it can meta-learn efficient exploration in
an epsilon-greedy Q-learning agent, without backpropagating through the update
rule. <br><br></div></a>
</td></tr>
<tr id=" 116 " class="entry"><td>
<a onclick="toggleVisibility('abstract116');">[|&bull;|]</a><a href="http://arxiv.org/abs/2103.13834v2">
<b/> Self-Imitation Learning by Planning (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2103.13834v2">
Sha Luo, Hamidreza Kasaei, Lambert Schomaker &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract116');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract116');"><div id="abstract116" style="font-size: 1rem; color:black; display: none">
<u><I> 2103.13834v2 &nbsp; - &nbsp;
{exploration}
{imitation}
{motion planning}
{reinforcement}
{robot}
{skill}
{transfer}
</I></u><br> Imitation learning (IL) enables robots to acquire skills quickly by
transferring expert knowledge, which is widely adopted in reinforcement
learning (RL) to initialize exploration. However, in long-horizon motion
planning tasks, a challenging problem in deploying IL and RL methods is how to
generate and collect massive, broadly distributed data such that these methods
can generalize effectively. In this work, we solve this problem using our
proposed approach called {self-imitation learning by planning (SILP)}, where
demonstration data are collected automatically by planning on the visited
states from the current policy. SILP is inspired by the observation that
successfully visited states in the early reinforcement learning stage are
collision-free nodes in the graph-search based motion planner, so we can plan
and relabel robot's own trials as demonstrations for policy learning. Due to
these self-generated demonstrations, we relieve the human operator from the
laborious data preparation process required by IL and RL methods in solving
complex motion planning tasks. The evaluation results show that our SILP method
achieves higher success rates and enhances sample efficiency compared to
selected baselines, and the policy learned in simulation performs well in a
real-world placement task with changing goals and obstacles. <br><br></div></a>
</td></tr>
<tr id=" 117 " class="entry"><td>
<a onclick="toggleVisibility('abstract117');">[|&bull;|]</a><a href="http://arxiv.org/abs/2106.04615v2">
<b/> Vector Quantized Models for Planning (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2106.04615v2">
Sherjil Ozair, Yazhe Li, Ali Razavi, Ioannis Antonoglou, A&#228;ron van den Oord, Oriol Vinyals &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract117');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract117');"><div id="abstract117" style="font-size: 1rem; color:black; display: none">
<u><I> 2106.04615v2 &nbsp; - &nbsp;
{deep}
{model-based}
{offline}
</I></u><br> Recent developments in the field of model-based RL have proven successful in
a range of environments, especially ones where planning is essential. However,
such successes have been limited to deterministic fully-observed environments.
We present a new approach that handles stochastic and partially-observable
environments. Our key insight is to use discrete autoencoders to capture the
multiple possible effects of an action in a stochastic environment. We use a
stochastic variant of Monte Carlo tree search to plan over both the agent's
actions and the discrete latent variables representing the environment's
response. Our approach significantly outperforms an offline version of MuZero
on a stochastic interpretation of chess where the opponent is considered part
of the environment. We also show that our approach scales to DeepMind Lab, a
first-person 3D environment with large visual observations and partial
observability. <br><br></div></a>
</td></tr>
<tr id=" 118 " class="entry"><td>
<a onclick="toggleVisibility('abstract118');">[|&bull;|]</a><a href="http://arxiv.org/abs/2105.12196v1">
<b/> From Motor Control to Team Play in Simulated Humanoid Football (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2105.12196v1">
Siqi Liu, Guy Lever, Zhe Wang, Josh Merel, S. M. Ali Eslami, Daniel Hennes, Wojciech M. Czarnecki, Yuval Tassa, Shayegan Omidshafiei, Abbas Abdolmaleki, Noah Y. Siegel, Leonard Hasenclever, Luke Marris, Saran Tunyasuvunakool, H. Francis Song, Markus Wulfmeier, Paul Muller, Tuomas Haarnoja, Brendan D. Tracey, Karl Tuyls, Thore Graepel, Nicolas Heess &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract118');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract118');"><div id="abstract118" style="font-size: 1rem; color:black; display: none">
<u><I> 2105.12196v1 &nbsp; - &nbsp;
{control}
{humanoid}
{imitation}
{population}
{reinforcement}
{skill}
{transfer}
</I></u><br> Intelligent behaviour in the physical world exhibits structure at multiple
spatial and temporal scales. Although movements are ultimately executed at the
level of instantaneous muscle tensions or joint torques, they must be selected
to serve goals defined on much longer timescales, and in terms of relations
that extend far beyond the body itself, ultimately involving coordination with
other agents. Recent research in artificial intelligence has shown the promise
of learning-based approaches to the respective problems of complex movement,
longer-term planning and multi-agent coordination. However, there is limited
research aimed at their integration. We study this problem by training teams of
physically simulated humanoid avatars to play football in a realistic virtual
environment. We develop a method that combines imitation learning, single- and
multi-agent reinforcement learning and population-based training, and makes use
of transferable representations of behaviour for decision making at different
levels of abstraction. In a sequence of stages, players first learn to control
a fully articulated body to perform realistic, human-like movements such as
running and turning; they then acquire mid-level football skills such as
dribbling and shooting; finally, they develop awareness of others and play as a
team, bridging the gap between low-level motor control at a timescale of
milliseconds, and coordinated goal-directed behaviour as a team at the
timescale of tens of seconds. We investigate the emergence of behaviours at
different levels of abstraction, as well as the representations that underlie
these behaviours using several analysis techniques, including statistics from
real-world sports analytics. Our work constitutes a complete demonstration of
integrated decision-making at multiple scales in a physically embodied
multi-agent setting. See project video at https://youtu.be/KHMwq9pv7mg. <br><br></div></a>
</td></tr>
<tr id=" 119 " class="entry"><td>
<a onclick="toggleVisibility('abstract119');">[|&bull;|]</a><a href="http://arxiv.org/abs/2105.14750v3">
<b/> Active Hierarchical Exploration with Stable Subgoal Representation
Learning (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2105.14750v3">
Siyuan Li, Jin Zhang, Jianhao Wang, Yang Yu, Chongjie Zhang &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract119');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract119');"><div id="abstract119" style="font-size: 1rem; color:black; display: none">
<u><I> 2105.14750v3 &nbsp; - &nbsp;
{control}
{exploration}
{goal-conditioned}
{hierarchical}
{intrinsic}
{reinforcement}
{sparse}
</I></u><br> Goal-conditioned hierarchical reinforcement learning (GCHRL) provides a
promising approach to solving long-horizon tasks. Recently, its success has
been extended to more general settings by concurrently learning hierarchical
policies and subgoal representations. Although GCHRL possesses superior
exploration ability by decomposing tasks via subgoals, existing GCHRL methods
struggle in temporally extended tasks with sparse external rewards, since the
high-level policy learning relies on external rewards. As the high-level policy
selects subgoals in an online learned representation space, the dynamic change
of the subgoal space severely hinders effective high-level exploration. In this
paper, we propose a novel regularization that contributes to both stable and
efficient subgoal representation learning. Building upon the stable
representation, we design measures of novelty and potential for subgoals, and
develop an active hierarchical exploration strategy that seeks out new
promising subgoals and states without intrinsic rewards. Experimental results
show that our approach significantly outperforms state-of-the-art baselines in
continuous control tasks with sparse rewards. <br><br></div></a>
</td></tr>
<tr id=" 120 " class="entry"><td>
<a onclick="toggleVisibility('abstract120');">[|&bull;|]</a><a href="http://arxiv.org/abs/2102.12086v2">
<b/> Modern Koopman Theory for Dynamical Systems (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2102.12086v2">
Steven L. Brunton, Marko Budi&#353;i&#263;, Eurika Kaiser, J. Nathan Kutz &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract120');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract120');"><div id="abstract120" style="font-size: 1rem; color:black; display: none">
<u><I> 2102.12086v2 &nbsp; - &nbsp;
{control}
</I></u><br> The field of dynamical systems is being transformed by the mathematical tools
and algorithms emerging from modern computing and data science.
First-principles derivations and asymptotic reductions are giving way to
data-driven approaches that formulate models in operator theoretic or
probabilistic frameworks. Koopman spectral theory has emerged as a dominant
perspective over the past decade, in which nonlinear dynamics are represented
in terms of an infinite-dimensional linear operator acting on the space of all
possible measurement functions of the system. This linear representation of
nonlinear dynamics has tremendous potential to enable the prediction,
estimation, and control of nonlinear systems with standard textbook methods
developed for linear systems. However, obtaining finite-dimensional coordinate
systems and embeddings in which the dynamics appear approximately linear
remains a central open challenge. The success of Koopman analysis is due
primarily to three key factors: 1) there exists rigorous theory connecting it
to classical geometric approaches for dynamical systems, 2) the approach is
formulated in terms of measurements, making it ideal for leveraging big-data
and machine learning techniques, and 3) simple, yet powerful numerical
algorithms, such as the dynamic mode decomposition (DMD), have been developed
and extended to reduce Koopman theory to practice in real-world applications.
In this review, we provide an overview of modern Koopman operator theory,
describing recent theoretical and algorithmic developments and highlighting
these methods with a diverse range of applications. We also discuss key
advances and challenges in the rapidly growing field of machine learning that
are likely to drive future developments and significantly transform the
theoretical landscape of dynamical systems. <br><br></div></a>
</td></tr>
<tr id=" 121 " class="entry"><td>
<a onclick="toggleVisibility('abstract121');">[|&bull;|]</a><a href="http://arxiv.org/abs/2109.00157v2">
<b/> A Survey of Exploration Methods in Reinforcement Learning (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2109.00157v2">
Susan Amin, Maziar Gomrokchi, Harsh Satija, Herke van Hoof, Doina Precup &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract121');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract121');"><div id="abstract121" style="font-size: 1rem; color:black; display: none">
<u><I> 2109.00157v2 &nbsp; - &nbsp;
{control}
{exploration}
{reinforcement}
</I></u><br> Exploration is an essential component of reinforcement learning algorithms,
where agents need to learn how to predict and control unknown and often
stochastic environments. Reinforcement learning agents depend crucially on
exploration to obtain informative data for the learning process as the lack of
enough information could hinder effective learning. In this article, we provide
a survey of modern exploration methods in (Sequential) reinforcement learning,
as well as a taxonomy of exploration methods. <br><br></div></a>
</td></tr>
<tr id=" 122 " class="entry"><td>
<a onclick="toggleVisibility('abstract122');">[|&bull;|]</a><a href="http://arxiv.org/abs/2110.02034v2">
<b/> Dropout Q-Functions for Doubly Efficient Reinforcement Learning (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2110.02034v2">
Takuya Hiraoka, Takahisa Imagawa, Taisei Hashimoto, Takashi Onishi, Yoshimasa Tsuruoka &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract122');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract122');"><div id="abstract122" style="font-size: 1rem; color:black; display: none">
<u><I> 2110.02034v2 &nbsp; - &nbsp;
{reinforcement}
</I></u><br> Randomized ensembled double Q-learning (REDQ) (Chen et al., 2021b) has
recently achieved state-of-the-art sample efficiency on continuous-action
reinforcement learning benchmarks. This superior sample efficiency is made
possible by using a large Q-function ensemble. However, REDQ is much less
computationally efficient than non-ensemble counterparts such as Soft
Actor-Critic (SAC) (Haarnoja et al., 2018a). To make REDQ more computationally
efficient, we propose a method of improving computational efficiency called
DroQ, which is a variant of REDQ that uses a small ensemble of dropout
Q-functions. Our dropout Q-functions are simple Q-functions equipped with
dropout connection and layer normalization. Despite its simplicity of
implementation, our experimental results indicate that DroQ is doubly (sample
and computationally) efficient. It achieved comparable sample efficiency with
REDQ, much better computational efficiency than REDQ, and comparable
computational efficiency with that of SAC. <br><br></div></a>
</td></tr>
<tr id=" 123 " class="entry"><td>
<a onclick="toggleVisibility('abstract123');">[|&bull;|]</a><a href="http://arxiv.org/abs/2102.03765v5">
<b/> Tactical Optimism and Pessimism for Deep Reinforcement Learning (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2102.03765v5">
Ted Moskovitz, Jack Parker-Holder, Aldo Pacchiano, Michael Arbel, Michael I. Jordan &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract123');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract123');"><div id="abstract123" style="font-size: 1rem; color:black; display: none">
<u><I> 2102.03765v5 &nbsp; - &nbsp;
{control}
{deep}
{exploration}
{off-policy}
{reinforcement}
</I></u><br> In recent years, deep off-policy actor-critic algorithms have become a
dominant approach to reinforcement learning for continuous control. One of the
primary drivers of this improved performance is the use of pessimistic value
updates to address function approximation errors, which previously led to
disappointing performance. However, a direct consequence of pessimism is
reduced exploration, running counter to theoretical support for the efficacy of
optimism in the face of uncertainty. So which approach is best? In this work,
we show that the most effective degree of optimism can vary both across tasks
and over the course of learning. Inspired by this insight, we introduce a novel
deep actor-critic framework, Tactical Optimistic and Pessimistic (TOP)
estimation, which switches between optimistic and pessimistic value learning
online. This is achieved by formulating the selection as a multi-arm bandit
problem. We show in a series of continuous control tasks that TOP outperforms
existing methods which rely on a fixed degree of optimism, setting a new state
of the art in challenging pixel-based environments. Since our changes are
simple to implement, we believe these insights can easily be incorporated into
a multitude of off-policy algorithms. <br><br></div></a>
</td></tr>
<tr id=" 124 " class="entry"><td>
<a onclick="toggleVisibility('abstract124');">[|&bull;|]</a><a href="http://arxiv.org/abs/2102.08363v2">
<b/> COMBO: Conservative Offline Model-Based Policy Optimization (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2102.08363v2">
Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, Chelsea Finn &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract124');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract124');"><div id="abstract124" style="font-size: 1rem; color:black; display: none">
<u><I> 2102.08363v2 &nbsp; - &nbsp;
{deep}
{imitation}
{model-based}
{offline}
{reinforcement}
</I></u><br> Model-based algorithms, which learn a dynamics model from logged experience
and perform some sort of pessimistic planning under the learned model, have
emerged as a promising paradigm for offline reinforcement learning (offline
RL). However, practical variants of such model-based algorithms rely on
explicit uncertainty quantification for incorporating pessimism. Uncertainty
estimation with complex models, such as deep neural networks, can be difficult
and unreliable. We overcome this limitation by developing a new model-based
offline RL algorithm, COMBO, that regularizes the value function on
out-of-support state-action tuples generated via rollouts under the learned
model. This results in a conservative estimate of the value function for
out-of-support state-action tuples, without requiring explicit uncertainty
estimation. We theoretically show that our method optimizes a lower bound on
the true policy value, that this bound is tighter than that of prior methods,
and our approach satisfies a policy improvement guarantee in the offline
setting. Through experiments, we find that COMBO consistently performs as well
or better as compared to prior offline model-free and model-based methods on
widely studied offline RL benchmarks, including image-based tasks. <br><br></div></a>
</td></tr>
<tr id=" 125 " class="entry"><td>
<a onclick="toggleVisibility('abstract125');">[|&bull;|]</a><a href="http://arxiv.org/abs/2110.12080v1">
<b/> C-Planning: An Automatic Curriculum for Learning Goal-Reaching Tasks (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2110.12080v1">
Tianjun Zhang, Benjamin Eysenbach, Ruslan Salakhutdinov, Sergey Levine, Joseph E. Gonzalez &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract125');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract125');"><div id="abstract125" style="font-size: 1rem; color:black; display: none">
<u><I> 2110.12080v1 &nbsp; - &nbsp;
{goal-conditioned}
{offline}
{optimal}
{reinforcement}
</I></u><br> Goal-conditioned reinforcement learning (RL) can solve tasks in a wide range
of domains, including navigation and manipulation, but learning to reach
distant goals remains a central challenge to the field. Learning to reach such
goals is particularly hard without any offline data, expert demonstrations, and
reward shaping. In this paper, we propose an algorithm to solve the distant
goal-reaching task by using search at training time to automatically generate a
curriculum of intermediate states. Our algorithm, Classifier-Planning
(C-Planning), frames the learning of the goal-conditioned policies as
expectation maximization: the E-step corresponds to planning an optimal
sequence of waypoints using graph search, while the M-step aims to learn a
goal-conditioned policy to reach those waypoints. Unlike prior methods that
combine goal-conditioned RL with graph search, ours performs search only during
training and not testing, significantly decreasing the compute costs of
deploying the learned policy. Empirically, we demonstrate that our method is
more sample efficient than prior methods. Moreover, it is able to solve very
long horizons manipulation and navigation tasks, tasks that prior
goal-conditioned methods and methods based on graph search fail to solve. <br><br></div></a>
</td></tr>
<tr id=" 126 " class="entry"><td>
<a onclick="toggleVisibility('abstract126');">[|&bull;|]</a><a href="http://arxiv.org/abs/2103.05456v3">
<b/> Extended Tree Search for Robot Task and Motion Planning (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2103.05456v3">
Tianyu Ren, Georgia Chalvatzaki, Jan Peters &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract126');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract126');"><div id="abstract126" style="font-size: 1rem; color:black; display: none">
<u><I> 2103.05456v3 &nbsp; - &nbsp;
{exploration}
{hierarchical}
{intrinsic}
{motion planning}
{optimal}
{robot}
</I></u><br> Integrated task and motion planning (TAMP) is desirable for generalized
autonomy robots but it is challenging at the same time. TAMP requires the
planner to not only search in both the large symbolic task space and the
high-dimension motion space but also deal with the infeasible task actions due
to its intrinsic hierarchical process. We propose a novel decision-making
framework for TAMP by constructing an extended decision tree for both symbolic
task planning and high-dimension motion variable binding. We integrate top-k
planning for generating explicitly a skeleton space where a variety of
candidate skeleton plans are at disposal. Moreover, we effectively combine this
skeleton space with the resultant motion variable spaces into a single extended
decision space. Accordingly, we use Monte-Carlo Tree Search (MCTS) to ensure an
exploration-exploitation balance at each decision node and optimize globally to
produce optimal solutions. The proposed seamless combination of symbolic top-k
planning with streams, with the proved optimality of MCTS, leads to a powerful
planning algorithm that can handle the combinatorial complexity of long-horizon
manipulation tasks. We empirically evaluate our proposed algorithm in
challenging robot tasks with different domains that require multi-stage
decisions and show how our method can overcome the large task space and motion
space through its effective tree search compared to its most competitive
baseline method. <br><br></div></a>
</td></tr>
<tr id=" 127 " class="entry"><td>
<a onclick="toggleVisibility('abstract127');">[|&bull;|]</a><a href="http://arxiv.org/abs/2111.02552v1">
<b/> Is Bang-Bang Control All You Need? Solving Continuous Control with
Bernoulli Policies (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2111.02552v1">
Tim Seyde, Igor Gilitschenski, Wilko Schwarting, Bartolomeo Stellato, Martin Riedmiller, Markus Wulfmeier, Daniela Rus &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract127');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract127');"><div id="abstract127" style="font-size: 1rem; color:black; display: none">
<u><I> 2111.02552v1 &nbsp; - &nbsp;
{control}
{exploration}
{imitation}
{optimal}
{reinforcement}
{robot}
</I></u><br> Reinforcement learning (RL) for continuous control typically employs
distributions whose support covers the entire action space. In this work, we
investigate the colloquially known phenomenon that trained agents often prefer
actions at the boundaries of that space. We draw theoretical connections to the
emergence of bang-bang behavior in optimal control, and provide extensive
empirical evaluation across a variety of recent RL algorithms. We replace the
normal Gaussian by a Bernoulli distribution that solely considers the extremes
along each action dimension - a bang-bang controller. Surprisingly, this
achieves state-of-the-art performance on several continuous control benchmarks
- in contrast to robotic hardware, where energy and maintenance cost affect
controller choices. Since exploration, learning,and the final solution are
entangled in RL, we provide additional imitation learning experiments to reduce
the impact of exploration on our analysis. Finally, we show that our
observations generalize to environments that aim to model real-world challenges
and evaluate factors to mitigate the emergence of bang-bang solutions. Our
findings emphasize challenges for benchmarking continuous control algorithms,
particularly in light of potential real-world applications. <br><br></div></a>
</td></tr>
<tr id=" 128 " class="entry"><td>
<a onclick="toggleVisibility('abstract128');">[|&bull;|]</a><a href="http://arxiv.org/abs/2108.10470v2">
<b/> Isaac Gym: High Performance GPU-Based Physics Simulation For Robot
Learning (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2108.10470v2">
Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, Gavriel State &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract128');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract128');"><div id="abstract128" style="font-size: 1rem; color:black; display: none">
<u><I> 2108.10470v2 &nbsp; - &nbsp;
{robot}
</I></u><br> Isaac Gym offers a high performance learning platform to train policies for
wide variety of robotics tasks directly on GPU. Both physics simulation and the
neural network policy training reside on GPU and communicate by directly
passing data from physics buffers to PyTorch tensors without ever going through
any CPU bottlenecks. This leads to blazing fast training times for complex
robotics tasks on a single GPU with 2-3 orders of magnitude improvements
compared to conventional RL training that uses a CPU based simulator and GPU
for neural networks. We host the results and videos at
\url{https://sites.google.com/view/isaacgym-nvidia} and isaac gym can be
downloaded at \url{https://developer.nvidia.com/isaac-gym}. <br><br></div></a>
</td></tr>
<tr id=" 129 " class="entry"><td>
<a onclick="toggleVisibility('abstract129');">[|&bull;|]</a><a href="http://arxiv.org/abs/2104.10157v2">
<b/> VideoGPT: Video Generation using VQ-VAE and Transformers (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2104.10157v2">
Wilson Yan, Yunzhi Zhang, Pieter Abbeel, Aravind Srinivas &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract129');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract129');"><div id="abstract129" style="font-size: 1rem; color:black; display: none">
<u><I> 2104.10157v2 &nbsp; - &nbsp;
{robot}
</I></u><br> We present VideoGPT: a conceptually simple architecture for scaling
likelihood based generative modeling to natural videos. VideoGPT uses VQ-VAE
that learns downsampled discrete latent representations of a raw video by
employing 3D convolutions and axial self-attention. A simple GPT-like
architecture is then used to autoregressively model the discrete latents using
spatio-temporal position encodings. Despite the simplicity in formulation and
ease of training, our architecture is able to generate samples competitive with
state-of-the-art GAN models for video generation on the BAIR Robot dataset, and
generate high fidelity natural videos from UCF-101 and Tumbler GIF Dataset
(TGIF). We hope our proposed architecture serves as a reproducible reference
for a minimalistic implementation of transformer based video generation models.
Samples and code are available at
https://wilson1yan.github.io/videogpt/index.html <br><br></div></a>
</td></tr>
<tr id=" 130 " class="entry"><td>
<a onclick="toggleVisibility('abstract130');">[|&bull;|]</a><a href="http://arxiv.org/abs/2101.05982v2">
<b/> Randomized Ensembled Double Q-Learning: Learning Fast Without a Model (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2101.05982v2">
Xinyue Chen, Che Wang, Zijian Zhou, Keith Ross &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract130');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract130');"><div id="abstract130" style="font-size: 1rem; color:black; display: none">
<u><I> 2101.05982v2 &nbsp; - &nbsp;
{model-based}
</I></u><br> Using a high Update-To-Data (UTD) ratio, model-based methods have recently
achieved much higher sample efficiency than previous model-free methods for
continuous-action DRL benchmarks. In this paper, we introduce a simple
model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show
that its performance is just as good as, if not better than, a state-of-the-art
model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this
performance using fewer parameters than the model-based method, and with less
wall-clock run time. REDQ has three carefully integrated ingredients which
allow it to achieve its high performance: (i) a UTD ratio >> 1; (ii) an
ensemble of Q functions; (iii) in-target minimization across a random subset of
Q functions from the ensemble. Through carefully designed experiments, we
provide a detailed analysis of REDQ and related model-free algorithms. To our
knowledge, REDQ is the first successful model-free DRL algorithm for
continuous-action spaces using a UTD ratio >> 1. <br><br></div></a>
</td></tr>
<tr id=" 131 " class="entry"><td>
<a onclick="toggleVisibility('abstract131');">[|&bull;|]</a><a href="http://arxiv.org/abs/2104.02180v2">
<b/> AMP: Adversarial Motion Priors for Stylized Physics-Based Character
Control (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2104.02180v2">
Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, Angjoo Kanazawa &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract131');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract131');"><div id="abstract131" style="font-size: 1rem; color:black; display: none">
<u><I> 2104.02180v2 &nbsp; - &nbsp;
{control}
{imitation}
{reinforcement}
{skill}
</I></u><br> Synthesizing graceful and life-like behaviors for physically simulated
characters has been a fundamental challenge in computer animation. Data-driven
methods that leverage motion tracking are a prominent class of techniques for
producing high fidelity motions for a wide range of behaviors. However, the
effectiveness of these tracking-based methods often hinges on carefully
designed objective functions, and when applied to large and diverse motion
datasets, these methods require significant additional machinery to select the
appropriate motion for the character to track in a given scenario. In this
work, we propose to obviate the need to manually design imitation objectives
and mechanisms for motion selection by utilizing a fully automated approach
based on adversarial imitation learning. High-level task objectives that the
character should perform can be specified by relatively simple reward
functions, while the low-level style of the character's behaviors can be
specified by a dataset of unstructured motion clips, without any explicit clip
selection or sequencing. These motion clips are used to train an adversarial
motion prior, which specifies style-rewards for training the character through
reinforcement learning (RL). The adversarial RL procedure automatically selects
which motion to perform, dynamically interpolating and generalizing from the
dataset. Our system produces high-quality motions that are comparable to those
achieved by state-of-the-art tracking-based techniques, while also being able
to easily accommodate large datasets of unstructured motion clips. Composition
of disparate skills emerges automatically from the motion prior, without
requiring a high-level motion planner or other task-specific annotations of the
motion clips. We demonstrate the effectiveness of our framework on a diverse
cast of complex simulated characters and a challenging suite of motor control
tasks. <br><br></div></a>
</td></tr>
<tr id=" 132 " class="entry"><td>
<a onclick="toggleVisibility('abstract132');">[|&bull;|]</a><a href="http://arxiv.org/abs/2111.09159v3">
<b/> Aggressive Q-Learning with Ensembles: Achieving Both High Sample
Efficiency and High Asymptotic Performance (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2111.09159v3">
Yanqiu Wu, Xinyue Chen, Che Wang, Yiming Zhang, Keith W. Ross &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract132');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract132');"><div id="abstract132" style="font-size: 1rem; color:black; display: none">
<u><I> 2111.09159v3 &nbsp; - &nbsp;
{control}
{deep}
{model-based}
{reinforcement}
</I></u><br> Recent advances in model-free deep reinforcement learning (DRL) show that
simple model-free methods can be highly effective in challenging
high-dimensional continuous control tasks. In particular, Truncated Quantile
Critics (TQC) achieves state-of-the-art asymptotic training performance on the
MuJoCo benchmark with a distributional representation of critics; and
Randomized Ensemble Double Q-Learning (REDQ) achieves high sample efficiency
that is competitive with state-of-the-art model-based methods using a high
update-to-data ratio and target randomization. In this paper, we propose a
novel model-free algorithm, Aggressive Q-Learning with Ensembles (AQE), which
improves the sample-efficiency performance of REDQ and the asymptotic
performance of TQC, thereby providing overall state-of-the-art performance
during all stages of training. Moreover, AQE is very simple, requiring neither
distributional representation of critics nor target randomization. The
effectiveness of AQE is further supported by our extensive experiments,
ablations, and theoretical results. <br><br></div></a>
</td></tr>
<tr id=" 133 " class="entry"><td>
<a onclick="toggleVisibility('abstract133');">[|&bull;|]</a><a href="http://arxiv.org/abs/2104.07749v3">
<b/> Actionable Models: Unsupervised Offline Reinforcement Learning of
Robotic Skills (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2104.07749v3">
Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jake Varley, Alex Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract133');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract133');"><div id="abstract133" style="font-size: 1rem; color:black; display: none">
<u><I> 2104.07749v3 &nbsp; - &nbsp;
{exploration}
{goal-conditioned}
{offline}
{robot}
{skill}
</I></u><br> We consider the problem of learning useful robotic skills from previously
collected offline data without access to manually specified rewards or
additional online exploration, a setting that is becoming increasingly
important for scaling robot learning by reusing past robotic data. In
particular, we propose the objective of learning a functional understanding of
the environment by learning to reach any goal state in a given dataset. We
employ goal-conditioned Q-learning with hindsight relabeling and develop
several techniques that enable training in a particularly challenging offline
setting. We find that our method can operate on high-dimensional camera images
and learn a variety of skills on real robots that generalize to previously
unseen scenes and objects. We also show that our method can learn to reach
long-horizon goals across multiple episodes through goal chaining, and learn
rich representations that can help with downstream tasks through pre-training
or auxiliary objectives. The videos of our experiments can be found at
https://actionable-models.github.io <br><br></div></a>
</td></tr>
<tr id=" 134 " class="entry"><td>
<a onclick="toggleVisibility('abstract134');">[|&bull;|]</a><a href="http://arxiv.org/abs/2109.13841v2">
<b/> Bottom-Up Skill Discovery from Unsegmented Demonstrations for
Long-Horizon Robot Manipulation (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2109.13841v2">
Yifeng Zhu, Peter Stone, Yuke Zhu &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract134');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract134');"><div id="abstract134" style="font-size: 1rem; color:black; display: none">
<u><I> 2109.13841v2 &nbsp; - &nbsp;
{control}
{goal-conditioned}
{hierarchical}
{imitation}
{meta}
{multi-task}
{robot}
{skill}
</I></u><br> We tackle real-world long-horizon robot manipulation tasks through skill
discovery. We present a bottom-up approach to learning a library of reusable
skills from unsegmented demonstrations and use these skills to synthesize
prolonged robot behaviors. Our method starts with constructing a hierarchical
task structure from each demonstration through agglomerative clustering. From
the task structures of multi-task demonstrations, we identify skills based on
the recurring patterns and train goal-conditioned sensorimotor policies with
hierarchical imitation learning. Finally, we train a meta controller to compose
these skills to solve long-horizon manipulation tasks. The entire model can be
trained on a small set of human demonstrations collected within 30 minutes
without further annotations, making it amendable to real-world deployment. We
systematically evaluated our method in simulation environments and on a real
robot. Our method has shown superior performance over state-of-the-art
imitation learning methods in multi-stage manipulation tasks. Furthermore,
skills discovered from multi-task demonstrations boost the average task success
by $8\%$ compared to those discovered from individual tasks. <br><br></div></a>
</td></tr>
<tr id=" 135 " class="entry"><td>
<a onclick="toggleVisibility('abstract135');">[|&bull;|]</a><a href="http://arxiv.org/abs/2102.09430v4">
<b/> State Entropy Maximization with Random Encoders for Efficient
Exploration (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2102.09430v4">
Younggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, Pieter Abbeel, Kimin Lee &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract135');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract135');"><div id="abstract135" style="font-size: 1rem; color:black; display: none">
<u><I> 2102.09430v4 &nbsp; - &nbsp;
{control}
{deep}
{entropy}
{exploration}
{intrinsic}
{model-based}
{reinforcement}
</I></u><br> Recent exploration methods have proven to be a recipe for improving
sample-efficiency in deep reinforcement learning (RL). However, efficient
exploration in high-dimensional observation spaces still remains a challenge.
This paper presents Random Encoders for Efficient Exploration (RE3), an
exploration method that utilizes state entropy as an intrinsic reward. In order
to estimate state entropy in environments with high-dimensional observations,
we utilize a k-nearest neighbor entropy estimator in the low-dimensional
representation space of a convolutional encoder. In particular, we find that
the state entropy can be estimated in a stable and compute-efficient manner by
utilizing a randomly initialized encoder, which is fixed throughout training.
Our experiments show that RE3 significantly improves the sample-efficiency of
both model-free and model-based RL methods on locomotion and navigation tasks
from DeepMind Control Suite and MiniGrid benchmarks. We also show that RE3
allows learning diverse behaviors without extrinsic rewards, effectively
improving sample-efficiency in downstream tasks. Source code and videos are
available at https://sites.google.com/view/re3-rl. <br><br></div></a>
</td></tr>
<tr id=" 136 " class="entry"><td>
<a onclick="toggleVisibility('abstract136');">[|&bull;|]</a><a href="http://arxiv.org/abs/2111.07999v1">
<b/> Adversarial Skill Chaining for Long-Horizon Robot Manipulation via
Terminal State Regularization (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2111.07999v1">
Youngwoon Lee, Joseph J. Lim, Anima Anandkumar, Yuke Zhu &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract136');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract136');"><div id="abstract136" style="font-size: 1rem; color:black; display: none">
<u><I> 2111.07999v1 &nbsp; - &nbsp;
{reinforcement}
{skill}
</I></u><br> Skill chaining is a promising approach for synthesizing complex behaviors by
sequentially combining previously learned skills. Yet, a naive composition of
skills fails when a policy encounters a starting state never seen during its
training. For successful skill chaining, prior approaches attempt to widen the
policy's starting state distribution. However, these approaches require larger
state distributions to be covered as more policies are sequenced, and thus are
limited to short skill sequences. In this paper, we propose to chain multiple
policies without excessively large initial state distributions by regularizing
the terminal state distributions in an adversarial learning framework. We
evaluate our approach on two complex long-horizon manipulation tasks of
furniture assembly. Our results have shown that our method establishes the
first model-free reinforcement learning algorithm to solve these tasks; whereas
prior skill chaining approaches fail. The code and videos are available at
https://clvrai.com/skill-chaining <br><br></div></a>
</td></tr>
<tr id=" 137 " class="entry"><td>
<a onclick="toggleVisibility('abstract137');">[|&bull;|]</a><a href="http://arxiv.org/abs/2103.07607v2">
<b/> Solving Compositional Reinforcement Learning Problems via Task Reduction (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2103.07607v2">
Yunfei Li, Yilin Wu, Huazhe Xu, Xiaolong Wang, Yi Wu &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract137');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract137');"><div id="abstract137" style="font-size: 1rem; color:black; display: none">
<u><I> 2103.07607v2 &nbsp; - &nbsp;
{control}
{imitation}
{reinforcement}
{sparse}
</I></u><br> We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for
solving compositional reinforcement learning problems. SIR is based on two core
ideas: task reduction and self-imitation. Task reduction tackles a
hard-to-solve task by actively reducing it to an easier task whose solution is
known by the RL agent. Once the original hard task is successfully solved by
task reduction, the agent naturally obtains a self-generated solution
trajectory to imitate. By continuously collecting and imitating such
demonstrations, the agent is able to progressively expand the solved subspace
in the entire task space. Experiment results show that SIR can significantly
accelerate and improve learning on a variety of challenging sparse-reward
continuous-control problems with compositional structures. Code and videos are
available at https://sites.google.com/view/sir-compositional. <br><br></div></a>
</td></tr>
<tr id=" 138 " class="entry"><td>
<a onclick="toggleVisibility('abstract138');">[|&bull;|]</a><a href="http://arxiv.org/abs/2106.05969v3">
<b/> Dynamics-Regulated Kinematic Policy for Egocentric Pose Estimation (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2106.05969v3">
Zhengyi Luo, Ryo Hachiuma, Ye Yuan, Kris Kitani &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract138');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract138');"><div id="abstract138" style="font-size: 1rem; color:black; display: none">
<u><I> 2106.05969v3 &nbsp; - &nbsp;
{control}
{kinematic}
</I></u><br> We propose a method for object-aware 3D egocentric pose estimation that
tightly integrates kinematics modeling, dynamics modeling, and scene object
information. Unlike prior kinematics or dynamics-based approaches where the two
components are used disjointly, we synergize the two approaches via
dynamics-regulated training. At each timestep, a kinematic model is used to
provide a target pose using video evidence and simulation state. Then, a
prelearned dynamics model attempts to mimic the kinematic pose in a physics
simulator. By comparing the pose instructed by the kinematic model against the
pose generated by the dynamics model, we can use their misalignment to further
improve the kinematic model. By factoring in the 6DoF pose of objects (e.g.,
chairs, boxes) in the scene, we demonstrate for the first time, the ability to
estimate physically-plausible 3D human-object interactions using a single
wearable camera. We evaluate our egocentric pose estimation method in both
controlled laboratory settings and real-world scenarios. <br><br></div></a>
</td></tr>
<tr id=" 139 " class="entry"><td>
<a onclick="toggleVisibility('abstract139');">[|&bull;|]</a><a href="http://arxiv.org/abs/2105.00371v1">
<b/> Discovering Diverse Athletic Jumping Strategies (2021)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2105.00371v1">
Zhiqi Yin, Zeshi Yang, Michiel van de Panne, KangKang Yin &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract139');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract139');"><div id="abstract139" style="font-size: 1rem; color:black; display: none">
<u><I> 2105.00371v1 &nbsp; - &nbsp;
{control}
{deep}
{diversity}
{imitation}
{reinforcement}
{skill}
</I></u><br> We present a framework that enables the discovery of diverse and
natural-looking motion strategies for athletic skills such as the high jump.
The strategies are realized as control policies for physics-based characters.
Given a task objective and an initial character configuration, the combination
of physics simulation and deep reinforcement learning (DRL) provides a suitable
starting point for automatic control policy training. To facilitate the
learning of realistic human motions, we propose a Pose Variational Autoencoder
(P-VAE) to constrain the actions to a subspace of natural poses. In contrast to
motion imitation methods, a rich variety of novel strategies can naturally
emerge by exploring initial character states through a sample-efficient
Bayesian diversity search (BDS) algorithm. A second stage of optimization that
encourages novel policies can further enrich the unique strategies discovered.
Our method allows for the discovery of diverse and novel strategies for
athletic jumping motions such as high jumps and obstacle jumps with no motion
examples and less reward engineering than prior work. <br><br></div></a>
</td></tr>
<tr id=" 140 " class="entry"><td>
<a onclick="toggleVisibility('abstract140');">[|&bull;|]</a><a href="http://arxiv.org/abs/2004.12919v6">
<b/> First return, then explore (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2004.12919v6">
Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, Jeff Clune &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract140');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract140');"><div id="abstract140" style="font-size: 1rem; color:black; display: none">
<u><I> 2004.12919v6 &nbsp; - &nbsp;
{exploration}
{goal-conditioned}
{reinforcement}
{robot}
{sparse}
</I></u><br> The promise of reinforcement learning is to solve complex sequential decision
problems autonomously by specifying a high-level reward function only. However,
reinforcement learning algorithms struggle when, as is often the case, simple
and intuitive rewards provide sparse and deceptive feedback. Avoiding these
pitfalls requires thoroughly exploring the environment, but creating algorithms
that can do so remains one of the central challenges of the field. We
hypothesise that the main impediment to effective exploration originates from
algorithms forgetting how to reach previously visited states ("detachment") and
from failing to first return to a state before exploring from it
("derailment"). We introduce Go-Explore, a family of algorithms that addresses
these two challenges directly through the simple principles of explicitly
remembering promising states and first returning to such states before
intentionally exploring. Go-Explore solves all heretofore unsolved Atari games
and surpasses the state of the art on all hard-exploration games, with orders
of magnitude improvements on the grand challenges Montezuma's Revenge and
Pitfall. We also demonstrate the practical potential of Go-Explore on a
sparse-reward pick-and-place robotics task. Additionally, we show that adding a
goal-conditioned policy can further improve Go-Explore's exploration efficiency
and enable it to handle stochasticity throughout training. The substantial
performance gains from Go-Explore suggest that the simple principles of
remembering states, returning to them, and exploring from them are a powerful
and general approach to exploration, an insight that may prove critical to the
creation of truly intelligent learning agents. <br><br></div></a>
</td></tr>
<tr id=" 141 " class="entry"><td>
<a onclick="toggleVisibility('abstract141');">[|&bull;|]</a><a href="http://arxiv.org/abs/2002.06038v1">
<b/> Never Give Up: Learning Directed Exploration Strategies (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2002.06038v1">
Adri&#224; Puigdom&#232;nech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven Kapturowski, Olivier Tieleman, Mart&#237;n Arjovsky, Alexander Pritzel, Andew Bolt, Charles Blundell &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract141');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract141');"><div id="abstract141" style="font-size: 1rem; color:black; display: none">
<u><I> 2002.06038v1 &nbsp; - &nbsp;
{control}
{exploration}
{intrinsic}
{reinforcement}
{self-supervised}
{transfer}
</I></u><br> We propose a reinforcement learning agent to solve hard exploration games by
learning a range of directed exploratory policies. We construct an episodic
memory-based intrinsic reward using k-nearest neighbors over the agent's recent
experience to train the directed exploratory policies, thereby encouraging the
agent to repeatedly revisit all states in its environment. A self-supervised
inverse dynamics model is used to train the embeddings of the nearest neighbour
lookup, biasing the novelty signal towards what the agent can control. We
employ the framework of Universal Value Function Approximators (UVFA) to
simultaneously learn many directed exploration policies with the same neural
network, with different trade-offs between exploration and exploitation. By
using the same neural network for different degrees of
exploration/exploitation, transfer is demonstrated from predominantly
exploratory policies yielding effective exploitative policies. The proposed
method can be incorporated to run with modern distributed RL agents that
collect large amounts of experience from many actors running in parallel on
separate environment instances. Our method doubles the performance of the base
agent in all hard exploration in the Atari-57 suite while maintaining a very
high score across the remaining games, obtaining a median human normalised
score of 1344.0%. Notably, the proposed method is the first algorithm to
achieve non-zero rewards (with a mean score of 8,400) in the game of Pitfall!
without using demonstrations or hand-crafted features. <br><br></div></a>
</td></tr>
<tr id=" 142 " class="entry"><td>
<a onclick="toggleVisibility('abstract142');">[|&bull;|]</a><a href="http://arxiv.org/abs/2005.08290v3">
<b/> Longitudinal high-throughput TCR repertoire profiling reveals the
dynamics of T cell memory formation after mild COVID-19 infection (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2005.08290v3">
Anastasia A. Minervina, Ekaterina A. Komech, Aleksei Titov, Meriem Bensouda Koraichi, Elisa Rosati, Ilgar Z. Mamedov, Andre Franke, Grigory A. Efimov, Dmitriy M. Chudakov, Thierry Mora, Aleksandra M. Walczak, Yuri B. Lebedev, Mikhail V. Pogorelyy &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract142');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract142');"><div id="abstract142" style="font-size: 1rem; color:black; display: none">
<u><I> 2005.08290v3 &nbsp; - &nbsp;
{diversity}
</I></u><br> COVID-19 is a global pandemic caused by the SARS-CoV-2 coronavirus. T cells
play a key role in the adaptive antiviral immune response by killing infected
cells and facilitating the selection of virus-specific antibodies. However
neither the dynamics and cross-reactivity of the SARS-CoV-2-specific T cell
response nor the diversity of resulting immune memory are well understood. In
this study we use longitudinal high-throughput T cell receptor (TCR) sequencing
to track changes in the T cell repertoire following two mild cases of COVID-19.
In both donors we identified CD4+ and CD8+ T cell clones with transient clonal
expansion after infection. The antigen specificity of CD8+ TCR sequences to
SARS-CoV-2 epitopes was confirmed by both MHC tetramer binding and presence in
large database of SARS-CoV-2 epitope-specific TCRs. We describe characteristic
motifs in TCR sequences of COVID-19-reactive clones and show preferential
occurence of these motifs in publicly available large dataset of repertoires
from COVID-19 patients. We show that in both donors the majority of
infection-reactive clonotypes acquire memory phenotypes. Certain T cell clones
were detected in the memory fraction at the pre-infection timepoint, suggesting
participation of pre-existing cross-reactive memory T cells in the immune
response to SARS-CoV-2. <br><br></div></a>
</td></tr>
<tr id=" 143 " class="entry"><td>
<a onclick="toggleVisibility('abstract143');">[|&bull;|]</a><a href="http://arxiv.org/abs/2005.05719v2">
<b/> Smooth Exploration for Robotic Reinforcement Learning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2005.05719v2">
Antonin Raffin, Jens Kober, Freek Stulp &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract143');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract143');"><div id="abstract143" style="font-size: 1rem; color:black; display: none">
<u><I> 2005.05719v2 &nbsp; - &nbsp;
{control}
{deep}
{exploration}
{reinforcement}
{robot}
{skill}
</I></u><br> Reinforcement learning (RL) enables robots to learn skills from interactions
with the real world. In practice, the unstructured step-based exploration used
in Deep RL -- often very successful in simulation -- leads to jerky motion
patterns on real robots. Consequences of the resulting shaky behavior are poor
exploration, or even damage to the robot. We address these issues by adapting
state-dependent exploration (SDE) to current Deep RL algorithms. To enable this
adaptation, we propose two extensions to the original SDE, using more general
features and re-sampling the noise periodically, which leads to a new
exploration method generalized state-dependent exploration (gSDE). We evaluate
gSDE both in simulation, on PyBullet continuous control tasks, and directly on
three different real robots: a tendon-driven elastic robot, a quadruped and an
RC car. The noise sampling interval of gSDE permits to have a compromise
between performance and smoothness, which allows training directly on the real
robots without loss of performance. The code is available at
https://github.com/DLR-RM/stable-baselines3. <br><br></div></a>
</td></tr>
<tr id=" 144 " class="entry"><td>
<a onclick="toggleVisibility('abstract144');">[|&bull;|]</a><a href="http://arxiv.org/abs/2010.13611v3">
<b/> OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement
Learning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2010.13611v3">
Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, Ofir Nachum &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract144');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract144');"><div id="abstract144" style="font-size: 1rem; color:black; display: none">
<u><I> 2010.13611v3 &nbsp; - &nbsp;
{exploration}
{few-shot}
{imitation}
{offline}
{reinforcement}
{transfer}
</I></u><br> Reinforcement learning (RL) has achieved impressive performance in a variety
of online settings in which an agent's ability to query the environment for
transitions and rewards is effectively unlimited. However, in many practical
applications, the situation is reversed: an agent may have access to large
amounts of undirected offline experience data, while access to the online
environment is severely limited. In this work, we focus on this offline
setting. Our main insight is that, when presented with offline data composed of
a variety of behaviors, an effective way to leverage this data is to extract a
continuous space of recurring and temporally extended primitive behaviors
before using these primitives for downstream task learning. Primitives
extracted in this way serve two purposes: they delineate the behaviors that are
supported by the data from those that are not, making them useful for avoiding
distributional shift in offline RL; and they provide a degree of temporal
abstraction, which reduces the effective horizon yielding better learning in
theory, and improved offline RL in practice. In addition to benefiting offline
policy optimization, we show that performing offline primitive learning in this
way can also be leveraged for improving few-shot imitation learning as well as
exploration and transfer in online RL on a variety of benchmark domains.
Visualizations are available at https://sites.google.com/view/opal-iclr <br><br></div></a>
</td></tr>
<tr id=" 145 " class="entry"><td>
<a onclick="toggleVisibility('abstract145');">[|&bull;|]</a><a href="http://arxiv.org/abs/2004.12974v1">
<b/> Emergent Real-World Robotic Skills via Unsupervised Off-Policy
Reinforcement Learning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2004.12974v1">
Archit Sharma, Michael Ahn, Sergey Levine, Vikash Kumar, Karol Hausman, Shixiang Gu &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract145');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract145');"><div id="abstract145" style="font-size: 1rem; color:black; display: none">
<u><I> 2004.12974v1 &nbsp; - &nbsp;
{control}
{off-policy}
{reinforcement}
{robot}
{skill}
{unsupervised}
</I></u><br> Reinforcement learning provides a general framework for learning robotic
skills while minimizing engineering effort. However, most reinforcement
learning algorithms assume that a well-designed reward function is provided,
and learn a single behavior for that single reward function. Such reward
functions can be difficult to design in practice. Can we instead develop
efficient reinforcement learning methods that acquire diverse skills without
any reward function, and then repurpose these skills for downstream tasks? In
this paper, we demonstrate that a recently proposed unsupervised skill
discovery algorithm can be extended into an efficient off-policy method, making
it suitable for performing unsupervised reinforcement learning in the real
world. Firstly, we show that our proposed algorithm provides substantial
improvement in learning efficiency, making reward-free real-world training
feasible. Secondly, we move beyond the simulation environments and evaluate the
algorithm on real physical hardware. On quadrupeds, we observe that locomotion
skills with diverse gaits and different orientations emerge without any rewards
or demonstrations. We also demonstrate that the learned skills can be composed
using model predictive control for goal-oriented navigation, without any
additional training. <br><br></div></a>
</td></tr>
<tr id=" 146 " class="entry"><td>
<a onclick="toggleVisibility('abstract146');">[|&bull;|]</a><a href="http://arxiv.org/abs/2005.04269v1">
<b/> Controlling Overestimation Bias with Truncated Mixture of Continuous
Distributional Quantile Critics (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2005.04269v1">
Arsenii Kuznetsov, Pavel Shvechikov, Alexander Grishin, Dmitry Vetrov &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract146');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract146');"><div id="abstract146" style="font-size: 1rem; color:black; display: none">
<u><I> 2005.04269v1 &nbsp; - &nbsp;
{control}
{humanoid}
{off-policy}
</I></u><br> The overestimation bias is one of the major impediments to accurate
off-policy learning. This paper investigates a novel way to alleviate the
overestimation bias in a continuous control setting. Our method---Truncated
Quantile Critics, TQC,---blends three ideas: distributional representation of a
critic, truncation of critics prediction, and ensembling of multiple critics.
Distributional representation and truncation allow for arbitrary granular
overestimation control, while ensembling provides additional score
improvements. TQC outperforms the current state of the art on all environments
from the continuous control benchmark suite, demonstrating 25% improvement on
the most challenging Humanoid environment. <br><br></div></a>
</td></tr>
<tr id=" 147 " class="entry"><td>
<a onclick="toggleVisibility('abstract147');">[|&bull;|]</a><a href="http://arxiv.org/abs/2006.09359v6">
<b/> AWAC: Accelerating Online Reinforcement Learning with Offline Datasets (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2006.09359v6">
Ashvin Nair, Abhishek Gupta, Murtaza Dalal, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract147');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract147');"><div id="abstract147" style="font-size: 1rem; color:black; display: none">
<u><I> 2006.09359v6 &nbsp; - &nbsp;
{control}
{exploration}
{offline}
{optimal}
{reinforcement}
{robot}
{skill}
</I></u><br> Reinforcement learning (RL) provides an appealing formalism for learning
control policies from experience. However, the classic active formulation of RL
necessitates a lengthy active exploration process for each behavior, making it
difficult to apply in real-world settings such as robotic control. If we can
instead allow RL algorithms to effectively use previously collected data to aid
the online learning process, such applications could be made substantially more
practical: the prior data would provide a starting point that mitigates
challenges due to exploration and sample complexity, while the online training
enables the agent to perfect the desired skill. Such prior data could either
constitute expert demonstrations or sub-optimal prior data that illustrates
potentially useful transitions. While a number of prior methods have either
used optimal demonstrations to bootstrap RL, or have used sub-optimal data to
train purely offline, it remains exceptionally difficult to train a policy with
offline data and actually continue to improve it further with online RL. In
this paper we analyze why this problem is so challenging, and propose an
algorithm that combines sample efficient dynamic programming with maximum
likelihood policy updates, providing a simple and effective framework that is
able to leverage large amounts of offline data and then quickly perform online
fine-tuning of RL policies. We show that our method, advantage weighted actor
critic (AWAC), enables rapid learning of skills with a combination of prior
demonstration data and online experience. We demonstrate these benefits on
simulated and real-world robotics domains, including dexterous manipulation
with a real multi-fingered hand, drawer opening with a robotic arm, and
rotating a valve. Our results show that incorporating prior data can reduce the
time required to learn a range of robotic skills to practical time-scales. <br><br></div></a>
</td></tr>
<tr id=" 148 " class="entry"><td>
<a onclick="toggleVisibility('abstract148');">[|&bull;|]</a><a href="http://arxiv.org/abs/2011.10024v1">
<b/> Parrot: Data-Driven Behavioral Priors for Reinforcement Learning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2011.10024v1">
Avi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract148');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract148');"><div id="abstract148" style="font-size: 1rem; color:black; display: none">
<u><I> 2011.10024v1 &nbsp; - &nbsp;
{control}
{reinforcement}
{robot}
{sparse}
</I></u><br> Reinforcement learning provides a general framework for flexible decision
making and control, but requires extensive data collection for each new task
that an agent needs to learn. In other machine learning fields, such as natural
language processing or computer vision, pre-training on large, previously
collected datasets to bootstrap learning for new tasks has emerged as a
powerful paradigm to reduce data requirements when learning a new task. In this
paper, we ask the following question: how can we enable similarly useful
pre-training for RL agents? We propose a method for pre-training behavioral
priors that can capture complex input-output relationships observed in
successful trials from a wide range of previously seen tasks, and we show how
this learned prior can be used for rapidly learning new tasks without impeding
the RL agent's ability to try out novel behaviors. We demonstrate the
effectiveness of our approach in challenging robotic manipulation domains
involving image observations and sparse reward functions, where our method
outperforms prior works by a substantial margin. <br><br></div></a>
</td></tr>
<tr id=" 149 " class="entry"><td>
<a onclick="toggleVisibility('abstract149');">[|&bull;|]</a><a href="http://arxiv.org/abs/2003.07305v1">
<b/> DisCor: Corrective Feedback in Reinforcement Learning via Distribution
Correction (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2003.07305v1">
Aviral Kumar, Abhishek Gupta, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract149');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract149');"><div id="abstract149" style="font-size: 1rem; color:black; display: none">
<u><I> 2003.07305v1 &nbsp; - &nbsp;
{deep}
{multi-task}
{on-policy}
{optimal}
{reinforcement}
{sparse}
</I></u><br> Deep reinforcement learning can learn effective policies for a wide range of
tasks, but is notoriously difficult to use due to instability and sensitivity
to hyperparameters. The reasons for this remain unclear. When using standard
supervised methods (e.g., for bandits), on-policy data collection provides
"hard negatives" that correct the model in precisely those states and actions
that the policy is likely to visit. We call this phenomenon "corrective
feedback." We show that bootstrapping-based Q-learning algorithms do not
necessarily benefit from this corrective feedback, and training on the
experience collected by the algorithm is not sufficient to correct errors in
the Q-function. In fact, Q-learning and related methods can exhibit
pathological interactions between the distribution of experience collected by
the agent and the policy induced by training on that experience, leading to
potential instability, sub-optimal convergence, and poor results when learning
from noisy, sparse or delayed rewards. We demonstrate the existence of this
problem, both theoretically and empirically. We then show that a specific
correction to the data distribution can mitigate this issue. Based on these
observations, we propose a new algorithm, DisCor, which computes an
approximation to this optimal distribution and uses it to re-weight the
transitions used for training, resulting in substantial improvements in a range
of challenging RL settings, such as multi-task learning and learning from noisy
reward signals. Blog post presenting a summary of this work is available at:
https://bair.berkeley.edu/blog/2020/03/16/discor/. <br><br></div></a>
</td></tr>
<tr id=" 150 " class="entry"><td>
<a onclick="toggleVisibility('abstract150');">[|&bull;|]</a><a href="http://arxiv.org/abs/2004.12524v1">
<b/> Sequential Interpretability: Methods, Applications, and Future Direction
for Understanding Deep Learning Models in the Context of Sequential Data (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2004.12524v1">
Benjamin Shickel, Parisa Rashidi &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract150');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract150');"><div id="abstract150" style="font-size: 1rem; color:black; display: none">
<u><I> 2004.12524v1 &nbsp; - &nbsp;
{deep}
{evolution}
{imitation}
</I></u><br> Deep learning continues to revolutionize an ever-growing number of critical
application areas including healthcare, transportation, finance, and basic
sciences. Despite their increased predictive power, model transparency and
human explainability remain a significant challenge due to the "black box"
nature of modern deep learning models. In many cases the desired balance
between interpretability and performance is predominately task specific.
Human-centric domains such as healthcare necessitate a renewed focus on
understanding how and why these frameworks are arriving at critical and
potentially life-or-death decisions. Given the quantity of research and
empirical successes of deep learning for computer vision, most of the existing
interpretability research has focused on image processing techniques.
Comparatively, less attention has been paid to interpreting deep learning
frameworks using sequential data. Given recent deep learning advancements in
highly sequential domains such as natural language processing and physiological
signal processing, the need for deep sequential explanations is at an all-time
high. In this paper, we review current techniques for interpreting deep
learning techniques involving sequential data, identify similarities to
non-sequential methods, and discuss current limitations and future avenues of
sequential interpretability research. <br><br></div></a>
</td></tr>
<tr id=" 151 " class="entry"><td>
<a onclick="toggleVisibility('abstract151');">[|&bull;|]</a><a href="http://arxiv.org/abs/2011.00440v2">
<b/> Learning When to Switch: Composing Controllers to Traverse a Sequence of
Terrain Artifacts (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2011.00440v2">
Brendan Tidd, Nicolas Hudson, Akansel Cosgun, Jurgen Leitner &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract151');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract151');"><div id="abstract151" style="font-size: 1rem; color:black; display: none">
<u><I> 2011.00440v2 &nbsp; - &nbsp;
{control}
{deep}
{reinforcement}
{robot}
</I></u><br> Legged robots often use separate control policiesthat are highly engineered
for traversing difficult terrain suchas stairs, gaps, and steps, where
switching between policies isonly possible when the robot is in a region that
is commonto adjacent controllers. Deep Reinforcement Learning (DRL)is a
promising alternative to hand-crafted control design,though typically requires
the full set of test conditions to beknown before training. DRL policies can
result in complex(often unrealistic) behaviours that have few or no
overlappingregions between adjacent policies, making it difficult to
switchbehaviours. In this work we develop multiple DRL policieswith Curriculum
Learning (CL), each that can traverse asingle respective terrain condition,
while ensuring an overlapbetween policies. We then train a network for each
destinationpolicy that estimates the likelihood of successfully switchingfrom
any other policy. We evaluate our switching methodon a previously unseen
combination of terrain artifacts andshow that it performs better than heuristic
methods. Whileour method is trained on individual terrain types, it
performscomparably to a Deep Q Network trained on the full set ofterrain
conditions. This approach allows the development ofseparate policies in
constrained conditions with embedded priorknowledge about each behaviour, that
is scalable to any numberof behaviours, and prepares DRL methods for
applications inthe real world <br><br></div></a>
</td></tr>
<tr id=" 152 " class="entry"><td>
<a onclick="toggleVisibility('abstract152');">[|&bull;|]</a><a href="http://arxiv.org/abs/2010.06491v1">
<b/> Broadly-Exploring, Local-Policy Trees for Long-Horizon Task Planning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2010.06491v1">
Brian Ichter, Pierre Sermanet, Corey Lynch &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract152');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract152');"><div id="abstract152" style="font-size: 1rem; color:black; display: none">
<u><I> 2010.06491v1 &nbsp; - &nbsp;
{exploration}
{goal-conditioned}
{model-based}
{motion planning}
</I></u><br> Long-horizon planning in realistic environments requires the ability to
reason over sequential tasks in high-dimensional state spaces with complex
dynamics. Classical motion planning algorithms, such as rapidly-exploring
random trees, are capable of efficiently exploring large state spaces and
computing long-horizon, sequential plans. However, these algorithms are
generally challenged with complex, stochastic, and high-dimensional state
spaces as well as in the presence of narrow passages, which naturally emerge in
tasks that interact with the environment. Machine learning offers a promising
solution for its ability to learn general policies that can handle complex
interactions and high-dimensional observations. However, these policies are
generally limited in horizon length. Our approach, Broadly-Exploring,
Local-policy Trees (BELT), merges these two approaches to leverage the
strengths of both through a task-conditioned, model-based tree search. BELT
uses an RRT-inspired tree search to efficiently explore the state space.
Locally, the exploration is guided by a task-conditioned, learned policy
capable of performing general short-horizon tasks. This task space can be quite
general and abstract; its only requirements are to be sampleable and to
well-cover the space of useful tasks. This search is aided by a
task-conditioned model that temporally extends dynamics propagation to allow
long-horizon search and sequential reasoning over tasks. BELT is demonstrated
experimentally to be able to plan long-horizon, sequential trajectories with a
goal conditioned policy and generate plans that are robust. <br><br></div></a>
</td></tr>
<tr id=" 153 " class="entry"><td>
<a onclick="toggleVisibility('abstract153');">[|&bull;|]</a><a href="http://arxiv.org/abs/2006.11419v4">
<b/> FISAR: Forward Invariant Safe Reinforcement Learning with a Deep Neural
Network-Based Optimize (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2006.11419v4">
Chuangchuang Sun, Dong-Ki Kim, Jonathan P. How &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract153');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract153');"><div id="abstract153" style="font-size: 1rem; color:black; display: none">
<u><I> 2006.11419v4 &nbsp; - &nbsp;
{deep}
{reinforcement}
</I></u><br> This paper investigates reinforcement learning with constraints, which are
indispensable in safety-critical environments. To drive the constraint
violation monotonically decrease, we take the constraints as Lyapunov functions
and impose new linear constraints on the policy parameters' updating dynamics.
As a result, the original safety set can be forward-invariant. However, because
the new guaranteed-feasible constraints are imposed on the updating dynamics
instead of the original policy parameters, classic optimization algorithms are
no longer applicable. To address this, we propose to learn a generic deep
neural network (DNN)-based optimizer to optimize the objective while satisfying
the linear constraints. The constraint-satisfaction is achieved via projection
onto a polytope formulated by multiple linear inequality constraints, which can
be solved analytically with our newly designed metric. To the best of our
knowledge, this is the \textit{first} DNN-based optimizer for constrained
optimization with the forward invariance guarantee. We show that our optimizer
trains a policy to decrease the constraint violation and maximize the
cumulative reward monotonically. Results on numerical constrained optimization
and obstacle-avoidance navigation validate the theoretical findings. <br><br></div></a>
</td></tr>
<tr id=" 154 " class="entry"><td>
<a onclick="toggleVisibility('abstract154');">[|&bull;|]</a><a href="http://arxiv.org/abs/2010.02193v4">
<b/> Mastering Atari with Discrete World Models (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2010.02193v4">
Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, Jimmy Ba &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract154');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract154');"><div id="abstract154" style="font-size: 1rem; color:black; display: none">
<u><I> 2010.02193v4 &nbsp; - &nbsp;
{humanoid}
{reinforcement}
{robot}
</I></u><br> Intelligent agents need to generalize from past experience to achieve goals
in complex environments. World models facilitate such generalization and allow
learning behaviors from imagined outcomes to increase sample-efficiency. While
learning world models from image inputs has recently become feasible for some
tasks, modeling Atari games accurately enough to derive successful behaviors
has remained an open challenge for many years. We introduce DreamerV2, a
reinforcement learning agent that learns behaviors purely from predictions in
the compact latent space of a powerful world model. The world model uses
discrete representations and is trained separately from the policy. DreamerV2
constitutes the first agent that achieves human-level performance on the Atari
benchmark of 55 tasks by learning behaviors inside a separately trained world
model. With the same computational budget and wall-clock time, Dreamer V2
reaches 200M frames and surpasses the final performance of the top single-GPU
agents IQN and Rainbow. DreamerV2 is also applicable to tasks with continuous
actions, where it learns an accurate world model of a complex humanoid robot
and solves stand-up and walking from only pixel inputs. <br><br></div></a>
</td></tr>
<tr id=" 155 " class="entry"><td>
<a onclick="toggleVisibility('abstract155');">[|&bull;|]</a><a href="http://arxiv.org/abs/2011.01975v1">
<b/> Rearrangement: A Challenge for Embodied AI (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2011.01975v1">
Dhruv Batra, Angel X. Chang, Sonia Chernova, Andrew J. Davison, Jia Deng, Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, Manolis Savva, Hao Su &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract155');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract155');"><div id="abstract155" style="font-size: 1rem; color:black; display: none">
<u><I> 2011.01975v1 &nbsp; - &nbsp;
{exploration}
{transfer}
</I></u><br> We describe a framework for research and evaluation in Embodied AI. Our
proposal is based on a canonical task: Rearrangement. A standard task can focus
the development of new techniques and serve as a source of trained models that
can be transferred to other settings. In the rearrangement task, the goal is to
bring a given physical environment into a specified state. The goal state can
be specified by object poses, by images, by a description in language, or by
letting the agent experience the environment in the goal state. We characterize
rearrangement scenarios along different axes and describe metrics for
benchmarking rearrangement performance. To facilitate research and exploration,
we present experimental testbeds of rearrangement scenarios in four different
simulation environments. We anticipate that other datasets will be released and
new simulation platforms will be built to support training of rearrangement
agents and their deployment on physical systems. <br><br></div></a>
</td></tr>
<tr id=" 156 " class="entry"><td>
<a onclick="toggleVisibility('abstract156');">[|&bull;|]</a><a href="http://arxiv.org/abs/2010.14274v1">
<b/> Behavior Priors for Efficient Reinforcement Learning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2010.14274v1">
Dhruva Tirumala, Alexandre Galashov, Hyeonwoo Noh, Leonard Hasenclever, Razvan Pascanu, Jonathan Schwarz, Guillaume Desjardins, Wojciech Marian Czarnecki, Arun Ahuja, Yee Whye Teh, Nicolas Heess &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract156');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract156');"><div id="abstract156" style="font-size: 1rem; color:black; display: none">
<u><I> 2010.14274v1 &nbsp; - &nbsp;
{control}
{hierarchical}
{multi-task}
{reinforcement}
{transfer}
</I></u><br> As we deploy reinforcement learning agents to solve increasingly challenging
problems, methods that allow us to inject prior knowledge about the structure
of the world and effective solution strategies becomes increasingly important.
In this work we consider how information and architectural constraints can be
combined with ideas from the probabilistic modeling literature to learn
behavior priors that capture the common movement and interaction patterns that
are shared across a set of related tasks or contexts. For example the day-to
day behavior of humans comprises distinctive locomotion and manipulation
patterns that recur across many different situations and goals. We discuss how
such behavior patterns can be captured using probabilistic trajectory models
and how these can be integrated effectively into reinforcement learning
schemes, e.g.\ to facilitate multi-task and transfer learning. We then extend
these ideas to latent variable models and consider a formulation to learn
hierarchical priors that capture different aspects of the behavior in reusable
modules. We discuss how such latent variable formulations connect to related
work on hierarchical reinforcement learning (HRL) and mutual information and
curiosity based objectives, thereby offering an alternative perspective on
existing ideas. We demonstrate the effectiveness of our framework by applying
it to a range of simulated continuous control domains. <br><br></div></a>
</td></tr>
<tr id=" 157 " class="entry"><td>
<a onclick="toggleVisibility('abstract157');">[|&bull;|]</a><a href="http://arxiv.org/abs/2002.08484v3">
<b/> Estimating Training Data Influence by Tracing Gradient Descent (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2002.08484v3">
Garima Pruthi, Frederick Liu, Mukund Sundararajan, Satyen Kale &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract157');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract157');"><div id="abstract157" style="font-size: 1rem; color:black; display: none">
<u><I> 2002.08484v3 &nbsp; - &nbsp;
{deep}
{gradient}
</I></u><br> We introduce a method called TracIn that computes the influence of a training
example on a prediction made by the model. The idea is to trace how the loss on
the test point changes during the training process whenever the training
example of interest was utilized. We provide a scalable implementation of
TracIn via: (a) a first-order gradient approximation to the exact computation,
(b) saved checkpoints of standard training procedures, and (c) cherry-picking
layers of a deep neural network. In contrast with previously proposed methods,
TracIn is simple to implement; all it needs is the ability to work with
gradients, checkpoints, and loss functions. The method is general. It applies
to any machine learning model trained using stochastic gradient descent or a
variant of it, agnostic of architecture, domain and task. We expect the method
to be widely useful within processes that study and improve training data. <br><br></div></a>
</td></tr>
<tr id=" 158 " class="entry"><td>
<a onclick="toggleVisibility('abstract158');">[|&bull;|]</a><a href="http://arxiv.org/abs/2004.11667v1">
<b/> PBCS : Efficient Exploration and Exploitation Using a Synergy between
Reinforcement Learning and Motion Planning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2004.11667v1">
Guillaume Matheron, Nicolas Perrin, Olivier Sigaud &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract158');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract158');"><div id="abstract158" style="font-size: 1rem; color:black; display: none">
<u><I> 2004.11667v1 &nbsp; - &nbsp;
{control}
{exploration}
{motion planning}
{reinforcement}
{skill}
</I></u><br> The exploration-exploitation trade-off is at the heart of reinforcement
learning (RL). However, most continuous control benchmarks used in recent RL
research only require local exploration. This led to the development of
algorithms that have basic exploration capabilities, and behave poorly in
benchmarks that require more versatile exploration. For instance, as
demonstrated in our empirical study, state-of-the-art RL algorithms such as
DDPG and TD3 are unable to steer a point mass in even small 2D mazes. In this
paper, we propose a new algorithm called "Plan, Backplay, Chain Skills" (PBCS)
that combines motion planning and reinforcement learning to solve hard
exploration environments. In a first phase, a motion planning algorithm is used
to find a single good trajectory, then an RL algorithm is trained using a
curriculum derived from the trajectory, by combining a variant of the Backplay
algorithm and skill chaining. We show that this method outperforms
state-of-the-art RL algorithms in 2D maze environments of various sizes, and is
able to improve on the trajectory obtained by the motion planning phase. <br><br></div></a>
</td></tr>
<tr id=" 159 " class="entry"><td>
<a onclick="toggleVisibility('abstract159');">[|&bull;|]</a><a href="http://arxiv.org/abs/2005.10934v3">
<b/> LEAF: Latent Exploration Along the Frontier (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2005.10934v3">
Homanga Bharadhwaj, Animesh Garg, Florian Shkurti &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract159');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract159');"><div id="abstract159" style="font-size: 1rem; color:black; display: none">
<u><I> 2005.10934v3 &nbsp; - &nbsp;
{deep}
{exploration}
{robot}
{self-supervised}
</I></u><br> Self-supervised goal proposal and reaching is a key component for exploration
and efficient policy learning algorithms. Such a self-supervised approach
without access to any oracle goal sampling distribution requires deep
exploration and commitment so that long horizon plans can be efficiently
discovered. In this paper, we propose an exploration framework, which learns a
dynamics-aware manifold of reachable states. For a goal, our proposed method
deterministically visits a state at the current frontier of reachable states
(commitment/reaching) and then stochastically explores to reach the goal
(exploration). This allocates exploration budget near the frontier of the
reachable region instead of its interior. We target the challenging problem of
policy learning from initial and goal states specified as images, and do not
assume any access to the underlying ground-truth states of the robot and the
environment. To keep track of reachable latent states, we propose a
distance-conditioned reachability network that is trained to infer whether one
state is reachable from another within the specified latent space distance.
Given an initial state, we obtain a frontier of reachable states from that
state. By incorporating a curriculum for sampling easier goals (closer to the
start state) before more difficult goals, we demonstrate that the proposed
self-supervised exploration algorithm, superior performance compared to
existing baselines on a set of challenging robotic
environments.https://sites.google.com/view/leaf-exploration <br><br></div></a>
</td></tr>
<tr id=" 160 " class="entry"><td>
<a onclick="toggleVisibility('abstract160');">[|&bull;|]</a><a href="http://arxiv.org/abs/2004.13649v4">
<b/> Image Augmentation Is All You Need: Regularizing Deep Reinforcement
Learning from Pixels (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2004.13649v4">
Ilya Kostrikov, Denis Yarats, Rob Fergus &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract160');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract160');"><div id="abstract160" style="font-size: 1rem; color:black; display: none">
<u><I> 2004.13649v4 &nbsp; - &nbsp;
{contrastive}
{control}
{deep}
{model-based}
{reinforcement}
</I></u><br> We propose a simple data augmentation technique that can be applied to
standard model-free reinforcement learning algorithms, enabling robust learning
directly from pixels without the need for auxiliary losses or pre-training. The
approach leverages input perturbations commonly used in computer vision tasks
to regularize the value function. Existing model-free approaches, such as Soft
Actor-Critic (SAC), are not able to train deep networks effectively from image
pixels. However, the addition of our augmentation method dramatically improves
SAC's performance, enabling it to reach state-of-the-art performance on the
DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC)
methods and recently proposed contrastive learning (CURL). Our approach can be
combined with any model-free reinforcement learning algorithm, requiring only
minor modifications. An implementation can be found at
https://sites.google.com/view/data-regularized-q. <br><br></div></a>
</td></tr>
<tr id=" 161 " class="entry"><td>
<a onclick="toggleVisibility('abstract161');">[|&bull;|]</a><a href="http://arxiv.org/abs/2002.00632v3">
<b/> Effective Diversity in Population Based Reinforcement Learning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2002.00632v3">
Jack Parker-Holder, Aldo Pacchiano, Krzysztof Choromanski, Stephen Roberts &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract161');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract161');"><div id="abstract161" style="font-size: 1rem; color:black; display: none">
<u><I> 2002.00632v3 &nbsp; - &nbsp;
{diversity}
{evolution}
{exploration}
{gradient}
{population}
{reinforcement}
</I></u><br> Exploration is a key problem in reinforcement learning, since agents can only
learn from data they acquire in the environment. With that in mind, maintaining
a population of agents is an attractive method, as it allows data be collected
with a diverse set of behaviors. This behavioral diversity is often boosted via
multi-objective loss functions. However, those approaches typically leverage
mean field updates based on pairwise distances, which makes them susceptible to
cycling behaviors and increased redundancy. In addition, explicitly boosting
diversity often has a detrimental impact on optimizing already fruitful
behaviors for rewards. As such, the reward-diversity trade off typically relies
on heuristics. Finally, such methods require behavioral representations, often
handcrafted and domain specific. In this paper, we introduce an approach to
optimize all members of a population simultaneously. Rather than using pairwise
distance, we measure the volume of the entire population in a behavioral
manifold, defined by task-agnostic behavioral embeddings. In addition, our
algorithm Diversity via Determinants (DvD), adapts the degree of diversity
during training using online learning techniques. We introduce both
evolutionary and gradient-based instantiations of DvD and show they effectively
improve exploration without reducing performance when better exploration is not
required. <br><br></div></a>
</td></tr>
<tr id=" 162 " class="entry"><td>
<a onclick="toggleVisibility('abstract162');">[|&bull;|]</a><a href="http://arxiv.org/abs/2011.04021v2">
<b/> On the role of planning in model-based deep reinforcement learning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2011.04021v2">
Jessica B. Hamrick, Abram L. Friesen, Feryal Behbahani, Arthur Guez, Fabio Viola, Sims Witherspoon, Thomas Anthony, Lars Buesing, Petar Veli&#269;kovi&#263;, Th&#233;ophane Weber &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract162');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract162');"><div id="abstract162" style="font-size: 1rem; color:black; display: none">
<u><I> 2011.04021v2 &nbsp; - &nbsp;
{control}
{deep}
{diversity}
{model-based}
{reinforcement}
</I></u><br> Model-based planning is often thought to be necessary for deep, careful
reasoning and generalization in artificial agents. While recent successes of
model-based reinforcement learning (MBRL) with deep function approximation have
strengthened this hypothesis, the resulting diversity of model-based methods
has also made it difficult to track which components drive success and why. In
this paper, we seek to disentangle the contributions of recent methods by
focusing on three questions: (1) How does planning benefit MBRL agents? (2)
Within planning, what choices drive performance? (3) To what extent does
planning improve generalization? To answer these questions, we study the
performance of MuZero (Schrittwieser et al., 2019), a state-of-the-art MBRL
algorithm with strong connections and overlapping components with many other
MBRL algorithms. We perform a number of interventions and ablations of MuZero
across a wide range of environments, including control tasks, Atari, and 9x9
Go. Our results suggest the following: (1) Planning is most useful in the
learning process, both for policy updates and for providing a more useful data
distribution. (2) Using shallow trees with simple Monte-Carlo rollouts is as
performant as more complex methods, except in the most difficult reasoning
tasks. (3) Planning alone is insufficient to drive strong generalization. These
results indicate where and how to utilize planning in reinforcement learning
settings, and highlight a number of open questions for future MBRL research. <br><br></div></a>
</td></tr>
<tr id=" 163 " class="entry"><td>
<a onclick="toggleVisibility('abstract163');">[|&bull;|]</a><a href="http://arxiv.org/abs/2010.05545v1">
<b/> Local Search for Policy Iteration in Continuous Control (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2010.05545v1">
Jost Tobias Springenberg, Nicolas Heess, Daniel Mankowitz, Josh Merel, Arunkumar Byravan, Abbas Abdolmaleki, Jackie Kay, Jonas Degrave, Julian Schrittwieser, Yuval Tassa, Jonas Buchli, Dan Belov, Martin Riedmiller &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract163');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract163');"><div id="abstract163" style="font-size: 1rem; color:black; display: none">
<u><I> 2010.05545v1 &nbsp; - &nbsp;
{control}
{model-based}
{reinforcement}
</I></u><br> We present an algorithm for local, regularized, policy improvement in
reinforcement learning (RL) that allows us to formulate model-based and
model-free variants in a single framework. Our algorithm can be interpreted as
a natural extension of work on KL-regularized RL and introduces a form of tree
search for continuous action spaces. We demonstrate that additional computation
spent on model-based policy improvement during learning can improve data
efficiency, and confirm that model-based policy improvement during action
selection can also be beneficial. Quantitatively, our algorithm improves data
efficiency on several continuous control benchmarks (when a model is learned in
parallel), and it provides significant improvements in wall-clock time in
high-dimensional domains (when a ground truth model is available). The unified
framework also helps us to better understand the space of model-based and
model-free algorithms. In particular, we demonstrate that some benefits
attributed to model-based RL can be obtained without a model, simply by
utilizing more computation. <br><br></div></a>
</td></tr>
<tr id=" 164 " class="entry"><td>
<a onclick="toggleVisibility('abstract164');">[|&bull;|]</a><a href="http://arxiv.org/abs/2004.07219v4">
<b/> D4RL: Datasets for Deep Data-Driven Reinforcement Learning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2004.07219v4">
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract164');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract164');"><div id="abstract164" style="font-size: 1rem; color:black; display: none">
<u><I> 2004.07219v4 &nbsp; - &nbsp;
{control}
{offline}
{reinforcement}
</I></u><br> The offline reinforcement learning (RL) setting (also known as full batch
RL), where a policy is learned from a static dataset, is compelling as progress
enables RL methods to take advantage of large, previously-collected datasets,
much like how the rise of large datasets has fueled results in supervised
learning. However, existing online RL benchmarks are not tailored towards the
offline setting and existing offline RL benchmarks are restricted to data
generated by partially-trained agents, making progress in offline RL difficult
to measure. In this work, we introduce benchmarks specifically designed for the
offline setting, guided by key properties of datasets relevant to real-world
applications of offline RL. With a focus on dataset collection, examples of
such properties include: datasets generated via hand-designed controllers and
human demonstrators, multitask datasets where an agent performs different tasks
in the same environment, and datasets collected with mixtures of policies. By
moving beyond simple benchmark tasks and data collected by partially-trained RL
agents, we reveal important and unappreciated deficiencies of existing
algorithms. To facilitate research, we have released our benchmark tasks and
datasets with a comprehensive evaluation of existing algorithms, an evaluation
protocol, and open-source examples. This serves as a common starting point for
the community to identify shortcomings in existing offline RL methods and a
collaborative route for progress in this emerging area. <br><br></div></a>
</td></tr>
<tr id=" 165 " class="entry"><td>
<a onclick="toggleVisibility('abstract165');">[|&bull;|]</a><a href="http://arxiv.org/abs/2009.04416v1">
<b/> Phasic Policy Gradient (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2009.04416v1">
Karl Cobbe, Jacob Hilton, Oleg Klimov, John Schulman &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract165');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract165');"><div id="abstract165" style="font-size: 1rem; color:black; display: none">
<u><I> 2009.04416v1 &nbsp; - &nbsp;
{distill}
{gradient}
{on-policy}
{reinforcement}
</I></u><br> We introduce Phasic Policy Gradient (PPG), a reinforcement learning framework
which modifies traditional on-policy actor-critic methods by separating policy
and value function training into distinct phases. In prior methods, one must
choose between using a shared network or separate networks to represent the
policy and value function. Using separate networks avoids interference between
objectives, while using a shared network allows useful features to be shared.
PPG is able to achieve the best of both worlds by splitting optimization into
two phases, one that advances training and one that distills features. PPG also
enables the value function to be more aggressively optimized with a higher
level of sample reuse. Compared to PPO, we find that PPG significantly improves
sample efficiency on the challenging Procgen Benchmark. <br><br></div></a>
</td></tr>
<tr id=" 166 " class="entry"><td>
<a onclick="toggleVisibility('abstract166');">[|&bull;|]</a><a href="http://arxiv.org/abs/2010.11944v1">
<b/> Accelerating Reinforcement Learning with Learned Skill Priors (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2010.11944v1">
Karl Pertsch, Youngwoon Lee, Joseph J. Lim &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract166');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract166');"><div id="abstract166" style="font-size: 1rem; color:black; display: none">
<u><I> 2010.11944v1 &nbsp; - &nbsp;
{deep}
{entropy}
{offline}
{reinforcement}
{robot}
{skill}
{transfer}
</I></u><br> Intelligent agents rely heavily on prior experience when learning a new task,
yet most modern reinforcement learning (RL) approaches learn every task from
scratch. One approach for leveraging prior knowledge is to transfer skills
learned on prior tasks to the new task. However, as the amount of prior
experience increases, the number of transferable skills grows too, making it
challenging to explore the full set of available skills during downstream
learning. Yet, intuitively, not all skills should be explored with equal
probability; for example information about the current state can hint which
skills are promising to explore. In this work, we propose to implement this
intuition by learning a prior over skills. We propose a deep latent variable
model that jointly learns an embedding space of skills and the skill prior from
offline agent experience. We then extend common maximum-entropy RL approaches
to use skill priors to guide downstream learning. We validate our approach,
SPiRL (Skill-Prior RL), on complex navigation and robotic manipulation tasks
and show that learned skill priors are essential for effective skill transfer
from rich datasets. Videos and code are available at https://clvrai.com/spirl. <br><br></div></a>
</td></tr>
<tr id=" 167 " class="entry"><td>
<a onclick="toggleVisibility('abstract167');">[|&bull;|]</a><a href="http://arxiv.org/abs/2003.01629v2">
<b/> Can Increasing Input Dimensionality Improve Deep Reinforcement Learning? (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2003.01629v2">
Kei Ota, Tomoaki Oiki, Devesh K. Jha, Toshisada Mariyama, Daniel Nikovski &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract167');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract167');"><div id="abstract167" style="font-size: 1rem; color:black; display: none">
<u><I> 2003.01629v2 &nbsp; - &nbsp;
{deep}
{reinforcement}
</I></u><br> Deep reinforcement learning (RL) algorithms have recently achieved remarkable
successes in various sequential decision making tasks, leveraging advances in
methods for training large deep networks. However, these methods usually
require large amounts of training data, which is often a big problem for
real-world applications. One natural question to ask is whether learning good
representations for states and using larger networks helps in learning better
policies. In this paper, we try to study if increasing input dimensionality
helps improve performance and sample efficiency of model-free deep RL
algorithms. To do so, we propose an online feature extractor network (OFENet)
that uses neural nets to produce good representations to be used as inputs to
deep RL algorithms. Even though the high dimensionality of input is usually
supposed to make learning of RL agents more difficult, we show that the RL
agents in fact learn more efficiently with the high-dimensional representation
than with the lower-dimensional state observations. We believe that stronger
feature propagation together with larger networks (and thus larger search
space) allows RL agents to learn more complex functions of states and thus
improves the sample efficiency. Through numerical experiments, we show that the
proposed method outperforms several other state-of-the-art algorithms in terms
of both sample efficiency and performance. Codes for the proposed method are
available at http://www.merl.com/research/license/OFENet . <br><br></div></a>
</td></tr>
<tr id=" 168 " class="entry"><td>
<a onclick="toggleVisibility('abstract168');">[|&bull;|]</a><a href="http://arxiv.org/abs/2012.03548v3">
<b/> Reset-Free Lifelong Learning with Skill-Space Planning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2012.03548v3">
Kevin Lu, Aditya Grover, Pieter Abbeel, Igor Mordatch &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract168');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract168');"><div id="abstract168" style="font-size: 1rem; color:black; display: none">
<u><I> 2012.03548v3 &nbsp; - &nbsp;
{intrinsic}
{offline}
{reinforcement}
{skill}
{unsupervised}
</I></u><br> The objective of lifelong reinforcement learning (RL) is to optimize agents
which can continuously adapt and interact in changing environments. However,
current RL approaches fail drastically when environments are non-stationary and
interactions are non-episodic. We propose Lifelong Skill Planning (LiSP), an
algorithmic framework for non-episodic lifelong RL based on planning in an
abstract space of higher-order skills. We learn the skills in an unsupervised
manner using intrinsic rewards and plan over the learned skills using a learned
dynamics model. Moreover, our framework permits skill discovery even from
offline data, thereby reducing the need for excessive real-world interactions.
We demonstrate empirically that LiSP successfully enables long-horizon planning
and learns agents that can avoid catastrophic failures even in challenging
non-stationary and non-episodic environments derived from gridworld and MuJoCo
benchmarks. <br><br></div></a>
</td></tr>
<tr id=" 169 " class="entry"><td>
<a onclick="toggleVisibility('abstract169');">[|&bull;|]</a><a href="http://arxiv.org/abs/2005.12729v1">
<b/> Implementation Matters in Deep Policy Gradients: A Case Study on PPO and
TRPO (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2005.12729v1">
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Aleksander Madry &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract169');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract169');"><div id="abstract169" style="font-size: 1rem; color:black; display: none">
<u><I> 2005.12729v1 &nbsp; - &nbsp;
{deep}
{gradient}
{on-policy}
{reinforcement}
</I></u><br> We study the roots of algorithmic progress in deep policy gradient algorithms
through a case study on two popular algorithms: Proximal Policy Optimization
(PPO) and Trust Region Policy Optimization (TRPO). Specifically, we investigate
the consequences of "code-level optimizations:" algorithm augmentations found
only in implementations or described as auxiliary details to the core
algorithm. Seemingly of secondary importance, such optimizations turn out to
have a major impact on agent behavior. Our results show that they (a) are
responsible for most of PPO's gain in cumulative reward over TRPO, and (b)
fundamentally change how RL methods function. These insights show the
difficulty and importance of attributing performance gains in deep
reinforcement learning. Code for reproducing our results is available at
https://github.com/MadryLab/implementation-matters . <br><br></div></a>
</td></tr>
<tr id=" 170 " class="entry"><td>
<a onclick="toggleVisibility('abstract170');">[|&bull;|]</a><a href="http://arxiv.org/abs/2001.00449v1">
<b/> Continuous-Discrete Reinforcement Learning for Hybrid Control in
Robotics (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2001.00449v1">
Michael Neunert, Abbas Abdolmaleki, Markus Wulfmeier, Thomas Lampe, Jost Tobias Springenberg, Roland Hafner, Francesco Romano, Jonas Buchli, Nicolas Heess, Martin Riedmiller &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract170');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract170');"><div id="abstract170" style="font-size: 1rem; color:black; display: none">
<u><I> 2001.00449v1 &nbsp; - &nbsp;
{control}
{exploration}
{meta}
{optimal}
{reinforcement}
{robot}
</I></u><br> Many real-world control problems involve both discrete decision variables -
such as the choice of control modes, gear switching or digital outputs - as
well as continuous decision variables - such as velocity setpoints, control
gains or analogue outputs. However, when defining the corresponding optimal
control or reinforcement learning problem, it is commonly approximated with
fully continuous or fully discrete action spaces. These simplifications aim at
tailoring the problem to a particular algorithm or solver which may only
support one type of action space. Alternatively, expert heuristics are used to
remove discrete actions from an otherwise continuous space. In contrast, we
propose to treat hybrid problems in their 'native' form by solving them with
hybrid reinforcement learning, which optimizes for discrete and continuous
actions simultaneously. In our experiments, we first demonstrate that the
proposed approach efficiently solves such natively hybrid reinforcement
learning problems. We then show, both in simulation and on robotic hardware,
the benefits of removing possibly imperfect expert-designed heuristics. Lastly,
hybrid reinforcement learning encourages us to rethink problem definitions. We
propose reformulating control problems, e.g. by adding meta actions, to improve
exploration or reduce mechanical wear and tear. <br><br></div></a>
</td></tr>
<tr id=" 171 " class="entry"><td>
<a onclick="toggleVisibility('abstract171');">[|&bull;|]</a><a href="http://arxiv.org/abs/2005.13143v2">
<b/> Euclideanizing Flows: Diffeomorphic Reduction for Learning Stable
Dynamical Systems (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2005.13143v2">
Muhammad Asif Rana, Anqi Li, Dieter Fox, Byron Boots, Fabio Ramos, Nathan Ratliff &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract171');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract171');"><div id="abstract171" style="font-size: 1rem; color:black; display: none">
<u><I> 2005.13143v2 &nbsp; - &nbsp;
{robot}
</I></u><br> Robotic tasks often require motions with complex geometric structures. We
present an approach to learn such motions from a limited number of human
demonstrations by exploiting the regularity properties of human motions e.g.
stability, smoothness, and boundedness. The complex motions are encoded as
rollouts of a stable dynamical system, which, under a change of coordinates
defined by a diffeomorphism, is equivalent to a simple, hand-specified
dynamical system. As an immediate result of using diffeomorphisms, the
stability property of the hand-specified dynamical system directly carry over
to the learned dynamical system. Inspired by recent works in density
estimation, we propose to represent the diffeomorphism as a composition of
simple parameterized diffeomorphisms. Additional structure is imposed to
provide guarantees on the smoothness of the generated motions. The efficacy of
this approach is demonstrated through validation on an established benchmark as
well demonstrations collected on a real-world robotic system. <br><br></div></a>
</td></tr>
<tr id=" 172 " class="entry"><td>
<a onclick="toggleVisibility('abstract172');">[|&bull;|]</a><a href="http://arxiv.org/abs/2006.14135v2">
<b/> Explainable CNN-attention Networks (C-Attention Network) for Automated
Detection of Alzheimer's Disease (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2006.14135v2">
Ning Wang, Mingxuan Chen, K. P. Subbalakshmi &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract172');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract172');"><div id="abstract172" style="font-size: 1rem; color:black; display: none">
<u><I> 2006.14135v2 &nbsp; - &nbsp;
{deep}
{explainable}
</I></u><br> In this work, we propose three explainable deep learning architectures to
automatically detect patients with Alzheimer`s disease based on their language
abilities. The architectures use: (1) only the part-of-speech features; (2)
only language embedding features and (3) both of these feature classes via a
unified architecture. We use self-attention mechanisms and interpretable
1-dimensional ConvolutionalNeural Network (CNN) to generate two types of
explanations of the model`s action: intra-class explanation and inter-class
explanation. The inter-class explanation captures the relative importance of
each of the different features in that class, while the inter-class explanation
captures the relative importance between the classes. Note that although we
have considered two classes of features in this paper, the architecture is
easily expandable to more classes because of its modularity. Extensive
experimentation and comparison with several recent models show that our method
outperforms these methods with an accuracy of 92.2% and F1 score of 0.952on the
DementiaBank dataset while being able to generate explanations. We show by
examples, how to generate these explanations using attention values. <br><br></div></a>
</td></tr>
<tr id=" 173 " class="entry"><td>
<a onclick="toggleVisibility('abstract173');">[|&bull;|]</a><a href="http://arxiv.org/abs/2011.04483v1">
<b/> A Theory of Universal Learning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2011.04483v1">
Olivier Bousquet, Steve Hanneke, Shay Moran, Ramon van Handel, Amir Yehudayoff &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract173');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract173');"><div id="abstract173" style="font-size: 1rem; color:black; display: none">
<u><I> 2011.04483v1 &nbsp; - &nbsp;
{optimal}
</I></u><br> How quickly can a given class of concepts be learned from examples? It is
common to measure the performance of a supervised machine learning algorithm by
plotting its "learning curve", that is, the decay of the error rate as a
function of the number of training examples. However, the classical theoretical
framework for understanding learnability, the PAC model of Vapnik-Chervonenkis
and Valiant, does not explain the behavior of learning curves: the
distribution-free PAC model of learning can only bound the upper envelope of
the learning curves over all possible data distributions. This does not match
the practice of machine learning, where the data source is typically fixed in
any given scenario, while the learner may choose the number of training
examples on the basis of factors such as computational resources and desired
accuracy.
In this paper, we study an alternative learning model that better captures
such practical aspects of machine learning, but still gives rise to a complete
theory of the learnable in the spirit of the PAC model. More precisely, we
consider the problem of universal learning, which aims to understand the
performance of learning algorithms on every data distribution, but without
requiring uniformity over the distribution. The main result of this paper is a
remarkable trichotomy: there are only three possible rates of universal
learning. More precisely, we show that the learning curves of any given concept
class decay either at an exponential, linear, or arbitrarily slow rates.
Moreover, each of these cases is completely characterized by appropriate
combinatorial parameters, and we exhibit optimal learning algorithms that
achieve the best possible rate in each case.
For concreteness, we consider in this paper only the realizable case, though
analogous results are expected to extend to more general learning scenarios. <br><br></div></a>
</td></tr>
<tr id=" 174 " class="entry"><td>
<a onclick="toggleVisibility('abstract174');">[|&bull;|]</a><a href="http://arxiv.org/abs/2006.13258v6">
<b/> Adversarial Soft Advantage Fitting: Imitation Learning without Policy
Optimization (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2006.13258v6">
Paul Barde, Julien Roy, Wonseok Jeon, Joelle Pineau, Christopher Pal, Derek Nowrouzezahrai &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract174');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract174');"><div id="abstract174" style="font-size: 1rem; color:black; display: none">
<u><I> 2006.13258v6 &nbsp; - &nbsp;
{imitation}
{optimal}
{reinforcement}
</I></u><br> Adversarial Imitation Learning alternates between learning a discriminator --
which tells apart expert's demonstrations from generated ones -- and a
generator's policy to produce trajectories that can fool this discriminator.
This alternated optimization is known to be delicate in practice since it
compounds unstable adversarial training with brittle and sample-inefficient
reinforcement learning. We propose to remove the burden of the policy
optimization steps by leveraging a novel discriminator formulation.
Specifically, our discriminator is explicitly conditioned on two policies: the
one from the previous generator's iteration and a learnable policy. When
optimized, this discriminator directly learns the optimal generator's policy.
Consequently, our discriminator's update solves the generator's optimization
problem for free: learning a policy that imitates the expert does not require
an additional optimization loop. This formulation effectively cuts by half the
implementation and computational burden of Adversarial Imitation Learning
algorithms by removing the Reinforcement Learning phase altogether. We show on
a variety of tasks that our simpler approach is competitive to prevalent
Imitation Learning methods. <br><br></div></a>
</td></tr>
<tr id=" 175 " class="entry"><td>
<a onclick="toggleVisibility('abstract175');">[|&bull;|]</a><a href="http://arxiv.org/abs/2002.02693v1">
<b/> Ready Policy One: World Building Through Active Learning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2002.02693v1">
Philip Ball, Jack Parker-Holder, Aldo Pacchiano, Krzysztof Choromanski, Stephen Roberts &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract175');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract175');"><div id="abstract175" style="font-size: 1rem; color:black; display: none">
<u><I> 2002.02693v1 &nbsp; - &nbsp;
{control}
{exploration}
{model-based}
{reinforcement}
</I></u><br> Model-Based Reinforcement Learning (MBRL) offers a promising direction for
sample efficient learning, often achieving state of the art results for
continuous control tasks. However, many existing MBRL methods rely on combining
greedy policies with exploration heuristics, and even those which utilize
principled exploration bonuses construct dual objectives in an ad hoc fashion.
In this paper we introduce Ready Policy One (RP1), a framework that views MBRL
as an active learning problem, where we aim to improve the world model in the
fewest samples possible. RP1 achieves this by utilizing a hybrid objective
function, which crucially adapts during optimization, allowing the algorithm to
trade off reward v.s. exploration at different stages of learning. In addition,
we introduce a principled mechanism to terminate sample collection once we have
a rich enough trajectory batch to improve the model. We rigorously evaluate our
method on a variety of continuous control tasks, and demonstrate statistically
significant gains over existing approaches. <br><br></div></a>
</td></tr>
<tr id=" 176 " class="entry"><td>
<a onclick="toggleVisibility('abstract176');">[|&bull;|]</a><a href="http://arxiv.org/abs/2001.06940v1">
<b/> Reinforcement Learning with Probabilistically Complete Exploration (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2001.06940v1">
Philippe Morere, Gilad Francis, Tom Blau, Fabio Ramos &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract176');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract176');"><div id="abstract176" style="font-size: 1rem; color:black; display: none">
<u><I> 2001.06940v1 &nbsp; - &nbsp;
{exploration}
{intrinsic}
{reinforcement}
{sparse}
</I></u><br> Balancing exploration and exploitation remains a key challenge in
reinforcement learning (RL). State-of-the-art RL algorithms suffer from high
sample complexity, particularly in the sparse reward case, where they can do no
better than to explore in all directions until the first positive rewards are
found. To mitigate this, we propose Rapidly Randomly-exploring Reinforcement
Learning (R3L). We formulate exploration as a search problem and leverage
widely-used planning algorithms such as Rapidly-exploring Random Tree (RRT) to
find initial solutions. These solutions are used as demonstrations to
initialize a policy, then refined by a generic RL algorithm, leading to faster
and more stable convergence. We provide theoretical guarantees of R3L
exploration finding successful solutions, as well as bounds for its sampling
complexity. We experimentally demonstrate the method outperforms classic and
intrinsic exploration techniques, requiring only a fraction of exploration
samples and achieving better asymptotic performance. <br><br></div></a>
</td></tr>
<tr id=" 177 " class="entry"><td>
<a onclick="toggleVisibility('abstract177');">[|&bull;|]</a><a href="http://arxiv.org/abs/2010.05113v2">
<b/> Contrastive Representation Learning: A Framework and Review (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2010.05113v2">
Phuc H. Le-Khac, Graham Healy, Alan F. Smeaton &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract177');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract177');"><div id="abstract177" style="font-size: 1rem; color:black; display: none">
<u><I> 2010.05113v2 &nbsp; - &nbsp;
{contrastive}
{reinforcement}
{self-supervised}
</I></u><br> Contrastive Learning has recently received interest due to its success in
self-supervised representation learning in the computer vision domain. However,
the origins of Contrastive Learning date as far back as the 1990s and its
development has spanned across many fields and domains including Metric
Learning and natural language processing. In this paper we provide a
comprehensive literature review and we propose a general Contrastive
Representation Learning framework that simplifies and unifies many different
contrastive learning methods. We also provide a taxonomy for each of the
components of contrastive learning in order to summarise it and distinguish it
from other forms of machine learning. We then discuss the inductive biases
which are present in any contrastive learning system and we analyse our
framework under different views from various sub-fields of Machine Learning.
Examples of how contrastive learning has been applied in computer vision,
natural language processing, audio processing, and others, as well as in
Reinforcement Learning are also presented. Finally, we discuss the challenges
and some of the most promising future research directions ahead. <br><br></div></a>
</td></tr>
<tr id=" 178 " class="entry"><td>
<a onclick="toggleVisibility('abstract178');">[|&bull;|]</a><a href="http://arxiv.org/abs/2004.11362v5">
<b/> Supervised Contrastive Learning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2004.11362v5">
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract178');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract178');"><div id="abstract178" style="font-size: 1rem; color:black; display: none">
<u><I> 2004.11362v5 &nbsp; - &nbsp;
{contrastive}
{deep}
{entropy}
{self-supervised}
{unsupervised}
</I></u><br> Contrastive learning applied to self-supervised representation learning has
seen a resurgence in recent years, leading to state of the art performance in
the unsupervised training of deep image models. Modern batch contrastive
approaches subsume or significantly outperform traditional contrastive losses
such as triplet, max-margin and the N-pairs loss. In this work, we extend the
self-supervised batch contrastive approach to the fully-supervised setting,
allowing us to effectively leverage label information. Clusters of points
belonging to the same class are pulled together in embedding space, while
simultaneously pushing apart clusters of samples from different classes. We
analyze two possible versions of the supervised contrastive (SupCon) loss,
identifying the best-performing formulation of the loss. On ResNet-200, we
achieve top-1 accuracy of 81.4% on the ImageNet dataset, which is 0.8% above
the best number reported for this architecture. We show consistent
outperformance over cross-entropy on other datasets and two ResNet variants.
The loss shows benefits for robustness to natural corruptions and is more
stable to hyperparameter settings such as optimizers and data augmentations.
Our loss function is simple to implement, and reference TensorFlow code is
released at https://t.ly/supcon. <br><br></div></a>
</td></tr>
<tr id=" 179 " class="entry"><td>
<a onclick="toggleVisibility('abstract179');">[|&bull;|]</a><a href="http://arxiv.org/abs/2002.02089v1">
<b/> Soft Hindsight Experience Replay (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2002.02089v1">
Qiwei He, Liansheng Zhuang, Houqiang Li &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract179');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract179');"><div id="abstract179" style="font-size: 1rem; color:black; display: none">
<u><I> 2002.02089v1 &nbsp; - &nbsp;
{control}
{deep}
{entropy}
{reinforcement}
{replay}
{robot}
{sparse}
</I></u><br> Efficient learning in the environment with sparse rewards is one of the most
important challenges in Deep Reinforcement Learning (DRL). In continuous DRL
environments such as robotic arms control, Hindsight Experience Replay (HER)
has been shown an effective solution. However, due to the brittleness of
deterministic methods, HER and its variants typically suffer from a major
challenge for stability and convergence, which significantly affects the final
performance. This challenge severely limits the applicability of such methods
to complex real-world domains. To tackle this challenge, in this paper, we
propose Soft Hindsight Experience Replay (SHER), a novel approach based on HER
and Maximum Entropy Reinforcement Learning (MERL), combining the failed
experiences reuse and maximum entropy probabilistic inference model. We
evaluate SHER on Open AI Robotic manipulation tasks with sparse rewards.
Experimental results show that, in contrast to HER and its variants, our
proposed SHER achieves state-of-the-art performance, especially in the
difficult HandManipulation tasks. Furthermore, our SHER method is more stable,
achieving very similar performance across different random seeds. <br><br></div></a>
</td></tr>
<tr id=" 180 " class="entry"><td>
<a onclick="toggleVisibility('abstract180');">[|&bull;|]</a><a href="http://arxiv.org/abs/2006.04678v2">
<b/> Primal Wasserstein Imitation Learning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2006.04678v2">
Robert Dadashi, L&#233;onard Hussenot, Matthieu Geist, Olivier Pietquin &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract180');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract180');"><div id="abstract180" style="font-size: 1rem; color:black; display: none">
<u><I> 2006.04678v2 &nbsp; - &nbsp;
{control}
{imitation}
{offline}
</I></u><br> Imitation Learning (IL) methods seek to match the behavior of an agent with
that of an expert. In the present work, we propose a new IL method based on a
conceptually simple algorithm: Primal Wasserstein Imitation Learning (PWIL),
which ties to the primal form of the Wasserstein distance between the expert
and the agent state-action distributions. We present a reward function which is
derived offline, as opposed to recent adversarial IL algorithms that learn a
reward function through interactions with the environment, and which requires
little fine-tuning. We show that we can recover expert behavior on a variety of
continuous control tasks of the MuJoCo domain in a sample efficient manner in
terms of agent interactions and of expert interactions with the environment.
Finally, we show that the behavior of the agent we train matches the behavior
of the expert with the Wasserstein distance, rather than the commonly used
proxy of performance. <br><br></div></a>
</td></tr>
<tr id=" 181 " class="entry"><td>
<a onclick="toggleVisibility('abstract181');">[|&bull;|]</a><a href="http://arxiv.org/abs/2008.12228v1">
<b/> Towards General and Autonomous Learning of Core Skills: A Case Study in
Locomotion (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2008.12228v1">
Roland Hafner, Tim Hertweck, Philipp Kl&#246;ppner, Michael Bloesch, Michael Neunert, Markus Wulfmeier, Saran Tunyasuvunakool, Nicolas Heess, Martin Riedmiller &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract181');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract181');"><div id="abstract181" style="font-size: 1rem; color:black; display: none">
<u><I> 2008.12228v1 &nbsp; - &nbsp;
{control}
{multi-task}
{off-policy}
{reinforcement}
{robot}
{skill}
</I></u><br> Modern Reinforcement Learning (RL) algorithms promise to solve difficult
motor control problems directly from raw sensory inputs. Their attraction is
due in part to the fact that they can represent a general class of methods that
allow to learn a solution with a reasonably set reward and minimal prior
knowledge, even in situations where it is difficult or expensive for a human
expert. For RL to truly make good on this promise, however, we need algorithms
and learning setups that can work across a broad range of problems with minimal
problem specific adjustments or engineering. In this paper, we study this idea
of generality in the locomotion domain. We develop a learning framework that
can learn sophisticated locomotion behavior for a wide spectrum of legged
robots, such as bipeds, tripeds, quadrupeds and hexapods, including wheeled
variants. Our learning framework relies on a data-efficient, off-policy
multi-task RL algorithm and a small set of reward functions that are
semantically identical across robots. To underline the general applicability of
the method, we keep the hyper-parameter settings and reward definitions
constant across experiments and rely exclusively on on-board sensing. For nine
different types of robots, including a real-world quadruped robot, we
demonstrate that the same algorithm can rapidly learn diverse and reusable
locomotion skills without any platform specific adjustments or additional
instrumentation of the learning setup. <br><br></div></a>
</td></tr>
<tr id=" 182 " class="entry"><td>
<a onclick="toggleVisibility('abstract182');">[|&bull;|]</a><a href="http://arxiv.org/abs/2009.13579v3">
<b/> Novelty Search in Representational Space for Sample Efficient
Exploration (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2009.13579v3">
Ruo Yu Tao, Vincent Fran&#231;ois-Lavet, Joelle Pineau &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract182');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract182');"><div id="abstract182" style="font-size: 1rem; color:black; display: none">
<u><I> 2009.13579v3 &nbsp; - &nbsp;
{control}
{exploration}
{intrinsic}
{model-based}
{sparse}
</I></u><br> We present a new approach for efficient exploration which leverages a
low-dimensional encoding of the environment learned with a combination of
model-based and model-free objectives. Our approach uses intrinsic rewards that
are based on the distance of nearest neighbors in the low dimensional
representational space to gauge novelty. We then leverage these intrinsic
rewards for sample-efficient exploration with planning routines in
representational space for hard exploration tasks with sparse rewards. One key
element of our approach is the use of information theoretic principles to shape
our representations in a way so that our novelty reward goes beyond pixel
similarity. We test our approach on a number of maze tasks, as well as a
control problem and show that our exploration approach is more sample-efficient
compared to strong baselines. <br><br></div></a>
</td></tr>
<tr id=" 183 " class="entry"><td>
<a onclick="toggleVisibility('abstract183');">[|&bull;|]</a><a href="http://arxiv.org/abs/2002.08803v1">
<b/> Support-weighted Adversarial Imitation Learning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2002.08803v1">
Ruohan Wang, Carlo Ciliberto, Pierluigi Amadori, Yiannis Demiris &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract183');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract183');"><div id="abstract183" style="font-size: 1rem; color:black; display: none">
<u><I> 2002.08803v1 &nbsp; - &nbsp;
{control}
{imitation}
{reinforcement}
</I></u><br> Adversarial Imitation Learning (AIL) is a broad family of imitation learning
methods designed to mimic expert behaviors from demonstrations. While AIL has
shown state-of-the-art performance on imitation learning with only small number
of demonstrations, it faces several practical challenges such as potential
training instability and implicit reward bias. To address the challenges, we
propose Support-weighted Adversarial Imitation Learning (SAIL), a general
framework that extends a given AIL algorithm with information derived from
support estimation of the expert policies. SAIL improves the quality of the
reinforcement signals by weighing the adversarial reward with a confidence
score from support estimation of the expert policy. We also show that SAIL is
always at least as efficient as the underlying AIL algorithm that SAIL uses for
learning the adversarial reward. Empirically, we show that the proposed method
achieves better performance and training stability than baseline methods on a
wide range of benchmark control tasks. <br><br></div></a>
</td></tr>
<tr id=" 184 " class="entry"><td>
<a onclick="toggleVisibility('abstract184');">[|&bull;|]</a><a href="http://arxiv.org/abs/2002.08550v3">
<b/> Learning to Walk in the Real World with Minimal Human Effort (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2002.08550v3">
Sehoon Ha, Peng Xu, Zhenyu Tan, Sergey Levine, Jie Tan &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract184');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract184');"><div id="abstract184" style="font-size: 1rem; color:black; display: none">
<u><I> 2002.08550v3 &nbsp; - &nbsp;
{control}
{deep}
{multi-task}
{reinforcement}
{robot}
{skill}
</I></u><br> Reliable and stable locomotion has been one of the most fundamental
challenges for legged robots. Deep reinforcement learning (deep RL) has emerged
as a promising method for developing such control policies autonomously. In
this paper, we develop a system for learning legged locomotion policies with
deep RL in the real world with minimal human effort. The key difficulties for
on-robot learning systems are automatic data collection and safety. We overcome
these two challenges by developing a multi-task learning procedure and a
safety-constrained RL framework. We tested our system on the task of learning
to walk on three different terrains: flat ground, a soft mattress, and a
doormat with crevices. Our system can automatically and efficiently learn
locomotion skills on a Minitaur robot with little human intervention. The
supplemental video can be found at: \url{https://youtu.be/cwyiq6dCgOc}. <br><br></div></a>
</td></tr>
<tr id=" 185 " class="entry"><td>
<a onclick="toggleVisibility('abstract185');">[|&bull;|]</a><a href="http://arxiv.org/abs/2006.01419v2">
<b/> Diversity Actor-Critic: Sample-Aware Entropy Regularization for
Sample-Efficient Exploration (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2006.01419v2">
Seungyul Han, Youngchul Sung &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract185');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract185');"><div id="abstract185" style="font-size: 1rem; color:black; display: none">
<u><I> 2006.01419v2 &nbsp; - &nbsp;
{diversity}
{entropy}
{exploration}
{reinforcement}
{replay}
</I></u><br> In this paper, sample-aware policy entropy regularization is proposed to
enhance the conventional policy entropy regularization for better exploration.
Exploiting the sample distribution obtainable from the replay buffer, the
proposed sample-aware entropy regularization maximizes the entropy of the
weighted sum of the policy action distribution and the sample action
distribution from the replay buffer for sample-efficient exploration. A
practical algorithm named diversity actor-critic (DAC) is developed by applying
policy iteration to the objective function with the proposed sample-aware
entropy regularization. Numerical results show that DAC significantly
outperforms existing recent algorithms for reinforcement learning. <br><br></div></a>
</td></tr>
<tr id=" 186 " class="entry"><td>
<a onclick="toggleVisibility('abstract186');">[|&bull;|]</a><a href="http://arxiv.org/abs/2011.00072v2">
<b/> Learning Stable Normalizing-Flow Control for Robotic Manipulation (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2011.00072v2">
Shahbaz Abdul Khader, Hang Yin, Pietro Falco, Danica Kragic &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract186');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract186');"><div id="abstract186" style="font-size: 1rem; color:black; display: none">
<u><I> 2011.00072v2 &nbsp; - &nbsp;
{control}
{deep}
{exploration}
{reinforcement}
{robot}
{skill}
</I></u><br> Reinforcement Learning (RL) of robotic manipulation skills, despite its
impressive successes, stands to benefit from incorporating domain knowledge
from control theory. One of the most important properties that is of interest
is control stability. Ideally, one would like to achieve stability guarantees
while staying within the framework of state-of-the-art deep RL algorithms. Such
a solution does not exist in general, especially one that scales to complex
manipulation tasks. We contribute towards closing this gap by introducing
$\textit{normalizing-flow}$ control structure, that can be deployed in any
latest deep RL algorithms. While stable exploration is not guaranteed, our
method is designed to ultimately produce deterministic controllers with
provable stability. In addition to demonstrating our method on challenging
contact-rich manipulation tasks, we also show that it is possible to achieve
considerable exploration efficiency--reduced state space coverage and actuation
efforts--without losing learning efficiency. <br><br></div></a>
</td></tr>
<tr id=" 187 " class="entry"><td>
<a onclick="toggleVisibility('abstract187');">[|&bull;|]</a><a href="http://arxiv.org/abs/2002.09571v2">
<b/> Learning to Continually Learn (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2002.09571v2">
Shawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth O. Stanley, Jeff Clune, Nick Cheney &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract187');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract187');"><div id="abstract187" style="font-size: 1rem; color:black; display: none">
<u><I> 2002.09571v2 &nbsp; - &nbsp;
{control}
{deep}
{lifelong learning}
{meta}
</I></u><br> Continual lifelong learning requires an agent or model to learn many
sequentially ordered tasks, building on previous knowledge without
catastrophically forgetting it. Much work has gone towards preventing the
default tendency of machine learning models to catastrophically forget, yet
virtually all such work involves manually-designed solutions to the problem. We
instead advocate meta-learning a solution to catastrophic forgetting, allowing
AI to learn to continually learn. Inspired by neuromodulatory processes in the
brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It
differentiates through a sequential learning process to meta-learn an
activation-gating function that enables context-dependent selective activation
within a deep neural network. Specifically, a neuromodulatory (NM) neural
network gates the forward pass of another (otherwise normal) neural network
called the prediction learning network (PLN). The NM network also thus
indirectly controls selective plasticity (i.e. the backward pass of) the PLN.
ANML enables continual learning without catastrophic forgetting at scale: it
produces state-of-the-art continual learning performance, sequentially learning
as many as 600 classes (over 9,000 SGD updates). <br><br></div></a>
</td></tr>
<tr id=" 188 " class="entry"><td>
<a onclick="toggleVisibility('abstract188');">[|&bull;|]</a><a href="http://arxiv.org/abs/2012.06644v2">
<b/> Regularizing Action Policies for Smooth Control with Reinforcement
Learning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2012.06644v2">
Siddharth Mysore, Bassel Mabsout, Renato Mancuso, Kate Saenko &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract188');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract188');"><div id="abstract188" style="font-size: 1rem; color:black; display: none">
<u><I> 2012.06644v2 &nbsp; - &nbsp;
{control}
{deep}
{on-policy}
{reinforcement}
</I></u><br> A critical problem with the practical utility of controllers trained with
deep Reinforcement Learning (RL) is the notable lack of smoothness in the
actions learned by the RL policies. This trend often presents itself in the
form of control signal oscillation and can result in poor control, high power
consumption, and undue system wear. We introduce Conditioning for Action Policy
Smoothness (CAPS), an effective yet intuitive regularization on action
policies, which offers consistent improvement in the smoothness of the learned
state-to-action mappings of neural network controllers, reflected in the
elimination of high-frequency components in the control signal. Tested on a
real system, improvements in controller smoothness on a quadrotor drone
resulted in an almost 80% reduction in power consumption while consistently
training flight-worthy controllers. Project website: http://ai.bu.edu/caps <br><br></div></a>
</td></tr>
<tr id=" 189 " class="entry"><td>
<a onclick="toggleVisibility('abstract189');">[|&bull;|]</a><a href="http://arxiv.org/abs/2007.02832v1">
<b/> Maximum Entropy Gain Exploration for Long Horizon Multi-goal
Reinforcement Learning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2007.02832v1">
Silviu Pitis, Harris Chan, Stephen Zhao, Bradly Stadie, Jimmy Ba &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract189');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract189');"><div id="abstract189" style="font-size: 1rem; color:black; display: none">
<u><I> 2007.02832v1 &nbsp; - &nbsp;
{entropy}
{exploration}
{intrinsic}
{reinforcement}
{sparse}
</I></u><br> What goals should a multi-goal reinforcement learning agent pursue during
training in long-horizon tasks? When the desired (test time) goal distribution
is too distant to offer a useful learning signal, we argue that the agent
should not pursue unobtainable goals. Instead, it should set its own intrinsic
goals that maximize the entropy of the historical achieved goal distribution.
We propose to optimize this objective by having the agent pursue past achieved
goals in sparsely explored areas of the goal space, which focuses exploration
on the frontier of the achievable goal set. We show that our strategy achieves
an order of magnitude better sample efficiency than the prior state of the art
on long-horizon multi-goal tasks including maze navigation and block stacking. <br><br></div></a>
</td></tr>
<tr id=" 190 " class="entry"><td>
<a onclick="toggleVisibility('abstract190');">[|&bull;|]</a><a href="http://arxiv.org/abs/2007.07170v2">
<b/> Goal-Aware Prediction: Learning to Model What Matters (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2007.07170v2">
Suraj Nair, Silvio Savarese, Chelsea Finn &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract190');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract190');"><div id="abstract190" style="font-size: 1rem; color:black; display: none">
<u><I> 2007.07170v2 &nbsp; - &nbsp;
{control}
{reinforcement}
{self-supervised}
</I></u><br> Learned dynamics models combined with both planning and policy learning
algorithms have shown promise in enabling artificial agents to learn to perform
many diverse tasks with limited supervision. However, one of the fundamental
challenges in using a learned forward dynamics model is the mismatch between
the objective of the learned model (future state reconstruction), and that of
the downstream planner or policy (completing a specified task). This issue is
exacerbated by vision-based control tasks in diverse real-world environments,
where the complexity of the real world dwarfs model capacity. In this paper, we
propose to direct prediction towards task relevant information, enabling the
model to be aware of the current task and encouraging it to only model relevant
quantities of the state space, resulting in a learning objective that more
closely matches the downstream task. Further, we do so in an entirely
self-supervised manner, without the need for a reward function or image labels.
We find that our method more effectively models the relevant parts of the scene
conditioned on the goal, and as a result outperforms standard task-agnostic
dynamics models and model-free reinforcement learning. <br><br></div></a>
</td></tr>
<tr id=" 191 " class="entry"><td>
<a onclick="toggleVisibility('abstract191');">[|&bull;|]</a><a href="http://arxiv.org/abs/2012.13658v2">
<b/> Locally Persistent Exploration in Continuous Control Tasks with Sparse
Rewards (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2012.13658v2">
Susan Amin, Maziar Gomrokchi, Hossein Aboutalebi, Harsh Satija, Doina Precup &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract191');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract191');"><div id="abstract191" style="font-size: 1rem; color:black; display: none">
<u><I> 2012.13658v2 &nbsp; - &nbsp;
{control}
{exploration}
{reinforcement}
{sparse}
</I></u><br> A major challenge in reinforcement learning is the design of exploration
strategies, especially for environments with sparse reward structures and
continuous state and action spaces. Intuitively, if the reinforcement signal is
very scarce, the agent should rely on some form of short-term memory in order
to cover its environment efficiently. We propose a new exploration method,
based on two intuitions: (1) the choice of the next exploratory action should
depend not only on the (Markovian) state of the environment, but also on the
agent's trajectory so far, and (2) the agent should utilize a measure of spread
in the state space to avoid getting stuck in a small region. Our method
leverages concepts often used in statistical physics to provide explanations
for the behavior of simplified (polymer) chains in order to generate persistent
(locally self-avoiding) trajectories in state space. We discuss the theoretical
properties of locally self-avoiding walks and their ability to provide a kind
of short-term memory through a decaying temporal correlation within the
trajectory. We provide empirical evaluations of our approach in a simulated 2D
navigation task, as well as higher-dimensional MuJoCo continuous control
locomotion tasks with sparse rewards. <br><br></div></a>
</td></tr>
<tr id=" 192 " class="entry"><td>
<a onclick="toggleVisibility('abstract192');">[|&bull;|]</a><a href="http://arxiv.org/abs/2007.13363v2">
<b/> Learning Compositional Neural Programs for Continuous Control (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2007.13363v2">
Thomas Pierrot, Nicolas Perrin, Feryal Behbahani, Alexandre Laterre, Olivier Sigaud, Karim Beguir, Nando de Freitas &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract192');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract192');"><div id="abstract192" style="font-size: 1rem; color:black; display: none">
<u><I> 2007.13363v2 &nbsp; - &nbsp;
{control}
{goal-conditioned}
{hierarchical}
{off-policy}
{reinforcement}
{replay}
{sparse}
</I></u><br> We propose a novel solution to challenging sparse-reward, continuous control
problems that require hierarchical planning at multiple levels of abstraction.
Our solution, dubbed AlphaNPI-X, involves three separate stages of learning.
First, we use off-policy reinforcement learning algorithms with experience
replay to learn a set of atomic goal-conditioned policies, which can be easily
repurposed for many tasks. Second, we learn self-models describing the effect
of the atomic policies on the environment. Third, the self-models are harnessed
to learn recursive compositional programs with multiple levels of abstraction.
The key insight is that the self-models enable planning by imagination,
obviating the need for interaction with the world when learning higher-level
compositional programs. To accomplish the third stage of learning, we extend
the AlphaNPI algorithm, which applies AlphaZero to learn recursive neural
programmer-interpreters. We empirically show that AlphaNPI-X can effectively
learn to tackle challenging sparse manipulation tasks, such as stacking
multiple blocks, where powerful model-free baselines fail. <br><br></div></a>
</td></tr>
<tr id=" 193 " class="entry"><td>
<a onclick="toggleVisibility('abstract193');">[|&bull;|]</a><a href="http://arxiv.org/abs/2006.08505v5">
<b/> Diversity Policy Gradient for Sample Efficient Quality-Diversity
Optimization (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2006.08505v5">
Thomas Pierrot, Valentin Mac&#233;, F&#233;lix Chalumeau, Arthur Flajolet, Geoffrey Cideron, Karim Beguir, Antoine Cully, Olivier Sigaud, Nicolas Perrin-Gilbert &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract193');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract193');"><div id="abstract193" style="font-size: 1rem; color:black; display: none">
<u><I> 2006.08505v5 &nbsp; - &nbsp;
{control}
{diversity}
{evolution}
{exploration}
{gradient}
{robot}
</I></u><br> A fascinating aspect of nature lies in its ability to produce a large and
diverse collection of organisms that are all high-performing in their niche. By
contrast, most AI algorithms focus on finding a single efficient solution to a
given problem. Aiming for diversity in addition to performance is a convenient
way to deal with the exploration-exploitation trade-off that plays a central
role in learning. It also allows for increased robustness when the returned
collection contains several working solutions to the considered problem, making
it well-suited for real applications such as robotics. Quality-Diversity (QD)
methods are evolutionary algorithms designed for this purpose. This paper
proposes a novel algorithm, QDPG, which combines the strength of Policy
Gradient algorithms and Quality Diversity approaches to produce a collection of
diverse and high-performing neural policies in continuous control environments.
The main contribution of this work is the introduction of a Diversity Policy
Gradient (DPG) that exploits information at the time-step level to drive
policies towards more diversity in a sample-efficient manner. Specifically,
QDPG selects neural controllers from a MAP-Elites grid and uses two
gradient-based mutation operators to improve both quality and diversity. Our
results demonstrate that QDPG is significantly more sample-efficient than its
evolutionary competitors. <br><br></div></a>
</td></tr>
<tr id=" 194 " class="entry"><td>
<a onclick="toggleVisibility('abstract194');">[|&bull;|]</a><a href="http://arxiv.org/abs/2010.14641v3">
<b/> Learning to Plan Optimistically: Uncertainty-Guided Deep Exploration via
Latent Model Ensembles (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2010.14641v3">
Tim Seyde, Wilko Schwarting, Sertac Karaman, Daniela Rus &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract194');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract194');"><div id="abstract194" style="font-size: 1rem; color:black; display: none">
<u><I> 2010.14641v3 &nbsp; - &nbsp;
{control}
{deep}
{exploration}
{robot}
{sparse}
</I></u><br> Learning complex robot behaviors through interaction requires structured
exploration. Planning should target interactions with the potential to optimize
long-term performance, while only reducing uncertainty where conducive to this
objective. This paper presents Latent Optimistic Value Exploration (LOVE), a
strategy that enables deep exploration through optimism in the face of
uncertain long-term rewards. We combine latent world models with value function
estimation to predict infinite-horizon returns and recover associated
uncertainty via ensembling. The policy is then trained on an upper confidence
bound (UCB) objective to identify and select the interactions most promising to
improve long-term performance. We apply LOVE to visual robot control tasks in
continuous action spaces and demonstrate on average more than 20% improved
sample efficiency in comparison to state-of-the-art and other exploration
objectives. In sparse and hard to explore environments we achieve an average
improvement of over 30%. <br><br></div></a>
</td></tr>
<tr id=" 195 " class="entry"><td>
<a onclick="toggleVisibility('abstract195');">[|&bull;|]</a><a href="http://arxiv.org/abs/2001.08116v1">
<b/> Q-Learning in enormous action spaces via amortized approximate
maximization (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2001.08116v1">
Tom Van de Wiele, David Warde-Farley, Andriy Mnih, Volodymyr Mnih &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract195');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract195');"><div id="abstract195" style="font-size: 1rem; color:black; display: none">
<u><I> 2001.08116v1 &nbsp; - &nbsp;
{control}
</I></u><br> Applying Q-learning to high-dimensional or continuous action spaces can be
difficult due to the required maximization over the set of possible actions.
Motivated by techniques from amortized inference, we replace the expensive
maximization over all actions with a maximization over a small subset of
possible actions sampled from a learned proposal distribution. The resulting
approach, which we dub Amortized Q-learning (AQL), is able to handle discrete,
continuous, or hybrid action spaces while maintaining the benefits of
Q-learning. Our experiments on continuous control tasks with up to 21
dimensional actions show that AQL outperforms D3PG (Barth-Maron et al, 2018)
and QT-Opt (Kalashnikov et al, 2018). Experiments on structured discrete action
spaces demonstrate that AQL can efficiently learn good policies in spaces with
thousands of discrete actions. <br><br></div></a>
</td></tr>
<tr id=" 196 " class="entry"><td>
<a onclick="toggleVisibility('abstract196');">[|&bull;|]</a><a href="http://arxiv.org/abs/2001.02907v1">
<b/> Population-Guided Parallel Policy Search for Reinforcement Learning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2001.02907v1">
Whiyoung Jung, Giseung Park, Youngchul Sung &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract196');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract196');"><div id="abstract196" style="font-size: 1rem; color:black; display: none">
<u><I> 2001.02907v1 &nbsp; - &nbsp;
{deep}
{gradient}
{off-policy}
{population}
{reinforcement}
{replay}
{sparse}
</I></u><br> In this paper, a new population-guided parallel learning scheme is proposed
to enhance the performance of off-policy reinforcement learning (RL). In the
proposed scheme, multiple identical learners with their own value-functions and
policies share a common experience replay buffer, and search a good policy in
collaboration with the guidance of the best policy information. The key point
is that the information of the best policy is fused in a soft manner by
constructing an augmented loss function for policy update to enlarge the
overall search region by the multiple learners. The guidance by the previous
best policy and the enlarged range enable faster and better policy search.
Monotone improvement of the expected cumulative return by the proposed scheme
is proved theoretically. Working algorithms are constructed by applying the
proposed scheme to the twin delayed deep deterministic (TD3) policy gradient
algorithm. Numerical results show that the constructed algorithm outperforms
most of the current state-of-the-art RL algorithms, and the gain is significant
in the case of sparse reward environment. <br><br></div></a>
</td></tr>
<tr id=" 197 " class="entry"><td>
<a onclick="toggleVisibility('abstract197');">[|&bull;|]</a><a href="http://arxiv.org/abs/2002.06473v1">
<b/> Universal Value Density Estimation for Imitation Learning and
Goal-Conditioned Reinforcement Learning (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2002.06473v1">
Yannick Schroecker, Charles Isbell &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract197');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract197');"><div id="abstract197" style="font-size: 1rem; color:black; display: none">
<u><I> 2002.06473v1 &nbsp; - &nbsp;
{goal-conditioned}
{imitation}
{reinforcement}
</I></u><br> This work considers two distinct settings: imitation learning and
goal-conditioned reinforcement learning. In either case, effective solutions
require the agent to reliably reach a specified state (a goal), or set of
states (a demonstration). Drawing a connection between probabilistic long-term
dynamics and the desired value function, this work introduces an approach which
utilizes recent advances in density estimation to effectively learn to reach a
given state. As our first contribution, we use this approach for
goal-conditioned reinforcement learning and show that it is both efficient and
does not suffer from hindsight bias in stochastic domains. As our second
contribution, we extend the approach to imitation learning and show that it
achieves state-of-the art demonstration sample-efficiency on standard benchmark
tasks. <br><br></div></a>
</td></tr>
<tr id=" 198 " class="entry"><td>
<a onclick="toggleVisibility('abstract198');">[|&bull;|]</a><a href="http://arxiv.org/abs/2006.07781v1">
<b/> Non-local Policy Optimization via Diversity-regularized Collaborative
Exploration (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2006.07781v1">
Zhenghao Peng, Hao Sun, Bolei Zhou &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract198');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract198');"><div id="abstract198" style="font-size: 1rem; color:black; display: none">
<u><I> 2006.07781v1 &nbsp; - &nbsp;
{diversity}
{exploration}
{off-policy}
{on-policy}
{reinforcement}
</I></u><br> Conventional Reinforcement Learning (RL) algorithms usually have one single
agent learning to solve the task independently. As a result, the agent can only
explore a limited part of the state-action space while the learned behavior is
highly correlated to the agent's previous experience, making the training prone
to a local minimum. In this work, we empower RL with the capability of teamwork
and propose a novel non-local policy optimization framework called
Diversity-regularized Collaborative Exploration (DiCE). DiCE utilizes a group
of heterogeneous agents to explore the environment simultaneously and share the
collected experiences. A regularization mechanism is further designed to
maintain the diversity of the team and modulate the exploration. We implement
the framework in both on-policy and off-policy settings and the experimental
results show that DiCE can achieve substantial improvement over the baselines
in the MuJoCo locomotion tasks. <br><br></div></a>
</td></tr>
<tr id=" 199 " class="entry"><td>
<a onclick="toggleVisibility('abstract199');">[|&bull;|]</a><a href="http://arxiv.org/abs/2009.07888v7">
<b/> Transfer Learning in Deep Reinforcement Learning: A Survey (2020)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/2009.07888v7">
Zhuangdi Zhu, Kaixiang Lin, Anil K. Jain, Jiayu Zhou &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract199');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract199');"><div id="abstract199" style="font-size: 1rem; color:black; display: none">
<u><I> 2009.07888v7 &nbsp; - &nbsp;
{deep}
{reinforcement}
{robot}
{transfer}
</I></u><br> Reinforcement learning is a learning paradigm for solving sequential
decision-making problems. Recent years have witnessed remarkable progress in
reinforcement learning upon the fast development of deep neural networks. Along
with the promising prospects of reinforcement learning in numerous domains such
as robotics and game-playing, transfer learning has arisen to tackle various
challenges faced by reinforcement learning, by transferring knowledge from
external expertise to facilitate the efficiency and effectiveness of the
learning process. In this survey, we systematically investigate the recent
progress of transfer learning approaches in the context of deep reinforcement
learning. Specifically, we provide a framework for categorizing the
state-of-the-art transfer learning approaches, under which we analyze their
goals, methodologies, compatible reinforcement learning backbones, and
practical applications. We also draw connections between transfer learning and
other relevant topics from the reinforcement learning perspective and explore
their potential challenges that await future research progress. <br><br></div></a>
</td></tr>
<tr id=" 200 " class="entry"><td>
<a onclick="toggleVisibility('abstract200');">[|&bull;|]</a><a href="http://arxiv.org/abs/1912.06680v1">
<b/> Dota 2 with Large Scale Deep Reinforcement Learning (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1912.06680v1">
 OpenAI,  :, Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys&#322;aw D&#281;biak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal J&#243;zefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique P. d. O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, Susan Zhang &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract200');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract200');"><div id="abstract200" style="font-size: 1rem; color:black; display: none">
<u><I> 1912.06680v1 &nbsp; - &nbsp;
{reinforcement}
</I></u><br> On April 13th, 2019, OpenAI Five became the first AI system to defeat the
world champions at an esports game. The game of Dota 2 presents novel
challenges for AI systems such as long time horizons, imperfect information,
and complex, continuous state-action spaces, all challenges which will become
increasingly central to more capable AI systems. OpenAI Five leveraged existing
reinforcement learning techniques, scaled to learn from batches of
approximately 2 million frames every 2 seconds. We developed a distributed
training system and tools for continual training which allowed us to train
OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG),
OpenAI Five demonstrates that self-play reinforcement learning can achieve
superhuman performance on a difficult task. <br><br></div></a>
</td></tr>
<tr id=" 201 " class="entry"><td>
<a onclick="toggleVisibility('abstract201');">[|&bull;|]</a><a href="http://arxiv.org/abs/1901.10995v4">
<b/> Go-Explore: a New Approach for Hard-Exploration Problems (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1901.10995v4">
Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, Jeff Clune &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract201');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract201');"><div id="abstract201" style="font-size: 1rem; color:black; display: none">
<u><I> 1901.10995v4 &nbsp; - &nbsp;
{exploration}
{imitation}
{intrinsic}
{reinforcement}
{robot}
{sparse}
</I></u><br> A grand challenge in reinforcement learning is intelligent exploration,
especially when rewards are sparse or deceptive. Two Atari games serve as
benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall.
On both games, current RL algorithms perform poorly, even those with intrinsic
motivation, which is the dominant method to improve performance on
hard-exploration domains. To address this shortfall, we introduce a new
algorithm called Go-Explore. It exploits the following principles: (1) remember
previously visited states, (2) first return to a promising state (without
exploration), then explore from it, and (3) solve simulated environments
through any available means (including by introducing determinism), then
robustify via imitation learning. The combined effect of these principles is a
dramatic performance improvement on hard-exploration problems. On Montezuma's
Revenge, Go-Explore scores a mean of over 43k points, almost 4 times the
previous state of the art. Go-Explore can also harness human-provided domain
knowledge and, when augmented with it, scores a mean of over 650k points on
Montezuma's Revenge. Its max performance of nearly 18 million surpasses the
human world record, meeting even the strictest definition of "superhuman"
performance. On Pitfall, Go-Explore with domain knowledge is the first
algorithm to score above zero. Its mean score of almost 60k points exceeds
expert human performance. Because Go-Explore produces high-performing
demonstrations automatically and cheaply, it also outperforms imitation
learning work where humans provide solution demonstrations. Go-Explore opens up
many new research directions into improving it and weaving its insights into
current RL algorithms. It may also enable progress on previously unsolvable
hard-exploration problems in many domains, especially those that harness a
simulator during training (e.g. robotics). <br><br></div></a>
</td></tr>
<tr id=" 202 " class="entry"><td>
<a onclick="toggleVisibility('abstract202');">[|&bull;|]</a><a href="http://arxiv.org/abs/1911.01417v1">
<b/> Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing
Shaped Rewards (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1911.01417v1">
Alexander Trott, Stephan Zheng, Caiming Xiong, Richard Socher &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract202');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract202');"><div id="abstract202" style="font-size: 1rem; color:black; display: none">
<u><I> 1911.01417v1 &nbsp; - &nbsp;
{exploration}
{intrinsic}
{sparse}
</I></u><br> While using shaped rewards can be beneficial when solving sparse reward
tasks, their successful application often requires careful engineering and is
problem specific. For instance, in tasks where the agent must achieve some goal
state, simple distance-to-goal reward shaping often fails, as it renders
learning vulnerable to local optima. We introduce a simple and effective
model-free method to learn from shaped distance-to-goal rewards on tasks where
success depends on reaching a goal state. Our method introduces an auxiliary
distance-based reward based on pairs of rollouts to encourage diverse
exploration. This approach effectively prevents learning dynamics from
stabilizing around local optima induced by the naive distance-to-goal reward
shaping and enables policies to efficiently solve sparse reward tasks. Our
augmented objective does not require any additional reward engineering or
domain expertise to implement and converges to the original sparse objective as
the agent learns to solve the task. We demonstrate that our method successfully
solves a variety of hard-exploration tasks (including maze navigation and 3D
construction in a Minecraft environment), where naive distance-based reward
shaping otherwise fails, and intrinsic curiosity and reward relabeling
strategies exhibit poor performance. <br><br></div></a>
</td></tr>
<tr id=" 203 " class="entry"><td>
<a onclick="toggleVisibility('abstract203');">[|&bull;|]</a><a href="http://arxiv.org/abs/1910.00514v3">
<b/> Online Trajectory Planning Through Combined Trajectory Optimization and
Function Approximation: Application to the Exoskeleton Atalante (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1910.00514v3">
Alexis Duburcq, Yann Chevaleyre, Nicolas Bredeche, Guilhem Bo&#233;ris &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract203');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract203');"><div id="abstract203" style="font-size: 1rem; color:black; display: none">
<u><I> 1910.00514v3 &nbsp; - &nbsp;
{control}
{exoskeleton}
{offline}
{robot}
</I></u><br> Autonomous robots require online trajectory planning capability to operate in
the real world. Efficient offline trajectory planning methods already exist,
but are computationally demanding, preventing their use online. In this paper,
we present a novel algorithm called Guided Trajectory Learning that learns a
function approximation of solutions computed through trajectory optimization
while ensuring accurate and reliable predictions. This function approximation
is then used online to generate trajectories. This algorithm is designed to be
easy to implement, and practical since it does not require massive computing
power. It is readily applicable to any robotics systems and effortless to set
up on real hardware since robust control strategies are usually already
available. We demonstrate the computational performance of our algorithm on
flat-foot walking with the self-balanced exoskeleton Atalante. <br><br></div></a>
</td></tr>
<tr id=" 204 " class="entry"><td>
<a onclick="toggleVisibility('abstract204');">[|&bull;|]</a><a href="http://arxiv.org/abs/1907.01657v2">
<b/> Dynamics-Aware Unsupervised Discovery of Skills (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1907.01657v2">
Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, Karol Hausman &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract204');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract204');"><div id="abstract204" style="font-size: 1rem; color:black; display: none">
<u><I> 1907.01657v2 &nbsp; - &nbsp;
{goal-conditioned}
{hierarchical}
{model-based}
{reinforcement}
{skill}
{sparse}
{unsupervised}
</I></u><br> Conventionally, model-based reinforcement learning (MBRL) aims to learn a
global model for the dynamics of the environment. A good model can potentially
enable planning algorithms to generate a large variety of behaviors and solve
diverse tasks. However, learning an accurate model for complex dynamical
systems is difficult, and even then, the model might not generalize well
outside the distribution of states on which it was trained. In this work, we
combine model-based learning with model-free learning of primitives that make
model-based planning easy. To that end, we aim to answer the question: how can
we discover skills whose outcomes are easy to predict? We propose an
unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS),
which simultaneously discovers predictable behaviors and learns their dynamics.
Our method can leverage continuous skill spaces, theoretically, allowing us to
learn infinitely many behaviors even for high-dimensional state-spaces. We
demonstrate that zero-shot planning in the learned latent space significantly
outperforms standard MBRL and model-free goal-conditioned RL, can handle
sparse-reward tasks, and substantially improves over prior hierarchical RL
methods for unsupervised skill discovery. <br><br></div></a>
</td></tr>
<tr id=" 205 " class="entry"><td>
<a onclick="toggleVisibility('abstract205');">[|&bull;|]</a><a href="http://arxiv.org/abs/1910.01738v2">
<b/> State Representation Learning from Demonstration (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1910.01738v2">
Astrid Merckling, Alexandre Coninx, Loic Cressot, St&#233;phane Doncieux, Nicolas Perrin-Gilbert &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract205');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract205');"><div id="abstract205" style="font-size: 1rem; color:black; display: none">
<u><I> 1910.01738v2 &nbsp; - &nbsp;
{control}
{imitation}
{reinforcement}
{robot}
</I></u><br> Robots could learn their own state and world representation from perception
and experience without supervision. This desirable goal is the main focus of
our field of interest, state representation learning (SRL). Indeed, a compact
representation of such a state is beneficial to help robots grasp onto their
environment for interacting. The properties of this representation have a
strong impact on the adaptive capability of the agent. In this article we
present an approach based on imitation learning. The idea is to train several
policies that share the same representation to reproduce various
demonstrations. To do so, we use a multi-head neural network with a shared
state representation feeding a task-specific agent. If the demonstrations are
diverse, the trained representation will eventually contain the information
necessary for all tasks, while discarding irrelevant information. As such, it
will potentially become a compact state representation useful for new tasks. We
call this approach SRLfD (State Representation Learning from Demonstration).
Our experiments confirm that when a controller takes SRLfD-based
representations as input, it can achieve better performance than with other
representation strategies and promote more efficient reinforcement learning
(RL) than with an end-to-end RL strategy. <br><br></div></a>
</td></tr>
<tr id=" 206 " class="entry"><td>
<a onclick="toggleVisibility('abstract206');">[|&bull;|]</a><a href="http://arxiv.org/abs/1906.00949v2">
<b/> Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1906.00949v2">
Aviral Kumar, Justin Fu, George Tucker, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract206');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract206');"><div id="abstract206" style="font-size: 1rem; color:black; display: none">
<u><I> 1906.00949v2 &nbsp; - &nbsp;
{control}
{off-policy}
{on-policy}
{optimal}
{reinforcement}
</I></u><br> Off-policy reinforcement learning aims to leverage experience collected from
prior policies for sample-efficient learning. However, in practice, commonly
used off-policy approximate dynamic programming methods based on Q-learning and
actor-critic methods are highly sensitive to the data distribution, and can
make only limited progress without collecting additional on-policy data. As a
step towards more robust off-policy algorithms, we study the setting where the
off-policy experience is fixed and there is no further interaction with the
environment. We identify bootstrapping error as a key source of instability in
current methods. Bootstrapping error is due to bootstrapping from actions that
lie outside of the training data distribution, and it accumulates via the
Bellman backup operator. We theoretically analyze bootstrapping error, and
demonstrate how carefully constraining action selection in the backup can
mitigate it. Based on our analysis, we propose a practical algorithm,
bootstrapping error accumulation reduction (BEAR). We demonstrate that BEAR is
able to learn robustly from different off-policy distributions, including
random and suboptimal demonstrations, on a range of continuous control tasks. <br><br></div></a>
</td></tr>
<tr id=" 207 " class="entry"><td>
<a onclick="toggleVisibility('abstract207');">[|&bull;|]</a><a href="http://arxiv.org/abs/1905.06893v3">
<b/> Leveraging exploration in off-policy algorithms via normalizing flows (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1905.06893v3">
Bogdan Mazoure, Thang Doan, Audrey Durand, R Devon Hjelm, Joelle Pineau &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract207');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract207');"><div id="abstract207" style="font-size: 1rem; color:black; display: none">
<u><I> 1905.06893v3 &nbsp; - &nbsp;
{control}
{entropy}
{exploration}
{off-policy}
{optimal}
{reinforcement}
{sparse}
</I></u><br> The ability to discover approximately optimal policies in domains with sparse
rewards is crucial to applying reinforcement learning (RL) in many real-world
scenarios. Approaches such as neural density models and continuous exploration
(e.g., Go-Explore) have been proposed to maintain the high exploration rate
necessary to find high performing and generalizable policies. Soft
actor-critic(SAC) is another method for improving exploration that aims to
combine efficient learning via off-policy updates while maximizing the policy
entropy. In this work, we extend SAC to a richer class of probability
distributions (e.g., multimodal) through normalizing flows (NF) and show that
this significantly improves performance by accelerating the discovery of good
policies while using much smaller policy representations. Our approach, which
we call SAC-NF, is a simple, efficient,easy-to-implement modification and
improvement to SAC on continuous control baselines such as MuJoCo and PyBullet
Roboschool domains. Finally, SAC-NF does this while being significantly
parameter efficient, using as few as 5.5% the parameters for an equivalent SAC
model. <br><br></div></a>
</td></tr>
<tr id=" 208 " class="entry"><td>
<a onclick="toggleVisibility('abstract208');">[|&bull;|]</a><a href="http://arxiv.org/abs/1901.10691v2">
<b/> Probability Functional Descent: A Unifying Perspective on GANs,
Variational Inference, and Reinforcement Learning (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1901.10691v2">
Casey Chu, Jose Blanchet, Peter Glynn &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract208');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract208');"><div id="abstract208" style="font-size: 1rem; color:black; display: none">
<u><I> 1901.10691v2 &nbsp; - &nbsp;
{reinforcement}
</I></u><br> This paper provides a unifying view of a wide range of problems of interest
in machine learning by framing them as the minimization of functionals defined
on the space of probability measures. In particular, we show that generative
adversarial networks, variational inference, and actor-critic methods in
reinforcement learning can all be seen through the lens of our framework. We
then discuss a generic optimization algorithm for our formulation, called
probability functional descent (PFD), and show how this algorithm recovers
existing methods developed independently in the settings mentioned earlier. <br><br></div></a>
</td></tr>
<tr id=" 209 " class="entry"><td>
<a onclick="toggleVisibility('abstract209');">[|&bull;|]</a><a href="http://arxiv.org/abs/1903.01973v2">
<b/> Learning Latent Plans from Play (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1903.01973v2">
Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, Pierre Sermanet &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract209');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract209');"><div id="abstract209" style="font-size: 1rem; color:black; display: none">
<u><I> 1903.01973v2 &nbsp; - &nbsp;
{control}
{robot}
{self-supervised}
{skill}
</I></u><br> Acquiring a diverse repertoire of general-purpose skills remains an open
challenge for robotics. In this work, we propose self-supervising control on
top of human teleoperated play data as a way to scale up skill learning. Play
has two properties that make it attractive compared to conventional task
demonstrations. Play is cheap, as it can be collected in large quantities
quickly without task segmenting, labeling, or resetting to an initial state.
Play is naturally rich, covering ~4x more interaction space than task
demonstrations for the same amount of collection time. To learn control from
play, we introduce Play-LMP, a self-supervised method that learns to organize
play behaviors in a latent space, then reuse them at test time to achieve
specific goals. Combining self-supervised control with a diverse play dataset
shifts the focus of skill learning from a narrow and discrete set of tasks to
the full continuum of behaviors available in an environment. We find that this
combination generalizes well empirically---after self-supervising on unlabeled
play, our method substantially outperforms individual expert-trained policies
on 18 difficult user-specified visual manipulation tasks in a simulated robotic
tabletop environment. We additionally find that play-supervised models, unlike
their expert-trained counterparts, are more robust to perturbations and exhibit
retrying-till-success behaviors. Finally, we find that our agent organizes its
latent plan space around functional tasks, despite never being trained with
task labels. Videos, code and data are available at
learning-from-play.github.io <br><br></div></a>
</td></tr>
<tr id=" 210 " class="entry"><td>
<a onclick="toggleVisibility('abstract210');">[|&bull;|]</a><a href="http://arxiv.org/abs/1906.09807v4">
<b/> Proximal Distilled Evolutionary Reinforcement Learning (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1906.09807v4">
Cristian Bodnar, Ben Day, Pietro Li&#243; &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract210');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract210');"><div id="abstract210" style="font-size: 1rem; color:black; display: none">
<u><I> 1906.09807v4 &nbsp; - &nbsp;
{deep}
{distill}
{evolution}
{hierarchical}
{reinforcement}
{robot}
</I></u><br> Reinforcement Learning (RL) has achieved impressive performance in many
complex environments due to the integration with Deep Neural Networks (DNNs).
At the same time, Genetic Algorithms (GAs), often seen as a competing approach
to RL, had limited success in scaling up to the DNNs required to solve
challenging tasks. Contrary to this dichotomic view, in the physical world,
evolution and learning are complementary processes that continuously interact.
The recently proposed Evolutionary Reinforcement Learning (ERL) framework has
demonstrated mutual benefits to performance when combining the two methods.
However, ERL has not fully addressed the scalability problem of GAs. In this
paper, we show that this problem is rooted in an unfortunate combination of a
simple genetic encoding for DNNs and the use of traditional
biologically-inspired variation operators. When applied to these encodings, the
standard operators are destructive and cause catastrophic forgetting of the
traits the networks acquired. We propose a novel algorithm called Proximal
Distilled Evolutionary Reinforcement Learning (PDERL) that is characterised by
a hierarchical integration between evolution and learning. The main innovation
of PDERL is the use of learning-based variation operators that compensate for
the simplicity of the genetic representation. Unlike traditional operators, our
proposals meet the functional requirements of variation operators when applied
on directly-encoded DNNs. We evaluate PDERL in five robot locomotion settings
from the OpenAI gym. Our method outperforms ERL, as well as two
state-of-the-art RL algorithms, PPO and TD3, in all tested environments. <br><br></div></a>
</td></tr>
<tr id=" 211 " class="entry"><td>
<a onclick="toggleVisibility('abstract211');">[|&bull;|]</a><a href="http://arxiv.org/abs/1912.01603v3">
<b/> Dream to Control: Learning Behaviors by Latent Imagination (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1912.01603v3">
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract211');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract211');"><div id="abstract211" style="font-size: 1rem; color:black; display: none">
<u><I> 1912.01603v3 &nbsp; - &nbsp;
{control}
{deep}
{gradient}
{reinforcement}
</I></u><br> Learned world models summarize an agent's experience to facilitate learning
complex behaviors. While learning world models from high-dimensional sensory
inputs is becoming feasible through deep learning, there are many potential
ways for deriving behaviors from them. We present Dreamer, a reinforcement
learning agent that solves long-horizon tasks from images purely by latent
imagination. We efficiently learn behaviors by propagating analytic gradients
of learned state values back through trajectories imagined in the compact state
space of a learned world model. On 20 challenging visual control tasks, Dreamer
exceeds existing approaches in data-efficiency, computation time, and final
performance. <br><br></div></a>
</td></tr>
<tr id=" 212 " class="entry"><td>
<a onclick="toggleVisibility('abstract212');">[|&bull;|]</a><a href="http://arxiv.org/abs/1902.04706v2">
<b/> Simultaneously Learning Vision and Feature-based Control Policies for
Real-world Ball-in-a-Cup (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1902.04706v2">
Devin Schwab, Tobias Springenberg, Murilo F. Martins, Thomas Lampe, Michael Neunert, Abbas Abdolmaleki, Tim Hertweck, Roland Hafner, Francesco Nori, Martin Riedmiller &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract212');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract212');"><div id="abstract212" style="font-size: 1rem; color:black; display: none">
<u><I> 1902.04706v2 &nbsp; - &nbsp;
{control}
{imitation}
{multi-task}
{reinforcement}
{robot}
{transfer}
</I></u><br> We present a method for fast training of vision based control policies on
real robots. The key idea behind our method is to perform multi-task
Reinforcement Learning with auxiliary tasks that differ not only in the reward
to be optimized but also in the state-space in which they operate. In
particular, we allow auxiliary task policies to utilize task features that are
available only at training-time. This allows for fast learning of auxiliary
policies, which subsequently generate good data for training the main,
vision-based control policies. This method can be seen as an extension of the
Scheduled Auxiliary Control (SAC-X) framework. We demonstrate the efficacy of
our method by using both a simulated and real-world Ball-in-a-Cup game
controlled by a robot arm. In simulation, our approach leads to significant
learning speed-ups when compared to standard SAC-X. On the real robot we show
that the task can be learned from-scratch, i.e., with no transfer from
simulation and no imitation learning. Videos of our learned policies running on
the real robot can be found at
https://sites.google.com/view/rss-2019-sawyer-bic/. <br><br></div></a>
</td></tr>
<tr id=" 213 " class="entry"><td>
<a onclick="toggleVisibility('abstract213');">[|&bull;|]</a><a href="http://arxiv.org/abs/1912.06088v4">
<b/> Learning to Reach Goals via Iterated Supervised Learning (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1912.06088v4">
Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract213');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract213');"><div id="abstract213" style="font-size: 1rem; color:black; display: none">
<u><I> 1912.06088v4 &nbsp; - &nbsp;
{imitation}
{reinforcement}
{sparse}
</I></u><br> Current reinforcement learning (RL) algorithms can be brittle and difficult
to use, especially when learning goal-reaching behaviors from sparse rewards.
Although supervised imitation learning provides a simple and stable
alternative, it requires access to demonstrations from a human supervisor. In
this paper, we study RL algorithms that use imitation learning to acquire goal
reaching policies from scratch, without the need for expert demonstrations or a
value function. In lieu of demonstrations, we leverage the property that any
trajectory is a successful demonstration for reaching the final state in that
same trajectory. We propose a simple algorithm in which an agent continually
relabels and imitates the trajectories it generates to progressively learn
goal-reaching behaviors from scratch. Each iteration, the agent collects new
trajectories using the latest policy, and maximizes the likelihood of the
actions along these trajectories under the goal that was actually reached, so
as to improve the policy. We formally show that this iterated supervised
learning procedure optimizes a bound on the RL objective, derive performance
bounds of the learned policy, and empirically demonstrate improved
goal-reaching performance and robustness over current RL algorithms in several
benchmark tasks. <br><br></div></a>
</td></tr>
<tr id=" 214 " class="entry"><td>
<a onclick="toggleVisibility('abstract214');">[|&bull;|]</a><a href="http://arxiv.org/abs/1906.02691v3">
<b/> An Introduction to Variational Autoencoders (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1906.02691v3">
Diederik P. Kingma, Max Welling &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract214');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract214');"><div id="abstract214" style="font-size: 1rem; color:black; display: none">
<u><I> 1906.02691v3 &nbsp; - &nbsp;
{deep}
</I></u><br> Variational autoencoders provide a principled framework for learning deep
latent-variable models and corresponding inference models. In this work, we
provide an introduction to variational autoencoders and some important
extensions. <br><br></div></a>
</td></tr>
<tr id=" 215 " class="entry"><td>
<a onclick="toggleVisibility('abstract215');">[|&bull;|]</a><a href="http://arxiv.org/abs/1907.01298v2">
<b/> Modified Actor-Critics (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1907.01298v2">
Erinc Merdivan, Sten Hanke, Matthieu Geist &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract215');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract215');"><div id="abstract215" style="font-size: 1rem; color:black; display: none">
<u><I> 1907.01298v2 &nbsp; - &nbsp;
{deep}
{off-policy}
{on-policy}
{reinforcement}
</I></u><br> Recent successful deep reinforcement learning algorithms, such as Trust
Region Policy Optimization (TRPO) or Proximal Policy Optimization (PPO), are
fundamentally variations of conservative policy iteration (CPI). These
algorithms iterate policy evaluation followed by a softened policy improvement
step. As so, they are naturally on-policy. In this paper, we propose to combine
(any kind of) soft greediness with Modified Policy Iteration (MPI). The
proposed abstract framework applies repeatedly: (i) a partial policy evaluation
step that allows off-policy learning and (ii) any softened greedy step. Our
contribution can be seen as a new generic tool for the deep reinforcement
learning toolbox. As a proof of concept, we instantiate this framework with the
PPO greediness. Comparison to the original PPO shows that our algorithm is much
more sample efficient. We also show that it is competitive with the
state-of-art off-policy algorithm Soft Actor Critic (SAC). <br><br></div></a>
</td></tr>
<tr id=" 216 " class="entry"><td>
<a onclick="toggleVisibility('abstract216');">[|&bull;|]</a><a href="http://arxiv.org/abs/1908.04211v4">
<b/> On Identifiability in Transformers (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1908.04211v4">
Gino Brunner, Yang Liu, Dami&#225;n Pascual, Oliver Richter, Massimiliano Ciaramita, Roger Wattenhofer &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract216');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract216');"><div id="abstract216" style="font-size: 1rem; color:black; display: none">
<u><I> 1908.04211v4 &nbsp; - &nbsp;
{deep}
{gradient}
</I></u><br> In this paper we delve deep in the Transformer architecture by investigating
two of its core components: self-attention and contextual embeddings. In
particular, we study the identifiability of attention weights and token
embeddings, and the aggregation of context into hidden tokens. We show that,
for sequences longer than the attention head dimension, attention weights are
not identifiable. We propose effective attention as a complementary tool for
improving explanatory interpretations based on attention. Furthermore, we show
that input tokens retain to a large degree their identity across the model. We
also find evidence suggesting that identity information is mainly encoded in
the angle of the embeddings and gradually decreases with depth. Finally, we
demonstrate strong mixing of input information in the generation of contextual
embeddings by means of a novel quantification method based on gradient
attribution. Overall, we show that self-attention distributions are not
directly interpretable and present tools to better understand and further
investigate Transformer models. <br><br></div></a>
</td></tr>
<tr id=" 217 " class="entry"><td>
<a onclick="toggleVisibility('abstract217');">[|&bull;|]</a><a href="http://arxiv.org/abs/1903.02219v1">
<b/> Training in Task Space to Speed Up and Guide Reinforcement Learning (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1903.02219v1">
Guillaume Bellegarda, Katie Byl &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract217');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract217');"><div id="abstract217" style="font-size: 1rem; color:black; display: none">
<u><I> 1903.02219v1 &nbsp; - &nbsp;
{control}
{kinematic}
{reinforcement}
{robot}
</I></u><br> Recent breakthroughs in the reinforcement learning (RL) community have made
significant advances towards learning and deploying policies on real world
robotic systems. However, even with the current state-of-the-art algorithms and
computational resources, these algorithms are still plagued with high sample
complexity, and thus long training times, especially for high degree of freedom
(DOF) systems. There are also concerns arising from lack of perceived stability
or robustness guarantees from emerging policies. This paper aims at mitigating
these drawbacks by: (1) modeling a complex, high DOF system with a
representative simple one, (2) making explicit use of forward and inverse
kinematics without forcing the RL algorithm to "learn" them on its own, and (3)
learning locomotion policies in Cartesian space instead of joint space. In this
paper these methods are applied to JPL's Robosimian, but can be readily used on
any system with a base and end effector(s). These locomotion policies can be
produced in just a few minutes, trained on a single laptop. We compare the
robustness of the resulting learned policies to those of other control methods.
An accompanying video for this paper can be found at
https://youtu.be/xDxxSw5ahnc . <br><br></div></a>
</td></tr>
<tr id=" 218 " class="entry"><td>
<a onclick="toggleVisibility('abstract218');">[|&bull;|]</a><a href="http://arxiv.org/abs/1911.11679v1">
<b/> The problem with DDPG: understanding failures in deterministic
environments with sparse rewards (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1911.11679v1">
Guillaume Matheron, Nicolas Perrin, Olivier Sigaud &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract218');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract218');"><div id="abstract218" style="font-size: 1rem; color:black; display: none">
<u><I> 1911.11679v1 &nbsp; - &nbsp;
{control}
{reinforcement}
{sparse}
</I></u><br> In environments with continuous state and action spaces, state-of-the-art
actor-critic reinforcement learning algorithms can solve very complex problems,
yet can also fail in environments that seem trivial, but the reason for such
failures is still poorly understood. In this paper, we contribute a formal
explanation of these failures in the particular case of sparse reward and
deterministic environments. First, using a very elementary control problem, we
illustrate that the learning process can get stuck into a fixed point
corresponding to a poor solution. Then, generalizing from the studied example,
we provide a detailed analysis of the underlying mechanisms which results in a
new understanding of one of the convergence regimes of these algorithms. The
resulting perspective casts a new light on already existing solutions to the
issues we have highlighted, and suggests other potential approaches. <br><br></div></a>
</td></tr>
<tr id=" 219 " class="entry"><td>
<a onclick="toggleVisibility('abstract219');">[|&bull;|]</a><a href="http://arxiv.org/abs/1907.04799v2">
<b/> RL-RRT: Kinodynamic Motion Planning via Learning Reachability Estimators
from RL Policies (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1907.04799v2">
Hao-Tien Lewis Chiang, Jasmine Hsu, Marek Fiser, Lydia Tapia, Aleksandra Faust &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract219');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract219');"><div id="abstract219" style="font-size: 1rem; color:black; display: none">
<u><I> 1907.04799v2 &nbsp; - &nbsp;
{control}
{deep}
{motion planning}
{reinforcement}
{robot}
{transfer}
</I></u><br> This paper addresses two challenges facing sampling-based kinodynamic motion
planning: a way to identify good candidate states for local transitions and the
subsequent computationally intractable steering between these candidate states.
Through the combination of sampling-based planning, a Rapidly Exploring
Randomized Tree (RRT) and an efficient kinodynamic motion planner through
machine learning, we propose an efficient solution to long-range planning for
kinodynamic motion planning. First, we use deep reinforcement learning to learn
an obstacle-avoiding policy that maps a robot's sensor observations to actions,
which is used as a local planner during planning and as a controller during
execution. Second, we train a reachability estimator in a supervised manner,
which predicts the RL policy's time to reach a state in the presence of
obstacles. Lastly, we introduce RL-RRT that uses the RL policy as a local
planner, and the reachability estimator as the distance function to bias
tree-growth towards promising regions. We evaluate our method on three
kinodynamic systems, including physical robot experiments. Results across all
three robots tested indicate that RL-RRT outperforms state of the art
kinodynamic planners in efficiency, and also provides a shorter path finish
time than a steering function free method. The learned local planner policy and
accompanying reachability estimator demonstrate transferability to the
previously unseen experimental environments, making RL-RRT fast because the
expensive computations are replaced with simple neural network inference.
Video: https://youtu.be/dDMVMTOI8KY <br><br></div></a>
</td></tr>
<tr id=" 220 " class="entry"><td>
<a onclick="toggleVisibility('abstract220');">[|&bull;|]</a><a href="http://arxiv.org/abs/1906.07987v1">
<b/> Adaptive Temporal-Difference Learning for Policy Evaluation with
Per-State Uncertainty Estimates (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1906.07987v1">
Hugo Penedones, Carlos Riquelme, Damien Vincent, Hartmut Maennel, Timothy Mann, Andre Barreto, Sylvain Gelly, Gergely Neu &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract220');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract220');"><div id="abstract220" style="font-size: 1rem; color:black; display: none">
<u><I> 1906.07987v1 &nbsp; - &nbsp;
{on-policy}
{reinforcement}
</I></u><br> We consider the core reinforcement-learning problem of on-policy value
function approximation from a batch of trajectory data, and focus on various
issues of Temporal Difference (TD) learning and Monte Carlo (MC) policy
evaluation. The two methods are known to achieve complementary bias-variance
trade-off properties, with TD tending to achieve lower variance but potentially
higher bias. In this paper, we argue that the larger bias of TD can be a result
of the amplification of local approximation errors. We address this by
proposing an algorithm that adaptively switches between TD and MC in each
state, thus mitigating the propagation of errors. Our method is based on
learned confidence intervals that detect biases of TD estimates. We demonstrate
in a variety of policy evaluation tasks that this simple adaptive algorithm
performs competitively with the best approach in hindsight, suggesting that
learned confidence intervals are a powerful technique for adapting policy
evaluation to use TD or MC returns in a data-driven way. <br><br></div></a>
</td></tr>
<tr id=" 221 " class="entry"><td>
<a onclick="toggleVisibility('abstract221');">[|&bull;|]</a><a href="http://arxiv.org/abs/1901.03909v1">
<b/> Eliminating all bad Local Minima from Loss Landscapes without even
adding an Extra Unit (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1901.03909v1">
Jascha Sohl-Dickstein, Kenji Kawaguchi &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract221');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract221');"><div id="abstract221" style="font-size: 1rem; color:black; display: none">
<u><I> 1901.03909v1 &nbsp; - &nbsp;
</I></u><br> Recent work has noted that all bad local minima can be removed from neural
network loss landscapes, by adding a single unit with a particular
parameterization. We show that the core technique from these papers can be used
to remove all bad local minima from any loss landscape, so long as the global
minimum has a loss of zero. This procedure does not require the addition of
auxiliary units, or even that the loss be associated with a neural network. The
method of action involves all bad local minima being converted into bad
(non-local) minima at infinity in terms of auxiliary parameters. <br><br></div></a>
</td></tr>
<tr id=" 222 " class="entry"><td>
<a onclick="toggleVisibility('abstract222');">[|&bull;|]</a><a href="http://arxiv.org/abs/1901.07517v1">
<b/> Robust Recovery Controller for a Quadrupedal Robot using Deep
Reinforcement Learning (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1901.07517v1">
Joonho Lee, Jemin Hwangbo, Marco Hutter &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract222');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract222');"><div id="abstract222" style="font-size: 1rem; color:black; display: none">
<u><I> 1901.07517v1 &nbsp; - &nbsp;
{control}
{deep}
{hierarchical}
{reinforcement}
{robot}
</I></u><br> The ability to recover from a fall is an essential feature for a legged robot
to navigate in challenging environments robustly. Until today, there has been
very little progress on this topic. Current solutions mostly build upon
(heuristically) predefined trajectories, resulting in unnatural behaviors and
requiring considerable effort in engineering system-specific components. In
this paper, we present an approach based on model-free Deep Reinforcement
Learning (RL) to control recovery maneuvers of quadrupedal robots using a
hierarchical behavior-based controller. The controller consists of four neural
network policies including three behaviors and one behavior selector to
coordinate them. Each of them is trained individually in simulation and
deployed directly on a real system. We experimentally validate our approach on
the quadrupedal robot ANYmal, which is a dog-sized quadrupedal system with 12
degrees of freedom. With our method, ANYmal manifests dynamic and reactive
recovery behaviors to recover from an arbitrary fall configuration within less
than 5 seconds. We tested the recovery maneuver more than 100 times, and the
success rate was higher than 97 %. <br><br></div></a>
</td></tr>
<tr id=" 223 " class="entry"><td>
<a onclick="toggleVisibility('abstract223');">[|&bull;|]</a><a href="http://arxiv.org/abs/1911.06636v2">
<b/> Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body
Tasks (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1911.06636v2">
Josh Merel, Saran Tunyasuvunakool, Arun Ahuja, Yuval Tassa, Leonard Hasenclever, Vu Pham, Tom Erez, Greg Wayne, Nicolas Heess &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract223');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract223');"><div id="abstract223" style="font-size: 1rem; color:black; display: none">
<u><I> 1911.06636v2 &nbsp; - &nbsp;
{control}
{goal-conditioned}
{humanoid}
{reinforcement}
{robot}
{transfer}
</I></u><br> We address the longstanding challenge of producing flexible, realistic
humanoid character controllers that can perform diverse whole-body tasks
involving object interactions. This challenge is central to a variety of
fields, from graphics and animation to robotics and motor neuroscience. Our
physics-based environment uses realistic actuation and first-person perception
-- including touch sensors and egocentric vision -- with a view to producing
active-sensing behaviors (e.g. gaze direction), transferability to real robots,
and comparisons to the biology. We develop an integrated neural-network based
approach consisting of a motor primitive module, human demonstrations, and an
instructed reinforcement learning regime with curricula and task variations. We
demonstrate the utility of our approach for several tasks, including
goal-conditioned box carrying and ball catching, and we characterize its
behavioral robustness. The resulting controllers can be deployed in real-time
on a standard PC. See overview video, https://youtu.be/2rQAW-8gQQk . <br><br></div></a>
</td></tr>
<tr id=" 224 " class="entry"><td>
<a onclick="toggleVisibility('abstract224');">[|&bull;|]</a><a href="http://arxiv.org/abs/1903.08894v1">
<b/> Towards Characterizing Divergence in Deep Q-Learning (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1903.08894v1">
Joshua Achiam, Ethan Knight, Pieter Abbeel &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract224');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract224');"><div id="abstract224" style="font-size: 1rem; color:black; display: none">
<u><I> 1903.08894v1 &nbsp; - &nbsp;
{control}
{deep}
{gradient}
{off-policy}
{reinforcement}
</I></u><br> Deep Q-Learning (DQL), a family of temporal difference algorithms for
control, employs three techniques collectively known as the `deadly triad' in
reinforcement learning: bootstrapping, off-policy learning, and function
approximation. Prior work has demonstrated that together these can lead to
divergence in Q-learning algorithms, but the conditions under which divergence
occurs are not well-understood. In this note, we give a simple analysis based
on a linear approximation to the Q-value updates, which we believe provides
insight into divergence under the deadly triad. The central point in our
analysis is to consider when the leading order approximation to the deep-Q
update is or is not a contraction in the sup norm. Based on this analysis, we
develop an algorithm which permits stable deep Q-learning for continuous
control without any of the tricks conventionally used (such as target networks,
adaptive gradient optimizers, or using multiple Q functions). We demonstrate
that our algorithm performs above or near state-of-the-art on standard MuJoCo
benchmarks from the OpenAI Gym. <br><br></div></a>
</td></tr>
<tr id=" 225 " class="entry"><td>
<a onclick="toggleVisibility('abstract225');">[|&bull;|]</a><a href="http://arxiv.org/abs/1906.04493v3">
<b/> Generative Adversarial Networks are Special Cases of Artificial
Curiosity (1990) and also Closely Related to Predictability Minimization
(1991) (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1906.04493v3">
Juergen Schmidhuber &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract225');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract225');"><div id="abstract225" style="font-size: 1rem; color:black; display: none">
<u><I> 1906.04493v3 &nbsp; - &nbsp;
{self-supervised}
{unsupervised}
</I></u><br> I review unsupervised or self-supervised neural networks playing minimax
games in game-theoretic settings: (i) Artificial Curiosity (AC, 1990) is based
on two such networks. One network learns to generate a probability distribution
over outputs, the other learns to predict effects of the outputs. Each network
minimizes the objective function maximized by the other. (ii) Generative
Adversarial Networks (GANs, 2010-2014) are an application of AC where the
effect of an output is 1 if the output is in a given set, and 0 otherwise.
(iii) Predictability Minimization (PM, 1990s) models data distributions through
a neural encoder that maximizes the objective function minimized by a neural
predictor of the code components. I correct a previously published claim that
PM is not based on a minimax game. <br><br></div></a>
</td></tr>
<tr id=" 226 " class="entry"><td>
<a onclick="toggleVisibility('abstract226');">[|&bull;|]</a><a href="http://arxiv.org/abs/1911.08265v2">
<b/> Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1911.08265v2">
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, David Silver &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract226');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract226');"><div id="abstract226" style="font-size: 1rem; color:black; display: none">
<u><I> 1911.08265v2 &nbsp; - &nbsp;
{model-based}
{on-policy}
</I></u><br> Constructing agents with planning capabilities has long been one of the main
challenges in the pursuit of artificial intelligence. Tree-based planning
methods have enjoyed huge success in challenging domains, such as chess and Go,
where a perfect simulator is available. However, in real-world problems the
dynamics governing the environment are often complex and unknown. In this work
we present the MuZero algorithm which, by combining a tree-based search with a
learned model, achieves superhuman performance in a range of challenging and
visually complex domains, without any knowledge of their underlying dynamics.
MuZero learns a model that, when applied iteratively, predicts the quantities
most directly relevant to planning: the reward, the action-selection policy,
and the value function. When evaluated on 57 different Atari games - the
canonical video game environment for testing AI techniques, in which
model-based planning approaches have historically struggled - our new algorithm
achieved a new state of the art. When evaluated on Go, chess and shogi, without
any knowledge of the game rules, MuZero matched the superhuman performance of
the AlphaZero algorithm that was supplied with the game rules. <br><br></div></a>
</td></tr>
<tr id=" 227 " class="entry"><td>
<a onclick="toggleVisibility('abstract227');">[|&bull;|]</a><a href="http://arxiv.org/abs/1902.10250v1">
<b/> Diagnosing Bottlenecks in Deep Q-learning Algorithms (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1902.10250v1">
Justin Fu, Aviral Kumar, Matthew Soh, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract227');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract227');"><div id="abstract227" style="font-size: 1rem; color:black; display: none">
<u><I> 1902.10250v1 &nbsp; - &nbsp;
{control}
{deep}
{reinforcement}
</I></u><br> Q-learning methods represent a commonly used class of algorithms in
reinforcement learning: they are generally efficient and simple, and can be
combined readily with function approximators for deep reinforcement learning
(RL). However, the behavior of Q-learning methods with function approximation
is poorly understood, both theoretically and empirically. In this work, we aim
to experimentally investigate potential issues in Q-learning, by means of a
"unit testing" framework where we can utilize oracles to disentangle sources of
error. Specifically, we investigate questions related to function
approximation, sampling error and nonstationarity, and where available, verify
if trends found in oracle settings hold true with modern deep RL methods. We
find that large neural network architectures have many benefits with regards to
learning stability; offer several practical compensations for overfitting; and
develop a novel sampling method based on explicitly compensating for function
approximation error that yields fair improvement on high-dimensional continuous
control domains. <br><br></div></a>
</td></tr>
<tr id=" 228 " class="entry"><td>
<a onclick="toggleVisibility('abstract228');">[|&bull;|]</a><a href="http://arxiv.org/abs/1910.12807v1">
<b/> Better Exploration with Optimistic Actor-Critic (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1910.12807v1">
Kamil Ciosek, Quan Vuong, Robert Loftin, Katja Hofmann &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract228');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract228');"><div id="abstract228" style="font-size: 1rem; color:black; display: none">
<u><I> 1910.12807v1 &nbsp; - &nbsp;
{control}
{exploration}
{reinforcement}
</I></u><br> Actor-critic methods, a type of model-free Reinforcement Learning, have been
successfully applied to challenging tasks in continuous control, often
achieving state-of-the art performance. However, wide-scale adoption of these
methods in real-world domains is made difficult by their poor sample
efficiency. We address this problem both theoretically and empirically. On the
theoretical side, we identify two phenomena preventing efficient exploration in
existing state-of-the-art algorithms such as Soft Actor Critic. First,
combining a greedy actor update with a pessimistic estimate of the critic leads
to the avoidance of actions that the agent does not know about, a phenomenon we
call pessimistic underexploration. Second, current algorithms are directionally
uninformed, sampling actions with equal probability in opposite directions from
the current mean. This is wasteful, since we typically need actions taken along
certain directions much more than others. To address both of these phenomena,
we introduce a new algorithm, Optimistic Actor Critic, which approximates a
lower and upper confidence bound on the state-action value function. This
allows us to apply the principle of optimism in the face of uncertainty to
perform directed exploration using the upper bound while still using the lower
bound to avoid overestimation. We evaluate OAC in several challenging
continuous control tasks, achieving state-of the art sample efficiency. <br><br></div></a>
</td></tr>
<tr id=" 229 " class="entry"><td>
<a onclick="toggleVisibility('abstract229');">[|&bull;|]</a><a href="http://arxiv.org/abs/1903.08254v1">
<b/> Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic
Context Variables (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1903.08254v1">
Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract229');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract229');"><div id="abstract229" style="font-size: 1rem; color:black; display: none">
<u><I> 1903.08254v1 &nbsp; - &nbsp;
{control}
{deep}
{exploration}
{meta}
{off-policy}
{on-policy}
{reinforcement}
{skill}
{sparse}
</I></u><br> Deep reinforcement learning algorithms require large amounts of experience to
learn an individual task. While in principle meta-reinforcement learning
(meta-RL) algorithms enable agents to learn new skills from small amounts of
experience, several major challenges preclude their practicality. Current
methods rely heavily on on-policy experience, limiting their sample efficiency.
The also lack mechanisms to reason about task uncertainty when adapting to new
tasks, limiting their effectiveness in sparse reward problems. In this paper,
we address these challenges by developing an off-policy meta-RL algorithm that
disentangles task inference and control. In our approach, we perform online
probabilistic filtering of latent task variables to infer how to solve a new
task from small amounts of experience. This probabilistic interpretation
enables posterior sampling for structured and efficient exploration. We
demonstrate how to integrate these task variables with off-policy RL algorithms
to achieve both meta-training and adaptation efficiency. Our method outperforms
prior algorithms in sample efficiency by 20-100X as well as in asymptotic
performance on several meta-RL benchmarks. <br><br></div></a>
</td></tr>
<tr id=" 230 " class="entry"><td>
<a onclick="toggleVisibility('abstract230');">[|&bull;|]</a><a href="http://arxiv.org/abs/1907.08225v4">
<b/> Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill
Discovery (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1907.08225v4">
Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract230');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract230');"><div id="abstract230" style="font-size: 1rem; color:black; display: none">
<u><I> 1907.08225v4 &nbsp; - &nbsp;
{gradient}
{reinforcement}
{robot}
{skill}
{unsupervised}
</I></u><br> Reinforcement learning requires manual specification of a reward function to
learn a task. While in principle this reward function only needs to specify the
task goal, in practice reinforcement learning can be very time-consuming or
even infeasible unless the reward function is shaped so as to provide a smooth
gradient towards a successful outcome. This shaping is difficult to specify by
hand, particularly when the task is learned from raw observations, such as
images. In this paper, we study how we can automatically learn dynamical
distances: a measure of the expected number of time steps to reach a given goal
state from any other state. These dynamical distances can be used to provide
well-shaped reward functions for reaching new goals, making it possible to
learn complex tasks efficiently. We show that dynamical distances can be used
in a semi-supervised regime, where unsupervised interaction with the
environment is used to learn the dynamical distances, while a small amount of
preference supervision is used to determine the task goal, without any manually
engineered reward function or goal examples. We evaluate our method both on a
real-world robot and in simulation. We show that our method can learn to turn a
valve with a real-world 9-DoF hand, using raw image observations and just ten
preference labels, without any other supervision. Videos of the learned skills
can be found on the project website:
https://sites.google.com/view/dynamical-distance-learning. <br><br></div></a>
</td></tr>
<tr id=" 231 " class="entry"><td>
<a onclick="toggleVisibility('abstract231');">[|&bull;|]</a><a href="http://arxiv.org/abs/1901.11530v2">
<b/> A Geometric Perspective on Optimal Representations for Reinforcement
Learning (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1901.11530v2">
Marc G. Bellemare, Will Dabney, Robert Dadashi, Adrien Ali Taiga, Pablo Samuel Castro, Nicolas Le Roux, Dale Schuurmans, Tor Lattimore, Clare Lyle &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract231');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract231');"><div id="abstract231" style="font-size: 1rem; color:black; display: none">
<u><I> 1901.11530v2 &nbsp; - &nbsp;
{reinforcement}
</I></u><br> We propose a new perspective on representation learning in reinforcement
learning based on geometric properties of the space of value functions. We
leverage this perspective to provide formal evidence regarding the usefulness
of value functions as auxiliary tasks. Our formulation considers adapting the
representation to minimize the (linear) approximation of the value function of
all stationary policies for a given environment. We show that this optimization
reduces to making accurate predictions regarding a special class of value
functions which we call adversarial value functions (AVFs). We demonstrate that
using value functions as auxiliary tasks corresponds to an expected-error
relaxation of our formulation, with AVFs a natural candidate, and identify a
close relationship with proto-value functions (Mahadevan, 2005). We highlight
characteristics of AVFs and their usefulness as auxiliary tasks in a series of
experiments on the four-room domain. <br><br></div></a>
</td></tr>
<tr id=" 232 " class="entry"><td>
<a onclick="toggleVisibility('abstract232');">[|&bull;|]</a><a href="http://arxiv.org/abs/1901.11275v2">
<b/> A Theory of Regularized Markov Decision Processes (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1901.11275v2">
Matthieu Geist, Bruno Scherrer, Olivier Pietquin &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract232');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract232');"><div id="abstract232" style="font-size: 1rem; color:black; display: none">
<u><I> 1901.11275v2 &nbsp; - &nbsp;
{deep}
{entropy}
{reinforcement}
</I></u><br> Many recent successful (deep) reinforcement learning algorithms make use of
regularization, generally based on entropy or Kullback-Leibler divergence. We
propose a general theory of regularized Markov Decision Processes that
generalizes these approaches in two directions: we consider a larger class of
regularizers, and we consider the general modified policy iteration approach,
encompassing both policy iteration and value iteration. The core building
blocks of this theory are a notion of regularized Bellman operator and the
Legendre-Fenchel transform, a classical tool of convex optimization. This
approach allows for error propagation analyses of general algorithmic schemes
of which (possibly variants of) classical algorithms such as Trust Region
Policy Optimization, Soft Q-learning, Stochastic Actor Critic or Dynamic Policy
Programming are special cases. This also draws connections to proximal convex
optimization, especially to Mirror Descent. <br><br></div></a>
</td></tr>
<tr id=" 233 " class="entry"><td>
<a onclick="toggleVisibility('abstract233');">[|&bull;|]</a><a href="http://arxiv.org/abs/1906.04556v2">
<b/> Exploiting the Sign of the Advantage Function to Learn Deterministic
Policies in Continuous Domains (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1906.04556v2">
Matthieu Zimmer, Paul Weng &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract233');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract233');"><div id="abstract233" style="font-size: 1rem; color:black; display: none">
<u><I> 1906.04556v2 &nbsp; - &nbsp;
{control}
{deep}
{gradient}
</I></u><br> In the context of learning deterministic policies in continuous domains, we
revisit an approach, which was first proposed in Continuous Actor Critic
Learning Automaton (CACLA) and later extended in Neural Fitted Actor Critic
(NFAC). This approach is based on a policy update different from that of
deterministic policy gradient (DPG). Previous work has observed its excellent
performance empirically, but a theoretical justification is lacking. To fill
this gap, we provide a theoretical explanation to motivate this unorthodox
policy update by relating it to another update and making explicit the
objective function of the latter. We furthermore discuss in depth the
properties of these updates to get a deeper understanding of the overall
approach. In addition, we extend it and propose a new trust region algorithm,
Penalized NFAC (PeNFAC). Finally, we experimentally demonstrate in several
classic control problems that it surpasses the state-of-the-art algorithms to
learn deterministic policies. <br><br></div></a>
</td></tr>
<tr id=" 234 " class="entry"><td>
<a onclick="toggleVisibility('abstract234');">[|&bull;|]</a><a href="http://arxiv.org/abs/1906.08253v3">
<b/> When to Trust Your Model: Model-Based Policy Optimization (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1906.08253v3">
Michael Janner, Justin Fu, Marvin Zhang, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract234');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract234');"><div id="abstract234" style="font-size: 1rem; color:black; display: none">
<u><I> 1906.08253v3 &nbsp; - &nbsp;
{model-based}
{off-policy}
{on-policy}
{reinforcement}
</I></u><br> Designing effective model-based reinforcement learning algorithms is
difficult because the ease of data generation must be weighed against the bias
of model-generated data. In this paper, we study the role of model usage in
policy optimization both theoretically and empirically. We first formulate and
analyze a model-based reinforcement learning algorithm with a guarantee of
monotonic improvement at each step. In practice, this analysis is overly
pessimistic and suggests that real off-policy data is always preferable to
model-generated on-policy data, but we show that an empirical estimate of model
generalization can be incorporated into such analysis to justify model usage.
Motivated by this analysis, we then demonstrate that a simple procedure of
using short model-generated rollouts branched from real data has the benefits
of more complicated model-based algorithms without the usual pitfalls. In
particular, this approach surpasses the sample efficiency of prior model-based
methods, matches the asymptotic performance of the best model-free algorithms,
and scales to horizons that cause other model-based methods to fail entirely. <br><br></div></a>
</td></tr>
<tr id=" 235 " class="entry"><td>
<a onclick="toggleVisibility('abstract235');">[|&bull;|]</a><a href="http://arxiv.org/abs/1906.07794v1">
<b/> Convolutional neural network models for cancer type prediction based on
gene expression (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1906.07794v1">
Milad Mostavi, Yu-Chiao Chiu, Yufei Huang, Yidong Chen &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract235');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract235');"><div id="abstract235" style="font-size: 1rem; color:black; display: none">
<u><I> 1906.07794v1 &nbsp; - &nbsp;
</I></u><br> Background Precise prediction of cancer types is vital for cancer diagnosis
and therapy. Important cancer marker genes can be inferred through predictive
model. Several studies have attempted to build machine learning models for this
task however none has taken into consideration the effects of tissue of origin
that can potentially bias the identification of cancer markers. Results In this
paper, we introduced several Convolutional Neural Network (CNN) models that
take unstructured gene expression inputs to classify tumor and non-tumor
samples into their designated cancer types or as normal. Based on different
designs of gene embeddings and convolution schemes, we implemented three CNN
models: 1D-CNN, 2D-Vanilla-CNN, and 2D-Hybrid-CNN. The models were trained and
tested on combined 10,340 samples of 33 cancer types and 731 matched normal
tissues of The Cancer Genome Atlas (TCGA). Our models achieved excellent
prediction accuracies (93.9-95.0%) among 34 classes (33 cancers and normal).
Furthermore, we interpreted one of the models, known as 1D-CNN model, with a
guided saliency technique and identified a total of 2,090 cancer markers (108
per class). The concordance of differential expression of these markers between
the cancer type they represent and others is confirmed. In breast cancer, for
instance, our model identified well-known markers, such as GATA3 and ESR1.
Finally, we extended the 1D-CNN model for prediction of breast cancer subtypes
and achieved an average accuracy of 88.42% among 5 subtypes. The codes can be
found at https://github.com/chenlabgccri/CancerTypePrediction. <br><br></div></a>
</td></tr>
<tr id=" 236 " class="entry"><td>
<a onclick="toggleVisibility('abstract236');">[|&bull;|]</a><a href="http://arxiv.org/abs/1909.12397v3">
<b/> CAQL: Continuous Action Q-Learning (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1909.12397v3">
Moonkyung Ryu, Yinlam Chow, Ross Anderson, Christian Tjandraatmadja, Craig Boutilier &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract236');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract236');"><div id="abstract236" style="font-size: 1rem; color:black; display: none">
<u><I> 1909.12397v3 &nbsp; - &nbsp;
{control}
{deep}
{entropy}
{gradient}
{optimal}
{reinforcement}
</I></u><br> Value-based reinforcement learning (RL) methods like Q-learning have shown
success in a variety of domains. One challenge in applying Q-learning to
continuous-action RL problems, however, is the continuous action maximization
(max-Q) required for optimal Bellman backup. In this work, we develop CAQL, a
(class of) algorithm(s) for continuous-action Q-learning that can use several
plug-and-play optimizers for the max-Q problem. Leveraging recent optimization
results for deep neural networks, we show that max-Q can be solved optimally
using mixed-integer programming (MIP). When the Q-function representation has
sufficient power, MIP-based optimization gives rise to better policies and is
more robust than approximate methods (e.g., gradient ascent, cross-entropy
search). We further develop several techniques to accelerate inference in CAQL,
which despite their approximate nature, perform well. We compare CAQL with
state-of-the-art RL algorithms on benchmark continuous-control problems that
have different degrees of action constraints and show that CAQL outperforms
policy-based methods in heavily constrained environments, often dramatically. <br><br></div></a>
</td></tr>
<tr id=" 237 " class="entry"><td>
<a onclick="toggleVisibility('abstract237');">[|&bull;|]</a><a href="http://arxiv.org/abs/1909.10618v2">
<b/> Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning? (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1909.10618v2">
Ofir Nachum, Haoran Tang, Xingyu Lu, Shixiang Gu, Honglak Lee, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract237');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract237');"><div id="abstract237" style="font-size: 1rem; color:black; display: none">
<u><I> 1909.10618v2 &nbsp; - &nbsp;
{exploration}
{hierarchical}
{reinforcement}
</I></u><br> Hierarchical reinforcement learning has demonstrated significant success at
solving difficult reinforcement learning (RL) tasks. Previous works have
motivated the use of hierarchy by appealing to a number of intuitive benefits,
including learning over temporally extended transitions, exploring over
temporally extended periods, and training and exploring in a more semantically
meaningful action space, among others. However, in fully observed, Markovian
settings, it is not immediately clear why hierarchical RL should provide
benefits over standard "shallow" RL architectures. In this work, we isolate and
evaluate the claimed benefits of hierarchical RL on a suite of tasks
encompassing locomotion, navigation, and manipulation. Surprisingly, we find
that most of the observed benefits of hierarchy can be attributed to improved
exploration, as opposed to easier policy learning or imposed hierarchical
structures. Given this insight, we present exploration techniques inspired by
hierarchy that achieve performance competitive with hierarchical RL while at
the same time being much simpler to use and implement. <br><br></div></a>
</td></tr>
<tr id=" 238 " class="entry"><td>
<a onclick="toggleVisibility('abstract238');">[|&bull;|]</a><a href="http://arxiv.org/abs/1906.02771v1">
<b/> Improving Exploration in Soft-Actor-Critic with Normalizing Flows
Policies (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1906.02771v1">
Patrick Nadeem Ward, Ariella Smofsky, Avishek Joey Bose &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract238');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract238');"><div id="abstract238" style="font-size: 1rem; color:black; display: none">
<u><I> 1906.02771v1 &nbsp; - &nbsp;
{deep}
{entropy}
{exploration}
{off-policy}
{reinforcement}
{sparse}
</I></u><br> Deep Reinforcement Learning (DRL) algorithms for continuous action spaces are
known to be brittle toward hyperparameters as well as \cut{being}sample
inefficient. Soft Actor Critic (SAC) proposes an off-policy deep actor critic
algorithm within the maximum entropy RL framework which offers greater
stability and empirical gains. The choice of policy distribution, a factored
Gaussian, is motivated by \cut{chosen due}its easy re-parametrization rather
than its modeling power. We introduce Normalizing Flow policies within the SAC
framework that learn more expressive classes of policies than simple factored
Gaussians. \cut{We also present a series of stabilization tricks that enable
effective training of these policies in the RL setting.}We show empirically on
continuous grid world tasks that our approach increases stability and is better
suited to difficult exploration in sparse reward settings. <br><br></div></a>
</td></tr>
<tr id=" 239 " class="entry"><td>
<a onclick="toggleVisibility('abstract239');">[|&bull;|]</a><a href="http://arxiv.org/abs/1912.11032v1">
<b/> Towards Practical Multi-Object Manipulation using Relational
Reinforcement Learning (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1912.11032v1">
Richard Li, Allan Jabri, Trevor Darrell, Pulkit Agrawal &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract239');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract239');"><div id="abstract239" style="font-size: 1rem; color:black; display: none">
<u><I> 1912.11032v1 &nbsp; - &nbsp;
{imitation}
{reinforcement}
{robot}
{sparse}
{transfer}
</I></u><br> Learning robotic manipulation tasks using reinforcement learning with sparse
rewards is currently impractical due to the outrageous data requirements. Many
practical tasks require manipulation of multiple objects, and the complexity of
such tasks increases with the number of objects. Learning from a curriculum of
increasingly complex tasks appears to be a natural solution, but unfortunately,
does not work for many scenarios. We hypothesize that the inability of the
state-of-the-art algorithms to effectively utilize a task curriculum stems from
the absence of inductive biases for transferring knowledge from simpler to
complex tasks. We show that graph-based relational architectures overcome this
limitation and enable learning of complex tasks when provided with a simple
curriculum of tasks with increasing numbers of objects. We demonstrate the
utility of our framework on a simulated block stacking task. Starting from
scratch, our agent learns to stack six blocks into a tower. Despite using
step-wise sparse rewards, our method is orders of magnitude more data-efficient
and outperforms the existing state-of-the-art method that utilizes human
demonstrations. Furthermore, the learned policy exhibits zero-shot
generalization, successfully stacking blocks into taller towers and previously
unseen configurations such as pyramids, without any further training. <br><br></div></a>
</td></tr>
<tr id=" 240 " class="entry"><td>
<a onclick="toggleVisibility('abstract240');">[|&bull;|]</a><a href="http://arxiv.org/abs/1905.06144v2">
<b/> Feedback MPC for Torque-Controlled Legged Robots (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1905.06144v2">
Ruben Grandia, Farbod Farshidian, Ren&#233; Ranftl, Marco Hutter &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract240');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract240');"><div id="abstract240" style="font-size: 1rem; color:black; display: none">
<u><I> 1905.06144v2 &nbsp; - &nbsp;
{control}
{robot}
</I></u><br> The computational power of mobile robots is currently insufficient to achieve
torque level whole-body Model Predictive Control (MPC) at the update rates
required for complex dynamic systems such as legged robots. This problem is
commonly circumvented by using a fast tracking controller to compensate for
model errors between updates. In this work, we show that the feedback policy
from a Differential Dynamic Programming (DDP) based MPC algorithm is a viable
alternative to bridge the gap between the low MPC update rate and the actuation
command rate. We propose to augment the DDP approach with a relaxed barrier
function to address inequality constraints arising from the friction cone. A
frequency-dependent cost function is used to reduce the sensitivity to
high-frequency model errors and actuator bandwidth limits. We demonstrate that
our approach can find stable locomotion policies for the torque-controlled
quadruped, ANYmal, both in simulation and on hardware. <br><br></div></a>
</td></tr>
<tr id=" 241 " class="entry"><td>
<a onclick="toggleVisibility('abstract241');">[|&bull;|]</a><a href="http://arxiv.org/abs/1905.06750v2">
<b/> Random Expert Distillation: Imitation Learning via Expert Policy Support
Estimation (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1905.06750v2">
Ruohan Wang, Carlo Ciliberto, Pierluigi Amadori, Yiannis Demiris &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract241');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract241');"><div id="abstract241" style="font-size: 1rem; color:black; display: none">
<u><I> 1905.06750v2 &nbsp; - &nbsp;
{imitation}
{reinforcement}
</I></u><br> We consider the problem of imitation learning from a finite set of expert
trajectories, without access to reinforcement signals. The classical approach
of extracting the expert's reward function via inverse reinforcement learning,
followed by reinforcement learning is indirect and may be computationally
expensive. Recent generative adversarial methods based on matching the policy
distribution between the expert and the agent could be unstable during
training. We propose a new framework for imitation learning by estimating the
support of the expert policy to compute a fixed reward function, which allows
us to re-frame imitation learning within the standard reinforcement learning
setting. We demonstrate the efficacy of our reward function on both discrete
and continuous domains, achieving comparable or better performance than the
state of the art under different reinforcement learning algorithms. <br><br></div></a>
</td></tr>
<tr id=" 242 " class="entry"><td>
<a onclick="toggleVisibility('abstract242');">[|&bull;|]</a><a href="http://arxiv.org/abs/1906.01563v3">
<b/> Hamiltonian Neural Networks (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1906.01563v3">
Sam Greydanus, Misko Dzamba, Jason Yosinski &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract242');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract242');"><div id="abstract242" style="font-size: 1rem; color:black; display: none">
<u><I> 1906.01563v3 &nbsp; - &nbsp;
{unsupervised}
</I></u><br> Even though neural networks enjoy widespread use, they still struggle to
learn the basic laws of physics. How might we endow them with better inductive
biases? In this paper, we draw inspiration from Hamiltonian mechanics to train
models that learn and respect exact conservation laws in an unsupervised
manner. We evaluate our models on problems where conservation of energy is
important, including the two-body problem and pixel observations of a pendulum.
Our model trains faster and generalizes better than a regular neural network.
An interesting side effect is that our model is perfectly reversible in time. <br><br></div></a>
</td></tr>
<tr id=" 243 " class="entry"><td>
<a onclick="toggleVisibility('abstract243');">[|&bull;|]</a><a href="http://arxiv.org/abs/1902.10186v3">
<b/> Attention is not Explanation (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1902.10186v3">
Sarthak Jain, Byron C. Wallace &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract243');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract243');"><div id="abstract243" style="font-size: 1rem; color:black; display: none">
<u><I> 1902.10186v3 &nbsp; - &nbsp;
{gradient}
</I></u><br> Attention mechanisms have seen wide adoption in neural NLP models. In
addition to improving predictive performance, these are often touted as
affording transparency: models equipped with attention provide a distribution
over attended-to input units, and this is often presented (at least implicitly)
as communicating the relative importance of inputs. However, it is unclear what
relationship exists between attention weights and model outputs. In this work,
we perform extensive experiments across a variety of NLP tasks that aim to
assess the degree to which attention weights provide meaningful `explanations'
for predictions. We find that they largely do not. For example, learned
attention weights are frequently uncorrelated with gradient-based measures of
feature importance, and one can identify very different attention distributions
that nonetheless yield equivalent predictions. Our findings show that standard
attention modules do not provide meaningful explanations and should not be
treated as though they do. Code for all experiments is available at
https://github.com/successar/AttentionExplanation. <br><br></div></a>
</td></tr>
<tr id=" 244 " class="entry"><td>
<a onclick="toggleVisibility('abstract244');">[|&bull;|]</a><a href="http://arxiv.org/abs/1904.09237v1">
<b/> On the Convergence of Adam and Beyond (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1904.09237v1">
Sashank J. Reddi, Satyen Kale, Sanjiv Kumar &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract244');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract244');"><div id="abstract244" style="font-size: 1rem; color:black; display: none">
<u><I> 1904.09237v1 &nbsp; - &nbsp;
{deep}
{gradient}
{optimal}
</I></u><br> Several recently proposed stochastic optimization methods that have been
successfully used in training deep networks such as RMSProp, Adam, Adadelta,
Nadam are based on using gradient updates scaled by square roots of exponential
moving averages of squared past gradients. In many applications, e.g. learning
with large output spaces, it has been empirically observed that these
algorithms fail to converge to an optimal solution (or a critical point in
nonconvex settings). We show that one cause for such failures is the
exponential moving average used in the algorithms. We provide an explicit
example of a simple convex optimization setting where Adam does not converge to
the optimal solution, and describe the precise problems with the previous
analysis of Adam algorithm. Our analysis suggests that the convergence issues
can be fixed by endowing such algorithms with `long-term memory' of past
gradients, and propose new variants of the Adam algorithm which not only fix
the convergence issues but often also lead to improved empirical performance. <br><br></div></a>
</td></tr>
<tr id=" 245 " class="entry"><td>
<a onclick="toggleVisibility('abstract245');">[|&bull;|]</a><a href="http://arxiv.org/abs/1909.12892v2">
<b/> Automated curricula through setter-solver interactions (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1909.12892v2">
Sebastien Racaniere, Andrew K. Lampinen, Adam Santoro, David P. Reichert, Vlad Firoiu, Timothy P. Lillicrap &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract245');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract245');"><div id="abstract245" style="font-size: 1rem; color:black; display: none">
<u><I> 1909.12892v2 &nbsp; - &nbsp;
{goal-conditioned}
{reinforcement}
{sparse}
</I></u><br> Reinforcement learning algorithms use correlations between policies and
rewards to improve agent performance. But in dynamic or sparsely rewarding
environments these correlations are often too small, or rewarding events are
too infrequent to make learning feasible. Human education instead relies on
curricula--the breakdown of tasks into simpler, static challenges with dense
rewards--to build up to complex behaviors. While curricula are also useful for
artificial agents, hand-crafting them is time consuming. This has lead
researchers to explore automatic curriculum generation. Here we explore
automatic curriculum generation in rich, dynamic environments. Using a
setter-solver paradigm we show the importance of considering goal validity,
goal feasibility, and goal coverage to construct useful curricula. We
demonstrate the success of our approach in rich but sparsely rewarding 2D and
3D environments, where an agent is tasked to achieve a single goal selected
from a set of possible goals that varies between episodes, and identify
challenges for future work. Finally, we demonstrate the value of a novel
technique that guides agents towards a desired goal distribution. Altogether,
these results represent a substantial step towards applying automatic task
curricula to learn complex, otherwise unlearnable goals, and to our knowledge
are the first to demonstrate automated curriculum generation for
goal-conditioned agents in environments where the possible goals vary between
episodes. <br><br></div></a>
</td></tr>
<tr id=" 246 " class="entry"><td>
<a onclick="toggleVisibility('abstract246');">[|&bull;|]</a><a href="http://arxiv.org/abs/1905.11108v3">
<b/> SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1905.11108v3">
Siddharth Reddy, Anca D. Dragan, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract246');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract246');"><div id="abstract246" style="font-size: 1rem; color:black; display: none">
<u><I> 1905.11108v3 &nbsp; - &nbsp;
{imitation}
{off-policy}
{reinforcement}
</I></u><br> Learning to imitate expert behavior from demonstrations can be challenging,
especially in environments with high-dimensional, continuous observations and
unknown dynamics. Supervised learning methods based on behavioral cloning (BC)
suffer from distribution shift: because the agent greedily imitates
demonstrated actions, it can drift away from demonstrated states due to error
accumulation. Recent methods based on reinforcement learning (RL), such as
inverse RL and generative adversarial imitation learning (GAIL), overcome this
issue by training an RL agent to match the demonstrations over a long horizon.
Since the true reward function for the task is unknown, these methods learn a
reward function from the demonstrations, often using complex and brittle
approximation techniques that involve adversarial training. We propose a simple
alternative that still uses RL, but does not require learning a reward
function. The key idea is to provide the agent with an incentive to match the
demonstrations over a long horizon, by encouraging it to return to demonstrated
states upon encountering new, out-of-distribution states. We accomplish this by
giving the agent a constant reward of r=+1 for matching the demonstrated action
in a demonstrated state, and a constant reward of r=0 for all other behavior.
Our method, which we call soft Q imitation learning (SQIL), can be implemented
with a handful of minor modifications to any standard Q-learning or off-policy
actor-critic algorithm. Theoretically, we show that SQIL can be interpreted as
a regularized variant of BC that uses a sparsity prior to encourage
long-horizon imitation. Empirically, we show that SQIL outperforms BC and
achieves competitive results compared to GAIL, on a variety of image-based and
low-dimensional tasks in Box2D, Atari, and MuJoCo. <br><br></div></a>
</td></tr>
<tr id=" 247 " class="entry"><td>
<a onclick="toggleVisibility('abstract247');">[|&bull;|]</a><a href="http://arxiv.org/abs/1909.07543v3">
<b/> Attraction-Repulsion Actor-Critic for Continuous Control Reinforcement
Learning (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1909.07543v3">
Thang Doan, Bogdan Mazoure, Moloud Abdar, Audrey Durand, Joelle Pineau, R Devon Hjelm &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract247');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract247');"><div id="abstract247" style="font-size: 1rem; color:black; display: none">
<u><I> 1909.07543v3 &nbsp; - &nbsp;
{control}
{optimal}
{population}
{reinforcement}
</I></u><br> Continuous control tasks in reinforcement learning are important because they
provide an important framework for learning in high-dimensional state spaces
with deceptive rewards, where the agent can easily become trapped into
suboptimal solutions. One way to avoid local optima is to use a population of
agents to ensure coverage of the policy space, yet learning a population with
the "best" coverage is still an open problem. In this work, we present a novel
approach to population-based RL in continuous control that leverages properties
of normalizing flows to perform attractive and repulsive operations between
current members of the population and previously observed policies. Empirical
results on the MuJoCo suite demonstrate a high performance gain for our
algorithm compared to prior work, including Soft-Actor Critic (SAC). <br><br></div></a>
</td></tr>
<tr id=" 248 " class="entry"><td>
<a onclick="toggleVisibility('abstract248');">[|&bull;|]</a><a href="http://arxiv.org/abs/1909.11188v2">
<b/> Towards Variable Assistance for Lower Body Exoskeletons (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1909.11188v2">
Thomas Gurriet, Maegan Tucker, Alexis Duburcq, Guilhem Boeris, Aaron D. Ames &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract248');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract248');"><div id="abstract248" style="font-size: 1rem; color:black; display: none">
<u><I> 1909.11188v2 &nbsp; - &nbsp;
{control}
{exoskeleton}
</I></u><br> This paper presents and experimentally demonstrates a novel framework for
variable assistance on lower body exoskeletons, based upon safety-critical
control methods. Existing work has shown that providing some freedom of
movement around a nominal gait, instead of rigidly following it, accelerates
the spinal learning process of people with a walking impediment when using a
lower body exoskeleton. With this as motivation, we present a method to
accurately control how much a subject is allowed to deviate from a given gait
while ensuring robustness to patient perturbation. This method leverages
control barrier functions to force certain joints to remain inside predefined
trajectory tubes in a minimally invasive way. The effectiveness of the method
is demonstrated experimentally with able-bodied subjects and the Atalante lower
body exoskeleton. <br><br></div></a>
</td></tr>
<tr id=" 249 " class="entry"><td>
<a onclick="toggleVisibility('abstract249');">[|&bull;|]</a><a href="http://arxiv.org/abs/1905.12941v2">
<b/> Learning Compositional Neural Programs with Recursive Tree Search and
Planning (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1905.12941v2">
Thomas Pierrot, Guillaume Ligner, Scott Reed, Olivier Sigaud, Nicolas Perrin, Alexandre Laterre, David Kas, Karim Beguir, Nando de Freitas &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract249');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract249');"><div id="abstract249" style="font-size: 1rem; color:black; display: none">
<u><I> 1905.12941v2 &nbsp; - &nbsp;
{hierarchical}
{reinforcement}
{sparse}
</I></u><br> We propose a novel reinforcement learning algorithm, AlphaNPI, that
incorporates the strengths of Neural Programmer-Interpreters (NPI) and
AlphaZero. NPI contributes structural biases in the form of modularity,
hierarchy and recursion, which are helpful to reduce sample complexity, improve
generalization and increase interpretability. AlphaZero contributes powerful
neural network guided search algorithms, which we augment with recursion.
AlphaNPI only assumes a hierarchical program specification with sparse rewards:
1 when the program execution satisfies the specification, and 0 otherwise.
Using this specification, AlphaNPI is able to train NPI models effectively with
RL for the first time, completely eliminating the need for strong supervision
in the form of execution traces. The experiments show that AlphaNPI can sort as
well as previous strongly supervised NPI variants. The AlphaNPI agent is also
trained on a Tower of Hanoi puzzle with two disks and is shown to generalize to
puzzles with an arbitrary number of disk <br><br></div></a>
</td></tr>
<tr id=" 250 " class="entry"><td>
<a onclick="toggleVisibility('abstract250');">[|&bull;|]</a><a href="http://arxiv.org/abs/1907.02057v1">
<b/> Benchmarking Model-Based Reinforcement Learning (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1907.02057v1">
Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, Jimmy Ba &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract250');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract250');"><div id="abstract250" style="font-size: 1rem; color:black; display: none">
<u><I> 1907.02057v1 &nbsp; - &nbsp;
{model-based}
{reinforcement}
</I></u><br> Model-based reinforcement learning (MBRL) is widely seen as having the
potential to be significantly more sample efficient than model-free RL.
However, research in model-based RL has not been very standardized. It is
fairly common for authors to experiment with self-designed environments, and
there are several separate lines of research, which are sometimes
closed-sourced or not reproducible. Accordingly, it is an open question how
these various existing MBRL algorithms perform relative to each other. To
facilitate research in MBRL, in this paper we gather a wide collection of MBRL
algorithms and propose over 18 benchmarking environments specially designed for
MBRL. We benchmark these algorithms with unified problem settings, including
noisy environments. Beyond cataloguing performance, we explore and unify the
underlying algorithmic differences across MBRL algorithms. We characterize
three key research challenges for future MBRL research: the dynamics
bottleneck, the planning horizon dilemma, and the early-termination dilemma.
Finally, to maximally facilitate future research on MBRL, we open-source our
benchmark in http://www.cs.toronto.edu/~tingwuwang/mbrl.html. <br><br></div></a>
</td></tr>
<tr id=" 251 " class="entry"><td>
<a onclick="toggleVisibility('abstract251');">[|&bull;|]</a><a href="http://arxiv.org/abs/1909.01387v1">
<b/> Making Efficient Use of Demonstrations to Solve Hard Exploration
Problems (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1909.01387v1">
Tom Le Paine, Caglar Gulcehre, Bobak Shahriari, Misha Denil, Matt Hoffman, Hubert Soyer, Richard Tanburn, Steven Kapturowski, Neil Rabinowitz, Duncan Williams, Gabriel Barth-Maron, Ziyu Wang, Nando de Freitas, Worlds Team &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract251');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract251');"><div id="abstract251" style="font-size: 1rem; color:black; display: none">
<u><I> 1909.01387v1 &nbsp; - &nbsp;
{exploration}
</I></u><br> This paper introduces R2D3, an agent that makes efficient use of
demonstrations to solve hard exploration problems in partially observable
environments with highly variable initial conditions. We also introduce a suite
of eight tasks that combine these three properties, and show that R2D3 can
solve several of the tasks where other state of the art methods (both with and
without demonstrations) fail to see even a single successful trajectory after
tens of billions of steps of exploration. <br><br></div></a>
</td></tr>
<tr id=" 252 " class="entry"><td>
<a onclick="toggleVisibility('abstract252');">[|&bull;|]</a><a href="http://arxiv.org/abs/1906.12189v1">
<b/> Learning-based Model Predictive Control for Safe Exploration and
Reinforcement Learning (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1906.12189v1">
Torsten Koller, Felix Berkenkamp, Matteo Turchetta, Joschka Boedecker, Andreas Krause &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract252');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract252');"><div id="abstract252" style="font-size: 1rem; color:black; display: none">
<u><I> 1906.12189v1 &nbsp; - &nbsp;
{control}
{reinforcement}
</I></u><br> Reinforcement learning has been successfully used to solve difficult tasks in
complex unknown environments. However, these methods typically do not provide
any safety guarantees during the learning process. This is particularly
problematic, since reinforcement learning agent actively explore their
environment. This prevents their use in safety-critical, real-world
applications. In this paper, we present a learning-based model predictive
control scheme that provides high-probability safety guarantees throughout the
learning process. Based on a reliable statistical model, we construct provably
accurate confidence intervals on predicted trajectories. Unlike previous
approaches, we allow for input-dependent uncertainties. Based on these reliable
predictions, we guarantee that trajectories satisfy safety constraints.
Moreover, we use a terminal set constraint to recursively guarantee the
existence of safe control actions at every iteration. We evaluate the resulting
algorithm to safely explore the dynamics of an inverted pendulum and to solve a
reinforcement learning task on a cart-pole system with safety constraints. <br><br></div></a>
</td></tr>
<tr id=" 253 " class="entry"><td>
<a onclick="toggleVisibility('abstract253');">[|&bull;|]</a><a href="http://arxiv.org/abs/1903.03698v4">
<b/> Skew-Fit: State-Covering Self-Supervised Reinforcement Learning (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1903.03698v4">
Vitchyr H. Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract253');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract253');"><div id="abstract253" style="font-size: 1rem; color:black; display: none">
<u><I> 1903.03698v4 &nbsp; - &nbsp;
{entropy}
{exploration}
{robot}
{self-supervised}
{skill}
</I></u><br> Autonomous agents that must exhibit flexible and broad capabilities will need
to be equipped with large repertoires of skills. Defining each skill with a
manually-designed reward function limits this repertoire and imposes a manual
engineering burden. Self-supervised agents that set their own goals can
automate this process, but designing appropriate goal setting objectives can be
difficult, and often involves heuristic design decisions. In this paper, we
propose a formal exploration objective for goal-reaching policies that
maximizes state coverage. We show that this objective is equivalent to
maximizing goal reaching performance together with the entropy of the goal
distribution, where goals correspond to full state observations. To instantiate
this principle, we present an algorithm called Skew-Fit for learning a
maximum-entropy goal distributions. We prove that, under regularity conditions,
Skew-Fit converges to a uniform distribution over the set of valid states, even
when we do not know this set beforehand. Our experiments show that combining
Skew-Fit for learning goal distributions with existing goal-reaching methods
outperforms a variety of prior methods on open-sourced visual goal-reaching
tasks. Moreover, we demonstrate that Skew-Fit enables a real-world robot to
learn to open a door, entirely from scratch, from pixels, and without any
manually-designed reward function. <br><br></div></a>
</td></tr>
<tr id=" 254 " class="entry"><td>
<a onclick="toggleVisibility('abstract254');">[|&bull;|]</a><a href="http://arxiv.org/abs/1910.01215v4">
<b/> ES-MAML: Simple Hessian-Free Meta Learning (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1910.01215v4">
Xingyou Song, Wenbo Gao, Yuxiang Yang, Krzysztof Choromanski, Aldo Pacchiano, Yunhao Tang &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract254');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract254');"><div id="abstract254" style="font-size: 1rem; color:black; display: none">
<u><I> 1910.01215v4 &nbsp; - &nbsp;
{evolution}
{gradient}
{meta}
{on-policy}
</I></u><br> We introduce ES-MAML, a new framework for solving the model agnostic meta
learning (MAML) problem based on Evolution Strategies (ES). Existing algorithms
for MAML are based on policy gradients, and incur significant difficulties when
attempting to estimate second derivatives using backpropagation on stochastic
policies. We show how ES can be applied to MAML to obtain an algorithm which
avoids the problem of estimating second derivatives, and is also conceptually
simple and easy to implement. Moreover, ES-MAML can handle new types of
nonsmooth adaptation operators, and other techniques for improving performance
and estimation of ES methods become applicable. We show empirically that
ES-MAML is competitive with existing methods and often yields better adaptation
with fewer queries. <br><br></div></a>
</td></tr>
<tr id=" 255 " class="entry"><td>
<a onclick="toggleVisibility('abstract255');">[|&bull;|]</a><a href="http://arxiv.org/abs/1910.00177v3">
<b/> Advantage-Weighted Regression: Simple and Scalable Off-Policy
Reinforcement Learning (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1910.00177v3">
Xue Bin Peng, Aviral Kumar, Grace Zhang, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract255');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract255');"><div id="abstract255" style="font-size: 1rem; color:black; display: none">
<u><I> 1910.00177v3 &nbsp; - &nbsp;
{control}
{off-policy}
{reinforcement}
{replay}
</I></u><br> In this paper, we aim to develop a simple and scalable reinforcement learning
algorithm that uses standard supervised learning methods as subroutines. Our
goal is an algorithm that utilizes only simple and convergent maximum
likelihood loss functions, while also being able to leverage off-policy data.
Our proposed approach, which we refer to as advantage-weighted regression
(AWR), consists of two standard supervised learning steps: one to regress onto
target values for a value function, and another to regress onto weighted target
actions for the policy. The method is simple and general, can accommodate
continuous and discrete actions, and can be implemented in just a few lines of
code on top of standard supervised learning methods. We provide a theoretical
motivation for AWR and analyze its properties when incorporating off-policy
data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym
benchmark tasks, and show that it achieves competitive performance compared to
a number of well-established state-of-the-art RL algorithms. AWR is also able
to acquire more effective policies than most off-policy algorithms when
learning from purely static datasets with no additional environmental
interactions. Furthermore, we demonstrate our algorithm on challenging
continuous control tasks with highly complex simulated characters. <br><br></div></a>
</td></tr>
<tr id=" 256 " class="entry"><td>
<a onclick="toggleVisibility('abstract256');">[|&bull;|]</a><a href="http://arxiv.org/abs/1905.09808v1">
<b/> MCP: Learning Composable Hierarchical Control with Multiplicative
Compositional Policies (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1905.09808v1">
Xue Bin Peng, Michael Chang, Grace Zhang, Pieter Abbeel, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract256');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract256');"><div id="abstract256" style="font-size: 1rem; color:black; display: none">
<u><I> 1905.09808v1 &nbsp; - &nbsp;
{control}
{humanoid}
{imitation}
{skill}
{transfer}
</I></u><br> Humans are able to perform a myriad of sophisticated tasks by drawing upon
skills acquired through prior experience. For autonomous agents to have this
capability, they must be able to extract reusable skills from past experience
that can be recombined in new ways for subsequent tasks. Furthermore, when
controlling complex high-dimensional morphologies, such as humanoid bodies,
tasks often require coordination of multiple skills simultaneously. Learning
discrete primitives for every combination of skills quickly becomes
prohibitive. Composable primitives that can be recombined to create a large
variety of behaviors can be more suitable for modeling this combinatorial
explosion. In this work, we propose multiplicative compositional policies
(MCP), a method for learning reusable motor skills that can be composed to
produce a range of complex behaviors. Our method factorizes an agent's skills
into a collection of primitives, where multiple primitives can be activated
simultaneously via multiplicative composition. This flexibility allows the
primitives to be transferred and recombined to elicit new behaviors as
necessary for novel tasks. We demonstrate that MCP is able to extract
composable skills for highly complex simulated characters from pre-training
tasks, such as motion imitation, and then reuse these skills to solve
challenging continuous control tasks, such as dribbling a soccer ball to a
goal, and picking up an object and transporting it to a target location. <br><br></div></a>
</td></tr>
<tr id=" 257 " class="entry"><td>
<a onclick="toggleVisibility('abstract257');">[|&bull;|]</a><a href="http://arxiv.org/abs/1902.00183v2">
<b/> Learning Action Representations for Reinforcement Learning (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1902.00183v2">
Yash Chandak, Georgios Theocharous, James Kostas, Scott Jordan, Philip S. Thomas &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract257');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract257');"><div id="abstract257" style="font-size: 1rem; color:black; display: none">
<u><I> 1902.00183v2 &nbsp; - &nbsp;
{reinforcement}
</I></u><br> Most model-free reinforcement learning methods leverage state representations
(embeddings) for generalization, but either ignore structure in the space of
actions or assume the structure is provided a priori. We show how a policy can
be decomposed into a component that acts in a low-dimensional space of action
representations and a component that transforms these representations into
actual actions. These representations improve generalization over large, finite
action sets by allowing the agent to infer the outcomes of actions similar to
actions already taken. We provide an algorithm to both learn and use action
representations and provide conditions for its convergence. The efficacy of the
proposed method is demonstrated on large-scale real-world problems. <br><br></div></a>
</td></tr>
<tr id=" 258 " class="entry"><td>
<a onclick="toggleVisibility('abstract258');">[|&bull;|]</a><a href="http://arxiv.org/abs/1901.10031v2">
<b/> Lyapunov-based Safe Policy Optimization for Continuous Control (2019)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1901.10031v2">
Yinlam Chow, Ofir Nachum, Aleksandra Faust, Edgar Duenez-Guzman, Mohammad Ghavamzadeh &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract258');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract258');"><div id="abstract258" style="font-size: 1rem; color:black; display: none">
<u><I> 1901.10031v2 &nbsp; - &nbsp;
{deep}
{gradient}
{off-policy}
{on-policy}
{reinforcement}
{robot}
</I></u><br> We study continuous action reinforcement learning problems in which it is
crucial that the agent interacts with the environment only through safe
policies, i.e.,~policies that do not take the agent to undesirable situations.
We formulate these problems as constrained Markov decision processes (CMDPs)
and present safe policy optimization algorithms that are based on a Lyapunov
approach to solve them. Our algorithms can use any standard policy gradient
(PG) method, such as deep deterministic policy gradient (DDPG) or proximal
policy optimization (PPO), to train a neural network policy, while guaranteeing
near-constraint satisfaction for every policy update by projecting either the
policy parameter or the action onto the set of feasible solutions induced by
the state-dependent linearized Lyapunov constraints. Compared to the existing
constrained PG algorithms, ours are more data efficient as they are able to
utilize both on-policy and off-policy data. Moreover, our action-projection
algorithm often leads to less conservative policy updates and allows for
natural integration into an end-to-end PG training pipeline. We evaluate our
algorithms and compare them with the state-of-the-art baselines on several
simulated (MuJoCo) tasks, as well as a real-world indoor robot navigation
problem, demonstrating their effectiveness in terms of balancing performance
and constraint satisfaction. Videos of the experiments can be found in the
following link:
https://drive.google.com/file/d/1pzuzFqWIE710bE2U6DmS59AfRzqK2Kek/view?usp=sharing. <br><br></div></a>
</td></tr>
<tr id=" 259 " class="entry"><td>
<a onclick="toggleVisibility('abstract259');">[|&bull;|]</a><a href="http://arxiv.org/abs/1807.03748v2">
<b/> Representation Learning with Contrastive Predictive Coding (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1807.03748v2">
Aaron van den Oord, Yazhe Li, Oriol Vinyals &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract259');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract259');"><div id="abstract259" style="font-size: 1rem; color:black; display: none">
<u><I> 1807.03748v2 &nbsp; - &nbsp;
{contrastive}
{reinforcement}
{unsupervised}
</I></u><br> While supervised learning has enabled great progress in many applications,
unsupervised learning has not seen such widespread adoption, and remains an
important and challenging endeavor for artificial intelligence. In this work,
we propose a universal unsupervised learning approach to extract useful
representations from high-dimensional data, which we call Contrastive
Predictive Coding. The key insight of our model is to learn such
representations by predicting the future in latent space by using powerful
autoregressive models. We use a probabilistic contrastive loss which induces
the latent space to capture information that is maximally useful to predict
future samples. It also makes the model tractable by using negative sampling.
While most prior work has focused on evaluating representations for a
particular modality, we demonstrate that our approach is able to learn useful
representations achieving strong performance on four distinct domains: speech,
images, text and reinforcement learning in 3D environments. <br><br></div></a>
</td></tr>
<tr id=" 260 " class="entry"><td>
<a onclick="toggleVisibility('abstract260');">[|&bull;|]</a><a href="http://arxiv.org/abs/1812.02256v1">
<b/> Relative Entropy Regularized Policy Iteration (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1812.02256v1">
Abbas Abdolmaleki, Jost Tobias Springenberg, Jonas Degrave, Steven Bohez, Yuval Tassa, Dan Belov, Nicolas Heess, Martin Riedmiller &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract260');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract260');"><div id="abstract260" style="font-size: 1rem; color:black; display: none">
<u><I> 1812.02256v1 &nbsp; - &nbsp;
{control}
{deep}
{evolution}
{gradient}
{off-policy}
{reinforcement}
</I></u><br> We present an off-policy actor-critic algorithm for Reinforcement Learning
(RL) that combines ideas from gradient-free optimization via stochastic search
with learned action-value function. The result is a simple procedure consisting
of three steps: i) policy evaluation by estimating a parametric action-value
function; ii) policy improvement via the estimation of a local non-parametric
policy; and iii) generalization by fitting a parametric policy. Each step can
be implemented in different ways, giving rise to several algorithm variants.
Our algorithm draws on connections to existing literature on black-box
optimization and 'RL as an inference' and it can be seen either as an extension
of the Maximum a Posteriori Policy Optimisation algorithm (MPO) [Abdolmaleki et
al., 2018a], or as an extension of Trust Region Covariance Matrix Adaptation
Evolutionary Strategy (CMA-ES) [Abdolmaleki et al., 2017b; Hansen et al., 1997]
to a policy iteration scheme. Our comparison on 31 continuous control tasks
from parkour suite [Heess et al., 2017], DeepMind control suite [Tassa et al.,
2018] and OpenAI Gym [Brockman et al., 2016] with diverse properties, limited
amount of compute and a single set of hyperparameters, demonstrate the
effectiveness of our method and the state of art results. Videos, summarizing
results, can be found at goo.gl/HtvJKR . <br><br></div></a>
</td></tr>
<tr id=" 261 " class="entry"><td>
<a onclick="toggleVisibility('abstract261');">[|&bull;|]</a><a href="http://arxiv.org/abs/1802.07245v1">
<b/> Meta-Reinforcement Learning of Structured Exploration Strategies (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1802.07245v1">
Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract261');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract261');"><div id="abstract261" style="font-size: 1rem; color:black; display: none">
<u><I> 1802.07245v1 &nbsp; - &nbsp;
{deep}
{exploration}
{gradient}
{meta}
{reinforcement}
{robot}
</I></u><br> Exploration is a fundamental challenge in reinforcement learning (RL). Many
of the current exploration methods for deep RL use task-agnostic objectives,
such as information gain or bonuses based on state visitation. However, many
practical applications of RL involve learning more than a single task, and
prior tasks can be used to inform how exploration should be performed in new
tasks. In this work, we explore how prior tasks can inform an agent about how
to explore effectively in new situations. We introduce a novel gradient-based
fast adaptation algorithm -- model agnostic exploration with structured noise
(MAESN) -- to learn exploration strategies from prior experience. The prior
experience is used both to initialize a policy and to acquire a latent
exploration space that can inject structured stochasticity into a policy,
producing exploration strategies that are informed by prior knowledge and are
more effective than random action-space noise. We show that MAESN is more
effective at learning exploration strategies when compared to prior meta-RL
methods, RL without learned exploration strategies, and task-agnostic
exploration methods. We evaluate our method on a variety of simulated tasks:
locomotion with a wheeled robot, locomotion with a quadrupedal walker, and
object manipulation. <br><br></div></a>
</td></tr>
<tr id=" 262 " class="entry"><td>
<a onclick="toggleVisibility('abstract262');">[|&bull;|]</a><a href="http://arxiv.org/abs/1803.02999v3">
<b/> On First-Order Meta-Learning Algorithms (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1803.02999v3">
Alex Nichol, Joshua Achiam, John Schulman &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract262');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract262');"><div id="abstract262" style="font-size: 1rem; color:black; display: none">
<u><I> 1803.02999v3 &nbsp; - &nbsp;
{few-shot}
{meta}
</I></u><br> This paper considers meta-learning problems, where there is a distribution of
tasks, and we would like to obtain an agent that performs well (i.e., learns
quickly) when presented with a previously unseen task sampled from this
distribution. We analyze a family of algorithms for learning a parameter
initialization that can be fine-tuned quickly on a new task, using only
first-order derivatives for the meta-learning updates. This family includes and
generalizes first-order MAML, an approximation to MAML obtained by ignoring
second-order derivatives. It also includes Reptile, a new algorithm that we
introduce here, which works by repeatedly sampling a task, training on it, and
moving the initialization towards the trained weights on that task. We expand
on the results from Finn et al. showing that first-order meta-learning
algorithms perform well on some well-established benchmarks for few-shot
classification, and we provide theoretical analysis aimed at understanding why
these algorithms work. <br><br></div></a>
</td></tr>
<tr id=" 263 " class="entry"><td>
<a onclick="toggleVisibility('abstract263');">[|&bull;|]</a><a href="http://arxiv.org/abs/1808.05832v1">
<b/> Importance mixing: Improving sample reuse in evolutionary policy search
methods (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1808.05832v1">
Alo&#239;s Pourchot, Nicolas Perrin, Olivier Sigaud &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract263');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract263');"><div id="abstract263" style="font-size: 1rem; color:black; display: none">
<u><I> 1808.05832v1 &nbsp; - &nbsp;
{deep}
{evolution}
{reinforcement}
</I></u><br> Deep neuroevolution, that is evolutionary policy search methods based on deep
neural networks, have recently emerged as a competitor to deep reinforcement
learning algorithms due to their better parallelization capabilities. However,
these methods still suffer from a far worse sample efficiency. In this paper we
investigate whether a mechanism known as "importance mixing" can significantly
improve their sample efficiency. We provide a didactic presentation of
importance mixing and we explain how it can be extended to reuse more samples.
Then, from an empirical comparison based on a simple benchmark, we show that,
though it actually provides better sample efficiency, it is still far from the
sample efficiency of deep reinforcement learning, though it is more stable. <br><br></div></a>
</td></tr>
<tr id=" 264 " class="entry"><td>
<a onclick="toggleVisibility('abstract264');">[|&bull;|]</a><a href="http://arxiv.org/abs/1806.01242v1">
<b/> Graph networks as learnable physics engines for inference and control (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1806.01242v1">
Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Riedmiller, Raia Hadsell, Peter Battaglia &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract264');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract264');"><div id="abstract264" style="font-size: 1rem; color:black; display: none">
<u><I> 1806.01242v1 &nbsp; - &nbsp;
{gradient}
{offline}
</I></u><br> Understanding and interacting with everyday physical scenes requires rich
knowledge about the structure of the world, represented either implicitly in a
value or policy function, or explicitly in a transition model. Here we
introduce a new class of learnable models--based on graph networks--which
implement an inductive bias for object- and relation-centric representations of
complex, dynamical systems. Our results show that as a forward model, our
approach supports accurate predictions from real and simulated data, and
surprisingly strong and efficient generalization, across eight distinct
physical systems which we varied parametrically and structurally. We also found
that our inference model can perform system identification. Our models are also
differentiable, and support online planning via gradient-based trajectory
optimization, as well as offline policy optimization. Our framework offers new
opportunities for harnessing and exploiting rich knowledge about the world, and
takes a key step toward building machines with more human-like representations
of the world. <br><br></div></a>
</td></tr>
<tr id=" 265 " class="entry"><td>
<a onclick="toggleVisibility('abstract265');">[|&bull;|]</a><a href="http://arxiv.org/abs/1811.02553v4">
<b/> A Closer Look at Deep Policy Gradients (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1811.02553v4">
Andrew Ilyas, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Aleksander Madry &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract265');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract265');"><div id="abstract265" style="font-size: 1rem; color:black; display: none">
<u><I> 1811.02553v4 &nbsp; - &nbsp;
{deep}
{gradient}
</I></u><br> We study how the behavior of deep policy gradient algorithms reflects the
conceptual framework motivating their development. To this end, we propose a
fine-grained analysis of state-of-the-art methods based on key elements of this
framework: gradient estimation, value prediction, and optimization landscapes.
Our results show that the behavior of deep policy gradient algorithms often
deviates from what their motivating framework would predict: the surrogate
objective does not match the true reward landscape, learned value estimators
fail to fit the true value function, and gradient estimates poorly correlate
with the "true" gradient. The mismatch between predicted and empirical behavior
we uncover highlights our poor understanding of current methods, and indicates
the need to move beyond current benchmark-centric evaluation methods. <br><br></div></a>
</td></tr>
<tr id=" 266 " class="entry"><td>
<a onclick="toggleVisibility('abstract266');">[|&bull;|]</a><a href="http://arxiv.org/abs/1804.00379v2">
<b/> Recall Traces: Backtracking Models for Efficient Reinforcement Learning (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1804.00379v2">
Anirudh Goyal, Philemon Brakel, William Fedus, Soumye Singhal, Timothy Lillicrap, Sergey Levine, Hugo Larochelle, Yoshua Bengio &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract266');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract266');"><div id="abstract266" style="font-size: 1rem; color:black; display: none">
<u><I> 1804.00379v2 &nbsp; - &nbsp;
{off-policy}
</I></u><br> In many environments only a tiny subset of all states yield high reward. In
these cases, few of the interactions with the environment provide a relevant
learning signal. Hence, we may want to preferentially train on those
high-reward states and the probable trajectories leading to them. To this end,
we advocate for the use of a backtracking model that predicts the preceding
states that terminate at a given high-reward state. We can train a model which,
starting from a high value state (or one that is estimated to have high value),
predicts and sample for which the (state, action)-tuples may have led to that
high value state. These traces of (state, action) pairs, which we refer to as
Recall Traces, sampled from this backtracking model starting from a high value
state, are informative as they terminate in good states, and hence we can use
these traces to improve a policy. We provide a variational interpretation for
this idea and a practical algorithm in which the backtracking model samples
from an approximate posterior distribution over trajectories which lead to
large rewards. Our method improves the sample efficiency of both on- and
off-policy RL algorithms across several environments and tasks. <br><br></div></a>
</td></tr>
<tr id=" 267 " class="entry"><td>
<a onclick="toggleVisibility('abstract267');">[|&bull;|]</a><a href="http://arxiv.org/abs/1804.07127v1">
<b/> Hierarchical Behavioral Repertoires with Unsupervised Descriptors (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1804.07127v1">
Antoine Cully, Yiannis Demiris &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract267');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract267');"><div id="abstract267" style="font-size: 1rem; color:black; display: none">
<u><I> 1804.07127v1 &nbsp; - &nbsp;
{control}
{hierarchical}
{humanoid}
{robot}
{transfer}
{unsupervised}
</I></u><br> Enabling artificial agents to automatically learn complex, versatile and
high-performing behaviors is a long-lasting challenge. This paper presents a
step in this direction with hierarchical behavioral repertoires that stack
several behavioral repertoires to generate sophisticated behaviors. Each
repertoire of this architecture uses the lower repertoires to create complex
behaviors as sequences of simpler ones, while only the lowest repertoire
directly controls the agent's movements. This paper also introduces a novel
approach to automatically define behavioral descriptors thanks to an
unsupervised neural network that organizes the produced high-level behaviors.
The experiments show that the proposed architecture enables a robot to learn
how to draw digits in an unsupervised manner after having learned to draw lines
and arcs. Compared to traditional behavioral repertoires, the proposed
architecture reduces the dimensionality of the optimization problems by orders
of magnitude and provides behaviors with a twice better fitness. More
importantly, it enables the transfer of knowledge between robots: a
hierarchical repertoire evolved for a robotic arm to draw digits can be
transferred to a humanoid robot by simply changing the lowest layer of the
hierarchy. This enables the humanoid to draw digits although it has never been
trained for this task. <br><br></div></a>
</td></tr>
<tr id=" 268 " class="entry"><td>
<a onclick="toggleVisibility('abstract268');">[|&bull;|]</a><a href="http://arxiv.org/abs/1802.06070v6">
<b/> Diversity is All You Need: Learning Skills without a Reward Function (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1802.06070v6">
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract268');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract268');"><div id="abstract268" style="font-size: 1rem; color:black; display: none">
<u><I> 1802.06070v6 &nbsp; - &nbsp;
{diversity}
{entropy}
{exploration}
{hierarchical}
{reinforcement}
{robot}
{skill}
{sparse}
{unsupervised}
</I></u><br> Intelligent creatures can explore their environments and learn useful skills
without supervision. In this paper, we propose DIAYN ('Diversity is All You
Need'), a method for learning useful skills without a reward function. Our
proposed method learns skills by maximizing an information theoretic objective
using a maximum entropy policy. On a variety of simulated robotic tasks, we
show that this simple objective results in the unsupervised emergence of
diverse skills, such as walking and jumping. In a number of reinforcement
learning benchmark environments, our method is able to learn a skill that
solves the benchmark task despite never receiving the true task reward. We show
how pretrained skills can provide a good parameter initialization for
downstream tasks, and can be composed hierarchically to solve complex, sparse
reward tasks. Our results suggest that unsupervised discovery of skills can
serve as an effective pretraining mechanism for overcoming challenges of
exploration and data efficiency in reinforcement learning. <br><br></div></a>
</td></tr>
<tr id=" 269 " class="entry"><td>
<a onclick="toggleVisibility('abstract269');">[|&bull;|]</a><a href="http://arxiv.org/abs/1807.10366v2">
<b/> Robot Motion Planning in Learned Latent Spaces (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1807.10366v2">
Brian Ichter, Marco Pavone &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract269');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract269');"><div id="abstract269" style="font-size: 1rem; color:black; display: none">
<u><I> 1807.10366v2 &nbsp; - &nbsp;
{control}
{exploration}
{humanoid}
{motion planning}
{robot}
</I></u><br> This paper presents Latent Sampling-based Motion Planning (L-SBMP), a
methodology towards computing motion plans for complex robotic systems by
learning a plannable latent representation. Recent works in control of robotic
systems have effectively leveraged local, low-dimensional embeddings of
high-dimensional dynamics. In this paper we combine these recent advances with
techniques from sampling-based motion planning (SBMP) in order to design a
methodology capable of planning for high-dimensional robotic systems beyond the
reach of traditional approaches (e.g., humanoids, or even systems where
planning occurs in the visual space). Specifically, the learned latent space is
constructed through an autoencoding network, a dynamics network, and a
collision checking network, which mirror the three main algorithmic primitives
of SBMP, namely state sampling, local steering, and collision checking.
Notably, these networks can be trained through only raw data of the system's
states and actions along with a supervising collision checker. Building upon
these networks, an RRT-based algorithm is used to plan motions directly in the
latent space - we refer to this exploration algorithm as Learned Latent RRT
(L2RRT). This algorithm globally explores the latent space and is capable of
generalizing to new environments. The overall methodology is demonstrated on
two planning problems, namely a visual planning problem, whereby planning
happens in the visual (pixel) space, and a humanoid robot planning problem. <br><br></div></a>
</td></tr>
<tr id=" 270 " class="entry"><td>
<a onclick="toggleVisibility('abstract270');">[|&bull;|]</a><a href="http://arxiv.org/abs/1807.06919v5">
<b/> Backplay: "Man muss immer umkehren" (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1807.06919v5">
Cinjon Resnick, Roberta Raileanu, Sanyam Kapoor, Alexander Peysakhovich, Kyunghyun Cho, Joan Bruna &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract270');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract270');"><div id="abstract270" style="font-size: 1rem; color:black; display: none">
<u><I> 1807.06919v5 &nbsp; - &nbsp;
{reinforcement}
{sparse}
</I></u><br> Model-free reinforcement learning (RL) requires a large number of trials to
learn a good policy, especially in environments with sparse rewards. We explore
a method to improve the sample efficiency when we have access to
demonstrations. Our approach, Backplay, uses a single demonstration to
construct a curriculum for a given task. Rather than starting each training
episode in the environment's fixed initial state, we start the agent near the
end of the demonstration and move the starting point backwards during the
course of training until we reach the initial state. Our contributions are that
we analytically characterize the types of environments where Backplay can
improve training speed, demonstrate the effectiveness of Backplay both in large
grid worlds and a complex four player zero-sum game (Pommerman), and show that
Backplay compares favorably to other competitive methods known to improve
sample efficiency. This includes reward shaping, behavioral cloning, and
reverse curriculum generation. <br><br></div></a>
</td></tr>
<tr id=" 271 " class="entry"><td>
<a onclick="toggleVisibility('abstract271');">[|&bull;|]</a><a href="http://arxiv.org/abs/1803.10122v4">
<b/> World Models (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1803.10122v4">
David Ha, J&#252;rgen Schmidhuber &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract271');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract271');"><div id="abstract271" style="font-size: 1rem; color:black; display: none">
<u><I> 1803.10122v4 &nbsp; - &nbsp;
{reinforcement}
{transfer}
{unsupervised}
</I></u><br> We explore building generative neural network models of popular reinforcement
learning environments. Our world model can be trained quickly in an
unsupervised manner to learn a compressed spatial and temporal representation
of the environment. By using features extracted from the world model as inputs
to an agent, we can train a very compact and simple policy that can solve the
required task. We can even train our agent entirely inside of its own
hallucinated dream generated by its world model, and transfer this policy back
into the actual environment.
An interactive version of this paper is available at
https://worldmodels.github.io/ <br><br></div></a>
</td></tr>
<tr id=" 272 " class="entry"><td>
<a onclick="toggleVisibility('abstract272');">[|&bull;|]</a><a href="http://arxiv.org/abs/1811.11359v1">
<b/> Unsupervised Control Through Non-Parametric Discriminative Rewards (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1811.11359v1">
David Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, Volodymyr Mnih &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract272');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract272');"><div id="abstract272" style="font-size: 1rem; color:black; display: none">
<u><I> 1811.11359v1 &nbsp; - &nbsp;
{control}
{deep}
{goal-conditioned}
{reinforcement}
{unsupervised}
</I></u><br> Learning to control an environment without hand-crafted rewards or expert
data remains challenging and is at the frontier of reinforcement learning
research. We present an unsupervised learning algorithm to train agents to
achieve perceptually-specified goals using only a stream of observations and
actions. Our agent simultaneously learns a goal-conditioned policy and a goal
achievement reward function that measures how similar a state is to the goal
state. This dual optimization leads to a co-operative game, giving rise to a
learned reward function that reflects similarity in controllable aspects of the
environment instead of distance in the space of observations. We demonstrate
the efficacy of our agent to learn, in an unsupervised manner, to reach a
diverse set of goals on three domains -- Atari, the DeepMind Control Suite and
DeepMind Lab. <br><br></div></a>
</td></tr>
<tr id=" 273 " class="entry"><td>
<a onclick="toggleVisibility('abstract273');">[|&bull;|]</a><a href="http://arxiv.org/abs/1806.05695v1">
<b/> Evolving simple programs for playing Atari games (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1806.05695v1">
Dennis G Wilson, Sylvain Cussat-Blanc, Herv&#233; Luga, Julian F Miller &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract273');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract273');"><div id="abstract273" style="font-size: 1rem; color:black; display: none">
<u><I> 1806.05695v1 &nbsp; - &nbsp;
{control}
</I></u><br> Cartesian Genetic Programming (CGP) has previously shown capabilities in
image processing tasks by evolving programs with a function set specialized for
computer vision. A similar approach can be applied to Atari playing. Programs
are evolved using mixed type CGP with a function set suited for matrix
operations, including image processing, but allowing for controller behavior to
emerge. While the programs are relatively small, many controllers are
competitive with state of the art methods for the Atari benchmark set and
require less training time. By evaluating the programs of the best evolved
individuals, simple but effective strategies can be found. <br><br></div></a>
</td></tr>
<tr id=" 274 " class="entry"><td>
<a onclick="toggleVisibility('abstract274');">[|&bull;|]</a><a href="http://arxiv.org/abs/1812.07626v1">
<b/> Universal Successor Features Approximators (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1812.07626v1">
Diana Borsa, Andr&#233; Barreto, John Quan, Daniel Mankowitz, R&#233;mi Munos, Hado van Hasselt, David Silver, Tom Schaul &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract274');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract274');"><div id="abstract274" style="font-size: 1rem; color:black; display: none">
<u><I> 1812.07626v1 &nbsp; - &nbsp;
{reinforcement}
{skill}
{transfer}
</I></u><br> The ability of a reinforcement learning (RL) agent to learn about many reward
functions at the same time has many potential benefits, such as the
decomposition of complex tasks into simpler ones, the exchange of information
between tasks, and the reuse of skills. We focus on one aspect in particular,
namely the ability to generalise to unseen tasks. Parametric generalisation
relies on the interpolation power of a function approximator that is given the
task description as input; one of its most common form are universal value
function approximators (UVFAs). Another way to generalise to new tasks is to
exploit structure in the RL problem itself. Generalised policy improvement
(GPI) combines solutions of previous tasks into a policy for the unseen task;
this relies on instantaneous policy evaluation of old policies under the new
reward function, which is made possible through successor features (SFs). Our
proposed universal successor features approximators (USFAs) combine the
advantages of all of these, namely the scalability of UVFAs, the instant
inference of SFs, and the strong generalisation of GPI. We discuss the
challenges involved in training a USFA, its generalisation properties and
demonstrate its practical benefits and transfer abilities on a large-scale
domain in which the agent has to navigate in a first-person perspective
three-dimensional environment. <br><br></div></a>
</td></tr>
<tr id=" 275 " class="entry"><td>
<a onclick="toggleVisibility('abstract275');">[|&bull;|]</a><a href="http://arxiv.org/abs/1806.10293v3">
<b/> QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic
Manipulation (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1806.10293v3">
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract275');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract275');"><div id="abstract275" style="font-size: 1rem; color:black; display: none">
<u><I> 1806.10293v3 &nbsp; - &nbsp;
{control}
{deep}
{reinforcement}
{robot}
{self-supervised}
{skill}
</I></u><br> In this paper, we study the problem of learning vision-based dynamic
manipulation skills using a scalable reinforcement learning approach. We study
this problem in the context of grasping, a longstanding challenge in robotic
manipulation. In contrast to static learning behaviors that choose a grasp
point and then execute the desired grasp, our method enables closed-loop
vision-based control, whereby the robot continuously updates its grasp strategy
based on the most recent observations to optimize long-horizon grasp success.
To that end, we introduce QT-Opt, a scalable self-supervised vision-based
reinforcement learning framework that can leverage over 580k real-world grasp
attempts to train a deep neural network Q-function with over 1.2M parameters to
perform closed-loop, real-world grasping that generalizes to 96% grasp success
on unseen objects. Aside from attaining a very high success rate, our method
exhibits behaviors that are quite distinct from more standard grasping systems:
using only RGB vision-based perception from an over-the-shoulder camera, our
method automatically learns regrasping strategies, probes objects to find the
most effective grasps, learns to reposition objects and perform other
non-prehensile pre-grasp manipulations, and responds dynamically to
disturbances and perturbations. <br><br></div></a>
</td></tr>
<tr id=" 276 " class="entry"><td>
<a onclick="toggleVisibility('abstract276');">[|&bull;|]</a><a href="http://arxiv.org/abs/1806.04613v1">
<b/> Improving Regression Performance with Distributional Losses (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1806.04613v1">
Ehsan Imani, Martha White &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract276');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract276');"><div id="abstract276" style="font-size: 1rem; color:black; display: none">
<u><I> 1806.04613v1 &nbsp; - &nbsp;
{distill}
{gradient}
{reinforcement}
</I></u><br> There is growing evidence that converting targets to soft targets in
supervised learning can provide considerable gains in performance. Much of this
work has considered classification, converting hard zero-one values to soft
labels---such as by adding label noise, incorporating label ambiguity or using
distillation. In parallel, there is some evidence from a regression setting in
reinforcement learning that learning distributions can improve performance. In
this work, we investigate the reasons for this improvement, in a regression
setting. We introduce a novel distributional regression loss, and similarly
find it significantly improves prediction accuracy. We investigate several
common hypotheses, around reducing overfitting and improved representations. We
instead find evidence for an alternative hypothesis: this loss is easier to
optimize, with better behaved gradients, resulting in improved generalization.
We provide theoretical support for this alternative hypothesis, by
characterizing the norm of the gradients of this loss. <br><br></div></a>
</td></tr>
<tr id=" 277 " class="entry"><td>
<a onclick="toggleVisibility('abstract277');">[|&bull;|]</a><a href="http://arxiv.org/abs/1811.12359v4">
<b/> Challenging Common Assumptions in the Unsupervised Learning of
Disentangled Representations (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1811.12359v4">
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar R&#228;tsch, Sylvain Gelly, Bernhard Sch&#246;lkopf, Olivier Bachem &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract277');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract277');"><div id="abstract277" style="font-size: 1rem; color:black; display: none">
<u><I> 1811.12359v4 &nbsp; - &nbsp;
{unsupervised}
</I></u><br> The key idea behind the unsupervised learning of disentangled representations
is that real-world data is generated by a few explanatory factors of variation
which can be recovered by unsupervised learning algorithms. In this paper, we
provide a sober look at recent progress in the field and challenge some common
assumptions. We first theoretically show that the unsupervised learning of
disentangled representations is fundamentally impossible without inductive
biases on both the models and the data. Then, we train more than 12000 models
covering most prominent methods and evaluation metrics in a reproducible
large-scale experimental study on seven different data sets. We observe that
while the different methods successfully enforce properties ``encouraged'' by
the corresponding losses, well-disentangled models seemingly cannot be
identified without supervision. Furthermore, increased disentanglement does not
seem to lead to a decreased sample complexity of learning for downstream tasks.
Our results suggest that future work on disentanglement learning should be
explicit about the role of inductive biases and (implicit) supervision,
investigate concrete benefits of enforcing disentanglement of the learned
representations, and consider a reproducible experimental setup covering
several data sets. <br><br></div></a>
</td></tr>
<tr id=" 278 " class="entry"><td>
<a onclick="toggleVisibility('abstract278');">[|&bull;|]</a><a href="http://arxiv.org/abs/1803.00567v4">
<b/> Computational Optimal Transport (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1803.00567v4">
Gabriel Peyr&#233;, Marco Cuturi &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract278');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract278');"><div id="abstract278" style="font-size: 1rem; color:black; display: none">
<u><I> 1803.00567v4 &nbsp; - &nbsp;
{optimal}
</I></u><br> Optimal transport (OT) theory can be informally described using the words of
the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in
hand has to move a large pile of sand lying on a construction site. The goal of
the worker is to erect with all that sand a target pile with a prescribed shape
(for example, that of a giant sand castle). Naturally, the worker wishes to
minimize her total effort, quantified for instance as the total distance or
time spent carrying shovelfuls of sand. Mathematicians interested in OT cast
that problem as that of comparing two probability distributions, two different
piles of sand of the same volume. They consider all of the many possible ways
to morph, transport or reshape the first pile into the second, and associate a
"global" cost to every such transport, using the "local" consideration of how
much it costs to move a grain of sand from one place to another. Recent years
have witnessed the spread of OT in several fields, thanks to the emergence of
approximate solvers that can scale to sizes and dimensions that are relevant to
data sciences. Thanks to this newfound scalability, OT is being increasingly
used to unlock various problems in imaging sciences (such as color or texture
processing), computer vision and graphics (for shape manipulation) or machine
learning (for regression, classification and density fitting). This short book
reviews OT with a bias toward numerical methods and their applications in data
sciences, and sheds lights on the theoretical properties of OT that make it
particularly useful for some of these applications. <br><br></div></a>
</td></tr>
<tr id=" 279 " class="entry"><td>
<a onclick="toggleVisibility('abstract279');">[|&bull;|]</a><a href="http://arxiv.org/abs/1801.03954v2">
<b/> Model-Based Action Exploration for Learning Dynamic Motion Skills (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1801.03954v2">
Glen Berseth, Michiel van de Panne &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract279');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract279');"><div id="abstract279" style="font-size: 1rem; color:black; display: none">
<u><I> 1801.03954v2 &nbsp; - &nbsp;
{control}
{deep}
{reinforcement}
{robot}
</I></u><br> Deep reinforcement learning has achieved great strides in solving challenging
motion control tasks. Recently, there has been significant work on methods for
exploiting the data gathered during training, but there has been less work on
how to best generate the data to learn from. For continuous action domains, the
most common method for generating exploratory actions involves sampling from a
Gaussian distribution centred around the mean action output by a policy.
Although these methods can be quite capable, they do not scale well with the
dimensionality of the action space, and can be dangerous to apply on hardware.
We consider learning a forward dynamics model to predict the result,
($x_{t+1}$), of taking a particular action, ($u$), given a specific observation
of the state, ($x_{t}$). With this model we perform internal look-ahead
predictions of outcomes and seek actions we believe have a reasonable chance of
success. This method alters the exploratory action space, thereby increasing
learning speed and enables higher quality solutions to difficult problems, such
as robotic locomotion and juggling. <br><br></div></a>
</td></tr>
<tr id=" 280 " class="entry"><td>
<a onclick="toggleVisibility('abstract280');">[|&bull;|]</a><a href="http://arxiv.org/abs/1810.12281v1">
<b/> Three Mechanisms of Weight Decay Regularization (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1810.12281v1">
Guodong Zhang, Chaoqi Wang, Bowen Xu, Roger Grosse &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract280');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract280');"><div id="abstract280" style="font-size: 1rem; color:black; display: none">
<u><I> 1810.12281v1 &nbsp; - &nbsp;
</I></u><br> Weight decay is one of the standard tricks in the neural network toolbox, but
the reasons for its regularization effect are poorly understood, and recent
results have cast doubt on the traditional interpretation in terms of $L_2$
regularization. Literal weight decay has been shown to outperform $L_2$
regularization for optimizers for which they differ. We empirically investigate
weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a
variety of network architectures. We identify three distinct mechanisms by
which weight decay exerts a regularization effect, depending on the particular
optimization algorithm and architecture: (1) increasing the effective learning
rate, (2) approximately regularizing the input-output Jacobian norm, and (3)
reducing the effective damping coefficient for second-order optimization. Our
results provide insight into how to improve the regularization of neural
networks. <br><br></div></a>
</td></tr>
<tr id=" 281 " class="entry"><td>
<a onclick="toggleVisibility('abstract281');">[|&bull;|]</a><a href="http://arxiv.org/abs/1812.02648v1">
<b/> Deep Reinforcement Learning and the Deadly Triad (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1812.02648v1">
Hado van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, Joseph Modayil &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract281');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract281');"><div id="abstract281" style="font-size: 1rem; color:black; display: none">
<u><I> 1812.02648v1 &nbsp; - &nbsp;
{deep}
{off-policy}
{reinforcement}
{replay}
</I></u><br> We know from reinforcement learning theory that temporal difference learning
can fail in certain cases. Sutton and Barto (2018) identify a deadly triad of
function approximation, bootstrapping, and off-policy learning. When these
three properties are combined, learning can diverge with the value estimates
becoming unbounded. However, several algorithms successfully combine these
three properties, which indicates that there is at least a partial gap in our
understanding. In this work, we investigate the impact of the deadly triad in
practice, in the context of a family of popular deep reinforcement learning
models - deep Q-networks trained with experience replay - analysing how the
components of this system play a role in the emergence of the deadly triad, and
in the agent's performance <br><br></div></a>
</td></tr>
<tr id=" 282 " class="entry"><td>
<a onclick="toggleVisibility('abstract282');">[|&bull;|]</a><a href="http://arxiv.org/abs/1809.02925v2">
<b/> Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward
Bias in Adversarial Imitation Learning (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1809.02925v2">
Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, Jonathan Tompson &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract282');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract282');"><div id="abstract282" style="font-size: 1rem; color:black; display: none">
<u><I> 1809.02925v2 &nbsp; - &nbsp;
{imitation}
{off-policy}
{optimal}
{reinforcement}
</I></u><br> We identify two issues with the family of algorithms based on the Adversarial
Imitation Learning framework. The first problem is implicit bias present in the
reward functions used in these algorithms. While these biases might work well
for some environments, they can also lead to sub-optimal behavior in others.
Secondly, even though these algorithms can learn from few expert
demonstrations, they require a prohibitively large number of interactions with
the environment in order to imitate the expert for many real-world
applications. In order to address these issues, we propose a new algorithm
called Discriminator-Actor-Critic that uses off-policy Reinforcement Learning
to reduce policy-environment interaction sample complexity by an average factor
of 10. Furthermore, since our reward function is designed to be unbiased, we
can apply our algorithm to many problems without making any task-specific
adjustments. <br><br></div></a>
</td></tr>
<tr id=" 283 " class="entry"><td>
<a onclick="toggleVisibility('abstract283');">[|&bull;|]</a><a href="http://arxiv.org/abs/1811.11711v2">
<b/> Neural probabilistic motor primitives for humanoid control (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1811.11711v2">
Josh Merel, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu Pham, Greg Wayne, Yee Whye Teh, Nicolas Heess &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract283');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract283');"><div id="abstract283" style="font-size: 1rem; color:black; display: none">
<u><I> 1811.11711v2 &nbsp; - &nbsp;
{control}
{humanoid}
{imitation}
{offline}
</I></u><br> We focus on the problem of learning a single motor module that can flexibly
express a range of behaviors for the control of high-dimensional physically
simulated humanoids. To do this, we propose a motor architecture that has the
general structure of an inverse model with a latent-variable bottleneck. We
show that it is possible to train this model entirely offline to compress
thousands of expert policies and learn a motor primitive embedding space. The
trained neural probabilistic motor primitive system can perform one-shot
imitation of whole-body humanoid behaviors, robustly mimicking unseen
trajectories. Additionally, we demonstrate that it is also straightforward to
train controllers to reuse the learned motor primitive space to solve tasks,
and the resulting movements are relatively naturalistic. To support the
training of our model, we compare two approaches for offline policy cloning,
including an experience efficient method which we call linear feedback policy
cloning. We encourage readers to view a supplementary video (
https://youtu.be/CaDEf-QcKwA ) summarizing our results. <br><br></div></a>
</td></tr>
<tr id=" 284 " class="entry"><td>
<a onclick="toggleVisibility('abstract284');">[|&bull;|]</a><a href="http://arxiv.org/abs/1811.01848v3">
<b/> Plan Online, Learn Offline: Efficient Learning and Exploration via
Model-Based Control (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1811.01848v3">
Kendall Lowrey, Aravind Rajeswaran, Sham Kakade, Emanuel Todorov, Igor Mordatch &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract284');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract284');"><div id="abstract284" style="font-size: 1rem; color:black; display: none">
<u><I> 1811.01848v3 &nbsp; - &nbsp;
{control}
{exploration}
{humanoid}
{model-based}
{offline}
</I></u><br> We propose a plan online and learn offline (POLO) framework for the setting
where an agent, with an internal model, needs to continually act and learn in
the world. Our work builds on the synergistic relationship between local
model-based control, global value function learning, and exploration. We study
how local trajectory optimization can cope with approximation errors in the
value function, and can stabilize and accelerate value function learning.
Conversely, we also study how approximate value functions can help reduce the
planning horizon and allow for better policies beyond local solutions. Finally,
we also demonstrate how trajectory optimization can be used to perform
temporally coordinated exploration in conjunction with estimating uncertainty
in value function approximation. This exploration is critical for fast and
stable learning of the value function. Combining these components enable
solutions to complex simulated control tasks, like humanoid locomotion and
dexterous in-hand manipulation, in the equivalent of a few minutes of
experience in the real world. <br><br></div></a>
</td></tr>
<tr id=" 285 " class="entry"><td>
<a onclick="toggleVisibility('abstract285');">[|&bull;|]</a><a href="http://arxiv.org/abs/1805.12114v2">
<b/> Deep Reinforcement Learning in a Handful of Trials using Probabilistic
Dynamics Models (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1805.12114v2">
Kurtland Chua, Roberto Calandra, Rowan McAllister, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract285');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract285');"><div id="abstract285" style="font-size: 1rem; color:black; display: none">
<u><I> 1805.12114v2 &nbsp; - &nbsp;
{deep}
{model-based}
{reinforcement}
</I></u><br> Model-based reinforcement learning (RL) algorithms can attain excellent
sample efficiency, but often lag behind the best model-free algorithms in terms
of asymptotic performance. This is especially true with high-capacity
parametric function approximators, such as deep networks. In this paper, we
study how to bridge this gap, by employing uncertainty-aware dynamics models.
We propose a new algorithm called probabilistic ensembles with trajectory
sampling (PETS) that combines uncertainty-aware deep network dynamics models
with sampling-based uncertainty propagation. Our comparison to state-of-the-art
model-based and model-free deep RL algorithms shows that our approach matches
the asymptotic performance of model-free algorithms on several challenging
benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125
times fewer samples than Soft Actor Critic and Proximal Policy Optimization
respectively on the half-cheetah task). <br><br></div></a>
</td></tr>
<tr id=" 286 " class="entry"><td>
<a onclick="toggleVisibility('abstract286');">[|&bull;|]</a><a href="http://arxiv.org/abs/1801.06159v2">
<b/> When Does Stochastic Gradient Algorithm Work Well? (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1801.06159v2">
Lam M. Nguyen, Nam H. Nguyen, Dzung T. Phan, Jayant R. Kalagnanam, Katya Scheinberg &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract286');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract286');"><div id="abstract286" style="font-size: 1rem; color:black; display: none">
<u><I> 1801.06159v2 &nbsp; - &nbsp;
{deep}
{gradient}
{optimal}
</I></u><br> In this paper, we consider a general stochastic optimization problem which is
often at the core of supervised learning, such as deep learning and linear
classification. We consider a standard stochastic gradient descent (SGD) method
with a fixed, large step size and propose a novel assumption on the objective
function, under which this method has the improved convergence rates (to a
neighborhood of the optimal solutions). We then empirically demonstrate that
these assumptions hold for logistic regression and standard deep neural
networks on classical data sets. Thus our analysis helps to explain when
efficient behavior can be expected from the SGD method in training
classification models and deep neural networks. <br><br></div></a>
</td></tr>
<tr id=" 287 " class="entry"><td>
<a onclick="toggleVisibility('abstract287');">[|&bull;|]</a><a href="http://arxiv.org/abs/1802.01561v3">
<b/> IMPALA: Scalable Distributed Deep-RL with Importance Weighted
Actor-Learner Architectures (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1802.01561v3">
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract287');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract287');"><div id="abstract287" style="font-size: 1rem; color:black; display: none">
<u><I> 1802.01561v3 &nbsp; - &nbsp;
{deep}
{multi-task}
{off-policy}
{reinforcement}
{transfer}
</I></u><br> In this work we aim to solve a large collection of tasks using a single
reinforcement learning agent with a single set of parameters. A key challenge
is to handle the increased amount of data and extended training time. We have
developed a new distributed agent IMPALA (Importance Weighted Actor-Learner
Architecture) that not only uses resources more efficiently in single-machine
training but also scales to thousands of machines without sacrificing data
efficiency or resource utilisation. We achieve stable learning at high
throughput by combining decoupled acting and learning with a novel off-policy
correction method called V-trace. We demonstrate the effectiveness of IMPALA
for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the
DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available
Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our
results show that IMPALA is able to achieve better performance than previous
agents with less data, and crucially exhibits positive transfer between tasks
as a result of its multi-task approach. <br><br></div></a>
</td></tr>
<tr id=" 288 " class="entry"><td>
<a onclick="toggleVisibility('abstract288');">[|&bull;|]</a><a href="http://arxiv.org/abs/1806.01240v3">
<b/> Diffeomorphic Learning (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1806.01240v3">
Laurent Younes &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract288');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract288');"><div id="abstract288" style="font-size: 1rem; color:black; display: none">
<u><I> 1806.01240v3 &nbsp; - &nbsp;
</I></u><br> We introduce in this paper a learning paradigm in which the training data is
transformed by a diffeomorphic transformation before prediction. The learning
algorithm minimizes a cost function evaluating the prediction error on the
training set penalized by the distance between the diffeomorphism and the
identity. The approach borrows ideas from shape analysis where diffeomorphisms
are estimated for shape and image alignment, and brings them in a previously
unexplored setting, estimating, in particular diffeomorphisms in much larger
dimensions. After introducing the concept and describing a learning algorithm,
we present diverse applications, mostly with synthetic examples, demonstrating
the potential of the approach, as well as some insight on how it can be
improved. <br><br></div></a>
</td></tr>
<tr id=" 289 " class="entry"><td>
<a onclick="toggleVisibility('abstract289');">[|&bull;|]</a><a href="http://arxiv.org/abs/1809.02721v3">
<b/> Learning to Solve NP-Complete Problems - A Graph Neural Network for
Decision TSP (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1809.02721v3">
Marcelo O. R. Prates, Pedro H. C. Avelar, Henrique Lemos, Luis Lamb, Moshe Vardi &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract289');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract289');"><div id="abstract289" style="font-size: 1rem; color:black; display: none">
<u><I> 1809.02721v3 &nbsp; - &nbsp;
{optimal}
</I></u><br> Graph Neural Networks (GNN) are a promising technique for bridging
differential programming and combinatorial domains. GNNs employ trainable
modules which can be assembled in different configurations that reflect the
relational structure of each problem instance. In this paper, we show that GNNs
can learn to solve, with very little supervision, the decision variant of the
Traveling Salesperson Problem (TSP), a highly relevant $\mathcal{NP}$-Complete
problem. Our model is trained to function as an effective message-passing
algorithm in which edges (embedded with their weights) communicate with
vertices for a number of iterations after which the model is asked to decide
whether a route with cost $<C$ exists. We show that such a network can be
trained with sets of dual examples: given the optimal tour cost $C^{*}$, we
produce one decision instance with target cost $x\%$ smaller and one with
target cost $x\%$ larger than $C^{*}$. We were able to obtain $80\%$ accuracy
training with $-2\%,+2\%$ deviations, and the same trained model can generalize
for more relaxed deviations with increasing performance. We also show that the
model is capable of generalizing for larger problem sizes. Finally, we provide
a method for predicting the optimal route cost within $2\%$ deviation from the
ground truth. In summary, our work shows that Graph Neural Networks are
powerful enough to solve $\mathcal{NP}$-Complete problems which combine
symbolic and numeric data. <br><br></div></a>
</td></tr>
<tr id=" 290 " class="entry"><td>
<a onclick="toggleVisibility('abstract290');">[|&bull;|]</a><a href="http://arxiv.org/abs/1802.08163v1">
<b/> An Analysis of Categorical Distributional Reinforcement Learning (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1802.08163v1">
Mark Rowland, Marc G. Bellemare, Will Dabney, R&#233;mi Munos, Yee Whye Teh &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract290');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract290');"><div id="abstract290" style="font-size: 1rem; color:black; display: none">
<u><I> 1802.08163v1 &nbsp; - &nbsp;
{reinforcement}
</I></u><br> Distributional approaches to value-based reinforcement learning model the
entire distribution of returns, rather than just their expected values, and
have recently been shown to yield state-of-the-art empirical performance. This
was demonstrated by the recently proposed C51 algorithm, based on categorical
distributional reinforcement learning (CDRL) [Bellemare et al., 2017]. However,
the theoretical properties of CDRL algorithms are not yet well understood. In
this paper, we introduce a framework to analyse CDRL algorithms, establish the
importance of the projected distributional Bellman operator in distributional
RL, draw fundamental connections between CDRL and the Cram\'er distance, and
give a proof of convergence for sample-based categorical distributional
reinforcement learning algorithms. <br><br></div></a>
</td></tr>
<tr id=" 291 " class="entry"><td>
<a onclick="toggleVisibility('abstract291');">[|&bull;|]</a><a href="http://arxiv.org/abs/1802.10567v1">
<b/> Learning by Playing - Solving Sparse Reward Tasks from Scratch (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1802.10567v1">
Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Van de Wiele, Volodymyr Mnih, Nicolas Heess, Jost Tobias Springenberg &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract291');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract291');"><div id="abstract291" style="font-size: 1rem; color:black; display: none">
<u><I> 1802.10567v1 &nbsp; - &nbsp;
{control}
{off-policy}
{reinforcement}
{robot}
{sparse}
</I></u><br> We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in
the context of Reinforcement Learning (RL). SAC-X enables learning of complex
behaviors - from scratch - in the presence of multiple sparse reward signals.
To this end, the agent is equipped with a set of general auxiliary tasks, that
it attempts to learn simultaneously via off-policy RL. The key idea behind our
method is that active (learned) scheduling and execution of auxiliary policies
allows the agent to efficiently explore its environment - enabling it to excel
at sparse reward RL. Our experiments in several challenging robotic
manipulation settings demonstrate the power of our approach. <br><br></div></a>
</td></tr>
<tr id=" 292 " class="entry"><td>
<a onclick="toggleVisibility('abstract292');">[|&bull;|]</a><a href="http://arxiv.org/abs/1803.02348v3">
<b/> Smoothed Action Value Functions for Learning Gaussian Policies (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1803.02348v3">
Ofir Nachum, Mohammad Norouzi, George Tucker, Dale Schuurmans &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract292');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract292');"><div id="abstract292" style="font-size: 1rem; color:black; display: none">
<u><I> 1803.02348v3 &nbsp; - &nbsp;
{control}
{gradient}
{reinforcement}
</I></u><br> State-action value functions (i.e., Q-values) are ubiquitous in reinforcement
learning (RL), giving rise to popular algorithms such as SARSA and Q-learning.
We propose a new notion of action value defined by a Gaussian smoothed version
of the expected Q-value. We show that such smoothed Q-values still satisfy a
Bellman equation, making them learnable from experience sampled from an
environment. Moreover, the gradients of expected reward with respect to the
mean and covariance of a parameterized Gaussian policy can be recovered from
the gradient and Hessian of the smoothed Q-value function. Based on these
relationships, we develop new algorithms for training a Gaussian policy
directly from a learned smoothed Q-value approximator. The approach is
additionally amenable to proximal optimization by augmenting the objective with
a penalty on KL-divergence from a previous policy. We find that the ability to
learn both a mean and covariance during training leads to significantly
improved results on standard continuous control benchmarks. <br><br></div></a>
</td></tr>
<tr id=" 293 " class="entry"><td>
<a onclick="toggleVisibility('abstract293');">[|&bull;|]</a><a href="http://arxiv.org/abs/1810.01257v2">
<b/> Near-Optimal Representation Learning for Hierarchical Reinforcement
Learning (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1810.01257v2">
Ofir Nachum, Shixiang Gu, Honglak Lee, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract293');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract293');"><div id="abstract293" style="font-size: 1rem; color:black; display: none">
<u><I> 1810.01257v2 &nbsp; - &nbsp;
{control}
{goal-conditioned}
{hierarchical}
{optimal}
{reinforcement}
</I></u><br> We study the problem of representation learning in goal-conditioned
hierarchical reinforcement learning. In such hierarchical structures, a
higher-level controller solves tasks by iteratively communicating goals which a
lower-level policy is trained to reach. Accordingly, the choice of
representation -- the mapping of observation space to goal space -- is crucial.
To study this problem, we develop a notion of sub-optimality of a
representation, defined in terms of expected reward of the optimal hierarchical
policy using this representation. We derive expressions which bound the
sub-optimality and show how these expressions can be translated to
representation learning objectives which may be optimized in practice. Results
on a number of difficult continuous-control tasks show that our approach to
representation learning yields qualitatively better representations as well as
quantitatively better hierarchical policies, compared to existing methods (see
videos at https://sites.google.com/view/representation-hrl). <br><br></div></a>
</td></tr>
<tr id=" 294 " class="entry"><td>
<a onclick="toggleVisibility('abstract294');">[|&bull;|]</a><a href="http://arxiv.org/abs/1802.08322v2">
<b/> Feedback Control of an Exoskeleton for Paraplegics: Toward Robustly
Stable Hands-free Dynamic Walking (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1802.08322v2">
Omar Harib, Ayonga Hereid, Ayush Agrawal, Thomas Gurriet, Sylvain Finet, Guilhem Boeris, Alexis Duburcq, M. Eva Mungai, Matthieu Masselin, Aaron D. Ames, Koushil Sreenath, Jessy Grizzle &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract294');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract294');"><div id="abstract294" style="font-size: 1rem; color:black; display: none">
<u><I> 1802.08322v2 &nbsp; - &nbsp;
{control}
{exoskeleton}
</I></u><br> This manuscript presents control of a high-DOF fully actuated lower-limb
exoskeleton for paraplegic individuals. The key novelty is the ability for the
user to walk without the use of crutches or other external means of
stabilization. We harness the power of modern optimization techniques and
supervised machine learning to develop a smooth feedback control policy that
provides robust velocity regulation and perturbation rejection. Preliminary
evaluation of the stability and robustness of the proposed approach is
demonstrated through the Gazebo simulation environment. In addition,
preliminary experimental results with (complete) paraplegic individuals are
included for the previous version of the controller. <br><br></div></a>
</td></tr>
<tr id=" 295 " class="entry"><td>
<a onclick="toggleVisibility('abstract295');">[|&bull;|]</a><a href="http://arxiv.org/abs/1806.01261v3">
<b/> Relational inductive biases, deep learning, and graph networks (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1806.01261v3">
Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, Razvan Pascanu &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract295');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract295');"><div id="abstract295" style="font-size: 1rem; color:black; display: none">
<u><I> 1806.01261v3 &nbsp; - &nbsp;
{control}
{deep}
</I></u><br> Artificial intelligence (AI) has undergone a renaissance recently, making
major progress in key domains such as vision, language, control, and
decision-making. This has been due, in part, to cheap data and cheap compute
resources, which have fit the natural strengths of deep learning. However, many
defining characteristics of human intelligence, which developed under much
different pressures, remain out of reach for current approaches. In particular,
generalizing beyond one's experiences--a hallmark of human intelligence from
infancy--remains a formidable challenge for modern AI.
The following is part position paper, part review, and part unification. We
argue that combinatorial generalization must be a top priority for AI to
achieve human-like abilities, and that structured representations and
computations are key to realizing this objective. Just as biology uses nature
and nurture cooperatively, we reject the false choice between
"hand-engineering" and "end-to-end" learning, and instead advocate for an
approach which benefits from their complementary strengths. We explore how
using relational inductive biases within deep learning architectures can
facilitate learning about entities, relations, and rules for composing them. We
present a new building block for the AI toolkit with a strong relational
inductive bias--the graph network--which generalizes and extends various
approaches for neural networks that operate on graphs, and provides a
straightforward interface for manipulating structured knowledge and producing
structured behaviors. We discuss how graph networks can support relational
reasoning and combinatorial generalization, laying the foundation for more
sophisticated, interpretable, and flexible patterns of reasoning. As a
companion to this paper, we have released an open-source software library for
building graph networks, with demonstrations of how to use them in practice. <br><br></div></a>
</td></tr>
<tr id=" 296 " class="entry"><td>
<a onclick="toggleVisibility('abstract296');">[|&bull;|]</a><a href="http://arxiv.org/abs/1802.04821v2">
<b/> Evolved Policy Gradients (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1802.04821v2">
Rein Houthooft, Richard Y. Chen, Phillip Isola, Bradly C. Stadie, Filip Wolski, Jonathan Ho, Pieter Abbeel &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract296');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract296');"><div id="abstract296" style="font-size: 1rem; color:black; display: none">
<u><I> 1802.04821v2 &nbsp; - &nbsp;
{gradient}
{meta}
{reinforcement}
</I></u><br> We propose a metalearning approach for learning gradient-based reinforcement
learning (RL) algorithms. The idea is to evolve a differentiable loss function,
such that an agent, which optimizes its policy to minimize this loss, will
achieve high rewards. The loss is parametrized via temporal convolutions over
the agent's experience. Because this loss is highly flexible in its ability to
take into account the agent's history, it enables fast task learning. Empirical
results show that our evolved policy gradient algorithm (EPG) achieves faster
learning on several randomized environments compared to an off-the-shelf policy
gradient method. We also demonstrate that EPG's learned loss can generalize to
out-of-distribution test time tasks, and exhibits qualitatively different
behavior from other popular metalearning algorithms. <br><br></div></a>
</td></tr>
<tr id=" 297 " class="entry"><td>
<a onclick="toggleVisibility('abstract297');">[|&bull;|]</a><a href="http://arxiv.org/abs/1806.07366v5">
<b/> Neural Ordinary Differential Equations (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1806.07366v5">
Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract297');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract297');"><div id="abstract297" style="font-size: 1rem; color:black; display: none">
<u><I> 1806.07366v5 &nbsp; - &nbsp;
{deep}
</I></u><br> We introduce a new family of deep neural network models. Instead of
specifying a discrete sequence of hidden layers, we parameterize the derivative
of the hidden state using a neural network. The output of the network is
computed using a black-box differential equation solver. These continuous-depth
models have constant memory cost, adapt their evaluation strategy to each
input, and can explicitly trade numerical precision for speed. We demonstrate
these properties in continuous-depth residual networks and continuous-time
latent variable models. We also construct continuous normalizing flows, a
generative model that can train by maximum likelihood, without partitioning or
ordering the data dimensions. For training, we show how to scalably
backpropagate through any ODE solver, without access to its internal
operations. This allows end-to-end training of ODEs within larger models. <br><br></div></a>
</td></tr>
<tr id=" 298 " class="entry"><td>
<a onclick="toggleVisibility('abstract298');">[|&bull;|]</a><a href="http://arxiv.org/abs/1812.02900v3">
<b/> Off-Policy Deep Reinforcement Learning without Exploration (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1812.02900v3">
Scott Fujimoto, David Meger, Doina Precup &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract298');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract298');"><div id="abstract298" style="font-size: 1rem; color:black; display: none">
<u><I> 1812.02900v3 &nbsp; - &nbsp;
{control}
{deep}
{off-policy}
{on-policy}
{reinforcement}
</I></u><br> Many practical applications of reinforcement learning constrain agents to
learn from a fixed batch of data which has already been gathered, without
offering further possibility for data collection. In this paper, we demonstrate
that due to errors introduced by extrapolation, standard off-policy deep
reinforcement learning algorithms, such as DQN and DDPG, are incapable of
learning with data uncorrelated to the distribution under the current policy,
making them ineffective for this fixed batch setting. We introduce a novel
class of off-policy algorithms, batch-constrained reinforcement learning, which
restricts the action space in order to force the agent towards behaving close
to on-policy with respect to a subset of the given data. We present the first
continuous control deep reinforcement learning algorithm which can learn
effectively from arbitrary, fixed batch data, and empirically demonstrate the
quality of its behavior in several tasks. <br><br></div></a>
</td></tr>
<tr id=" 299 " class="entry"><td>
<a onclick="toggleVisibility('abstract299');">[|&bull;|]</a><a href="http://arxiv.org/abs/1802.09477v3">
<b/> Addressing Function Approximation Error in Actor-Critic Methods (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1802.09477v3">
Scott Fujimoto, Herke van Hoof, David Meger &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract299');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract299');"><div id="abstract299" style="font-size: 1rem; color:black; display: none">
<u><I> 1802.09477v3 &nbsp; - &nbsp;
{deep}
{optimal}
{reinforcement}
</I></u><br> In value-based reinforcement learning methods such as deep Q-learning,
function approximation errors are known to lead to overestimated value
estimates and suboptimal policies. We show that this problem persists in an
actor-critic setting and propose novel mechanisms to minimize its effects on
both the actor and the critic. Our algorithm builds on Double Q-learning, by
taking the minimum value between a pair of critics to limit overestimation. We
draw the connection between target networks and overestimation bias, and
suggest delaying policy updates to reduce per-update error and further improve
performance. We evaluate our method on the suite of OpenAI gym tasks,
outperforming the state of the art in every environment tested. <br><br></div></a>
</td></tr>
<tr id=" 300 " class="entry"><td>
<a onclick="toggleVisibility('abstract300');">[|&bull;|]</a><a href="http://arxiv.org/abs/1812.08288v3">
<b/> TD-Regularized Actor-Critic Methods (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1812.08288v3">
Simone Parisi, Voot Tangkaratt, Jan Peters, Mohammad Emtiyaz Khan &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract300');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract300');"><div id="abstract300" style="font-size: 1rem; color:black; display: none">
<u><I> 1812.08288v3 &nbsp; - &nbsp;
{reinforcement}
</I></u><br> Actor-critic methods can achieve incredible performance on difficult
reinforcement learning problems, but they are also prone to instability. This
is partly due to the interaction between the actor and critic during learning,
e.g., an inaccurate step taken by one of them might adversely affect the other
and destabilize the learning. To avoid such issues, we propose to regularize
the learning objective of the actor by penalizing the temporal difference (TD)
error of the critic. This improves stability by avoiding large steps in the
actor update whenever the critic is highly inaccurate. The resulting method,
which we call the TD-regularized actor-critic method, is a simple plug-and-play
approach to improve stability and overall performance of the actor-critic
methods. Evaluations on standard benchmarks confirm this. <br><br></div></a>
</td></tr>
<tr id=" 301 " class="entry"><td>
<a onclick="toggleVisibility('abstract301');">[|&bull;|]</a><a href="http://arxiv.org/abs/1810.03237v1">
<b/> Task-Embedded Control Networks for Few-Shot Imitation Learning (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1810.03237v1">
Stephen James, Michael Bloesch, Andrew J. Davison &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract301');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract301');"><div id="abstract301" style="font-size: 1rem; color:black; display: none">
<u><I> 1810.03237v1 &nbsp; - &nbsp;
{control}
{few-shot}
{meta}
{robot}
</I></u><br> Much like humans, robots should have the ability to leverage knowledge from
previously learned tasks in order to learn new tasks quickly in new and
unfamiliar environments. Despite this, most robot learning approaches have
focused on learning a single task, from scratch, with a limited notion of
generalisation, and no way of leveraging the knowledge to learn other tasks
more efficiently. One possible solution is meta-learning, but many of the
related approaches are limited in their ability to scale to a large number of
tasks and to learn further tasks without forgetting previously learned ones.
With this in mind, we introduce Task-Embedded Control Networks, which employ
ideas from metric learning in order to create a task embedding that can be used
by a robot to learn new tasks from one or more demonstrations. In the area of
visually-guided manipulation, we present simulation results in which we surpass
the performance of a state-of-the-art method when using only visual information
from each demonstration. Additionally, we demonstrate that our approach can
also be used in conjunction with domain randomisation to train our few-shot
learning ability in simulation and then deploy in the real world without any
additional training. Once deployed, the robot can learn new tasks from a single
real-world demonstration. <br><br></div></a>
</td></tr>
<tr id=" 302 " class="entry"><td>
<a onclick="toggleVisibility('abstract302');">[|&bull;|]</a><a href="http://arxiv.org/abs/1801.07022v2">
<b/> Capturability-based Pattern Generation for Walking with Variable Height (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1801.07022v2">
St&#233;phane Caron, Adrien Escande, Leonardo Lanari, Bastien Mallein &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract302');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract302');"><div id="abstract302" style="font-size: 1rem; color:black; display: none">
<u><I> 1801.07022v2 &nbsp; - &nbsp;
{control}
</I></u><br> Capturability analysis of the linear inverted pendulum (LIP) model enabled
walking with constrained height based on the capture point. We generalize this
analysis to the variable-height inverted pendulum (VHIP) and show how it
enables 3D walking over uneven terrains based on capture inputs. Thanks to a
tailored optimization scheme, we can compute these inputs fast enough for
real-time model predictive control. We implement this approach as open-source
software and demonstrate it in dynamic simulations. <br><br></div></a>
</td></tr>
<tr id=" 303 " class="entry"><td>
<a onclick="toggleVisibility('abstract303');">[|&bull;|]</a><a href="http://arxiv.org/abs/1805.04874v3">
<b/> GAN Q-learning (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1805.04874v3">
Thang Doan, Bogdan Mazoure, Clare Lyle &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract303');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract303');"><div id="abstract303" style="font-size: 1rem; color:black; display: none">
<u><I> 1805.04874v3 &nbsp; - &nbsp;
{deep}
{reinforcement}
</I></u><br> Distributional reinforcement learning (distributional RL) has seen empirical
success in complex Markov Decision Processes (MDPs) in the setting of nonlinear
function approximation. However, there are many different ways in which one can
leverage the distributional approach to reinforcement learning. In this paper,
we propose GAN Q-learning, a novel distributional RL method based on generative
adversarial networks (GANs) and analyze its performance in simple tabular
environments, as well as OpenAI Gym. We empirically show that our algorithm
leverages the flexibility and blackbox approach of deep learning models while
providing a viable alternative to traditional methods. <br><br></div></a>
</td></tr>
<tr id=" 304 " class="entry"><td>
<a onclick="toggleVisibility('abstract304');">[|&bull;|]</a><a href="http://arxiv.org/abs/1806.03884v2">
<b/> Fast Approximate Natural Gradient Descent in a Kronecker-factored
Eigenbasis (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1806.03884v2">
Thomas George, C&#233;sar Laurent, Xavier Bouthillier, Nicolas Ballas, Pascal Vincent &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract304');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract304');"><div id="abstract304" style="font-size: 1rem; color:black; display: none">
<u><I> 1806.03884v2 &nbsp; - &nbsp;
{deep}
{gradient}
</I></u><br> Optimization algorithms that leverage gradient covariance information, such
as variants of natural gradient descent (Amari, 1998), offer the prospect of
yielding more effective descent directions. For models with many parameters,
the covariance matrix they are based on becomes gigantic, making them
inapplicable in their original form. This has motivated research into both
simple diagonal approximations and more sophisticated factored approximations
such as KFAC (Heskes, 2000; Martens & Grosse, 2015; Grosse & Martens, 2016). In
the present work we draw inspiration from both to propose a novel approximation
that is provably better than KFAC and amendable to cheap partial updates. It
consists in tracking a diagonal variance, not in parameter coordinates, but in
a Kronecker-factored eigenbasis, in which the diagonal approximation is likely
to be more effective. Experiments show improvements over KFAC in optimization
speed for several deep network architectures. <br><br></div></a>
</td></tr>
<tr id=" 305 " class="entry"><td>
<a onclick="toggleVisibility('abstract305');">[|&bull;|]</a><a href="http://arxiv.org/abs/1810.08102v4">
<b/> First-order and second-order variants of the gradient descent in a
unified framework (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1810.08102v4">
Thomas Pierrot, Nicolas Perrin, Olivier Sigaud &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract305');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract305');"><div id="abstract305" style="font-size: 1rem; color:black; display: none">
<u><I> 1810.08102v4 &nbsp; - &nbsp;
{gradient}
</I></u><br> In this paper, we provide an overview of first-order and second-order
variants of the gradient descent method that are commonly used in machine
learning. We propose a general framework in which 6 of these variants can be
interpreted as different instances of the same approach. They are the vanilla
gradient descent, the classical and generalized Gauss-Newton methods, the
natural gradient descent method, the gradient covariance matrix approach, and
Newton's method. Besides interpreting these methods within a single framework,
we explain their specificities and show under which conditions some of them
coincide. <br><br></div></a>
</td></tr>
<tr id=" 306 " class="entry"><td>
<a onclick="toggleVisibility('abstract306');">[|&bull;|]</a><a href="http://arxiv.org/abs/1802.04181v2">
<b/> State Representation Learning for Control: An Overview (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1802.04181v2">
Timoth&#233;e Lesort, Natalia D&#237;az-Rodr&#237;guez, Jean-Fran&#231;ois Goudou, David Filliat &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract306');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract306');"><div id="abstract306" style="font-size: 1rem; color:black; display: none">
<u><I> 1802.04181v2 &nbsp; - &nbsp;
{control}
{reinforcement}
{robot}
</I></u><br> Representation learning algorithms are designed to learn abstract features
that characterize data. State representation learning (SRL) focuses on a
particular kind of representation learning where learned features are in low
dimension, evolve through time, and are influenced by actions of an agent. The
representation is learned to capture the variation in the environment generated
by the agent's actions; this kind of representation is particularly suitable
for robotics and control scenarios. In particular, the low dimension
characteristic of the representation helps to overcome the curse of
dimensionality, provides easier interpretation and utilization by humans and
can help improve performance and speed in policy learning algorithms such as
reinforcement learning.
This survey aims at covering the state-of-the-art on state representation
learning in the most recent years. It reviews different SRL methods that
involve interaction with the environment, their implementations and their
applications in robotics control tasks (simulated or real). In particular, it
highlights how generic learning objectives are differently exploited in the
reviewed algorithms. Finally, it discusses evaluation methods to assess the
representation learned and summarizes current and future lines of research. <br><br></div></a>
</td></tr>
<tr id=" 307 " class="entry"><td>
<a onclick="toggleVisibility('abstract307');">[|&bull;|]</a><a href="http://arxiv.org/abs/1812.06298v2">
<b/> Residual Policy Learning (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1812.06298v2">
Tom Silver, Kelsey Allen, Josh Tenenbaum, Leslie Kaelbling &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract307');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract307');"><div id="abstract307" style="font-size: 1rem; color:black; display: none">
<u><I> 1812.06298v2 &nbsp; - &nbsp;
{control}
{deep}
{reinforcement}
{robot}
{sparse}
</I></u><br> We present Residual Policy Learning (RPL): a simple method for improving
nondifferentiable policies using model-free deep reinforcement learning. RPL
thrives in complex robotic manipulation tasks where good but imperfect
controllers are available. In these tasks, reinforcement learning from scratch
remains data-inefficient or intractable, but learning a residual on top of the
initial controller can yield substantial improvements. We study RPL in six
challenging MuJoCo tasks involving partial observability, sensor noise, model
misspecification, and controller miscalibration. For initial controllers, we
consider both hand-designed policies and model-predictive controllers with
known or learned transition models. By combining learning with control
algorithms, RPL can perform long-horizon, sparse-reward tasks for which
reinforcement learning alone fails. Moreover, we find that RPL consistently and
substantially improves on the initial controllers. We argue that RPL is a
promising approach for combining the complementary strengths of deep
reinforcement learning and robotic control, pushing the boundaries of what
either can achieve independently. Video and code at
https://k-r-allen.github.io/residual-policy-learning/. <br><br></div></a>
</td></tr>
<tr id=" 308 " class="entry"><td>
<a onclick="toggleVisibility('abstract308');">[|&bull;|]</a><a href="http://arxiv.org/abs/1812.05905v2">
<b/> Soft Actor-Critic Algorithms and Applications (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1812.05905v2">
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract308');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract308');"><div id="abstract308" style="font-size: 1rem; color:black; display: none">
<u><I> 1812.05905v2 &nbsp; - &nbsp;
{control}
{deep}
{entropy}
{off-policy}
{on-policy}
{reinforcement}
{robot}
</I></u><br> Model-free deep reinforcement learning (RL) algorithms have been successfully
applied to a range of challenging sequential decision making and control tasks.
However, these methods typically suffer from two major challenges: high sample
complexity and brittleness to hyperparameters. Both of these challenges limit
the applicability of such methods to real-world domains. In this paper, we
describe Soft Actor-Critic (SAC), our recently introduced off-policy
actor-critic algorithm based on the maximum entropy RL framework. In this
framework, the actor aims to simultaneously maximize expected return and
entropy. That is, to succeed at the task while acting as randomly as possible.
We extend SAC to incorporate a number of modifications that accelerate training
and improve stability with respect to the hyperparameters, including a
constrained formulation that automatically tunes the temperature
hyperparameter. We systematically evaluate SAC on a range of benchmark tasks,
as well as real-world challenging tasks such as locomotion for a quadrupedal
robot and robotic manipulation with a dexterous hand. With these improvements,
SAC achieves state-of-the-art performance, outperforming prior on-policy and
off-policy methods in sample-efficiency and asymptotic performance.
Furthermore, we demonstrate that, in contrast to other off-policy algorithms,
our approach is very stable, achieving similar performance across different
random seeds. These results suggest that SAC is a promising candidate for
learning in real-world robotics tasks. <br><br></div></a>
</td></tr>
<tr id=" 309 " class="entry"><td>
<a onclick="toggleVisibility('abstract309');">[|&bull;|]</a><a href="http://arxiv.org/abs/1801.01290v2">
<b/> Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement
Learning with a Stochastic Actor (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1801.01290v2">
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract309');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract309');"><div id="abstract309" style="font-size: 1rem; color:black; display: none">
<u><I> 1801.01290v2 &nbsp; - &nbsp;
{control}
{deep}
{entropy}
{off-policy}
{on-policy}
{reinforcement}
</I></u><br> Model-free deep reinforcement learning (RL) algorithms have been demonstrated
on a range of challenging decision making and control tasks. However, these
methods typically suffer from two major challenges: very high sample complexity
and brittle convergence properties, which necessitate meticulous hyperparameter
tuning. Both of these challenges severely limit the applicability of such
methods to complex, real-world domains. In this paper, we propose soft
actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum
entropy reinforcement learning framework. In this framework, the actor aims to
maximize expected reward while also maximizing entropy. That is, to succeed at
the task while acting as randomly as possible. Prior deep RL methods based on
this framework have been formulated as Q-learning methods. By combining
off-policy updates with a stable stochastic actor-critic formulation, our
method achieves state-of-the-art performance on a range of continuous control
benchmark tasks, outperforming prior on-policy and off-policy methods.
Furthermore, we demonstrate that, in contrast to other off-policy algorithms,
our approach is very stable, achieving very similar performance across
different random seeds. <br><br></div></a>
</td></tr>
<tr id=" 310 " class="entry"><td>
<a onclick="toggleVisibility('abstract310');">[|&bull;|]</a><a href="http://arxiv.org/abs/1804.02808v2">
<b/> Latent Space Policies for Hierarchical Reinforcement Learning (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1804.02808v2">
Tuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract310');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract310');"><div id="abstract310" style="font-size: 1rem; color:black; display: none">
<u><I> 1804.02808v2 &nbsp; - &nbsp;
{control}
{deep}
{entropy}
{hierarchical}
{reinforcement}
{skill}
{sparse}
</I></u><br> We address the problem of learning hierarchical deep neural network policies
for reinforcement learning. In contrast to methods that explicitly restrict or
cripple lower layers of a hierarchy to force them to use higher-level
modulating signals, each layer in our framework is trained to directly solve
the task, but acquires a range of diverse strategies via a maximum entropy
reinforcement learning objective. Each layer is also augmented with latent
random variables, which are sampled from a prior distribution during the
training of that layer. The maximum entropy objective causes these latent
variables to be incorporated into the layer's policy, and the higher level
layer can directly control the behavior of the lower layer through this latent
space. Furthermore, by constraining the mapping from latent variables to
actions to be invertible, higher layers retain full expressivity: neither the
higher layers nor the lower layers are constrained in their behavior. Our
experimental evaluation demonstrates that we can improve on the performance of
single-layer policies on standard benchmark tasks simply by adding additional
layers, and that our method can solve more complex sparse-reward tasks by
learning higher-level policies on top of high-entropy skills optimized for
simple low-level objectives. <br><br></div></a>
</td></tr>
<tr id=" 311 " class="entry"><td>
<a onclick="toggleVisibility('abstract311');">[|&bull;|]</a><a href="http://arxiv.org/abs/1803.06773v1">
<b/> Composable Deep Reinforcement Learning for Robotic Manipulation (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1803.06773v1">
Tuomas Haarnoja, Vitchyr Pong, Aurick Zhou, Murtaza Dalal, Pieter Abbeel, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract311');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract311');"><div id="abstract311" style="font-size: 1rem; color:black; display: none">
<u><I> 1803.06773v1 &nbsp; - &nbsp;
{deep}
{entropy}
{exploration}
{optimal}
{reinforcement}
{robot}
{skill}
</I></u><br> Model-free deep reinforcement learning has been shown to exhibit good
performance in domains ranging from video games to simulated robotic
manipulation and locomotion. However, model-free methods are known to perform
poorly when the interaction time with the environment is limited, as is the
case for most real-world robotic tasks. In this paper, we study how maximum
entropy policies trained using soft Q-learning can be applied to real-world
robotic manipulation. The application of this method to real-world manipulation
is facilitated by two important features of soft Q-learning. First, soft
Q-learning can learn multimodal exploration strategies by learning policies
represented by expressive energy-based models. Second, we show that policies
learned with soft Q-learning can be composed to create new policies, and that
the optimality of the resulting policy can be bounded in terms of the
divergence between the composed policies. This compositionality provides an
especially valuable tool for real-world manipulation, where constructing new
policies by composing existing skills can provide a large gain in efficiency
over training from scratch. Our experimental evaluation demonstrates that soft
Q-learning is substantially more sample efficient than prior model-free deep
reinforcement learning methods, and that compositionality can be performed for
both simulated and real-world tasks. <br><br></div></a>
</td></tr>
<tr id=" 312 " class="entry"><td>
<a onclick="toggleVisibility('abstract312');">[|&bull;|]</a><a href="http://arxiv.org/abs/1804.03906v1">
<b/> Discovering the Elite Hypervolume by Leveraging Interspecies Correlation (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1804.03906v1">
Vassilis Vassiliades, Jean-Baptiste Mouret &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract312');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract312');"><div id="abstract312" style="font-size: 1rem; color:black; display: none">
<u><I> 1804.03906v1 &nbsp; - &nbsp;
{diversity}
{evolution}
{robot}
</I></u><br> Evolution has produced an astonishing diversity of species, each filling a
different niche. Algorithms like MAP-Elites mimic this divergent evolutionary
process to find a set of behaviorally diverse but high-performing solutions,
called the elites. Our key insight is that species in nature often share a
surprisingly large part of their genome, in spite of occupying very different
niches; similarly, the elites are likely to be concentrated in a specific
"elite hypervolume" whose shape is defined by their common features. In this
paper, we first introduce the elite hypervolume concept and propose two metrics
to characterize it: the genotypic spread and the genotypic similarity. We then
introduce a new variation operator, called "directional variation", that
exploits interspecies (or inter-elites) correlations to accelerate the
MAP-Elites algorithm. We demonstrate the effectiveness of this operator in
three problems (a toy function, a redundant robotic arm, and a hexapod robot). <br><br></div></a>
</td></tr>
<tr id=" 313 " class="entry"><td>
<a onclick="toggleVisibility('abstract313');">[|&bull;|]</a><a href="http://arxiv.org/abs/1811.12560v2">
<b/> An Introduction to Deep Reinforcement Learning (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1811.12560v2">
Vincent Francois-Lavet, Peter Henderson, Riashat Islam, Marc G. Bellemare, Joelle Pineau &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract313');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract313');"><div id="abstract313" style="font-size: 1rem; color:black; display: none">
<u><I> 1811.12560v2 &nbsp; - &nbsp;
{deep}
{reinforcement}
{robot}
</I></u><br> Deep reinforcement learning is the combination of reinforcement learning (RL)
and deep learning. This field of research has been able to solve a wide range
of complex decision-making tasks that were previously out of reach for a
machine. Thus, deep RL opens up many new applications in domains such as
healthcare, robotics, smart grids, finance, and many more. This manuscript
provides an introduction to deep reinforcement learning models, algorithms and
techniques. Particular focus is on the aspects related to generalization and
how deep RL can be used for practical applications. We assume the reader is
familiar with basic machine learning concepts. <br><br></div></a>
</td></tr>
<tr id=" 314 " class="entry"><td>
<a onclick="toggleVisibility('abstract314');">[|&bull;|]</a><a href="http://arxiv.org/abs/1801.08093v3">
<b/> Learning Symmetric and Low-energy Locomotion (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1801.08093v3">
Wenhao Yu, Greg Turk, C. Karen Liu &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract314');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract314');"><div id="abstract314" style="font-size: 1rem; color:black; display: none">
<u><I> 1801.08093v3 &nbsp; - &nbsp;
{control}
{deep}
{humanoid}
{reinforcement}
{skill}
</I></u><br> Learning locomotion skills is a challenging problem. To generate realistic
and smooth locomotion, existing methods use motion capture, finite state
machines or morphology-specific knowledge to guide the motion generation
algorithms. Deep reinforcement learning (DRL) is a promising approach for the
automatic creation of locomotion control. Indeed, a standard benchmark for DRL
is to automatically create a running controller for a biped character from a
simple reward function. Although several different DRL algorithms can
successfully create a running controller, the resulting motions usually look
nothing like a real runner. This paper takes a minimalist learning approach to
the locomotion problem, without the use of motion examples, finite state
machines, or morphology-specific knowledge. We introduce two modifications to
the DRL approach that, when used together, produce locomotion behaviors that
are symmetric, low-energy, and much closer to that of a real person. First, we
introduce a new term to the loss function (not the reward function) that
encourages symmetric actions. Second, we introduce a new curriculum learning
method that provides modulated physical assistance to help the character with
left/right balance and forward movement. The algorithm automatically computes
appropriate assistance to the character and gradually relaxes this assistance,
so that eventually the character learns to move entirely without help. Because
our method does not make use of motion capture data, it can be applied to a
variety of character morphologies. We demonstrate locomotion controllers for
the lower half of a biped, a full humanoid, a quadruped, and a hexapod. Our
results show that learned policies are able to produce symmetric, low-energy
gaits. In addition, speed-appropriate gait patterns emerge without any guidance
from motion examples or contact planning. <br><br></div></a>
</td></tr>
<tr id=" 315 " class="entry"><td>
<a onclick="toggleVisibility('abstract315');">[|&bull;|]</a><a href="http://arxiv.org/abs/1806.06923v1">
<b/> Implicit Quantile Networks for Distributional Reinforcement Learning (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1806.06923v1">
Will Dabney, Georg Ostrovski, David Silver, R&#233;mi Munos &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract315');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract315');"><div id="abstract315" style="font-size: 1rem; color:black; display: none">
<u><I> 1806.06923v1 &nbsp; - &nbsp;
{reinforcement}
</I></u><br> In this work, we build on recent advances in distributional reinforcement
learning to give a generally applicable, flexible, and state-of-the-art
distributional variant of DQN. We achieve this by using quantile regression to
approximate the full quantile function for the state-action return
distribution. By reparameterizing a distribution over the sample space, this
yields an implicitly defined return distribution and gives rise to a large
class of risk-sensitive policies. We demonstrate improved performance on the 57
Atari 2600 games in the ALE, and use our algorithm's implicitly defined
distributions to study the effects of risk-sensitive policies in Atari games. <br><br></div></a>
</td></tr>
<tr id=" 316 " class="entry"><td>
<a onclick="toggleVisibility('abstract316');">[|&bull;|]</a><a href="http://arxiv.org/abs/1805.08380v4">
<b/> Optimal transport natural gradient for statistical manifolds with
continuous sample space (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1805.08380v4">
Yifan Chen, Wuchen Li &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract316');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract316');"><div id="abstract316" style="font-size: 1rem; color:black; display: none">
<u><I> 1805.08380v4 &nbsp; - &nbsp;
{gradient}
</I></u><br> We study the Wasserstein natural gradient in parametric statistical models
with continuous sample spaces. Our approach is to pull back the
$L^2$-Wasserstein metric tensor in the probability density space to a parameter
space, equipping the latter with a positive definite metric tensor, under which
it becomes a Riemannian manifold, named the Wasserstein statistical manifold.
In general, it is not a totally geodesic sub-manifold of the density space, and
therefore its geodesics will differ from the Wasserstein geodesics, except for
the well-known Gaussian distribution case, a fact which can also be validated
under our framework. We use the sub-manifold geometry to derive a gradient flow
and natural gradient descent method in the parameter space. When parametrized
densities lie in $\bR$, the induced metric tensor establishes an explicit
formula. In optimization problems, we observe that the natural gradient descent
outperforms the standard gradient descent when the Wasserstein distance is the
objective function. In such a case, we prove that the resulting algorithm
behaves similarly to the Newton method in the asymptotic regime. The proof
calculates the exact Hessian formula for the Wasserstein distance, which
further motivates another preconditioner for the optimization process. To the
end, we present examples to illustrate the effectiveness of the natural
gradient in several parametric statistical models, including the Gaussian
measure, Gaussian mixture, Gamma distribution, and Laplace distribution. <br><br></div></a>
</td></tr>
<tr id=" 317 " class="entry"><td>
<a onclick="toggleVisibility('abstract317');">[|&bull;|]</a><a href="http://arxiv.org/abs/1812.02224v2">
<b/> Adapting Auxiliary Losses Using Gradient Similarity (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1812.02224v2">
Yunshu Du, Wojciech M. Czarnecki, Siddhant M. Jayakumar, Mehrdad Farajtabar, Razvan Pascanu, Balaji Lakshminarayanan &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract317');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract317');"><div id="abstract317" style="font-size: 1rem; color:black; display: none">
<u><I> 1812.02224v2 &nbsp; - &nbsp;
{gradient}
{multi-task}
{reinforcement}
</I></u><br> One approach to deal with the statistical inefficiency of neural networks is
to rely on auxiliary losses that help to build useful representations. However,
it is not always trivial to know if an auxiliary task will be helpful for the
main task and when it could start hurting. We propose to use the cosine
similarity between gradients of tasks as an adaptive weight to detect when an
auxiliary loss is helpful to the main loss. We show that our approach is
guaranteed to converge to critical points of the main task and demonstrate the
practical usefulness of the proposed algorithm in a few domains: multi-task
supervised learning on subsets of ImageNet, reinforcement learning on
gridworld, and reinforcement learning on Atari games. <br><br></div></a>
</td></tr>
<tr id=" 318 " class="entry"><td>
<a onclick="toggleVisibility('abstract318');">[|&bull;|]</a><a href="http://arxiv.org/abs/1810.12894v1">
<b/> Exploration by Random Network Distillation (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1810.12894v1">
Yuri Burda, Harrison Edwards, Amos Storkey, Oleg Klimov &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract318');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract318');"><div id="abstract318" style="font-size: 1rem; color:black; display: none">
<u><I> 1810.12894v1 &nbsp; - &nbsp;
{deep}
{distill}
{exploration}
{intrinsic}
{reinforcement}
</I></u><br> We introduce an exploration bonus for deep reinforcement learning methods
that is easy to implement and adds minimal overhead to the computation
performed. The bonus is the error of a neural network predicting features of
the observations given by a fixed randomly initialized neural network. We also
introduce a method to flexibly combine intrinsic and extrinsic rewards. We find
that the random network distillation (RND) bonus combined with this increased
flexibility enables significant progress on several hard exploration Atari
games. In particular we establish state of the art performance on Montezuma's
Revenge, a game famously difficult for deep reinforcement learning methods. To
the best of our knowledge, this is the first method that achieves better than
average human performance on this game without using demonstrations or having
access to the underlying state of the game, and occasionally completes the
first level. <br><br></div></a>
</td></tr>
<tr id=" 319 " class="entry"><td>
<a onclick="toggleVisibility('abstract319');">[|&bull;|]</a><a href="http://arxiv.org/abs/1801.00690v1">
<b/> DeepMind Control Suite (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1801.00690v1">
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, Martin Riedmiller &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract319');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract319');"><div id="abstract319" style="font-size: 1rem; color:black; display: none">
<u><I> 1801.00690v1 &nbsp; - &nbsp;
{control}
{deep}
{reinforcement}
</I></u><br> The DeepMind Control Suite is a set of continuous control tasks with a
standardised structure and interpretable rewards, intended to serve as
performance benchmarks for reinforcement learning agents. The tasks are written
in Python and powered by the MuJoCo physics engine, making them easy to use and
modify. We include benchmarks for several learning algorithms. The Control
Suite is publicly available at https://www.github.com/deepmind/dm_control . A
video summary of all tasks is available at http://youtu.be/rAai4QzcYbs . <br><br></div></a>
</td></tr>
<tr id=" 320 " class="entry"><td>
<a onclick="toggleVisibility('abstract320');">[|&bull;|]</a><a href="http://arxiv.org/abs/1811.11214v5">
<b/> Understanding the impact of entropy on policy optimization (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1811.11214v5">
Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, Dale Schuurmans &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract320');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract320');"><div id="abstract320" style="font-size: 1rem; color:black; display: none">
<u><I> 1811.11214v5 &nbsp; - &nbsp;
{entropy}
{exploration}
{gradient}
{reinforcement}
</I></u><br> Entropy regularization is commonly used to improve policy optimization in
reinforcement learning. It is believed to help with \emph{exploration} by
encouraging the selection of more stochastic policies. In this work, we analyze
this claim using new visualizations of the optimization landscape based on
randomly perturbing the loss function. We first show that even with access to
the exact gradient, policy optimization is difficult due to the geometry of the
objective function. Then, we qualitatively show that in some environments, a
policy with higher entropy can make the optimization landscape smoother,
thereby connecting local optima and enabling the use of larger learning rates.
This paper presents new tools for understanding the optimization landscape,
shows that policy entropy serves as a regularizer, and highlights the challenge
of designing general-purpose policy optimization algorithms. <br><br></div></a>
</td></tr>
<tr id=" 321 " class="entry"><td>
<a onclick="toggleVisibility('abstract321');">[|&bull;|]</a><a href="http://arxiv.org/abs/1811.06407v2">
<b/> Neural Predictive Belief Representations (2018)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1811.06407v2">
Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Bernardo A. Pires, R&#233;mi Munos &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract321');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract321');"><div id="abstract321" style="font-size: 1rem; color:black; display: none">
<u><I> 1811.06407v2 &nbsp; - &nbsp;
{contrastive}
{optimal}
{unsupervised}
</I></u><br> Unsupervised representation learning has succeeded with excellent results in
many applications. It is an especially powerful tool to learn a good
representation of environments with partial or noisy observations. In partially
observable domains it is important for the representation to encode a belief
state, a sufficient statistic of the observations seen so far. In this paper,
we investigate whether it is possible to learn such a belief representation
using modern neural architectures. Specifically, we focus on one-step frame
prediction and two variants of contrastive predictive coding (CPC) as the
objective functions to learn the representations. To evaluate these learned
representations, we test how well they can predict various pieces of
information about the underlying state of the environment, e.g., position of
the agent in a 3D maze. We show that all three methods are able to learn belief
representations of the environment, they encode not only the state information,
but also its uncertainty, a crucial aspect of belief states. We also find that
for CPC multi-step predictions and action-conditioning are critical for
accurate belief representations in visually complex environments. The ability
of neural representations to capture the belief information has the potential
to spur new advances for learning and planning in partially observable domains,
where leveraging uncertainty is essential for optimal decision making. <br><br></div></a>
</td></tr>
<tr id=" 322 " class="entry"><td>
<a onclick="toggleVisibility('abstract322');">[|&bull;|]</a><a href="http://arxiv.org/abs/1703.02949v1">
<b/> Learning Invariant Feature Spaces to Transfer Skills with Reinforcement
Learning (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1703.02949v1">
Abhishek Gupta, Coline Devin, YuXuan Liu, Pieter Abbeel, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract322');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract322');"><div id="abstract322" style="font-size: 1rem; color:black; display: none">
<u><I> 1703.02949v1 &nbsp; - &nbsp;
{reinforcement}
{robot}
{skill}
{transfer}
</I></u><br> People can learn a wide range of tasks from their own experience, but can
also learn from observing other creatures. This can accelerate acquisition of
new skills even when the observed agent differs substantially from the learning
agent in terms of morphology. In this paper, we examine how reinforcement
learning algorithms can transfer knowledge between morphologically different
agents (e.g., different robots). We introduce a problem formulation where two
agents are tasked with learning multiple skills by sharing information. Our
method uses the skills that were learned by both agents to train invariant
feature spaces that can then be used to transfer other skills from one agent to
another. The process of learning these invariant feature spaces can be viewed
as a kind of "analogy making", or implicit learning of partial correspondences
between two distinct domains. We evaluate our transfer learning algorithm in
two simulated robotic manipulation skills, and illustrate that we can transfer
knowledge between simulated robotic arms with different numbers of links, as
well as simulated arms with different actuation mechanisms, where one robot is
torque-driven while the other is tendon-driven. <br><br></div></a>
</td></tr>
<tr id=" 323 " class="entry"><td>
<a onclick="toggleVisibility('abstract323');">[|&bull;|]</a><a href="http://arxiv.org/abs/1706.03662v2">
<b/> Practical Gauss-Newton Optimisation for Deep Learning (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1706.03662v2">
Aleksandar Botev, Hippolyt Ritter, David Barber &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract323');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract323');"><div id="abstract323" style="font-size: 1rem; color:black; display: none">
<u><I> 1706.03662v2 &nbsp; - &nbsp;
{transfer}
</I></u><br> We present an efficient block-diagonal ap- proximation to the Gauss-Newton
matrix for feedforward neural networks. Our result- ing algorithm is
competitive against state- of-the-art first order optimisation methods, with
sometimes significant improvement in optimisation performance. Unlike
first-order methods, for which hyperparameter tuning of the optimisation
parameters is often a labo- rious process, our approach can provide good
performance even when used with default set- tings. A side result of our work
is that for piecewise linear transfer functions, the net- work objective
function can have no differ- entiable local maxima, which may partially explain
why such transfer functions facilitate effective optimisation. <br><br></div></a>
</td></tr>
<tr id=" 324 " class="entry"><td>
<a onclick="toggleVisibility('abstract324');">[|&bull;|]</a><a href="http://arxiv.org/abs/1712.00948v5">
<b/> Learning Multi-Level Hierarchies with Hindsight (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1712.00948v5">
Andrew Levy, George Konidaris, Robert Platt, Kate Saenko &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract324');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract324');"><div id="abstract324" style="font-size: 1rem; color:black; display: none">
<u><I> 1712.00948v5 &nbsp; - &nbsp;
{hierarchical}
{optimal}
{reinforcement}
{robot}
</I></u><br> Hierarchical agents have the potential to solve sequential decision making
tasks with greater sample efficiency than their non-hierarchical counterparts
because hierarchical agents can break down tasks into sets of subtasks that
only require short sequences of decisions. In order to realize this potential
of faster learning, hierarchical agents need to be able to learn their multiple
levels of policies in parallel so these simpler subproblems can be solved
simultaneously. Yet, learning multiple levels of policies in parallel is hard
because it is inherently unstable: changes in a policy at one level of the
hierarchy may cause changes in the transition and reward functions at higher
levels in the hierarchy, making it difficult to jointly learn multiple levels
of policies. In this paper, we introduce a new Hierarchical Reinforcement
Learning (HRL) framework, Hierarchical Actor-Critic (HAC), that can overcome
the instability issues that arise when agents try to jointly learn multiple
levels of policies. The main idea behind HAC is to train each level of the
hierarchy independently of the lower levels by training each level as if the
lower level policies are already optimal. We demonstrate experimentally in both
grid world and simulated robotics domains that our approach can significantly
accelerate learning relative to other non-hierarchical and hierarchical
methods. Indeed, our framework is the first to successfully learn 3-level
hierarchies in parallel in tasks with continuous state and action spaces. <br><br></div></a>
</td></tr>
<tr id=" 325 " class="entry"><td>
<a onclick="toggleVisibility('abstract325');">[|&bull;|]</a><a href="http://arxiv.org/abs/1708.09251v1">
<b/> Quality and Diversity Optimization: A Unifying Modular Framework (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1708.09251v1">
Antoine Cully, Yiannis Demiris &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract325');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract325');"><div id="abstract325" style="font-size: 1rem; color:black; display: none">
<u><I> 1708.09251v1 &nbsp; - &nbsp;
{diversity}
</I></u><br> The optimization of functions to find the best solution according to one or
several objectives has a central role in many engineering and research fields.
Recently, a new family of optimization algorithms, named Quality-Diversity
optimization, has been introduced, and contrasts with classic algorithms.
Instead of searching for a single solution, Quality-Diversity algorithms are
searching for a large collection of both diverse and high-performing solutions.
The role of this collection is to cover the range of possible solution types as
much as possible, and to contain the best solution for each type. The
contribution of this paper is threefold. Firstly, we present a unifying
framework of Quality-Diversity optimization algorithms that covers the two main
algorithms of this family (Multi-dimensional Archive of Phenotypic Elites and
the Novelty Search with Local Competition), and that highlights the large
variety of variants that can be investigated within this family. Secondly, we
propose algorithms with a new selection mechanism for Quality-Diversity
algorithms that outperforms all the algorithms tested in this paper. Lastly, we
present a new collection management that overcomes the erosion issues observed
when using unstructured collections. These three contributions are supported by
extensive experimental comparisons of Quality-Diversity algorithms on three
different experimental scenarios. <br><br></div></a>
</td></tr>
<tr id=" 326 " class="entry"><td>
<a onclick="toggleVisibility('abstract326');">[|&bull;|]</a><a href="http://arxiv.org/abs/1711.08946v2">
<b/> Action Branching Architectures for Deep Reinforcement Learning (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1711.08946v2">
Arash Tavakoli, Fabio Pardo, Petar Kormushev &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract326');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract326');"><div id="abstract326" style="font-size: 1rem; color:black; display: none">
<u><I> 1711.08946v2 &nbsp; - &nbsp;
{control}
{deep}
{gradient}
{reinforcement}
</I></u><br> Discrete-action algorithms have been central to numerous recent successes of
deep reinforcement learning. However, applying these algorithms to
high-dimensional action tasks requires tackling the combinatorial increase of
the number of possible actions with the number of action dimensions. This
problem is further exacerbated for continuous-action tasks that require fine
control of actions via discretization. In this paper, we propose a novel neural
architecture featuring a shared decision module followed by several network
branches, one for each action dimension. This approach achieves a linear
increase of the number of network outputs with the number of degrees of freedom
by allowing a level of independence for each individual action dimension. To
illustrate the approach, we present a novel agent, called Branching Dueling
Q-Network (BDQ), as a branching variant of the Dueling Double Deep Q-Network
(Dueling DDQN). We evaluate the performance of our agent on a set of
challenging continuous control tasks. The empirical results show that the
proposed agent scales gracefully to environments with increasing action
dimensionality and indicate the significance of the shared decision module in
coordination of the distributed action branches. Furthermore, we show that the
proposed agent performs competitively against a state-of-the-art continuous
control algorithm, Deep Deterministic Policy Gradient (DDPG). <br><br></div></a>
</td></tr>
<tr id=" 327 " class="entry"><td>
<a onclick="toggleVisibility('abstract327');">[|&bull;|]</a><a href="http://arxiv.org/abs/1703.02660v2">
<b/> Towards Generalization and Simplicity in Continuous Control (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1703.02660v2">
Aravind Rajeswaran, Kendall Lowrey, Emanuel Todorov, Sham Kakade &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract327');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract327');"><div id="abstract327" style="font-size: 1rem; color:black; display: none">
<u><I> 1703.02660v2 &nbsp; - &nbsp;
{control}
</I></u><br> This work shows that policies with simple linear and RBF parameterizations
can be trained to solve a variety of continuous control tasks, including the
OpenAI gym benchmarks. The performance of these trained policies are
competitive with state of the art results, obtained with more elaborate
parameterizations such as fully connected neural networks. Furthermore,
existing training and testing scenarios are shown to be very limited and prone
to over-fitting, thus giving rise to only trajectory-centric policies. Training
with a diverse initial state distribution is shown to produce more global
policies with better generalization. This allows for interactive control
scenarios where the system recovers from large on-line perturbations; as shown
in the supplementary video. <br><br></div></a>
</td></tr>
<tr id=" 328 " class="entry"><td>
<a onclick="toggleVisibility('abstract328');">[|&bull;|]</a><a href="http://arxiv.org/abs/1709.10087v2">
<b/> Learning Complex Dexterous Manipulation with Deep Reinforcement Learning
and Demonstrations (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1709.10087v2">
Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract328');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract328');"><div id="abstract328" style="font-size: 1rem; color:black; display: none">
<u><I> 1709.10087v2 &nbsp; - &nbsp;
{control}
{deep}
{reinforcement}
{robot}
</I></u><br> Dexterous multi-fingered hands are extremely versatile and provide a generic
way to perform a multitude of tasks in human-centric environments. However,
effectively controlling them remains challenging due to their high
dimensionality and large number of potential contacts. Deep reinforcement
learning (DRL) provides a model-agnostic approach to control complex dynamical
systems, but has not been shown to scale to high-dimensional dexterous
manipulation. Furthermore, deployment of DRL on physical systems remains
challenging due to sample inefficiency. Consequently, the success of DRL in
robotics has thus far been limited to simpler manipulators and tasks. In this
work, we show that model-free DRL can effectively scale up to complex
manipulation tasks with a high-dimensional 24-DoF hand, and solve them from
scratch in simulated experiments. Furthermore, with the use of a small number
of human demonstrations, the sample complexity can be significantly reduced,
which enables learning with sample sizes equivalent to a few hours of robot
experience. The use of demonstrations result in policies that exhibit very
natural movements and, surprisingly, are also substantially more robust. <br><br></div></a>
</td></tr>
<tr id=" 329 " class="entry"><td>
<a onclick="toggleVisibility('abstract329');">[|&bull;|]</a><a href="http://arxiv.org/abs/1706.03762v7">
<b/> Attention Is All You Need (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1706.03762v7">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract329');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract329');"><div id="abstract329" style="font-size: 1rem; color:black; display: none">
<u><I> 1706.03762v7 &nbsp; - &nbsp;
</I></u><br> The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks in an encoder-decoder configuration. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer, based
solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to be
superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014
English-to-German translation task, improving over the existing best results,
including ensembles by over 2 BLEU. On the WMT 2014 English-to-French
translation task, our model establishes a new single-model state-of-the-art
BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction
of the training costs of the best models from the literature. We show that the
Transformer generalizes well to other tasks by applying it successfully to
English constituency parsing both with large and limited training data. <br><br></div></a>
</td></tr>
<tr id=" 330 " class="entry"><td>
<a onclick="toggleVisibility('abstract330');">[|&bull;|]</a><a href="http://arxiv.org/abs/1709.10089v2">
<b/> Overcoming Exploration in Reinforcement Learning with Demonstrations (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1709.10089v2">
Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, Pieter Abbeel &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract330');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract330');"><div id="abstract330" style="font-size: 1rem; color:black; display: none">
<u><I> 1709.10089v2 &nbsp; - &nbsp;
{control}
{deep}
{exploration}
{gradient}
{optimal}
{reinforcement}
{replay}
{robot}
{sparse}
</I></u><br> Exploration in environments with sparse rewards has been a persistent problem
in reinforcement learning (RL). Many tasks are natural to specify with a sparse
reward, and manually shaping a reward function can result in suboptimal
performance. However, finding a non-zero reward is exponentially more difficult
with increasing task horizon or action dimensionality. This puts many
real-world tasks out of practical reach of RL methods. In this work, we use
demonstrations to overcome the exploration problem and successfully learn to
perform long-horizon, multi-step robotics tasks with continuous control such as
stacking blocks with a robot arm. Our method, which builds on top of Deep
Deterministic Policy Gradients and Hindsight Experience Replay, provides an
order of magnitude of speedup over RL on simulated robotics tasks. It is simple
to implement and makes only the additional assumption that we can collect a
small set of demonstrations. Furthermore, our method is able to solve tasks not
solvable by either RL or behavior cloning alone, and often ends up
outperforming the demonstrator policy. <br><br></div></a>
</td></tr>
<tr id=" 331 " class="entry"><td>
<a onclick="toggleVisibility('abstract331');">[|&bull;|]</a><a href="http://arxiv.org/abs/1707.05300v3">
<b/> Reverse Curriculum Generation for Reinforcement Learning (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1707.05300v3">
Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, Pieter Abbeel &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract331');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract331');"><div id="abstract331" style="font-size: 1rem; color:black; display: none">
<u><I> 1707.05300v3 &nbsp; - &nbsp;
{exploration}
{reinforcement}
{robot}
{sparse}
</I></u><br> Many relevant tasks require an agent to reach a certain state, or to
manipulate objects into a desired configuration. For example, we might want a
robot to align and assemble a gear onto an axle or insert and turn a key in a
lock. These goal-oriented tasks present a considerable challenge for
reinforcement learning, since their natural reward function is sparse and
prohibitive amounts of exploration are required to reach the goal and receive
some learning signal. Past approaches tackle these problems by exploiting
expert demonstrations or by manually designing a task-specific reward shaping
function to guide the learning agent. Instead, we propose a method to learn
these tasks without requiring any prior knowledge other than obtaining a single
state in which the task is achieved. The robot is trained in reverse, gradually
learning to reach the goal from a set of start states increasingly far from the
goal. Our method automatically generates a curriculum of start states that
adapts to the agent's performance, leading to efficient training on
goal-oriented tasks. We demonstrate our approach on difficult simulated
navigation and fine-grained manipulation problems, not solvable by
state-of-the-art reinforcement learning methods. <br><br></div></a>
</td></tr>
<tr id=" 332 " class="entry"><td>
<a onclick="toggleVisibility('abstract332');">[|&bull;|]</a><a href="http://arxiv.org/abs/1705.06366v5">
<b/> Automatic Goal Generation for Reinforcement Learning Agents (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1705.06366v5">
Carlos Florensa, David Held, Xinyang Geng, Pieter Abbeel &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract332');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract332');"><div id="abstract332" style="font-size: 1rem; color:black; display: none">
<u><I> 1705.06366v5 &nbsp; - &nbsp;
{reinforcement}
{sparse}
</I></u><br> Reinforcement learning is a powerful technique to train an agent to perform a
task. However, an agent that is trained using reinforcement learning is only
capable of achieving the single task that is specified via its reward function.
Such an approach does not scale well to settings in which an agent needs to
perform a diverse set of tasks, such as navigating to varying positions in a
room or moving objects to varying locations. Instead, we propose a method that
allows an agent to automatically discover the range of tasks that it is capable
of performing. We use a generator network to propose tasks for the agent to try
to achieve, specified as goal states. The generator network is optimized using
adversarial training to produce tasks that are always at the appropriate level
of difficulty for the agent. Our method thus automatically produces a
curriculum of tasks for the agent to learn. We show that, by using this
framework, an agent can efficiently and automatically learn to perform a wide
set of tasks without requiring any prior knowledge of its environment. Our
method can also learn to achieve tasks with sparse rewards, which traditionally
pose significant challenges. <br><br></div></a>
</td></tr>
<tr id=" 333 " class="entry"><td>
<a onclick="toggleVisibility('abstract333');">[|&bull;|]</a><a href="http://arxiv.org/abs/1709.02878v2">
<b/> TensorFlow Agents: Efficient Batched Reinforcement Learning in
TensorFlow (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1709.02878v2">
Danijar Hafner, James Davidson, Vincent Vanhoucke &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract333');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract333');"><div id="abstract333" style="font-size: 1rem; color:black; display: none">
<u><I> 1709.02878v2 &nbsp; - &nbsp;
{reinforcement}
</I></u><br> We introduce TensorFlow Agents, an efficient infrastructure paradigm for
building parallel reinforcement learning algorithms in TensorFlow. We simulate
multiple environments in parallel, and group them to perform the neural network
computation on a batch rather than individual observations. This allows the
TensorFlow execution engine to parallelize computation, without the need for
manual synchronization. Environments are stepped in separate Python processes
to progress them in parallel without interference of the global interpreter
lock. As part of this project, we introduce BatchPPO, an efficient
implementation of the proximal policy optimization algorithm. By open sourcing
TensorFlow Agents, we hope to provide a flexible starting point for future
projects that accelerates future research in the field. <br><br></div></a>
</td></tr>
<tr id=" 334 " class="entry"><td>
<a onclick="toggleVisibility('abstract334');">[|&bull;|]</a><a href="http://arxiv.org/abs/1712.06560v3">
<b/> Improving Exploration in Evolution Strategies for Deep Reinforcement
Learning via a Population of Novelty-Seeking Agents (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1712.06560v3">
Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth O. Stanley, Jeff Clune &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract334');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract334');"><div id="abstract334" style="font-size: 1rem; color:black; display: none">
<u><I> 1712.06560v3 &nbsp; - &nbsp;
{deep}
{diversity}
{evolution}
{exploration}
{gradient}
{population}
{reinforcement}
{robot}
{sparse}
</I></u><br> Evolution strategies (ES) are a family of black-box optimization algorithms
able to train deep neural networks roughly as well as Q-learning and policy
gradient methods on challenging deep reinforcement learning (RL) problems, but
are much faster (e.g. hours vs. days) because they parallelize better. However,
many RL problems require directed exploration because they have reward
functions that are sparse or deceptive (i.e. contain local optima), and it is
unknown how to encourage such exploration with ES. Here we show that algorithms
that have been invented to promote directed exploration in small-scale evolved
neural networks via populations of exploring agents, specifically novelty
search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to
improve its performance on sparse or deceptive deep RL tasks, while retaining
scalability. Our experiments confirm that the resultant new algorithms, NS-ES
and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES
to achieve higher performance on Atari and simulated robots learning to walk
around a deceptive trap. This paper thus introduces a family of fast, scalable
algorithms for reinforcement learning that are capable of directed exploration.
It also adds this new family of exploration algorithms to the RL toolbox and
raises the interesting possibility that analogous algorithms with multiple
simultaneous paths of exploration might also combine well with existing RL
algorithms outside ES. <br><br></div></a>
</td></tr>
<tr id=" 335 " class="entry"><td>
<a onclick="toggleVisibility('abstract335');">[|&bull;|]</a><a href="http://arxiv.org/abs/1702.01182v1">
<b/> Uncertainty-Aware Reinforcement Learning for Collision Avoidance (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1702.01182v1">
Gregory Kahn, Adam Villaflor, Vitchyr Pong, Pieter Abbeel, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract335');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract335');"><div id="abstract335" style="font-size: 1rem; color:black; display: none">
<u><I> 1702.01182v1 &nbsp; - &nbsp;
{model-based}
{reinforcement}
{robot}
</I></u><br> Reinforcement learning can enable complex, adaptive behavior to be learned
automatically for autonomous robotic platforms. However, practical deployment
of reinforcement learning methods must contend with the fact that the training
process itself can be unsafe for the robot. In this paper, we consider the
specific case of a mobile robot learning to navigate an a priori unknown
environment while avoiding collisions. In order to learn collision avoidance,
the robot must experience collisions at training time. However, high-speed
collisions, even at training time, could damage the robot. A successful
learning method must therefore proceed cautiously, experiencing only low-speed
collisions until it gains confidence. To this end, we present an
uncertainty-aware model-based learning algorithm that estimates the probability
of collision together with a statistical estimate of uncertainty. By
formulating an uncertainty-dependent cost function, we show that the algorithm
naturally chooses to proceed cautiously in unfamiliar environments, and
increases the velocity of the robot in settings where it has high confidence.
Our predictive model is based on bootstrapped neural networks using dropout,
allowing it to process raw sensory inputs from high-bandwidth sensors such as
cameras. Our experimental evaluation demonstrates that our method effectively
minimizes dangerous collisions at training time in an obstacle avoidance task
for a simulated and real-world quadrotor, and a real-world RC car. Videos of
the experiments can be found at https://sites.google.com/site/probcoll. <br><br></div></a>
</td></tr>
<tr id=" 336 " class="entry"><td>
<a onclick="toggleVisibility('abstract336');">[|&bull;|]</a><a href="http://arxiv.org/abs/1712.09913v3">
<b/> Visualizing the Loss Landscape of Neural Nets (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1712.09913v3">
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, Tom Goldstein &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract336');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract336');"><div id="abstract336" style="font-size: 1rem; color:black; display: none">
<u><I> 1712.09913v3 &nbsp; - &nbsp;
</I></u><br> Neural network training relies on our ability to find "good" minimizers of
highly non-convex loss functions. It is well-known that certain network
architecture designs (e.g., skip connections) produce loss functions that train
easier, and well-chosen training parameters (batch size, learning rate,
optimizer) produce minimizers that generalize better. However, the reasons for
these differences, and their effects on the underlying loss landscape, are not
well understood. In this paper, we explore the structure of neural loss
functions, and the effect of loss landscapes on generalization, using a range
of visualization methods. First, we introduce a simple "filter normalization"
method that helps us visualize loss function curvature and make meaningful
side-by-side comparisons between loss functions. Then, using a variety of
visualizations, we explore how network architecture affects the loss landscape,
and how training parameters affect the shape of minimizers. <br><br></div></a>
</td></tr>
<tr id=" 337 " class="entry"><td>
<a onclick="toggleVisibility('abstract337');">[|&bull;|]</a><a href="http://arxiv.org/abs/1706.04223v3">
<b/> Adversarially Regularized Autoencoders (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1706.04223v3">
Jake Zhao, Yoon Kim, Kelly Zhang, Alexander M. Rush, Yann LeCun &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract337');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract337');"><div id="abstract337" style="font-size: 1rem; color:black; display: none">
<u><I> 1706.04223v3 &nbsp; - &nbsp;
{control}
{deep}
{optimal}
{transfer}
</I></u><br> Deep latent variable models, trained using variational autoencoders or
generative adversarial networks, are now a key technique for representation
learning of continuous structures. However, applying similar methods to
discrete structures, such as text sequences or discretized images, has proven
to be more challenging. In this work, we propose a flexible method for training
deep latent variable models of discrete structures. Our approach is based on
the recently-proposed Wasserstein autoencoder (WAE) which formalizes the
adversarial autoencoder (AAE) as an optimal transport problem. We first extend
this framework to model discrete sequences, and then further explore different
learned priors targeting a controllable representation. This adversarially
regularized autoencoder (ARAE) allows us to generate natural textual outputs as
well as perform manipulations in the latent space to induce change in the
output space. Finally we show that the latent representation can be trained to
perform unaligned textual style transfer, giving improvements both in
automatic/human evaluation compared to existing methods. <br><br></div></a>
</td></tr>
<tr id=" 338 " class="entry"><td>
<a onclick="toggleVisibility('abstract338');">[|&bull;|]</a><a href="http://arxiv.org/abs/1703.09312v3">
<b/> Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point
Clouds and Analytic Grasp Metrics (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1703.09312v3">
Jeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea, Ken Goldberg &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract338');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract338');"><div id="abstract338" style="font-size: 1rem; color:black; display: none">
<u><I> 1703.09312v3 &nbsp; - &nbsp;
{deep}
{robot}
</I></u><br> To reduce data collection time for deep learning of robust robotic grasp
plans, we explore training from a synthetic dataset of 6.7 million point
clouds, grasps, and analytic grasp metrics generated from thousands of 3D
models from Dex-Net 1.0 in randomized poses on a table. We use the resulting
dataset, Dex-Net 2.0, to train a Grasp Quality Convolutional Neural Network
(GQ-CNN) model that rapidly predicts the probability of success of grasps from
depth images, where grasps are specified as the planar position, angle, and
depth of a gripper relative to an RGB-D sensor. Experiments with over 1,000
trials on an ABB YuMi comparing grasp planning methods on singulated objects
suggest that a GQ-CNN trained with only synthetic data from Dex-Net 2.0 can be
used to plan grasps in 0.8sec with a success rate of 93% on eight known objects
with adversarial geometry and is 3x faster than registering point clouds to a
precomputed dataset of objects and indexing grasps. The Dex-Net 2.0 grasp
planner also has the highest success rate on a dataset of 10 novel rigid
objects and achieves 99% precision (one false positive out of 69 grasps
classified as robust) on a dataset of 40 novel household objects, some of which
are articulated or deformable. Code, datasets, videos, and supplementary
material are available at http://berkeleyautomation.github.io/dex-net . <br><br></div></a>
</td></tr>
<tr id=" 339 " class="entry"><td>
<a onclick="toggleVisibility('abstract339');">[|&bull;|]</a><a href="http://arxiv.org/abs/1712.06563v3">
<b/> Safe Mutations for Deep and Recurrent Neural Networks through Output
Gradients (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1712.06563v3">
Joel Lehman, Jay Chen, Jeff Clune, Kenneth O. Stanley &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract339');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract339');"><div id="abstract339" style="font-size: 1rem; color:black; display: none">
<u><I> 1712.06563v3 &nbsp; - &nbsp;
{deep}
{evolution}
{exploration}
{gradient}
{reinforcement}
</I></u><br> While neuroevolution (evolving neural networks) has a successful track record
across a variety of domains from reinforcement learning to artificial life, it
is rarely applied to large, deep neural networks. A central reason is that
while random mutation generally works in low dimensions, a random perturbation
of thousands or millions of weights is likely to break existing functionality,
providing no learning signal even if some individual weight changes were
beneficial. This paper proposes a solution by introducing a family of safe
mutation (SM) operators that aim within the mutation operator itself to find a
degree of change that does not alter network behavior too much, but still
facilitates exploration. Importantly, these SM operators do not require any
additional interactions with the environment. The most effective SM variant
capitalizes on the intriguing opportunity to scale the degree of mutation of
each individual weight according to the sensitivity of the network's outputs to
that weight, which requires computing the gradient of outputs with respect to
the weights (instead of the gradient of error, as in conventional deep
learning). This safe mutation through gradients (SM-G) operator dramatically
increases the ability of a simple genetic algorithm-based neuroevolution method
to find solutions in high-dimensional domains that require deep and/or
recurrent neural networks (which tend to be particularly brittle to mutation),
including domains that require processing raw pixels. By improving our ability
to evolve deep neural networks, this new safer approach to mutation expands the
scope of domains amenable to neuroevolution. <br><br></div></a>
</td></tr>
<tr id=" 340 " class="entry"><td>
<a onclick="toggleVisibility('abstract340');">[|&bull;|]</a><a href="http://arxiv.org/abs/1707.06347v2">
<b/> Proximal Policy Optimization Algorithms (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1707.06347v2">
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract340');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract340');"><div id="abstract340" style="font-size: 1rem; color:black; display: none">
<u><I> 1707.06347v2 &nbsp; - &nbsp;
{gradient}
{on-policy}
{reinforcement}
{robot}
</I></u><br> We propose a new family of policy gradient methods for reinforcement
learning, which alternate between sampling data through interaction with the
environment, and optimizing a "surrogate" objective function using stochastic
gradient ascent. Whereas standard policy gradient methods perform one gradient
update per data sample, we propose a novel objective function that enables
multiple epochs of minibatch updates. The new methods, which we call proximal
policy optimization (PPO), have some of the benefits of trust region policy
optimization (TRPO), but they are much simpler to implement, more general, and
have better sample complexity (empirically). Our experiments test PPO on a
collection of benchmark tasks, including simulated robotic locomotion and Atari
game playing, and we show that PPO outperforms other online policy gradient
methods, and overall strikes a favorable balance between sample complexity,
simplicity, and wall-time. <br><br></div></a>
</td></tr>
<tr id=" 341 " class="entry"><td>
<a onclick="toggleVisibility('abstract341');">[|&bull;|]</a><a href="http://arxiv.org/abs/1708.05866v2">
<b/> A Brief Survey of Deep Reinforcement Learning (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1708.05866v2">
Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony Bharath &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract341');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract341');"><div id="abstract341" style="font-size: 1rem; color:black; display: none">
<u><I> 1708.05866v2 &nbsp; - &nbsp;
{control}
{deep}
{evolution}
{on-policy}
{reinforcement}
{robot}
</I></u><br> Deep reinforcement learning is poised to revolutionise the field of AI and
represents a step towards building autonomous systems with a higher level
understanding of the visual world. Currently, deep learning is enabling
reinforcement learning to scale to problems that were previously intractable,
such as learning to play video games directly from pixels. Deep reinforcement
learning algorithms are also applied to robotics, allowing control policies for
robots to be learned directly from camera inputs in the real world. In this
survey, we begin with an introduction to the general field of reinforcement
learning, then progress to the main streams of value-based and policy-based
methods. Our survey will cover central algorithms in deep reinforcement
learning, including the deep $Q$-network, trust region policy optimisation, and
asynchronous advantage actor-critic. In parallel, we highlight the unique
advantages of deep neural networks, focusing on visual understanding via
reinforcement learning. To conclude, we describe several current areas of
research within the field. <br><br></div></a>
</td></tr>
<tr id=" 342 " class="entry"><td>
<a onclick="toggleVisibility('abstract342');">[|&bull;|]</a><a href="http://arxiv.org/abs/1703.04933v2">
<b/> Sharp Minima Can Generalize For Deep Nets (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1703.04933v2">
Laurent Dinh, Razvan Pascanu, Samy Bengio, Yoshua Bengio &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract342');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract342');"><div id="abstract342" style="font-size: 1rem; color:black; display: none">
<u><I> 1703.04933v2 &nbsp; - &nbsp;
{deep}
{gradient}
</I></u><br> Despite their overwhelming capacity to overfit, deep learning architectures
tend to generalize relatively well to unseen data, allowing them to be deployed
in practice. However, explaining why this is the case is still an open area of
research. One standing hypothesis that is gaining popularity, e.g. Hochreiter &
Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the
loss function found by stochastic gradient based methods results in good
generalization. This paper argues that most notions of flatness are problematic
for deep models and can not be directly applied to explain generalization.
Specifically, when focusing on deep networks with rectifier units, we can
exploit the particular geometry of parameter space induced by the inherent
symmetries that these architectures exhibit to build equivalent models
corresponding to arbitrarily sharper minima. Furthermore, if we allow to
reparametrize a function, the geometry of its parameters can change drastically
without affecting its generalization properties. <br><br></div></a>
</td></tr>
<tr id=" 343 " class="entry"><td>
<a onclick="toggleVisibility('abstract343');">[|&bull;|]</a><a href="http://arxiv.org/abs/1705.05035v3">
<b/> Discrete Sequential Prediction of Continuous Actions for Deep RL (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1705.05035v3">
Luke Metz, Julian Ibarz, Navdeep Jaitly, James Davidson &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract343');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract343');"><div id="abstract343" style="font-size: 1rem; color:black; display: none">
<u><I> 1705.05035v3 &nbsp; - &nbsp;
{control}
{off-policy}
</I></u><br> It has long been assumed that high dimensional continuous control problems
cannot be solved effectively by discretizing individual dimensions of the
action space due to the exponentially large number of bins over which policies
would have to be learned. In this paper, we draw inspiration from the recent
success of sequence-to-sequence models for structured prediction problems to
develop policies over discretized spaces. Central to this method is the
realization that complex functions over high dimensional spaces can be modeled
by neural networks that predict one dimension at a time. Specifically, we show
how Q-values and policies over continuous spaces can be modeled using a next
step prediction model over discretized dimensions. With this parameterization,
it is possible to both leverage the compositional structure of action spaces
during learning, as well as compute maxima over action spaces (approximately).
On a simple example task we demonstrate empirically that our method can perform
global search, which effectively gets around the local optimization issues that
plague DDPG. We apply the technique to off-policy (Q-learning) methods and show
that our method can achieve the state-of-the-art for off-policy methods on
several continuous control tasks. <br><br></div></a>
</td></tr>
<tr id=" 344 " class="entry"><td>
<a onclick="toggleVisibility('abstract344');">[|&bull;|]</a><a href="http://arxiv.org/abs/1705.10743v1">
<b/> The Cramer Distance as a Solution to Biased Wasserstein Gradients (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1705.10743v1">
Marc G. Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan, Stephan Hoyer, R&#233;mi Munos &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract344');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract344');"><div id="abstract344" style="font-size: 1rem; color:black; display: none">
<u><I> 1705.10743v1 &nbsp; - &nbsp;
{gradient}
</I></u><br> The Wasserstein probability metric has received much attention from the
machine learning community. Unlike the Kullback-Leibler divergence, which
strictly measures change in probability, the Wasserstein metric reflects the
underlying geometry between outcomes. The value of being sensitive to this
geometry has been demonstrated, among others, in ordinal regression and
generative modelling. In this paper we describe three natural properties of
probability divergences that reflect requirements from machine learning: sum
invariance, scale sensitivity, and unbiased sample gradients. The Wasserstein
metric possesses the first two properties but, unlike the Kullback-Leibler
divergence, does not possess the third. We provide empirical evidence
suggesting that this is a serious issue in practice. Leveraging insights from
probabilistic forecasting we propose an alternative to the Wasserstein metric,
the Cram\'er distance. We show that the Cram\'er distance possesses all three
desired properties, combining the best of the Wasserstein and Kullback-Leibler
divergences. To illustrate the relevance of the Cram\'er distance in practice
we design a new algorithm, the Cram\'er Generative Adversarial Network (GAN),
and show that it performs significantly better than the related Wasserstein
GAN. <br><br></div></a>
</td></tr>
<tr id=" 345 " class="entry"><td>
<a onclick="toggleVisibility('abstract345');">[|&bull;|]</a><a href="http://arxiv.org/abs/1707.06887v1">
<b/> A Distributional Perspective on Reinforcement Learning (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1707.06887v1">
Marc G. Bellemare, Will Dabney, R&#233;mi Munos &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract345');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract345');"><div id="abstract345" style="font-size: 1rem; color:black; display: none">
<u><I> 1707.06887v1 &nbsp; - &nbsp;
{control}
{reinforcement}
</I></u><br> In this paper we argue for the fundamental importance of the value
distribution: the distribution of the random return received by a reinforcement
learning agent. This is in contrast to the common approach to reinforcement
learning which models the expectation of this return, or value. Although there
is an established body of literature studying the value distribution, thus far
it has always been used for a specific purpose such as implementing risk-aware
behaviour. We begin with theoretical results in both the policy evaluation and
control settings, exposing a significant distributional instability in the
latter. We then use the distributional perspective to design a new algorithm
which applies Bellman's equation to the learning of approximate value
distributions. We evaluate our algorithm using the suite of games from the
Arcade Learning Environment. We obtain both state-of-the-art results and
anecdotal evidence demonstrating the importance of the value distribution in
approximate reinforcement learning. Finally, we combine theoretical and
empirical evidence to highlight the ways in which the value distribution
impacts learning in the approximate setting. <br><br></div></a>
</td></tr>
<tr id=" 346 " class="entry"><td>
<a onclick="toggleVisibility('abstract346');">[|&bull;|]</a><a href="http://arxiv.org/abs/1707.01495v3">
<b/> Hindsight Experience Replay (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1707.01495v3">
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, Wojciech Zaremba &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract346');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract346');"><div id="abstract346" style="font-size: 1rem; color:black; display: none">
<u><I> 1707.01495v3 &nbsp; - &nbsp;
{off-policy}
{reinforcement}
{replay}
{robot}
{sparse}
</I></u><br> Dealing with sparse rewards is one of the biggest challenges in Reinforcement
Learning (RL). We present a novel technique called Hindsight Experience Replay
which allows sample-efficient learning from rewards which are sparse and binary
and therefore avoid the need for complicated reward engineering. It can be
combined with an arbitrary off-policy RL algorithm and may be seen as a form of
implicit curriculum.
We demonstrate our approach on the task of manipulating objects with a
robotic arm. In particular, we run experiments on three different tasks:
pushing, sliding, and pick-and-place, in each case using only binary rewards
indicating whether or not the task is completed. Our ablation studies show that
Hindsight Experience Replay is a crucial ingredient which makes training
possible in these challenging environments. We show that our policies trained
on a physics simulation can be deployed on a physical robot and successfully
complete the task. <br><br></div></a>
</td></tr>
<tr id=" 347 " class="entry"><td>
<a onclick="toggleVisibility('abstract347');">[|&bull;|]</a><a href="http://arxiv.org/abs/1711.09846v2">
<b/> Population Based Training of Neural Networks (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1711.09846v2">
Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, Koray Kavukcuoglu &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract347');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract347');"><div id="abstract347" style="font-size: 1rem; color:black; display: none">
<u><I> 1711.09846v2 &nbsp; - &nbsp;
{deep}
{optimal}
{population}
{reinforcement}
</I></u><br> Neural networks dominate the modern machine learning landscape, but their
training and success still suffer from sensitivity to empirical choices of
hyperparameters such as model architecture, loss function, and optimisation
algorithm. In this work we present \emph{Population Based Training (PBT)}, a
simple asynchronous optimisation algorithm which effectively utilises a fixed
computational budget to jointly optimise a population of models and their
hyperparameters to maximise performance. Importantly, PBT discovers a schedule
of hyperparameter settings rather than following the generally sub-optimal
strategy of trying to find a single fixed set to use for the whole course of
training. With just a small modification to a typical distributed
hyperparameter training framework, our method allows robust and reliable
training of models. We demonstrate the effectiveness of PBT on deep
reinforcement learning problems, showing faster wall-clock convergence and
higher final performance of agents by optimising over a suite of
hyperparameters. In addition, we show the same method can be applied to
supervised learning for machine translation, where PBT is used to maximise the
BLEU score directly, and also to training of Generative Adversarial Networks to
maximise the Inception score of generated images. In all cases PBT results in
the automatic discovery of hyperparameter schedules and model selection which
results in stable training and better final performance. <br><br></div></a>
</td></tr>
<tr id=" 348 " class="entry"><td>
<a onclick="toggleVisibility('abstract348');">[|&bull;|]</a><a href="http://arxiv.org/abs/1706.10295v3">
<b/> Noisy Networks for Exploration (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1706.10295v3">
Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, Shane Legg &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract348');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract348');"><div id="abstract348" style="font-size: 1rem; color:black; display: none">
<u><I> 1706.10295v3 &nbsp; - &nbsp;
{deep}
{entropy}
{exploration}
{gradient}
{reinforcement}
</I></u><br> We introduce NoisyNet, a deep reinforcement learning agent with parametric
noise added to its weights, and show that the induced stochasticity of the
agent's policy can be used to aid efficient exploration. The parameters of the
noise are learned with gradient descent along with the remaining network
weights. NoisyNet is straightforward to implement and adds little computational
overhead. We find that replacing the conventional exploration heuristics for
A3C, DQN and dueling agents (entropy reward and $\epsilon$-greedy respectively)
with NoisyNet yields substantially higher scores for a wide range of Atari
games, in some cases advancing the agent from sub to super-human performance. <br><br></div></a>
</td></tr>
<tr id=" 349 " class="entry"><td>
<a onclick="toggleVisibility('abstract349');">[|&bull;|]</a><a href="http://arxiv.org/abs/1707.08817v2">
<b/> Leveraging Demonstrations for Deep Reinforcement Learning on Robotics
Problems with Sparse Rewards (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1707.08817v2">
Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Roth&#246;rl, Thomas Lampe, Martin Riedmiller &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract349');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract349');"><div id="abstract349" style="font-size: 1rem; color:black; display: none">
<u><I> 1707.08817v2 &nbsp; - &nbsp;
{control}
{deep}
{exploration}
{gradient}
{model-based}
{reinforcement}
{replay}
{robot}
{sparse}
</I></u><br> We propose a general and model-free approach for Reinforcement Learning (RL)
on real robotics with sparse rewards. We build upon the Deep Deterministic
Policy Gradient (DDPG) algorithm to use demonstrations. Both demonstrations and
actual interactions are used to fill a replay buffer and the sampling ratio
between demonstrations and transitions is automatically tuned via a prioritized
replay mechanism. Typically, carefully engineered shaping rewards are required
to enable the agents to efficiently explore on high dimensional control
problems such as robotics. They are also required for model-based acceleration
methods relying on local solvers such as iLQG (e.g. Guided Policy Search and
Normalized Advantage Function). The demonstrations replace the need for
carefully engineered rewards, and reduce the exploration problem encountered by
classical RL approaches in these domains. Demonstrations are collected by a
robot kinesthetically force-controlled by a human demonstrator. Results on four
simulated insertion tasks show that DDPG from demonstrations out-performs DDPG,
and does not require engineered rewards. Finally, we demonstrate the method on
a real robotics task consisting of inserting a clip (flexible object) into a
rigid object. <br><br></div></a>
</td></tr>
<tr id=" 350 " class="entry"><td>
<a onclick="toggleVisibility('abstract350');">[|&bull;|]</a><a href="http://arxiv.org/abs/1707.02286v2">
<b/> Emergence of Locomotion Behaviours in Rich Environments (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1707.02286v2">
Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, S. M. Ali Eslami, Martin Riedmiller, David Silver &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract350');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract350');"><div id="abstract350" style="font-size: 1rem; color:black; display: none">
<u><I> 1707.02286v2 &nbsp; - &nbsp;
{gradient}
{reinforcement}
</I></u><br> The reinforcement learning paradigm allows, in principle, for complex
behaviours to be learned directly from simple reward signals. In practice,
however, it is common to carefully hand-design the reward function to encourage
a particular solution, or to derive it from demonstration data. In this paper
explore how a rich environment can help to promote the learning of complex
behavior. Specifically, we train agents in diverse environmental contexts, and
find that this encourages the emergence of robust behaviours that perform well
across a suite of tasks. We demonstrate this principle for locomotion --
behaviours that are known for their sensitivity to the choice of reward. We
train several simulated bodies on a diverse set of challenging terrains and
obstacles, using a simple reward function based on forward progress. Using a
novel scalable variant of policy gradient reinforcement learning, our agents
learn to run, jump, crouch and turn as required by the environment without
explicit reward-based guidance. A visual depiction of highlights of the learned
behavior can be viewed following https://youtu.be/hx_bgoTF7bs . <br><br></div></a>
</td></tr>
<tr id=" 351 " class="entry"><td>
<a onclick="toggleVisibility('abstract351');">[|&bull;|]</a><a href="http://arxiv.org/abs/1707.01891v3">
<b/> Trust-PCL: An Off-Policy Trust Region Method for Continuous Control (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1707.01891v3">
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, Dale Schuurmans &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract351');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract351');"><div id="abstract351" style="font-size: 1rem; color:black; display: none">
<u><I> 1707.01891v3 &nbsp; - &nbsp;
{control}
{entropy}
{off-policy}
{on-policy}
{optimal}
{reinforcement}
</I></u><br> Trust region methods, such as TRPO, are often used to stabilize policy
optimization algorithms in reinforcement learning (RL). While current trust
region strategies are effective for continuous control, they typically require
a prohibitively large amount of on-policy interaction with the environment. To
address this problem, we propose an off-policy trust region method, Trust-PCL.
The algorithm is the result of observing that the optimal policy and state
values of a maximum reward objective with a relative-entropy regularizer
satisfy a set of multi-step pathwise consistencies along any path. Thus,
Trust-PCL is able to maintain optimization stability while exploiting
off-policy data to improve sample efficiency. When evaluated on a number of
continuous control tasks, Trust-PCL improves the solution quality and sample
efficiency of TRPO. <br><br></div></a>
</td></tr>
<tr id=" 352 " class="entry"><td>
<a onclick="toggleVisibility('abstract352');">[|&bull;|]</a><a href="http://arxiv.org/abs/1705.07241v3">
<b/> Diffusion-based neuromodulation can eliminate catastrophic forgetting in
simple neural networks (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1705.07241v3">
Roby Velez, Jeff Clune &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract352');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract352');"><div id="abstract352" style="font-size: 1rem; color:black; display: none">
<u><I> 1705.07241v3 &nbsp; - &nbsp;
{diversity}
{skill}
</I></u><br> A long-term goal of AI is to produce agents that can learn a diversity of
skills throughout their lifetimes and continuously improve those skills via
experience. A longstanding obstacle towards that goal is catastrophic
forgetting, which is when learning new information erases previously learned
information. Catastrophic forgetting occurs in artificial neural networks
(ANNs), which have fueled most recent advances in AI. A recent paper proposed
that catastrophic forgetting in ANNs can be reduced by promoting modularity,
which can limit forgetting by isolating task information to specific clusters
of nodes and connections (functional modules). While the prior work did show
that modular ANNs suffered less from catastrophic forgetting, it was not able
to produce ANNs that possessed task-specific functional modules, thereby
leaving the main theory regarding modularity and forgetting untested. We
introduce diffusion-based neuromodulation, which simulates the release of
diffusing, neuromodulatory chemicals within an ANN that can modulate (i.e. up
or down regulate) learning in a spatial region. On the simple diagnostic
problem from the prior work, diffusion-based neuromodulation 1) induces
task-specific learning in groups of nodes and connections (task-specific
localized learning), which 2) produces functional modules for each subtask, and
3) yields higher performance by eliminating catastrophic forgetting. Overall,
our results suggest that diffusion-based neuromodulation promotes task-specific
localized learning and functional modularity, which can help solve the
challenging, but important problem of catastrophic forgetting. <br><br></div></a>
</td></tr>
<tr id=" 353 " class="entry"><td>
<a onclick="toggleVisibility('abstract353');">[|&bull;|]</a><a href="http://arxiv.org/abs/1710.09829v2">
<b/> Dynamic Routing Between Capsules (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1710.09829v2">
Sara Sabour, Nicholas Frosst, Geoffrey E Hinton &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract353');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract353');"><div id="abstract353" style="font-size: 1rem; color:black; display: none">
<u><I> 1710.09829v2 &nbsp; - &nbsp;
</I></u><br> A capsule is a group of neurons whose activity vector represents the
instantiation parameters of a specific type of entity such as an object or an
object part. We use the length of the activity vector to represent the
probability that the entity exists and its orientation to represent the
instantiation parameters. Active capsules at one level make predictions, via
transformation matrices, for the instantiation parameters of higher-level
capsules. When multiple predictions agree, a higher level capsule becomes
active. We show that a discrimininatively trained, multi-layer capsule system
achieves state-of-the-art performance on MNIST and is considerably better than
a convolutional net at recognizing highly overlapping digits. To achieve these
results we use an iterative routing-by-agreement mechanism: A lower-level
capsule prefers to send its output to higher level capsules whose activity
vectors have a big scalar product with the prediction coming from the
lower-level capsule. <br><br></div></a>
</td></tr>
<tr id=" 354 " class="entry"><td>
<a onclick="toggleVisibility('abstract354');">[|&bull;|]</a><a href="http://arxiv.org/abs/1705.07874v2">
<b/> A Unified Approach to Interpreting Model Predictions (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1705.07874v2">
Scott Lundberg, Su-In Lee &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract354');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract354');"><div id="abstract354" style="font-size: 1rem; color:black; display: none">
<u><I> 1705.07874v2 &nbsp; - &nbsp;
{deep}
</I></u><br> Understanding why a model makes a certain prediction can be as crucial as the
prediction's accuracy in many applications. However, the highest accuracy for
large modern datasets is often achieved by complex models that even experts
struggle to interpret, such as ensemble or deep learning models, creating a
tension between accuracy and interpretability. In response, various methods
have recently been proposed to help users interpret the predictions of complex
models, but it is often unclear how these methods are related and when one
method is preferable over another. To address this problem, we present a
unified framework for interpreting predictions, SHAP (SHapley Additive
exPlanations). SHAP assigns each feature an importance value for a particular
prediction. Its novel components include: (1) the identification of a new class
of additive feature importance measures, and (2) theoretical results showing
there is a unique solution in this class with a set of desirable properties.
The new class unifies six existing methods, notable because several recent
methods in the class lack the proposed desirable properties. Based on insights
from this unification, we present new methods that show improved computational
performance and/or better consistency with human intuition than previous
approaches. <br><br></div></a>
</td></tr>
<tr id=" 355 " class="entry"><td>
<a onclick="toggleVisibility('abstract355');">[|&bull;|]</a><a href="http://arxiv.org/abs/1711.01012v2">
<b/> Policy Optimization by Genetic Distillation (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1711.01012v2">
Tanmay Gangwani, Jian Peng &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract355');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract355');"><div id="abstract355" style="font-size: 1rem; color:black; display: none">
<u><I> 1711.01012v2 &nbsp; - &nbsp;
{deep}
{gradient}
{imitation}
{reinforcement}
</I></u><br> Genetic algorithms have been widely used in many practical optimization
problems. Inspired by natural selection, operators, including mutation,
crossover and selection, provide effective heuristics for search and black-box
optimization. However, they have not been shown useful for deep reinforcement
learning, possibly due to the catastrophic consequence of parameter crossovers
of neural networks. Here, we present Genetic Policy Optimization (GPO), a new
genetic algorithm for sample-efficient deep policy optimization. GPO uses
imitation learning for policy crossover in the state space and applies policy
gradient methods for mutation. Our experiments on MuJoCo tasks show that GPO as
a genetic algorithm is able to provide superior performance over the
state-of-the-art policy gradient methods and achieves comparable or higher
sample efficiency. <br><br></div></a>
</td></tr>
<tr id=" 356 " class="entry"><td>
<a onclick="toggleVisibility('abstract356');">[|&bull;|]</a><a href="http://arxiv.org/abs/1709.07932v3">
<b/> Expanding Motor Skills through Relay Neural Networks (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1709.07932v3">
Visak C. V. Kumar, Sehoon Ha, C. Karen Liu &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract356');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract356');"><div id="abstract356" style="font-size: 1rem; color:black; display: none">
<u><I> 1709.07932v3 &nbsp; - &nbsp;
{control}
{deep}
{reinforcement}
{robot}
{skill}
</I></u><br> While the recent advances in deep reinforcement learning have achieved
impressive results in learning motor skills, many of the trained policies are
only capable within a limited set of initial states. We propose a technique to
break down a complex robotic task to simpler subtasks and train them
sequentially such that the robot can expand its existing skill set gradually.
Our key idea is to build a tree of local control policies represented by neural
networks, which we refer as Relay Neural Networks. Starting from the root
policy that attempts to achieve the task from a small set of initial states,
each subsequent policy expands the set of successful initial states by driving
the new states to existing "good" states. Our algorithm utilizes the value
function of the policy to determine whether a state is "good" under each
policy. We take advantage of many existing policy search algorithms that learn
the value function simultaneously with the policy, such as those that use
actor-critic representations or those that use the advantage function to reduce
variance. We demonstrate that the relay networks can solve complex continuous
control problems for underactuated dynamic systems. <br><br></div></a>
</td></tr>
<tr id=" 357 " class="entry"><td>
<a onclick="toggleVisibility('abstract357');">[|&bull;|]</a><a href="http://arxiv.org/abs/1710.10044v1">
<b/> Distributional Reinforcement Learning with Quantile Regression (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1710.10044v1">
Will Dabney, Mark Rowland, Marc G. Bellemare, R&#233;mi Munos &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract357');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract357');"><div id="abstract357" style="font-size: 1rem; color:black; display: none">
<u><I> 1710.10044v1 &nbsp; - &nbsp;
{reinforcement}
</I></u><br> In reinforcement learning an agent interacts with the environment by taking
actions and observing the next state and reward. When sampled
probabilistically, these state transitions, rewards, and actions can all induce
randomness in the observed long-term return. Traditionally, reinforcement
learning algorithms average over this randomness to estimate the value
function. In this paper, we build on recent work advocating a distributional
approach to reinforcement learning in which the distribution over returns is
modeled explicitly instead of only estimating the mean. That is, we examine
methods of learning the value distribution instead of the value function. We
give results that close a number of gaps between the theoretical and
algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we
extend existing results to the approximate distribution setting. Second, we
present a novel distributional reinforcement learning algorithm consistent with
our theoretical formulation. Finally, we evaluate this new algorithm on the
Atari 2600 games, observing that it significantly outperforms many of the
recent improvements on DQN, including the related distributional algorithm C51. <br><br></div></a>
</td></tr>
<tr id=" 358 " class="entry"><td>
<a onclick="toggleVisibility('abstract358');">[|&bull;|]</a><a href="http://arxiv.org/abs/1712.08449v1">
<b/> True Asymptotic Natural Gradient Optimization (2017)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1712.08449v1">
Yann Ollivier &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract358');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract358');"><div id="abstract358" style="font-size: 1rem; color:black; display: none">
<u><I> 1712.08449v1 &nbsp; - &nbsp;
{gradient}
{optimal}
</I></u><br> We introduce a simple algorithm, True Asymptotic Natural Gradient
Optimization (TANGO), that converges to a true natural gradient descent in the
limit of small learning rates, without explicit Fisher matrix estimation.
For quadratic models the algorithm is also an instance of averaged stochastic
gradient, where the parameter is a moving average of a "fast", constant-rate
gradient descent. TANGO appears as a particular de-linearization of averaged
SGD, and is sometimes quite different on non-quadratic models. This further
connects averaged SGD and natural gradient, both of which are arguably optimal
asymptotically.
In large dimension, small learning rates will be required to approximate the
natural gradient well. Still, this shows it is possible to get arbitrarily
close to exact natural gradient descent with a lightweight algorithm. <br><br></div></a>
</td></tr>
<tr id=" 359 " class="entry"><td>
<a onclick="toggleVisibility('abstract359');">[|&bull;|]</a><a href="http://arxiv.org/abs/1605.01278v4">
<b/> A Bayesian Approach to Policy Recognition and State Representation
Learning (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1605.01278v4">
Adrian &#352;o&#353;i&#263;, Abdelhak M. Zoubir, Heinz Koeppl &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract359');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract359');"><div id="abstract359" style="font-size: 1rem; color:black; display: none">
<u><I> 1605.01278v4 &nbsp; - &nbsp;
{control}
{optimal}
</I></u><br> Learning from demonstration (LfD) is the process of building behavioral
models of a task from demonstrations provided by an expert. These models can be
used e.g. for system control by generalizing the expert demonstrations to
previously unencountered situations. Most LfD methods, however, make strong
assumptions about the expert behavior, e.g. they assume the existence of a
deterministic optimal ground truth policy or require direct monitoring of the
expert's controls, which limits their practical use as part of a general system
identification framework. In this work, we consider the LfD problem in a more
general setting where we allow for arbitrary stochastic expert policies,
without reasoning about the optimality of the demonstrations. Following a
Bayesian methodology, we model the full posterior distribution of possible
expert controllers that explain the provided demonstration data. Moreover, we
show that our methodology can be applied in a nonparametric context to infer
the complexity of the state representation used by the expert, and to learn
task-appropriate partitionings of the system state space. <br><br></div></a>
</td></tr>
<tr id=" 360 " class="entry"><td>
<a onclick="toggleVisibility('abstract360');">[|&bull;|]</a><a href="http://arxiv.org/abs/1610.09038v1">
<b/> Professor Forcing: A New Algorithm for Training Recurrent Networks (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1610.09038v1">
Alex Lamb, Anirudh Goyal, Ying Zhang, Saizheng Zhang, Aaron Courville, Yoshua Bengio &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract360');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract360');"><div id="abstract360" style="font-size: 1rem; color:black; display: none">
<u><I> 1610.09038v1 &nbsp; - &nbsp;
</I></u><br> The Teacher Forcing algorithm trains recurrent networks by supplying observed
sequence values as inputs during training and using the network's own
one-step-ahead predictions to do multi-step sampling. We introduce the
Professor Forcing algorithm, which uses adversarial domain adaptation to
encourage the dynamics of the recurrent network to be the same when training
the network and when sampling from the network over multiple time steps. We
apply Professor Forcing to language modeling, vocal synthesis on raw waveforms,
handwriting generation, and image generation. Empirically we find that
Professor Forcing acts as a regularizer, improving test likelihood on character
level Penn Treebank and sequential MNIST. We also find that the model
qualitatively improves samples, especially when sampling for a large number of
time steps. This is supported by human evaluation of sample quality. Trade-offs
between Professor Forcing and Scheduled Sampling are discussed. We produce
T-SNEs showing that Professor Forcing successfully makes the dynamics of the
network during training and sampling more similar. <br><br></div></a>
</td></tr>
<tr id=" 361 " class="entry"><td>
<a onclick="toggleVisibility('abstract361');">[|&bull;|]</a><a href="http://arxiv.org/abs/1606.05312v2">
<b/> Successor Features for Transfer in Reinforcement Learning (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1606.05312v2">
Andr&#233; Barreto, Will Dabney, R&#233;mi Munos, Jonathan J. Hunt, Tom Schaul, Hado van Hasselt, David Silver &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract361');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract361');"><div id="abstract361" style="font-size: 1rem; color:black; display: none">
<u><I> 1606.05312v2 &nbsp; - &nbsp;
{control}
{reinforcement}
{robot}
{transfer}
</I></u><br> Transfer in reinforcement learning refers to the notion that generalization
should occur not only within a task but also across tasks. We propose a
transfer framework for the scenario where the reward function changes between
tasks but the environment's dynamics remain the same. Our approach rests on two
key ideas: "successor features", a value function representation that decouples
the dynamics of the environment from the rewards, and "generalized policy
improvement", a generalization of dynamic programming's policy improvement
operation that considers a set of policies rather than a single one. Put
together, the two ideas lead to an approach that integrates seamlessly within
the reinforcement learning framework and allows the free exchange of
information across tasks. The proposed method also provides performance
guarantees for the transferred policy even before any learning has taken place.
We derive two theorems that set our approach in firm theoretical ground and
present experiments that show that it successfully promotes transfer in
practice, significantly outperforming alternative methods in a sequence of
navigation tasks and in the control of a simulated robotic arm. <br><br></div></a>
</td></tr>
<tr id=" 362 " class="entry"><td>
<a onclick="toggleVisibility('abstract362');">[|&bull;|]</a><a href="http://arxiv.org/abs/1611.02635v4">
<b/> A Lyapunov Analysis of Momentum Methods in Optimization (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1611.02635v4">
Ashia C. Wilson, Benjamin Recht, Michael I. Jordan &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract362');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract362');"><div id="abstract362" style="font-size: 1rem; color:black; display: none">
<u><I> 1611.02635v4 &nbsp; - &nbsp;
{gradient}
{optimal}
</I></u><br> Momentum methods play a significant role in optimization. Examples include
Nesterov's accelerated gradient method and the conditional gradient algorithm.
Several momentum methods are provably optimal under standard oracle models, and
all use a technique called estimate sequences to analyze their convergence
properties. The technique of estimate sequences has long been considered
difficult to understand, leading many researchers to generate alternative,
"more intuitive" methods and analyses. We show there is an equivalence between
the technique of estimate sequences and a family of Lyapunov functions in both
continuous and discrete time. This connection allows us to develop a simple and
unified analysis of many existing momentum algorithms, introduce several new
algorithms, and strengthen the connection between algorithms and
continuous-time dynamical systems. <br><br></div></a>
</td></tr>
<tr id=" 363 " class="entry"><td>
<a onclick="toggleVisibility('abstract363');">[|&bull;|]</a><a href="http://arxiv.org/abs/1609.07152v3">
<b/> Input Convex Neural Networks (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1609.07152v3">
Brandon Amos, Lei Xu, J. Zico Kolter &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract363');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract363');"><div id="abstract363" style="font-size: 1rem; color:black; display: none">
<u><I> 1609.07152v3 &nbsp; - &nbsp;
{deep}
{reinforcement}
</I></u><br> This paper presents the input convex neural network architecture. These are
scalar-valued (potentially deep) neural networks with constraints on the
network parameters such that the output of the network is a convex function of
(some of) the inputs. The networks allow for efficient inference via
optimization over some inputs to the network given others, and can be applied
to settings including structured prediction, data imputation, reinforcement
learning, and others. In this paper we lay the basic groundwork for these
models, proposing methods for inference, optimization and learning, and analyze
their representational power. We show that many existing neural network
architectures can be made input-convex with a minor modification, and develop
specialized optimization algorithms tailored to this setting. Finally, we
highlight the performance of the methods on multi-label prediction, image
completion, and reinforcement learning problems, where we show improvement over
the existing state of the art in many cases. <br><br></div></a>
</td></tr>
<tr id=" 364 " class="entry"><td>
<a onclick="toggleVisibility('abstract364');">[|&bull;|]</a><a href="http://arxiv.org/abs/1606.05908v3">
<b/> Tutorial on Variational Autoencoders (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1606.05908v3">
Carl Doersch &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract364');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract364');"><div id="abstract364" style="font-size: 1rem; color:black; display: none">
<u><I> 1606.05908v3 &nbsp; - &nbsp;
{gradient}
{unsupervised}
</I></u><br> In just three years, Variational Autoencoders (VAEs) have emerged as one of
the most popular approaches to unsupervised learning of complicated
distributions. VAEs are appealing because they are built on top of standard
function approximators (neural networks), and can be trained with stochastic
gradient descent. VAEs have already shown promise in generating many kinds of
complicated data, including handwritten digits, faces, house numbers, CIFAR
images, physical models of scenes, segmentation, and predicting the future from
static images. This tutorial introduces the intuitions behind VAEs, explains
the mathematics behind them, and describes some empirical behavior. No prior
knowledge of variational Bayesian methods is assumed. <br><br></div></a>
</td></tr>
<tr id=" 365 " class="entry"><td>
<a onclick="toggleVisibility('abstract365');">[|&bull;|]</a><a href="http://arxiv.org/abs/1603.00448v3">
<b/> Guided Cost Learning: Deep Inverse Optimal Control via Policy
Optimization (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1603.00448v3">
Chelsea Finn, Sergey Levine, Pieter Abbeel &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract365');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract365');"><div id="abstract365" style="font-size: 1rem; color:black; display: none">
<u><I> 1603.00448v3 &nbsp; - &nbsp;
{control}
{optimal}
{reinforcement}
{robot}
</I></u><br> Reinforcement learning can acquire complex behaviors from high-level
specifications. However, defining a cost function that can be optimized
effectively and encodes the correct task is challenging in practice. We explore
how inverse optimal control (IOC) can be used to learn behaviors from
demonstrations, with applications to torque control of high-dimensional robotic
systems. Our method addresses two key challenges in inverse optimal control:
first, the need for informative features and effective regularization to impose
structure on the cost, and second, the difficulty of learning the cost function
under unknown dynamics for high-dimensional continuous systems. To address the
former challenge, we present an algorithm capable of learning arbitrary
nonlinear cost functions, such as neural networks, without meticulous feature
engineering. To address the latter challenge, we formulate an efficient
sample-based approximation for MaxEnt IOC. We evaluate our method on a series
of simulated tasks and real-world robotic manipulation problems, demonstrating
substantial improvement over prior methods both in terms of task complexity and
sample efficiency. <br><br></div></a>
</td></tr>
<tr id=" 366 " class="entry"><td>
<a onclick="toggleVisibility('abstract366');">[|&bull;|]</a><a href="http://arxiv.org/abs/1606.04934v2">
<b/> Improving Variational Inference with Inverse Autoregressive Flow (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1606.04934v2">
Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, Max Welling &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract366');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract366');"><div id="abstract366" style="font-size: 1rem; color:black; display: none">
<u><I> 1606.04934v2 &nbsp; - &nbsp;
</I></u><br> The framework of normalizing flows provides a general strategy for flexible
variational inference of posteriors over latent variables. We propose a new
type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast
to earlier published flows, scales well to high-dimensional latent spaces. The
proposed flow consists of a chain of invertible transformations, where each
transformation is based on an autoregressive neural network. In experiments, we
show that IAF significantly improves upon diagonal Gaussian approximate
posteriors. In addition, we demonstrate that a novel type of variational
autoencoder, coupled with IAF, is competitive with neural autoregressive models
in terms of attained log-likelihood on natural images, while allowing
significantly faster synthesis. <br><br></div></a>
</td></tr>
<tr id=" 367 " class="entry"><td>
<a onclick="toggleVisibility('abstract367');">[|&bull;|]</a><a href="http://arxiv.org/abs/1607.07611v1">
<b/> Learning Null Space Projections in Operational Space Formulation (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1607.07611v1">
Hsiu-Chin Lin, Matthew Howard &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract367');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract367');"><div id="abstract367" style="font-size: 1rem; color:black; display: none">
<u><I> 1607.07611v1 &nbsp; - &nbsp;
{control}
{kinematic}
</I></u><br> In recent years, a number of tools have become available that recover the
underlying control policy from constrained movements. However, few have
explicitly considered learning the constraints of the motion and ways to cope
with unknown environment. In this paper, we consider learning the null space
projection matrix of a kinematically constrained system in the absence of any
prior knowledge either on the underlying policy, the geometry, or
dimensionality of the constraints. Our evaluations have demonstrated the
effectiveness of the proposed approach on problems of differing dimensionality,
and with different degrees of non-linearity. <br><br></div></a>
</td></tr>
<tr id=" 368 " class="entry"><td>
<a onclick="toggleVisibility('abstract368');">[|&bull;|]</a><a href="http://arxiv.org/abs/1602.04621v3">
<b/> Deep Exploration via Bootstrapped DQN (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1602.04621v3">
Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract368');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract368');"><div id="abstract368" style="font-size: 1rem; color:black; display: none">
<u><I> 1602.04621v3 &nbsp; - &nbsp;
{deep}
{exploration}
{reinforcement}
</I></u><br> Efficient exploration in complex environments remains a major challenge for
reinforcement learning. We propose bootstrapped DQN, a simple algorithm that
explores in a computationally and statistically efficient manner through use of
randomized value functions. Unlike dithering strategies such as epsilon-greedy
exploration, bootstrapped DQN carries out temporally-extended (or deep)
exploration; this can lead to exponentially faster learning. We demonstrate
these benefits in complex stochastic MDPs and in the large-scale Arcade
Learning Environment. Bootstrapped DQN substantially improves learning times
and performance across most Atari games. <br><br></div></a>
</td></tr>
<tr id=" 369 " class="entry"><td>
<a onclick="toggleVisibility('abstract369');">[|&bull;|]</a><a href="http://arxiv.org/abs/1607.05077v1">
<b/> Playing Atari Games with Deep Reinforcement Learning and Human
Checkpoint Replay (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1607.05077v1">
Ionel-Alexandru Hosu, Traian Rebedea &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract369');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract369');"><div id="abstract369" style="font-size: 1rem; color:black; display: none">
<u><I> 1607.05077v1 &nbsp; - &nbsp;
{control}
{deep}
{exploration}
{reinforcement}
{replay}
{sparse}
</I></u><br> This paper introduces a novel method for learning how to play the most
difficult Atari 2600 games from the Arcade Learning Environment using deep
reinforcement learning. The proposed method, human checkpoint replay, consists
in using checkpoints sampled from human gameplay as starting points for the
learning process. This is meant to compensate for the difficulties of current
exploration strategies, such as epsilon-greedy, to find successful control
policies in games with sparse rewards. Like other deep reinforcement learning
architectures, our model uses a convolutional neural network that receives only
raw pixel inputs to estimate the state value function. We tested our method on
Montezuma's Revenge and Private Eye, two of the most challenging games from the
Atari platform. The results we obtained show a substantial improvement compared
to previous learning approaches, as well as over a random player. We also
propose a method for training deep reinforcement learning agents using human
gameplay experience, which we call human experience replay. <br><br></div></a>
</td></tr>
<tr id=" 370 " class="entry"><td>
<a onclick="toggleVisibility('abstract370');">[|&bull;|]</a><a href="http://arxiv.org/abs/1611.07507v1">
<b/> Variational Intrinsic Control (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1611.07507v1">
Karol Gregor, Danilo Jimenez Rezende, Daan Wierstra &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract370');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract370');"><div id="abstract370" style="font-size: 1rem; color:black; display: none">
<u><I> 1611.07507v1 &nbsp; - &nbsp;
{gradient}
{intrinsic}
{reinforcement}
{unsupervised}
</I></u><br> In this paper we introduce a new unsupervised reinforcement learning method
for discovering the set of intrinsic options available to an agent. This set is
learned by maximizing the number of different states an agent can reliably
reach, as measured by the mutual information between the set of options and
option termination states. To this end, we instantiate two policy gradient
based algorithms, one that creates an explicit embedding space of options and
one that represents options implicitly. The algorithms also provide an explicit
measure of empowerment in a given state that can be used by an empowerment
maximizing agent. The algorithm scales well with function approximation and we
demonstrate the applicability of the algorithm on a range of tasks. <br><br></div></a>
</td></tr>
<tr id=" 371 " class="entry"><td>
<a onclick="toggleVisibility('abstract371');">[|&bull;|]</a><a href="http://arxiv.org/abs/1612.07139v4">
<b/> A Survey of Deep Network Solutions for Learning Control in Robotics:
From Reinforcement to Imitation (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1612.07139v4">
Lei Tai, Jingwei Zhang, Ming Liu, Joschka Boedecker, Wolfram Burgard &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract371');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract371');"><div id="abstract371" style="font-size: 1rem; color:black; display: none">
<u><I> 1612.07139v4 &nbsp; - &nbsp;
{control}
{deep}
{imitation}
{reinforcement}
{robot}
{transfer}
</I></u><br> Deep learning techniques have been widely applied, achieving state-of-the-art
results in various fields of study. This survey focuses on deep learning
solutions that target learning control policies for robotics applications. We
carry out our discussions on the two main paradigms for learning control with
deep networks: deep reinforcement learning and imitation learning. For deep
reinforcement learning (DRL), we begin from traditional reinforcement learning
algorithms, showing how they are extended to the deep context and effective
mechanisms that could be added on top of the DRL algorithms. We then introduce
representative works that utilize DRL to solve navigation and manipulation
tasks in robotics. We continue our discussion on methods addressing the
challenge of the reality gap for transferring DRL policies trained in
simulation to real-world scenarios, and summarize robotics simulation platforms
for conducting DRL research. For imitation leaning, we go through its three
main categories, behavior cloning, inverse reinforcement learning and
generative adversarial imitation learning, by introducing their formulations
and their corresponding robotics applications. Finally, we discuss the open
challenges and research frontiers. <br><br></div></a>
</td></tr>
<tr id=" 372 " class="entry"><td>
<a onclick="toggleVisibility('abstract372');">[|&bull;|]</a><a href="http://arxiv.org/abs/1606.01868v2">
<b/> Unifying Count-Based Exploration and Intrinsic Motivation (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1606.01868v2">
Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, Remi Munos &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract372');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract372');"><div id="abstract372" style="font-size: 1rem; color:black; display: none">
<u><I> 1606.01868v2 &nbsp; - &nbsp;
{exploration}
{intrinsic}
{reinforcement}
</I></u><br> We consider an agent's uncertainty about its environment and the problem of
generalizing this uncertainty across observations. Specifically, we focus on
the problem of exploration in non-tabular reinforcement learning. Drawing
inspiration from the intrinsic motivation literature, we use density models to
measure uncertainty, and propose a novel algorithm for deriving a pseudo-count
from an arbitrary density model. This technique enables us to generalize
count-based exploration algorithms to the non-tabular case. We apply our ideas
to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We
transform these pseudo-counts into intrinsic rewards and obtain significantly
improved exploration in a number of hard games, including the infamously
difficult Montezuma's Revenge. <br><br></div></a>
</td></tr>
<tr id=" 373 " class="entry"><td>
<a onclick="toggleVisibility('abstract373');">[|&bull;|]</a><a href="http://arxiv.org/abs/1611.05397v1">
<b/> Reinforcement Learning with Unsupervised Auxiliary Tasks (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1611.05397v1">
Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, Koray Kavukcuoglu &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract373');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract373');"><div id="abstract373" style="font-size: 1rem; color:black; display: none">
<u><I> 1611.05397v1 &nbsp; - &nbsp;
{deep}
{reinforcement}
{unsupervised}
</I></u><br> Deep reinforcement learning agents have achieved state-of-the-art results by
directly maximising cumulative reward. However, environments contain a much
wider variety of possible training signals. In this paper, we introduce an
agent that also maximises many other pseudo-reward functions simultaneously by
reinforcement learning. All of these tasks share a common representation that,
like unsupervised learning, continues to develop in the absence of extrinsic
rewards. We also introduce a novel mechanism for focusing this representation
upon extrinsic rewards, so that learning can rapidly adapt to the most relevant
aspects of the actual task. Our agent significantly outperforms the previous
state-of-the-art on Atari, averaging 880\% expert human performance, and a
challenging suite of first-person, three-dimensional \emph{Labyrinth} tasks
leading to a mean speedup in learning of 10$\times$ and averaging 87\% expert
human performance on Labyrinth. <br><br></div></a>
</td></tr>
<tr id=" 374 " class="entry"><td>
<a onclick="toggleVisibility('abstract374');">[|&bull;|]</a><a href="http://arxiv.org/abs/1609.05140v2">
<b/> The Option-Critic Architecture (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1609.05140v2">
Pierre-Luc Bacon, Jean Harb, Doina Precup &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract374');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract374');"><div id="abstract374" style="font-size: 1rem; color:black; display: none">
<u><I> 1609.05140v2 &nbsp; - &nbsp;
{gradient}
{reinforcement}
</I></u><br> Temporal abstraction is key to scaling up learning and planning in
reinforcement learning. While planning with temporally extended actions is well
understood, creating such abstractions autonomously from data has remained
challenging. We tackle this problem in the framework of options [Sutton, Precup
& Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options
and propose a new option-critic architecture capable of learning both the
internal policies and the termination conditions of options, in tandem with the
policy over options, and without the need to provide any additional rewards or
subgoals. Experimental results in both discrete and continuous environments
showcase the flexibility and efficiency of the framework. <br><br></div></a>
</td></tr>
<tr id=" 375 " class="entry"><td>
<a onclick="toggleVisibility('abstract375');">[|&bull;|]</a><a href="http://arxiv.org/abs/1609.04747v2">
<b/> An overview of gradient descent optimization algorithms (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1609.04747v2">
Sebastian Ruder &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract375');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract375');"><div id="abstract375" style="font-size: 1rem; color:black; display: none">
<u><I> 1609.04747v2 &nbsp; - &nbsp;
{gradient}
</I></u><br> Gradient descent optimization algorithms, while increasingly popular, are
often used as black-box optimizers, as practical explanations of their
strengths and weaknesses are hard to come by. This article aims to provide the
reader with intuitions with regard to the behaviour of different algorithms
that will allow her to put them to use. In the course of this overview, we look
at different variants of gradient descent, summarize challenges, introduce the
most common optimization algorithms, review architectures in a parallel and
distributed setting, and investigate additional strategies for optimizing
gradient descent. <br><br></div></a>
</td></tr>
<tr id=" 376 " class="entry"><td>
<a onclick="toggleVisibility('abstract376');">[|&bull;|]</a><a href="http://arxiv.org/abs/1603.02199v4">
<b/> Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning
and Large-Scale Data Collection (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1603.02199v4">
Sergey Levine, Peter Pastor, Alex Krizhevsky, Deirdre Quillen &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract376');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract376');"><div id="abstract376" style="font-size: 1rem; color:black; display: none">
<u><I> 1603.02199v4 &nbsp; - &nbsp;
{control}
{robot}
</I></u><br> We describe a learning-based approach to hand-eye coordination for robotic
grasping from monocular images. To learn hand-eye coordination for grasping, we
trained a large convolutional neural network to predict the probability that
task-space motion of the gripper will result in successful grasps, using only
monocular camera images and independently of camera calibration or the current
robot pose. This requires the network to observe the spatial relationship
between the gripper and objects in the scene, thus learning hand-eye
coordination. We then use this network to servo the gripper in real time to
achieve successful grasps. To train our network, we collected over 800,000
grasp attempts over the course of two months, using between 6 and 14 robotic
manipulators at any given time, with differences in camera placement and
hardware. Our experimental evaluation demonstrates that our method achieves
effective real-time control, can successfully grasp novel objects, and corrects
mistakes by continuous servoing. <br><br></div></a>
</td></tr>
<tr id=" 377 " class="entry"><td>
<a onclick="toggleVisibility('abstract377');">[|&bull;|]</a><a href="http://arxiv.org/abs/1603.00748v1">
<b/> Continuous Deep Q-Learning with Model-based Acceleration (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1603.00748v1">
Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, Sergey Levine &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract377');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract377');"><div id="abstract377" style="font-size: 1rem; color:black; display: none">
<u><I> 1603.00748v1 &nbsp; - &nbsp;
{control}
{deep}
{gradient}
{reinforcement}
{replay}
{robot}
</I></u><br> Model-free reinforcement learning has been successfully applied to a range of
challenging problems, and has recently been extended to handle large neural
network policies and value functions. However, the sample complexity of
model-free algorithms, particularly when using high-dimensional function
approximators, tends to limit their applicability to physical systems. In this
paper, we explore algorithms and representations to reduce the sample
complexity of deep reinforcement learning for continuous control tasks. We
propose two complementary techniques for improving the efficiency of such
algorithms. First, we derive a continuous variant of the Q-learning algorithm,
which we call normalized adantage functions (NAF), as an alternative to the
more commonly used policy gradient and actor-critic methods. NAF representation
allows us to apply Q-learning with experience replay to continuous tasks, and
substantially improves performance on a set of simulated robotic control tasks.
To further improve the efficiency of our approach, we explore the use of
learned models for accelerating model-free reinforcement learning. We show that
iteratively refitted local linear models are especially effective for this, and
demonstrate substantially faster learning on domains where such models are
applicable. <br><br></div></a>
</td></tr>
<tr id=" 378 " class="entry"><td>
<a onclick="toggleVisibility('abstract378');">[|&bull;|]</a><a href="http://arxiv.org/abs/1607.00485v1">
<b/> Group Sparse Regularization for Deep Neural Networks (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1607.00485v1">
Simone Scardapane, Danilo Comminiello, Amir Hussain, Aurelio Uncini &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract378');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract378');"><div id="abstract378" style="font-size: 1rem; color:black; display: none">
<u><I> 1607.00485v1 &nbsp; - &nbsp;
{deep}
{sparse}
</I></u><br> In this paper, we consider the joint task of simultaneously optimizing (i)
the weights of a deep neural network, (ii) the number of neurons for each
hidden layer, and (iii) the subset of active input features (i.e., feature
selection). While these problems are generally dealt with separately, we
present a simple regularized formulation allowing to solve all three of them in
parallel, using standard optimization routines. Specifically, we extend the
group Lasso penalty (originated in the linear regression literature) in order
to impose group-level sparsity on the network's connections, where each group
is defined as the set of outgoing weights from a unit. Depending on the
specific case, the weights can be related to an input variable, to a hidden
neuron, or to a bias unit, thus performing simultaneously all the
aforementioned tasks in order to obtain a compact network. We perform an
extensive experimental evaluation, by comparing with classical weight decay and
Lasso penalties. We show that a sparse version of the group Lasso penalty is
able to achieve competitive performances, while at the same time resulting in
extremely compact networks with a smaller number of input features. We evaluate
both on a toy dataset for handwritten digit recognition, and on multiple
realistic large-scale classification problems. <br><br></div></a>
</td></tr>
<tr id=" 379 " class="entry"><td>
<a onclick="toggleVisibility('abstract379');">[|&bull;|]</a><a href="http://arxiv.org/abs/1609.03759v2">
<b/> 3D Simulation for Robot Arm Control with Deep Q-Learning (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1609.03759v2">
Stephen James, Edward Johns &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract379');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract379');"><div id="abstract379" style="font-size: 1rem; color:black; display: none">
<u><I> 1609.03759v2 &nbsp; - &nbsp;
{control}
{deep}
{reinforcement}
{robot}
{transfer}
</I></u><br> Recent trends in robot arm control have seen a shift towards end-to-end
solutions, using deep reinforcement learning to learn a controller directly
from raw sensor data, rather than relying on a hand-crafted, modular pipeline.
However, the high dimensionality of the state space often means that it is
impractical to generate sufficient training data with real-world experiments.
As an alternative solution, we propose to learn a robot controller in
simulation, with the potential of then transferring this to a real robot.
Building upon the recent success of deep Q-networks, we present an approach
which uses 3D simulations to train a 7-DOF robotic arm in a control task
without any prior knowledge. The controller accepts images of the environment
as its only input, and outputs motor actions for the task of locating and
grasping a cube, over a range of initial configurations. To encourage efficient
learning, a structured reward function is designed with intermediate rewards.
We also present preliminary results in direct transfer of policies over to a
real robot, without any further training. <br><br></div></a>
</td></tr>
<tr id=" 380 " class="entry"><td>
<a onclick="toggleVisibility('abstract380');">[|&bull;|]</a><a href="http://arxiv.org/abs/1606.02396v1">
<b/> Deep Successor Reinforcement Learning (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1606.02396v1">
Tejas D. Kulkarni, Ardavan Saeedi, Simanta Gautam, Samuel J. Gershman &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract380');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract380');"><div id="abstract380" style="font-size: 1rem; color:black; display: none">
<u><I> 1606.02396v1 &nbsp; - &nbsp;
{deep}
{model-based}
{reinforcement}
</I></u><br> Learning robust value functions given raw observations and rewards is now
possible with model-free and model-based deep reinforcement learning
algorithms. There is a third alternative, called Successor Representations
(SR), which decomposes the value function into two components -- a reward
predictor and a successor map. The successor map represents the expected future
state occupancy from any given state and the reward predictor maps states to
scalar rewards. The value function of a state can be computed as the inner
product between the successor map and the reward weights. In this paper, we
present DSR, which generalizes SR within an end-to-end deep reinforcement
learning framework. DSR has several appealing properties including: increased
sensitivity to distal reward changes due to factorization of reward and world
dynamics, and the ability to extract bottleneck states (subgoals) given
successor maps trained under a random policy. We show the efficacy of our
approach on two diverse environments given raw pixel observations -- simple
grid-world domains (MazeBase) and the Doom game engine. <br><br></div></a>
</td></tr>
<tr id=" 381 " class="entry"><td>
<a onclick="toggleVisibility('abstract381');">[|&bull;|]</a><a href="http://arxiv.org/abs/1602.07868v3">
<b/> Weight Normalization: A Simple Reparameterization to Accelerate Training
of Deep Neural Networks (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1602.07868v3">
Tim Salimans, Diederik P. Kingma &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract381');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract381');"><div id="abstract381" style="font-size: 1rem; color:black; display: none">
<u><I> 1602.07868v3 &nbsp; - &nbsp;
{deep}
{gradient}
{reinforcement}
</I></u><br> We present weight normalization: a reparameterization of the weight vectors
in a neural network that decouples the length of those weight vectors from
their direction. By reparameterizing the weights in this way we improve the
conditioning of the optimization problem and we speed up convergence of
stochastic gradient descent. Our reparameterization is inspired by batch
normalization but does not introduce any dependencies between the examples in a
minibatch. This means that our method can also be applied successfully to
recurrent models such as LSTMs and to noise-sensitive applications such as deep
reinforcement learning or generative models, for which batch normalization is
less well suited. Although our method is much simpler, it still provides much
of the speed-up of full batch normalization. In addition, the computational
overhead of our method is lower, permitting more optimization steps to be taken
in the same amount of time. We demonstrate the usefulness of our method on
applications in supervised image recognition, generative modelling, and deep
reinforcement learning. <br><br></div></a>
</td></tr>
<tr id=" 382 " class="entry"><td>
<a onclick="toggleVisibility('abstract382');">[|&bull;|]</a><a href="http://arxiv.org/abs/1602.01783v2">
<b/> Asynchronous Methods for Deep Reinforcement Learning (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1602.01783v2">
Volodymyr Mnih, Adri&#224; Puigdom&#232;nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract382');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract382');"><div id="abstract382" style="font-size: 1rem; color:black; display: none">
<u><I> 1602.01783v2 &nbsp; - &nbsp;
{control}
{deep}
{gradient}
{reinforcement}
</I></u><br> We propose a conceptually simple and lightweight framework for deep
reinforcement learning that uses asynchronous gradient descent for optimization
of deep neural network controllers. We present asynchronous variants of four
standard reinforcement learning algorithms and show that parallel
actor-learners have a stabilizing effect on training allowing all four methods
to successfully train neural network controllers. The best performing method,
an asynchronous variant of actor-critic, surpasses the current state-of-the-art
on the Atari domain while training for half the time on a single multi-core CPU
instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds
on a wide variety of continuous motor control problems as well as on a new task
of navigating random 3D mazes using a visual input. <br><br></div></a>
</td></tr>
<tr id=" 383 " class="entry"><td>
<a onclick="toggleVisibility('abstract383');">[|&bull;|]</a><a href="http://arxiv.org/abs/1606.03657v1">
<b/> InfoGAN: Interpretable Representation Learning by Information Maximizing
Generative Adversarial Nets (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1606.03657v1">
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract383');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract383');"><div id="abstract383" style="font-size: 1rem; color:black; display: none">
<u><I> 1606.03657v1 &nbsp; - &nbsp;
{unsupervised}
</I></u><br> This paper describes InfoGAN, an information-theoretic extension to the
Generative Adversarial Network that is able to learn disentangled
representations in a completely unsupervised manner. InfoGAN is a generative
adversarial network that also maximizes the mutual information between a small
subset of the latent variables and the observation. We derive a lower bound to
the mutual information objective that can be optimized efficiently, and show
that our training procedure can be interpreted as a variation of the Wake-Sleep
algorithm. Specifically, InfoGAN successfully disentangles writing styles from
digit shapes on the MNIST dataset, pose from lighting of 3D rendered images,
and background digits from the central digit on the SVHN dataset. It also
discovers visual concepts that include hair styles, presence/absence of
eyeglasses, and emotions on the CelebA face dataset. Experiments show that
InfoGAN learns interpretable representations that are competitive with
representations learned by existing fully supervised methods. <br><br></div></a>
</td></tr>
<tr id=" 384 " class="entry"><td>
<a onclick="toggleVisibility('abstract384');">[|&bull;|]</a><a href="http://arxiv.org/abs/1611.01055v1">
<b/> Learning Locomotion Skills Using DeepRL: Does the Choice of Action Space
Matter? (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1611.01055v1">
Xue Bin Peng, Michiel van de Panne &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract384');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract384');"><div id="abstract384" style="font-size: 1rem; color:black; display: none">
<u><I> 1611.01055v1 &nbsp; - &nbsp;
{deep}
{imitation}
{reinforcement}
</I></u><br> The use of deep reinforcement learning allows for high-dimensional state
descriptors, but little is known about how the choice of action representation
impacts the learning difficulty and the resulting performance. We compare the
impact of four different action parameterizations (torques, muscle-activations,
target joint angles, and target joint-angle velocities) in terms of learning
time, policy robustness, motion quality, and policy query rates. Our results
are evaluated on a gait-cycle imitation task for multiple planar articulated
figures and multiple gaits. We demonstrate that the local feedback provided by
higher-level action parameterizations can significantly impact the learning,
robustness, and quality of the resulting policies. <br><br></div></a>
</td></tr>
<tr id=" 385 " class="entry"><td>
<a onclick="toggleVisibility('abstract385');">[|&bull;|]</a><a href="http://arxiv.org/abs/1611.01224v2">
<b/> Sample Efficient Actor-Critic with Experience Replay (2016)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1611.01224v2">
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, Nando de Freitas &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract385');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract385');"><div id="abstract385" style="font-size: 1rem; color:black; display: none">
<u><I> 1611.01224v2 &nbsp; - &nbsp;
{control}
{deep}
{reinforcement}
{replay}
</I></u><br> This paper presents an actor-critic deep reinforcement learning agent with
experience replay that is stable, sample efficient, and performs remarkably
well on challenging environments, including the discrete 57-game Atari domain
and several continuous control problems. To achieve this, the paper introduces
several innovations, including truncated importance sampling with bias
correction, stochastic dueling network architectures, and a new trust region
policy optimization method. <br><br></div></a>
</td></tr>
<tr id=" 386 " class="entry"><td>
<a onclick="toggleVisibility('abstract386');">[|&bull;|]</a><a href="http://arxiv.org/abs/1509.06113v3">
<b/> Deep Spatial Autoencoders for Visuomotor Learning (2015)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1509.06113v3">
Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, Pieter Abbeel &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract386');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract386');"><div id="abstract386" style="font-size: 1rem; color:black; display: none">
<u><I> 1509.06113v3 &nbsp; - &nbsp;
{control}
{deep}
{reinforcement}
{robot}
{skill}
</I></u><br> Reinforcement learning provides a powerful and flexible framework for
automated acquisition of robotic motion skills. However, applying reinforcement
learning requires a sufficiently detailed representation of the state,
including the configuration of task-relevant objects. We present an approach
that automates state-space construction by learning a state representation
directly from camera images. Our method uses a deep spatial autoencoder to
acquire a set of feature points that describe the environment for the current
task, such as the positions of objects, and then learns a motion skill with
these feature points using an efficient reinforcement learning method based on
local linear models. The resulting controller reacts continuously to the
learned feature points, allowing the robot to dynamically manipulate objects in
the world with closed-loop control. We demonstrate our method with a PR2 robot
on tasks that include pushing a free-standing toy block, picking up a bag of
rice using a spatula, and hanging a loop of rope on a hook at various
positions. In each task, our method automatically learns to track task-relevant
objects and manipulate their configuration with the robot's arm. <br><br></div></a>
</td></tr>
<tr id=" 387 " class="entry"><td>
<a onclick="toggleVisibility('abstract387');">[|&bull;|]</a><a href="http://arxiv.org/abs/1505.05770v6">
<b/> Variational Inference with Normalizing Flows (2015)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1505.05770v6">
Danilo Jimenez Rezende, Shakir Mohamed &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract387');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract387');"><div id="abstract387" style="font-size: 1rem; color:black; display: none">
<u><I> 1505.05770v6 &nbsp; - &nbsp;
</I></u><br> The choice of approximate posterior distribution is one of the core problems
in variational inference. Most applications of variational inference employ
simple families of posterior approximations in order to allow for efficient
inference, focusing on mean-field or other simple structured approximations.
This restriction has a significant impact on the quality of inferences made
using variational methods. We introduce a new approach for specifying flexible,
arbitrarily complex and scalable approximate posterior distributions. Our
approximations are distributions constructed through a normalizing flow,
whereby a simple initial density is transformed into a more complex one by
applying a sequence of invertible transformations until a desired level of
complexity is attained. We use this view of normalizing flows to develop
categories of finite and infinitesimal flows and provide a unified view of
approaches for constructing rich posterior approximations. We demonstrate that
the theoretical advantages of having posteriors that better match the true
posterior, combined with the scalability of amortized variational approaches,
provides a clear improvement in performance and applicability of variational
inference. <br><br></div></a>
</td></tr>
<tr id=" 388 " class="entry"><td>
<a onclick="toggleVisibility('abstract388');">[|&bull;|]</a><a href="http://arxiv.org/abs/1509.03005v1">
<b/> Compatible Value Gradients for Reinforcement Learning of Continuous Deep
Policies (2015)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1509.03005v1">
David Balduzzi, Muhammad Ghifary &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract388');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract388');"><div id="abstract388" style="font-size: 1rem; color:black; display: none">
<u><I> 1509.03005v1 &nbsp; - &nbsp;
{deep}
{gradient}
{reinforcement}
</I></u><br> This paper proposes GProp, a deep reinforcement learning algorithm for
continuous policies with compatible function approximation. The algorithm is
based on two innovations. Firstly, we present a temporal-difference based
method for learning the gradient of the value-function. Secondly, we present
the deviator-actor-critic (DAC) model, which comprises three neural networks
that estimate the value function, its gradient, and determine the actor's
policy respectively. We evaluate GProp on two challenging tasks: a contextual
bandit problem constructed from nonparametric regression datasets that is
designed to probe the ability of reinforcement learning algorithms to
accurately estimate gradients; and the octopus arm, a challenging reinforcement
learning benchmark. GProp is competitive with fully supervised methods on the
bandit task and achieves the best performance to date on the octopus arm. <br><br></div></a>
</td></tr>
<tr id=" 389 " class="entry"><td>
<a onclick="toggleVisibility('abstract389');">[|&bull;|]</a><a href="http://arxiv.org/abs/1509.01149v3">
<b/> Model Predictive Path Integral Control using Covariance Variable
Importance Sampling (2015)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1509.01149v3">
Grady Williams, Andrew Aldrich, Evangelos Theodorou &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract389');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract389');"><div id="abstract389" style="font-size: 1rem; color:black; display: none">
<u><I> 1509.01149v3 &nbsp; - &nbsp;
{control}
</I></u><br> In this paper we develop a Model Predictive Path Integral (MPPI) control
algorithm based on a generalized importance sampling scheme and perform
parallel optimization via sampling using a Graphics Processing Unit (GPU). The
proposed generalized importance sampling scheme allows for changes in the drift
and diffusion terms of stochastic diffusion processes and plays a significant
role in the performance of the model predictive control algorithm. We compare
the proposed algorithm in simulation with a model predictive control version of
differential dynamic programming. <br><br></div></a>
</td></tr>
<tr id=" 390 " class="entry"><td>
<a onclick="toggleVisibility('abstract390');">[|&bull;|]</a><a href="http://arxiv.org/abs/1507.00210v1">
<b/> Natural Neural Networks (2015)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1507.00210v1">
Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, Koray Kavukcuoglu &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract390');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract390');"><div id="abstract390" style="font-size: 1rem; color:black; display: none">
<u><I> 1507.00210v1 &nbsp; - &nbsp;
{gradient}
{unsupervised}
</I></u><br> We introduce Natural Neural Networks, a novel family of algorithms that speed
up convergence by adapting their internal representation during training to
improve conditioning of the Fisher matrix. In particular, we show a specific
example that employs a simple and efficient reparametrization of the neural
network weights by implicitly whitening the representation obtained at each
layer, while preserving the feed-forward computation of the network. Such
networks can be trained efficiently via the proposed Projected Natural Gradient
Descent algorithm (PRONG), which amortizes the cost of these reparametrizations
over many parameter updates and is closely related to the Mirror Descent online
learning algorithm. We highlight the benefits of our method on both
unsupervised and supervised learning tasks, and showcase its scalability by
training on the large-scale ImageNet Challenge dataset. <br><br></div></a>
</td></tr>
<tr id=" 391 " class="entry"><td>
<a onclick="toggleVisibility('abstract391');">[|&bull;|]</a><a href="http://arxiv.org/abs/1504.04909v1">
<b/> Illuminating search spaces by mapping elites (2015)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1504.04909v1">
Jean-Baptiste Mouret, Jeff Clune &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract391');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract391');"><div id="abstract391" style="font-size: 1rem; color:black; display: none">
<u><I> 1504.04909v1 &nbsp; - &nbsp;
{diversity}
{robot}
</I></u><br> Many fields use search algorithms, which automatically explore a search space
to find high-performing solutions: chemists search through the space of
molecules to discover new drugs; engineers search for stronger, cheaper, safer
designs, scientists search for models that best explain data, etc. The goal of
search algorithms has traditionally been to return the single
highest-performing solution in a search space. Here we describe a new,
fundamentally different type of algorithm that is more useful because it
provides a holistic view of how high-performing solutions are distributed
throughout a search space. It creates a map of high-performing solutions at
each point in a space defined by dimensions of variation that a user gets to
choose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites)
algorithm illuminates search spaces, allowing researchers to understand how
interesting attributes of solutions combine to affect performance, either
positively or, equally of interest, negatively. For example, a drug company may
wish to understand how performance changes as the size of molecules and their
cost-to-produce vary. MAP-Elites produces a large diversity of high-performing,
yet qualitatively different solutions, which can be more helpful than a single,
high-performing solution. Interestingly, because MAP-Elites explores more of
the search space, it also tends to find a better overall solution than
state-of-the-art search algorithms. We demonstrate the benefits of this new
algorithm in three different problem domains ranging from producing modular
neural networks to designing simulated and real soft robots. Because MAP-
Elites (1) illuminates the relationship between performance and dimensions of
interest in solutions, (2) returns a set of high-performing, yet diverse
solutions, and (3) improves finding a single, best solution, it will advance
science and engineering. <br><br></div></a>
</td></tr>
<tr id=" 392 " class="entry"><td>
<a onclick="toggleVisibility('abstract392');">[|&bull;|]</a><a href="http://arxiv.org/abs/1506.02438v6">
<b/> High-Dimensional Continuous Control Using Generalized Advantage
Estimation (2015)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1506.02438v6">
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract392');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract392');"><div id="abstract392" style="font-size: 1rem; color:black; display: none">
<u><I> 1506.02438v6 &nbsp; - &nbsp;
{gradient}
{kinematic}
{reinforcement}
{robot}
</I></u><br> Policy gradient methods are an appealing approach in reinforcement learning
because they directly optimize the cumulative reward and can straightforwardly
be used with nonlinear function approximators such as neural networks. The two
main challenges are the large number of samples typically required, and the
difficulty of obtaining stable and steady improvement despite the
nonstationarity of the incoming data. We address the first challenge by using
value functions to substantially reduce the variance of policy gradient
estimates at the cost of some bias, with an exponentially-weighted estimator of
the advantage function that is analogous to TD(lambda). We address the second
challenge by using trust region optimization procedure for both the policy and
the value function, which are represented by neural networks.
Our approach yields strong empirical results on highly challenging 3D
locomotion tasks, learning running gaits for bipedal and quadrupedal simulated
robots, and learning a policy for getting the biped to stand up from starting
out lying on the ground. In contrast to a body of prior work that uses
hand-crafted policy representations, our neural network policies map directly
from raw kinematics to joint torques. Our algorithm is fully model-free, and
the amount of simulated experience required for the learning tasks on 3D bipeds
corresponds to 1-2 weeks of real time. <br><br></div></a>
</td></tr>
<tr id=" 393 " class="entry"><td>
<a onclick="toggleVisibility('abstract393');">[|&bull;|]</a><a href="http://arxiv.org/abs/1502.05477v5">
<b/> Trust Region Policy Optimization (2015)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1502.05477v5">
John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract393');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract393');"><div id="abstract393" style="font-size: 1rem; color:black; display: none">
<u><I> 1502.05477v5 &nbsp; - &nbsp;
{gradient}
{on-policy}
{robot}
</I></u><br> We describe an iterative procedure for optimizing policies, with guaranteed
monotonic improvement. By making several approximations to the
theoretically-justified procedure, we develop a practical algorithm, called
Trust Region Policy Optimization (TRPO). This algorithm is similar to natural
policy gradient methods and is effective for optimizing large nonlinear
policies such as neural networks. Our experiments demonstrate its robust
performance on a wide variety of tasks: learning simulated robotic swimming,
hopping, and walking gaits; and playing Atari games using images of the screen
as input. Despite its approximations that deviate from the theory, TRPO tends
to give monotonic improvement, with little tuning of hyperparameters. <br><br></div></a>
</td></tr>
<tr id=" 394 " class="entry"><td>
<a onclick="toggleVisibility('abstract394');">[|&bull;|]</a><a href="http://arxiv.org/abs/1511.02540v1">
<b/> Speed learning on the fly (2015)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1511.02540v1">
Pierre-Yves Mass&#233;, Yann Ollivier &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract394');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract394');"><div id="abstract394" style="font-size: 1rem; color:black; display: none">
<u><I> 1511.02540v1 &nbsp; - &nbsp;
{gradient}
</I></u><br> The practical performance of online stochastic gradient descent algorithms is
highly dependent on the chosen step size, which must be tediously hand-tuned in
many applications. The same is true for more advanced variants of stochastic
gradients, such as SAGA, SVRG, or AdaGrad. Here we propose to adapt the step
size by performing a gradient descent on the step size itself, viewing the
whole performance of the learning trajectory as a function of step size.
Importantly, this adaptation can be computed online at little cost, without
having to iterate backward passes over the full data. <br><br></div></a>
</td></tr>
<tr id=" 395 " class="entry"><td>
<a onclick="toggleVisibility('abstract395');">[|&bull;|]</a><a href="http://arxiv.org/abs/1511.06279v4">
<b/> Neural Programmer-Interpreters (2015)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1511.06279v4">
Scott Reed, Nando de Freitas &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract395');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract395');"><div id="abstract395" style="font-size: 1rem; color:black; display: none">
<u><I> 1511.06279v4 &nbsp; - &nbsp;
</I></u><br> We propose the neural programmer-interpreter (NPI): a recurrent and
compositional neural network that learns to represent and execute programs. NPI
has three learnable components: a task-agnostic recurrent core, a persistent
key-value program memory, and domain-specific encoders that enable a single NPI
to operate in multiple perceptually diverse environments with distinct
affordances. By learning to compose lower-level programs to express
higher-level programs, NPI reduces sample complexity and increases
generalization ability compared to sequence-to-sequence LSTMs. The program
memory allows efficient learning of additional tasks by building on existing
programs. NPI can also harness the environment (e.g. a scratch pad with
read-write pointers) to cache intermediate results of computation, lessening
the long-term memory burden on recurrent hidden units. In this work we train
the NPI with fully-supervised execution traces; each program has example
sequences of calls to the immediate subprograms conditioned on the input.
Rather than training on a huge number of relatively weak labels, NPI learns
from a small number of rich examples. We demonstrate the capability of our
model to learn several types of compositional programs: addition, sorting, and
canonicalizing 3D models. Furthermore, a single NPI learns to execute these
programs and all 21 associated subprograms. <br><br></div></a>
</td></tr>
<tr id=" 396 " class="entry"><td>
<a onclick="toggleVisibility('abstract396');">[|&bull;|]</a><a href="http://arxiv.org/abs/1502.03167v3">
<b/> Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift (2015)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1502.03167v3">
Sergey Ioffe, Christian Szegedy &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract396');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract396');"><div id="abstract396" style="font-size: 1rem; color:black; display: none">
<u><I> 1502.03167v3 &nbsp; - &nbsp;
{deep}
</I></u><br> Training Deep Neural Networks is complicated by the fact that the
distribution of each layer's inputs changes during training, as the parameters
of the previous layers change. This slows down the training by requiring lower
learning rates and careful parameter initialization, and makes it notoriously
hard to train models with saturating nonlinearities. We refer to this
phenomenon as internal covariate shift, and address the problem by normalizing
layer inputs. Our method draws its strength from making normalization a part of
the model architecture and performing the normalization for each training
mini-batch. Batch Normalization allows us to use much higher learning rates and
be less careful about initialization. It also acts as a regularizer, in some
cases eliminating the need for Dropout. Applied to a state-of-the-art image
classification model, Batch Normalization achieves the same accuracy with 14
times fewer training steps, and beats the original model by a significant
margin. Using an ensemble of batch-normalized networks, we improve upon the
best published result on ImageNet classification: reaching 4.9% top-5
validation error (and 4.8% test error), exceeding the accuracy of human raters. <br><br></div></a>
</td></tr>
<tr id=" 397 " class="entry"><td>
<a onclick="toggleVisibility('abstract397');">[|&bull;|]</a><a href="http://arxiv.org/abs/1504.00702v5">
<b/> End-to-End Training of Deep Visuomotor Policies (2015)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1504.00702v5">
Sergey Levine, Chelsea Finn, Trevor Darrell, Pieter Abbeel &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract397');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract397');"><div id="abstract397" style="font-size: 1rem; color:black; display: none">
<u><I> 1504.00702v5 &nbsp; - &nbsp;
{control}
{deep}
{reinforcement}
{robot}
</I></u><br> Policy search methods can allow robots to learn control policies for a wide
range of tasks, but practical applications of policy search often require
hand-engineered components for perception, state estimation, and low-level
control. In this paper, we aim to answer the following question: does training
the perception and control systems jointly end-to-end provide better
performance than training each component separately? To this end, we develop a
method that can be used to learn policies that map raw image observations
directly to torques at the robot's motors. The policies are represented by deep
convolutional neural networks (CNNs) with 92,000 parameters, and are trained
using a partially observed guided policy search method, which transforms policy
search into supervised learning, with supervision provided by a simple
trajectory-centric reinforcement learning method. We evaluate our method on a
range of real-world manipulation tasks that require close coordination between
vision and control, such as screwing a cap onto a bottle, and present simulated
comparisons to a range of prior policy search methods. <br><br></div></a>
</td></tr>
<tr id=" 398 " class="entry"><td>
<a onclick="toggleVisibility('abstract398');">[|&bull;|]</a><a href="http://arxiv.org/abs/1509.02971v6">
<b/> Continuous control with deep reinforcement learning (2015)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1509.02971v6">
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract398');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract398');"><div id="abstract398" style="font-size: 1rem; color:black; display: none">
<u><I> 1509.02971v6 &nbsp; - &nbsp;
{deep}
{gradient}
</I></u><br> We adapt the ideas underlying the success of Deep Q-Learning to the
continuous action domain. We present an actor-critic, model-free algorithm
based on the deterministic policy gradient that can operate over continuous
action spaces. Using the same learning algorithm, network architecture and
hyper-parameters, our algorithm robustly solves more than 20 simulated physics
tasks, including classic problems such as cartpole swing-up, dexterous
manipulation, legged locomotion and car driving. Our algorithm is able to find
policies whose performance is competitive with those found by a planning
algorithm with full access to the dynamics of the domain and its derivatives.
We further demonstrate that for many of the tasks the algorithm can learn
policies end-to-end: directly from raw pixel inputs. <br><br></div></a>
</td></tr>
<tr id=" 399 " class="entry"><td>
<a onclick="toggleVisibility('abstract399');">[|&bull;|]</a><a href="http://arxiv.org/abs/1511.05952v4">
<b/> Prioritized Experience Replay (2015)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1511.05952v4">
Tom Schaul, John Quan, Ioannis Antonoglou, David Silver &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract399');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract399');"><div id="abstract399" style="font-size: 1rem; color:black; display: none">
<u><I> 1511.05952v4 &nbsp; - &nbsp;
{deep}
{reinforcement}
{replay}
</I></u><br> Experience replay lets online reinforcement learning agents remember and
reuse experiences from the past. In prior work, experience transitions were
uniformly sampled from a replay memory. However, this approach simply replays
transitions at the same frequency that they were originally experienced,
regardless of their significance. In this paper we develop a framework for
prioritizing experience, so as to replay important transitions more frequently,
and therefore learn more efficiently. We use prioritized experience replay in
Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved
human-level performance across many Atari games. DQN with prioritized
experience replay achieves a new state-of-the-art, outperforming DQN with
uniform replay on 41 out of 49 games. <br><br></div></a>
</td></tr>
<tr id=" 400 " class="entry"><td>
<a onclick="toggleVisibility('abstract400');">[|&bull;|]</a><a href="http://arxiv.org/abs/1507.07680v2">
<b/> Training recurrent networks online without backtracking (2015)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1507.07680v2">
Yann Ollivier, Corentin Tallec, Guillaume Charpiat &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract400');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract400');"><div id="abstract400" style="font-size: 1rem; color:black; display: none">
<u><I> 1507.07680v2 &nbsp; - &nbsp;
{evolution}
{gradient}
</I></u><br> We introduce the "NoBackTrack" algorithm to train the parameters of dynamical
systems such as recurrent neural networks. This algorithm works in an online,
memoryless setting, thus requiring no backpropagation through time, and is
scalable, avoiding the large computational and memory cost of maintaining the
full gradient of the current state with respect to the parameters.
The algorithm essentially maintains, at each time, a single search direction
in parameter space. The evolution of this search direction is partly stochastic
and is constructed in such a way to provide, at every time, an unbiased random
estimate of the gradient of the loss function with respect to the parameters.
Because the gradient estimate is unbiased, on average over time the parameter
is updated as it should.
The resulting gradient estimate can then be fed to a lightweight Kalman-like
filter to yield an improved algorithm. For recurrent neural networks, the
resulting algorithms scale linearly with the number of parameters.
Small-scale experiments confirm the suitability of the approach, showing that
the stochastic approximation of the gradient introduced in the algorithm is not
detrimental to learning. In particular, the Kalman-like version of NoBackTrack
is superior to backpropagation through time (BPTT) when the time span of
dependencies in the data is longer than the truncation span for BPTT. <br><br></div></a>
</td></tr>
<tr id=" 401 " class="entry"><td>
<a onclick="toggleVisibility('abstract401');">[|&bull;|]</a><a href="http://arxiv.org/abs/1412.1897v4">
<b/> Deep Neural Networks are Easily Fooled: High Confidence Predictions for
Unrecognizable Images (2014)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1412.1897v4">
Anh Nguyen, Jason Yosinski, Jeff Clune &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract401');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract401');"><div id="abstract401" style="font-size: 1rem; color:black; display: none">
<u><I> 1412.1897v4 &nbsp; - &nbsp;
{deep}
{evolution}
{gradient}
</I></u><br> Deep neural networks (DNNs) have recently been achieving state-of-the-art
performance on a variety of pattern-recognition tasks, most notably visual
classification problems. Given that DNNs are now able to classify objects in
images with near-human-level performance, questions naturally arise as to what
differences remain between computer and human vision. A recent study revealed
that changing an image (e.g. of a lion) in a way imperceptible to humans can
cause a DNN to label the image as something else entirely (e.g. mislabeling a
lion a library). Here we show a related result: it is easy to produce images
that are completely unrecognizable to humans, but that state-of-the-art DNNs
believe to be recognizable objects with 99.99% confidence (e.g. labeling with
certainty that white noise static is a lion). Specifically, we take
convolutional neural networks trained to perform well on either the ImageNet or
MNIST datasets and then find images with evolutionary algorithms or gradient
ascent that DNNs label with high confidence as belonging to each dataset class.
It is possible to produce images totally unrecognizable to human eyes that DNNs
believe with near certainty are familiar objects, which we call "fooling
images" (more generally, fooling examples). Our results shed light on
interesting differences between human vision and current DNNs, and raise
questions about the generality of DNN computer vision. <br><br></div></a>
</td></tr>
<tr id=" 402 " class="entry"><td>
<a onclick="toggleVisibility('abstract402');">[|&bull;|]</a><a href="http://arxiv.org/abs/1412.8690v2">
<b/> Breaking the Curse of Dimensionality with Convex Neural Networks (2014)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1412.8690v2">
Francis Bach &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract402');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract402');"><div id="abstract402" style="font-size: 1rem; color:black; display: none">
<u><I> 1412.8690v2 &nbsp; - &nbsp;
</I></u><br> We consider neural networks with a single hidden layer and non-decreasing
homogeneous activa-tion functions like the rectified linear units. By letting
the number of hidden units grow unbounded and using classical non-Euclidean
regularization tools on the output weights, we provide a detailed theoretical
analysis of their generalization performance, with a study of both the
approximation and the estimation errors. We show in particular that they are
adaptive to unknown underlying linear structures, such as the dependence on the
projection of the input variables onto a low-dimensional subspace. Moreover,
when using sparsity-inducing norms on the input weights, we show that
high-dimensional non-linear variable selection may be achieved, without any
strong assumption regarding the data and with a total number of variables
potentially exponential in the number of ob-servations. In addition, we provide
a simple geometric interpretation to the non-convex problem of addition of a
new unit, which is the core potentially hard computational element in the
framework of learning from continuously many basis functions. We provide simple
conditions for convex relaxations to achieve the same generalization error
bounds, even when constant-factor approxi-mations cannot be found (e.g.,
because it is NP-hard such as for the zero-homogeneous activation function). We
were not able to find strong enough convex relaxations and leave open the
existence or non-existence of polynomial-time algorithms. <br><br></div></a>
</td></tr>
<tr id=" 403 " class="entry"><td>
<a onclick="toggleVisibility('abstract403');">[|&bull;|]</a><a href="http://arxiv.org/abs/1412.1193v11">
<b/> New insights and perspectives on the natural gradient method (2014)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1412.1193v11">
James Martens &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract403');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract403');"><div id="abstract403" style="font-size: 1rem; color:black; display: none">
<u><I> 1412.1193v11 &nbsp; - &nbsp;
{gradient}
</I></u><br> Natural gradient descent is an optimization method traditionally motivated
from the perspective of information geometry, and works well for many
applications as an alternative to stochastic gradient descent. In this paper we
critically analyze this method and its properties, and show how it can be
viewed as a type of 2nd-order optimization method, with the Fisher information
matrix acting as a substitute for the Hessian. In many important cases, the
Fisher information matrix is shown to be equivalent to the Generalized
Gauss-Newton matrix, which both approximates the Hessian, but also has certain
properties that favor its use over the Hessian. This perspective turns out to
have significant implications for the design of a practical and robust natural
gradient optimizer, as it motivates the use of techniques like trust regions
and Tikhonov regularization. Additionally, we make a series of contributions to
the understanding of natural gradient and 2nd-order methods, including: a
thorough analysis of the convergence speed of stochastic natural gradient
descent (and more general stochastic 2nd-order methods) as applied to convex
quadratics, a critical examination of the oft-used "empirical" approximation of
the Fisher matrix, and an analysis of the (approximate) parameterization
invariance property possessed by natural gradient methods (which we show also
holds for certain other curvature, but notably not the Hessian). <br><br></div></a>
</td></tr>
<tr id=" 404 " class="entry"><td>
<a onclick="toggleVisibility('abstract404');">[|&bull;|]</a><a href="http://arxiv.org/abs/1412.7009v3">
<b/> Generative Class-conditional Autoencoders (2014)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1412.7009v3">
Jan Rudy, Graham Taylor &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract404');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract404');"><div id="abstract404" style="font-size: 1rem; color:black; display: none">
<u><I> 1412.7009v3 &nbsp; - &nbsp;
</I></u><br> Recent work by Bengio et al. (2013) proposes a sampling procedure for
denoising autoencoders which involves learning the transition operator of a
Markov chain. The transition operator is typically unimodal, which limits its
capacity to model complex data. In order to perform efficient sampling from
conditional distributions, we extend this work, both theoretically and
algorithmically, to gated autoencoders (Memisevic, 2013), The proposed model is
able to generate convincing class-conditional samples when trained on both the
MNIST and TFD datasets. <br><br></div></a>
</td></tr>
<tr id=" 405 " class="entry"><td>
<a onclick="toggleVisibility('abstract405');">[|&bull;|]</a><a href="http://arxiv.org/abs/1411.1792v1">
<b/> How transferable are features in deep neural networks? (2014)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1411.1792v1">
Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract405');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract405');"><div id="abstract405" style="font-size: 1rem; color:black; display: none">
<u><I> 1411.1792v1 &nbsp; - &nbsp;
{deep}
{transfer}
</I></u><br> Many deep neural networks trained on natural images exhibit a curious
phenomenon in common: on the first layer they learn features similar to Gabor
filters and color blobs. Such first-layer features appear not to be specific to
a particular dataset or task, but general in that they are applicable to many
datasets and tasks. Features must eventually transition from general to
specific by the last layer of the network, but this transition has not been
studied extensively. In this paper we experimentally quantify the generality
versus specificity of neurons in each layer of a deep convolutional neural
network and report a few surprising results. Transferability is negatively
affected by two distinct issues: (1) the specialization of higher layer neurons
to their original task at the expense of performance on the target task, which
was expected, and (2) optimization difficulties related to splitting networks
between co-adapted neurons, which was not expected. In an example network
trained on ImageNet, we demonstrate that either of these two issues may
dominate, depending on whether features are transferred from the bottom,
middle, or top of the network. We also document that the transferability of
features decreases as the distance between the base task and target task
increases, but that transferring features even from distant tasks can be better
than using random features. A final surprising result is that initializing a
network with transferred features from almost any number of layers can produce
a boost to generalization that lingers even after fine-tuning to the target
dataset. <br><br></div></a>
</td></tr>
<tr id=" 406 " class="entry"><td>
<a onclick="toggleVisibility('abstract406');">[|&bull;|]</a><a href="http://arxiv.org/abs/1310.5726v5">
<b/> Correlation and variable importance in random forests (2013)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1310.5726v5">
Baptiste Gregorutti, Bertrand Michel, Philippe Saint-Pierre &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract406');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract406');"><div id="abstract406" style="font-size: 1rem; color:black; display: none">
<u><I> 1310.5726v5 &nbsp; - &nbsp;
</I></u><br> This paper is about variable selection with the random forests algorithm in
presence of correlated predictors. In high-dimensional regression or
classification frameworks, variable selection is a difficult task, that becomes
even more challenging in the presence of highly correlated predictors. Firstly
we provide a theoretical study of the permutation importance measure for an
additive regression model. This allows us to describe how the correlation
between predictors impacts the permutation importance. Our results motivate the
use of the Recursive Feature Elimination (RFE) algorithm for variable selection
in this context. This algorithm recursively eliminates the variables using
permutation importance measure as a ranking criterion. Next various simulation
experiments illustrate the efficiency of the RFE algorithm for selecting a
small number of variables together with a good prediction error. Finally, this
selection algorithm is tested on the Landsat Satellite data from the UCI
Machine Learning Repository. <br><br></div></a>
</td></tr>
<tr id=" 407 " class="entry"><td>
<a onclick="toggleVisibility('abstract407');">[|&bull;|]</a><a href="http://arxiv.org/abs/1306.3532v4">
<b/> Fast Marching Tree: a Fast Marching Sampling-Based Method for Optimal
Motion Planning in Many Dimensions (2013)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1306.3532v4">
Lucas Janson, Edward Schmerling, Ashley Clark, Marco Pavone &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract407');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract407');"><div id="abstract407" style="font-size: 1rem; color:black; display: none">
<u><I> 1306.3532v4 &nbsp; - &nbsp;
{motion planning}
{optimal}
</I></u><br> In this paper we present a novel probabilistic sampling-based motion planning
algorithm called the Fast Marching Tree algorithm (FMT*). The algorithm is
specifically aimed at solving complex motion planning problems in
high-dimensional configuration spaces. This algorithm is proven to be
asymptotically optimal and is shown to converge to an optimal solution faster
than its state-of-the-art counterparts, chiefly PRM* and RRT*. The FMT*
algorithm performs a "lazy" dynamic programming recursion on a predetermined
number of probabilistically-drawn samples to grow a tree of paths, which moves
steadily outward in cost-to-arrive space. As a departure from previous analysis
approaches that are based on the notion of almost sure convergence, the FMT*
algorithm is analyzed under the notion of convergence in probability: the extra
mathematical flexibility of this approach allows for convergence rate
bounds--the first in the field of optimal sampling-based motion planning.
Specifically, for a certain selection of tuning parameters and configuration
spaces, we obtain a convergence rate bound of order $O(n^{-1/d+\rho})$, where
$n$ is the number of sampled points, $d$ is the dimension of the configuration
space, and $\rho$ is an arbitrarily small constant. We go on to demonstrate
asymptotic optimality for a number of variations on FMT*, namely when the
configuration space is sampled non-uniformly, when the cost is not arc length,
and when connections are made based on the number of nearest neighbors instead
of a fixed connection radius. Numerical experiments over a range of dimensions
and obstacle configurations confirm our theoretical and heuristic arguments by
showing that FMT*, for a given execution time, returns substantially better
solutions than either PRM* or RRT*, especially in high-dimensional
configuration spaces and in scenarios where collision-checking is expensive. <br><br></div></a>
</td></tr>
<tr id=" 408 " class="entry"><td>
<a onclick="toggleVisibility('abstract408');">[|&bull;|]</a><a href="http://arxiv.org/abs/1301.3584v7">
<b/> Revisiting Natural Gradient for Deep Networks (2013)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1301.3584v7">
Razvan Pascanu, Yoshua Bengio &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract408');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract408');"><div id="abstract408" style="font-size: 1rem; color:black; display: none">
<u><I> 1301.3584v7 &nbsp; - &nbsp;
{deep}
{gradient}
</I></u><br> We evaluate natural gradient, an algorithm originally proposed in Amari
(1997), for learning deep models. The contributions of this paper are as
follows. We show the connection between natural gradient and three other
recently proposed methods for training deep models: Hessian-Free (Martens,
2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA (Le Roux et
al., 2008). We describe how one can use unlabeled data to improve the
generalization error obtained by natural gradient and empirically evaluate the
robustness of the algorithm to the ordering of the training set compared to
stochastic gradient descent. Finally we extend natural gradient to incorporate
second order information alongside the manifold information and provide a
benchmark of the new algorithm using a truncated Newton approach for inverting
the metric matrix instead of using a diagonal approximation of it. <br><br></div></a>
</td></tr>
<tr id=" 409 " class="entry"><td>
<a onclick="toggleVisibility('abstract409');">[|&bull;|]</a><a href="http://arxiv.org/abs/1311.1839v2">
<b/> An Efficiently Solvable Quadratic Program for Stabilizing Dynamic
Locomotion (2013)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1311.1839v2">
Scott Kuindersma, Frank Permenter, Russ Tedrake &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract409');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract409');"><div id="abstract409" style="font-size: 1rem; color:black; display: none">
<u><I> 1311.1839v2 &nbsp; - &nbsp;
{control}
{humanoid}
{optimal}
{robot}
</I></u><br> We describe a whole-body dynamic walking controller implemented as a convex
quadratic program. The controller solves an optimal control problem using an
approximate value function derived from a simple walking model while respecting
the dynamic, input, and contact constraints of the full robot dynamics. By
exploiting sparsity and temporal structure in the optimization with a custom
active-set algorithm, we surpass the performance of the best available
off-the-shelf solvers and achieve 1kHz control rates for a 34-DOF humanoid. We
describe applications to balancing and walking tasks using the simulated Atlas
robot in the DARPA Virtual Robotics Challenge. <br><br></div></a>
</td></tr>
<tr id=" 410 " class="entry"><td>
<a onclick="toggleVisibility('abstract410');">[|&bull;|]</a><a href="http://arxiv.org/abs/1306.0514v4">
<b/> Riemannian metrics for neural networks II: recurrent networks and
learning symbolic data sequences (2013)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1306.0514v4">
Yann Ollivier &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract410');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract410');"><div id="abstract410" style="font-size: 1rem; color:black; display: none">
<u><I> 1306.0514v4 &nbsp; - &nbsp;
{evolution}
{gradient}
{sparse}
</I></u><br> Recurrent neural networks are powerful models for sequential data, able to
represent complex dependencies in the sequence that simpler models such as
hidden Markov models cannot handle. Yet they are notoriously hard to train.
Here we introduce a training procedure using a gradient ascent in a Riemannian
metric: this produces an algorithm independent from design choices such as the
encoding of parameters and unit activities. This metric gradient ascent is
designed to have an algorithmic cost close to backpropagation through time for
sparsely connected networks. We use this procedure on gated leaky neural
networks (GLNNs), a variant of recurrent neural networks with an architecture
inspired by finite automata and an evolution equation inspired by
continuous-time networks. GLNNs trained with a Riemannian gradient are
demonstrated to effectively capture a variety of structures in synthetic
problems: basic block nesting as in context-free grammars (an important feature
of natural languages, but difficult to learn), intersections of multiple
independent Markov-type relations, or long-distance relationships such as the
distant-XOR problem. This method does not require adjusting the network
structure or initial parameters: the network used is a sparse random graph and
the initialization is identical for all problems considered. <br><br></div></a>
</td></tr>
<tr id=" 411 " class="entry"><td>
<a onclick="toggleVisibility('abstract411');">[|&bull;|]</a><a href="http://arxiv.org/abs/1211.0358v2">
<b/> Deep Gaussian Processes (2012)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1211.0358v2">
Andreas C. Damianou, Neil D. Lawrence &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract411');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract411');"><div id="abstract411" style="font-size: 1rem; color:black; display: none">
<u><I> 1211.0358v2 &nbsp; - &nbsp;
{deep}
{gradient}
</I></u><br> In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a
deep belief network based on Gaussian process mappings. The data is modeled as
the output of a multivariate GP. The inputs to that Gaussian process are then
governed by another GP. A single layer model is equivalent to a standard GP or
the GP latent variable model (GP-LVM). We perform inference in the model by
approximate variational marginalization. This results in a strict lower bound
on the marginal likelihood of the model which we use for model selection
(number of layers and nodes per layer). Deep belief networks are typically
applied to relatively large data sets using stochastic gradient descent for
optimization. Our fully Bayesian treatment allows for the application of deep
models even when data is scarce. Model selection by our variational bound shows
that a five layer hierarchy is justified even when modelling a digit data set
containing only 150 examples. <br><br></div></a>
</td></tr>
<tr id=" 412 " class="entry"><td>
<a onclick="toggleVisibility('abstract412');">[|&bull;|]</a><a href="http://arxiv.org/abs/1201.4497v2">
<b/> A geometrical introduction to screw theory (2012)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1201.4497v2">
E. Minguzzi &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract412');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract412');"><div id="abstract412" style="font-size: 1rem; color:black; display: none">
<u><I> 1201.4497v2 &nbsp; - &nbsp;
</I></u><br> This work introduces screw theory, a venerable but yet little known theory
aimed at describing rigid body dynamics. This formulation of mechanics unifies
in the concept of screw the translational and rotational degrees of freedom of
the body. It captures a remarkable mathematical analogy between mechanical
momenta and linear velocities, and between forces and angular velocities. For
instance, it clarifies that angular velocities should be treated as applied
vectors and that, under the composition of motions, they sum with the same
rules of applied forces. This work provides a short and rigorous introduction
to screw theory intended to an undergraduate and general readership. <br><br></div></a>
</td></tr>
<tr id=" 413 " class="entry"><td>
<a onclick="toggleVisibility('abstract413');">[|&bull;|]</a><a href="http://arxiv.org/abs/1212.1524v2">
<b/> Layer-wise learning of deep generative models (2012)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1212.1524v2">
Ludovic Arnold, Yann Ollivier &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract413');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract413');"><div id="abstract413" style="font-size: 1rem; color:black; display: none">
<u><I> 1212.1524v2 &nbsp; - &nbsp;
{deep}
</I></u><br> When using deep, multi-layered architectures to build generative models of
data, it is difficult to train all layers at once. We propose a layer-wise
training procedure admitting a performance guarantee compared to the global
optimum. It is based on an optimistic proxy of future performance, the best
latent marginal. We interpret auto-encoders in this setting as generative
models, by showing that they train a lower bound of this criterion. We test the
new learning procedure against a state of the art method (stacked RBMs), and
find it to improve performance. Both theory and experiments highlight the
importance, when training deep architectures, of using an inference model (from
data to hidden variables) richer than the generative model (from hidden
variables to data). <br><br></div></a>
</td></tr>
<tr id=" 414 " class="entry"><td>
<a onclick="toggleVisibility('abstract414');">[|&bull;|]</a><a href="http://arxiv.org/abs/1202.6258v4">
<b/> A Stochastic Gradient Method with an Exponential Convergence Rate for
Finite Training Sets (2012)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1202.6258v4">
Nicolas Le Roux, Mark Schmidt, Francis Bach &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract414');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract414');"><div id="abstract414" style="font-size: 1rem; color:black; display: none">
<u><I> 1202.6258v4 &nbsp; - &nbsp;
{gradient}
</I></u><br> We propose a new stochastic gradient method for optimizing the sum of a
finite set of smooth functions, where the sum is strongly convex. While
standard stochastic gradient methods converge at sublinear rates for this
problem, the proposed method incorporates a memory of previous gradient values
in order to achieve a linear convergence rate. In a machine learning context,
numerical experiments indicate that the new algorithm can dramatically
outperform standard algorithms, both in terms of optimizing the training error
and reducing the test error quickly. <br><br></div></a>
</td></tr>
<tr id=" 415 " class="entry"><td>
<a onclick="toggleVisibility('abstract415');">[|&bull;|]</a><a href="http://arxiv.org/abs/1206.5538v3">
<b/> Representation Learning: A Review and New Perspectives (2012)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1206.5538v3">
Yoshua Bengio, Aaron Courville, Pascal Vincent &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract415');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract415');"><div id="abstract415" style="font-size: 1rem; color:black; display: none">
<u><I> 1206.5538v3 &nbsp; - &nbsp;
{deep}
{unsupervised}
</I></u><br> The success of machine learning algorithms generally depends on data
representation, and we hypothesize that this is because different
representations can entangle and hide more or less the different explanatory
factors of variation behind the data. Although specific domain knowledge can be
used to help design representations, learning with generic priors can also be
used, and the quest for AI is motivating the design of more powerful
representation-learning algorithms implementing such priors. This paper reviews
recent work in the area of unsupervised feature learning and deep learning,
covering advances in probabilistic models, auto-encoders, manifold learning,
and deep networks. This motivates longer-term unanswered questions about the
appropriate objectives for learning good representations, for computing
representations (i.e., inference), and the geometrical connections between
representation learning, density estimation and manifold learning. <br><br></div></a>
</td></tr>
<tr id=" 416 " class="entry"><td>
<a onclick="toggleVisibility('abstract416');">[|&bull;|]</a><a href="http://arxiv.org/abs/1111.4259v1">
<b/> Krylov Subspace Descent for Deep Learning (2011)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1111.4259v1">
Oriol Vinyals, Daniel Povey &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract416');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract416');"><div id="abstract416" style="font-size: 1rem; color:black; display: none">
<u><I> 1111.4259v1 &nbsp; - &nbsp;
{deep}
{gradient}
</I></u><br> In this paper, we propose a second order optimization method to learn models
where both the dimensionality of the parameter space and the number of training
samples is high. In our method, we construct on each iteration a Krylov
subspace formed by the gradient and an approximation to the Hessian matrix, and
then use a subset of the training data samples to optimize over this subspace.
As with the Hessian Free (HF) method of [7], the Hessian matrix is never
explicitly constructed, and is computed using a subset of data. In practice, as
in HF, we typically use a positive definite substitute for the Hessian matrix
such as the Gauss-Newton matrix. We investigate the effectiveness of our
proposed method on deep neural networks, and compare its performance to widely
used methods such as stochastic gradient descent, conjugate gradient descent
and L-BFGS, and also to HF. Our method leads to faster convergence than either
L-BFGS or HF, and generally performs better than either of them in
cross-validation accuracy. It is also simpler and more general than HF, as it
does not require a positive semi-definite approximation of the Hessian matrix
to work well nor the setting of a damping parameter. The chief drawback versus
HF is the need for memory to store a basis for the Krylov subspace. <br><br></div></a>
</td></tr>
<tr id=" 417 " class="entry"><td>
<a onclick="toggleVisibility('abstract417');">[|&bull;|]</a><a href="http://arxiv.org/abs/1106.3708v4">
<b/> Information-Geometric Optimization Algorithms: A Unifying Picture via
Invariance Principles (2011)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1106.3708v4">
Yann Ollivier, Ludovic Arnold, Anne Auger, Nikolaus Hansen &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract417');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract417');"><div id="abstract417" style="font-size: 1rem; color:black; display: none">
<u><I> 1106.3708v4 &nbsp; - &nbsp;
{diversity}
{entropy}
{evolution}
{gradient}
{intrinsic}
</I></u><br> We present a canonical way to turn any smooth parametric family of
probability distributions on an arbitrary search space $X$ into a
continuous-time black-box optimization method on $X$, the
\emph{information-geometric optimization} (IGO) method. Invariance as a design
principle minimizes the number of arbitrary choices. The resulting \emph{IGO
flow} conducts the natural gradient ascent of an adaptive, time-dependent,
quantile-based transformation of the objective function. It makes no
assumptions on the objective function to be optimized.
The IGO method produces explicit IGO algorithms through time discretization.
It naturally recovers versions of known algorithms and offers a systematic way
to derive new ones. The cross-entropy method is recovered in a particular case,
and can be extended into a smoothed, parametrization-independent maximum
likelihood update (IGO-ML). For Gaussian distributions on $\mathbb{R}^d$, IGO
is related to natural evolution strategies (NES) and recovers a version of the
CMA-ES algorithm. For Bernoulli distributions on $\{0,1\}^d$, we recover the
PBIL algorithm. From restricted Boltzmann machines, we obtain a novel algorithm
for optimization on $\{0,1\}^d$. All these algorithms are unified under a
single information-geometric optimization framework.
Thanks to its intrinsic formulation, the IGO method achieves invariance under
reparametrization of the search space $X$, under a change of parameters of the
probability distributions, and under increasing transformations of the
objective function.
Theory strongly suggests that IGO algorithms have minimal loss in diversity
during optimization, provided the initial diversity is high. First experiments
using restricted Boltzmann machines confirm this insight. Thus IGO seems to
provide, from information theory, an elegant way to spontaneously explore
several valleys of a fitness landscape in a single run. <br><br></div></a>
</td></tr>
<tr id=" 418 " class="entry"><td>
<a onclick="toggleVisibility('abstract418');">[|&bull;|]</a><a href="http://arxiv.org/abs/1010.3013v1">
<b/> Invariant Funnels around Trajectories using Sum-of-Squares Programming (2010)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/1010.3013v1">
Mark M. Tobenkin, Ian R. Manchester, Russ Tedrake &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract418');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract418');"><div id="abstract418" style="font-size: 1rem; color:black; display: none">
<u><I> 1010.3013v1 &nbsp; - &nbsp;
</I></u><br> This paper presents numerical methods for computing regions of finite-time
invariance (funnels) around solutions of polynomial differential equations.
First, we present a method which exactly certifies sufficient conditions for
invariance despite relying on approximate trajectories from numerical
integration. Our second method relaxes the constraints of the first by sampling
in time. In applications, this can recover almost identical funnels but is much
faster to compute. In both cases, funnels are verified using Sum-of-Squares
programming to search over a family of time-varying polynomial Lyapunov
functions. Initial candidate Lyapunov functions are constructed using the
linearization about the trajectory, and associated time-varying Lyapunov and
Riccati differential equations. The methods are compared on stabilized
trajectories of a six-state model of a satellite. <br><br></div></a>
</td></tr>
<tr id=" 419 " class="entry"><td>
<a onclick="toggleVisibility('abstract419');">[|&bull;|]</a><a href="http://arxiv.org/abs/0911.4625v1">
<b/> Hamilton-Jacobi formulation for reach-avoid differential games (2009)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/0911.4625v1">
Kostas Margellos, John Lygeros &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract419');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract419');"><div id="abstract419" style="font-size: 1rem; color:black; display: none">
<u><I> 0911.4625v1 &nbsp; - &nbsp;
{control}
{optimal}
</I></u><br> A new framework for formulating reachability problems with competing inputs,
nonlinear dynamics and state constraints as optimal control problems is
developed. Such reach-avoid problems arise in, among others, the study of
safety problems in hybrid systems. Earlier approaches to reach-avoid
computations are either restricted to linear systems, or face numerical
difficulties due to possible discontinuities in the Hamiltonian of the optimal
control problem. The main advantage of the approach proposed in this paper is
that it can be applied to a general class of target hitting continuous dynamic
games with nonlinear dynamics, and has very good properties in terms of its
numerical solution, since the value function and the Hamiltonian of the system
are both continuous. The performance of the proposed method is demonstrated by
applying it to a two aircraft collision avoidance scenario under target window
constraints and in the presence of wind disturbance. Target Windows are a novel
concept in air traffic management, and represent spatial and temporal
constraints, that the aircraft have to respect to meet their schedule. <br><br></div></a>
</td></tr>
<tr id=" 420 " class="entry"><td>
<a onclick="toggleVisibility('abstract420');">[|&bull;|]</a><a href="http://arxiv.org/abs/cs/0108021v1">
<b/> Computational Geometry Column 42 (2001)</b>
</a>
&nbsp; - &nbsp;
<I><a href="http://ar5iv.org/abs/cs/0108021v1">
Joseph S. B. Mitchell, Joseph O'Rourke &nbsp;
</a></I><span style="float:right"><a onclick="toggleVisibility('abstract420');">[|&bull;|]</a></span>
<a onclick="toggleVisibility('abstract420');"><div id="abstract420" style="font-size: 1rem; color:black; display: none">
<u><I> cs/0108021v1 &nbsp; - &nbsp;
</I></u><br> A compendium of thirty previously published open problems in computational
geometry is presented. <br><br></div></a>
</td></tr>
</tbody>

</table>































<!-- ======== End of the body ================================================================================================ -->

<BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR>



</body></html>

